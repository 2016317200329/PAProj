{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2022/11/12 09:54\n",
    "# @Author  : Wang Yujia\n",
    "# @File    : SA_for_PT_one_for_all.ipynb\n",
    "# @Description : for all settings and data, just infer one params"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0. What for\n",
    "1. 给所有的data和setting，只infer一组params\n",
    "\n",
    "# 1. Preparations\n",
    "## 1.1 全局设置"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# small fix from here\n",
    "# data path\n",
    "data_selected_path = \"../../data/SA_PT/datawithnp_PT_selected.csv\"\n",
    "# data_key path\n",
    "data_key_path = \"../../data/SA_PT/data_key_PT.csv\"\n",
    "\n",
    "# output path\n",
    "params_opitim_path = \"../../data/SA_PT/params_opitim.csv\"\n",
    "params_opitim_oneforall_path = \"../../data/SA_PT/params_opitim_oneforall.csv\"\n",
    "\n",
    "features_GT = ['bidincrement','bidfee','retail']\n",
    "# for SA\n",
    "# initial params\n",
    "table_5_M = [0.025,3.72]\n",
    "# table_5_M = [0,2]\n",
    "# lower/ upper bound\n",
    "lb = [-0.3,0.01]\n",
    "ub = [0.3,18]\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "from visdom import Visdom\n",
    "from SA_modified import SABoltzmann\n",
    "from sko.tools import set_run_mode\n",
    "from SA_for_PT_funcs_delta_eq1 import *\n",
    "\n",
    "# viz = Visdom(env='P',use_incoming_socket=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 data 读取\n",
    "1. 保存data_key\n",
    "2. 并且提取成功infer的data`params_all`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_selected_path, encoding=\"utf-8\")\n",
    "data_key = data[features_GT].copy()\n",
    "data_key.drop_duplicates(inplace=True,ignore_index=True)\n",
    "data_key.to_csv(data_key_path,header=True, encoding=\"utf-8\",index=False)\n",
    "\n",
    "# 有`N_uniq_auction`组setting\n",
    "N_uniq_auction= data_key.shape[0]\n",
    "\n",
    "print(\"For PT model, there are *{}* settings waiting to be inferred.\".format(N_uniq_auction))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 functions about 'key'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get key from i in 'data_key'\n",
    "def get_key_from_index(i,str=\"NotStr\"):\n",
    "    if(str==\"str\"):\n",
    "        key_i = list(data_key.iloc[i,:])\n",
    "        key_i_str = (str(key_i[0]),str(key_i[1]),str(key_i[2]))\n",
    "        return key_i_str\n",
    "    else:\n",
    "        key_i = data_key.iloc[i,:]\n",
    "        return key_i\n",
    "\n",
    "#features_GT = ['bidincrement','bidfee','retail']\n",
    "def select_data_fromkey(key_i_str):\n",
    "    return data[(data['bidincrement'] == key_i_str[0]) & (data['bidfee'] == key_i_str[1]) & (data['retail'] == key_i_str[2])].copy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Equi. condition in PT model\n",
    "1. 根据Eq(6)\n",
    "2. 注意分辨怎么代入上面的公式\n",
    "3. `delta = 1`时，公式可以大大化简，见ipad上的公式"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def f_Equi(t,v,d,b,alpha,labda):\n",
    "\n",
    "    tmp = v-d*t-C(t-1,b) - b\n",
    "\n",
    "    if (tmp>=0):\n",
    "        root = (labda*f(C(t-1,b),alpha) + f(tmp,alpha)) / (labda*f(C(t-1,b)+b,alpha) + f(tmp,alpha))\n",
    "        # if(np.isclose(root,0.0)):\n",
    "        #     print(f\"t:{t} ---- u = 0.0:{root} ---- alpha : {alpha}\")\n",
    "    else:\n",
    "        # print(\"tmp starts to < 0\", t)\n",
    "        root = (f(C(t-1,b),alpha) - f(-tmp,alpha)) / (f(C(t-1,b)+b,alpha) + f(-tmp,alpha))\n",
    "        # if(np.isclose(root,0.0)):\n",
    "        #     print(f\"t:{t} -- u = 0.0:{root} -- alpha : {alpha} -- lamda : {labda}\")\n",
    "\n",
    "    # if(root > 1.0):\n",
    "    #     print(f\"t:{t} ---- u > 1.0:{root} ---- alpha: {alpha}\")\n",
    "\n",
    "    #viz.line([[0.0,0.0]],[0],win = 'root compare',opts= dict(title='root in 2 methods'+str(t),legend=['simplify', 'sympy']))\n",
    "    #viz.line([[np.float(root1),np.float(root)]],[t],win = 'root compare', update='append')\n",
    "\n",
    "    return root"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. SA\n",
    "## 3.1 define loss function\n",
    "1. loss function: NLL for auctions with same `features_GT`\n",
    "2. “one for all”的不同之处在于，累加所有setting的情况求一个loss，不再区分`data_i`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def loss_func(params,other_params):\n",
    "    # start_time_loss = datetime.datetime.now()\n",
    "    alpha = params[0]\n",
    "    # delta = 1\n",
    "    labda = params[1]\n",
    "    # max_T,v,d,b,T_i,cnt_n_2_i = other_params\n",
    "    NLL=0.0\n",
    "    for i in range(0,N_uniq_auction):\n",
    "\n",
    "        # get i_th data_key\n",
    "        key_i = get_key_from_index(i)\n",
    "        # extract data with same `key_i` into a table\n",
    "        data_i = select_data_fromkey(key_i)\n",
    "        data_i.reset_index(drop=True,inplace=True)\n",
    "\n",
    "        T_i = data_i['N'].astype(int)          # auction duration sequence\n",
    "        max_T = int(max(T_i))                  # max duration value\n",
    "\n",
    "        cnt_n_2_i = data_i['cnt_n_2'].astype(int)       # Number of occurrences of different durations\n",
    "        # for a certain auction(like 'data_i'), 'cnt_uniq' should be all the same\n",
    "        # A_i = int(data_i['cnt_uniq'].unique())\n",
    "        # assert(A_i == sum(cnt_n_2_i),\"'cnt_uniq' does not match with sum of 'cnt_n_2'!\")\n",
    "\n",
    "        v = float(data_i['retail'].unique())\n",
    "        d = float(data_i['bidincrement'].unique())\n",
    "        b = float(data_i['bidfee'].unique())\n",
    "        assert max_T <= (v-b)/d, \"wrong!\"\n",
    "\n",
    "        flag = 1\n",
    "        # solve for U from Equi. condt.\n",
    "        U_i = [0] * (max_T + 1)\n",
    "        U_i[0] = 1\n",
    "\n",
    "        for t in range(1,max_T+1):\n",
    "\n",
    "            U_i[t] = f_Equi(t, v, d, b, alpha, labda)\n",
    "            if(flag & (U_i[t]<=0)):\n",
    "                # print(f\"t:{t} -- u starts to <= 0.0: {U_i[t]} -- alpha : {alpha} -- lamda : {labda}\")\n",
    "                flag = 0\n",
    "\n",
    "        # calculate NLL under this auction setting & PT params\n",
    "        nll = 0.0\n",
    "        if(U_i[0]==1):\n",
    "            U_i.pop(0)\n",
    "        for idx in range(0,data_i.shape[0]):\n",
    "            # sum up the log prob among all durations of this auction\n",
    "            # nll1 += np.cumprod(U_i)[T_i[idx]-2]*U_i[T_i[idx]-1]* cnt_n_2_i[idx]\n",
    "            # NLL1 == nll\n",
    "            nll += ( np.sum( np.log( U_i[0:(T_i[idx]-1)] ) ) + np.log(1-U_i[(T_i[idx]-1)]) )* cnt_n_2_i[idx]\n",
    "        # print('> The loss costs {time_costs}s \\n'.format(time_costs=(datetime.datetime.now() - start_time_loss).total_seconds()))\n",
    "        NLL += nll\n",
    "    print(f\"NLL loss is {NLL}\")\n",
    "    return float(-NLL)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 do SA\n",
    "1. 要对所有settings做一次infer\n",
    "2. 具体的：对每个setting `i`\n",
    "    - 每一个setting `i` 可以提取出来一个`data_i`，代表所有auction\n",
    "    - 每一个`data_i`中的`cnt_uniq`，也就是`A`，是相同的，表示setting `i` 进行的拍卖总次数.【但是这个`A`在计算loss的时候派不上用场】\n",
    "    - `N`表示duration，因此paper公式里的$T_a$即`N[a]`\n",
    "    - 因此有`A = sum(data_i['cnt_n_2'])`，其中的'cnt_n_2'表示了该行对应的`duration=N`发生的次数\n",
    "    - 按照上文，求解`U[i]_t` which is a array with shape of (max(N)),也就是求解paper里的`p_t`\n",
    "3.每次进行`L`次对参数的试探寻找，每次寻找对应一个温度一组新的参数。\n",
    "    - 优化的完成/退出条件：温度小于`T_min`或者最低温度保持`max_stay_counter`次的不变\n",
    "    - 鉴于温度小于`T_min`很难达到，因此基本上对一组参数进行优化要进行L*max_stay_counter+1次运算（loss运算）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "params_opitim = pd.DataFrame(columns=['trial_time','alpha','delta','labda','initial_loss','final_loss','avg_loss'])\n",
    "max_trial = 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Perform SA for all settings at once\n",
    "for i in range(0,max_trial):\n",
    "    print(f\"> For the *{i}_th* trial\")\n",
    "    print(\"> Initilizing SA....... \\n\")\n",
    "    set_run_mode(loss_func, 'cached')\n",
    "    set_run_mode(loss_func, 'multithreading')\n",
    "\n",
    "    initial_t = max(data['cnt_n_2'].astype(int))        # initial temperature\n",
    "    # L=30, max_stay_counter=15\n",
    "    # other_params = [max_T,v,d,b,T_i,cnt_n_2_i]\n",
    "    sa_boltzmann = SABoltzmann(func=loss_func, x0=table_5_M, other_params = [],T_max=initial_t, T_min=1, learn_rate=0.2, L=20, max_stay_counter=10,\n",
    "    lb=lb, ub=ub)\n",
    "\n",
    "    print(\"> Now do SA....... \\n\")\n",
    "    best_x, best_y = sa_boltzmann.run()\n",
    "    # print('> The whole inference process costs {time_costs}s \\n'.format(time_costs=(datetime.datetime.now() - start_time).total_seconds()))\n",
    "\n",
    "    print(\"> SA ENDS....... \\n\")\n",
    "    # if(i%10 == 0):\n",
    "    #     viz.line([0.0]*(sa_boltzmann.iter_cycle+1),[0]*(sa_boltzmann.iter_cycle+1),env = \"Loss of PT_2\",win = 'Loss of '+str(i),opts= dict(title='Loss of '+str(i)))\n",
    "    #     viz.line(np.array(sa_boltzmann.generation_best_Y),np.arange(0,sa_boltzmann.iter_cycle+1),env = \"Loss of PT_2\",win = 'Loss of '+str(i), update='append')\n",
    "\n",
    "    # append the opitimized params into the df\n",
    "    df_tmp = pd.DataFrame([ [i,best_x[0],1,best_x[1],sa_boltzmann.generation_best_Y[0],best_y,best_y/data.shape[0]] ],\n",
    "                        columns=['trial_time','alpha','delta','labda','initial_loss','final_loss','avg_loss'])\n",
    "    params_opitim = params_opitim.append(df_tmp,ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save 'params_opitim' for later check\n",
    "params_opitim.to_csv(params_opitim_path, header=True, encoding=\"utf-8\",index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
