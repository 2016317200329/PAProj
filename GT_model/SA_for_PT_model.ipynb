{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2022/10/09 16:19\n",
    "# @Author  : Wang Yujia\n",
    "# @File    : SA_for_PT_model.ipynb\n",
    "# @Description : Parameter estimation for PT_model using Simulated Annealing"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0. What for\n",
    "\n",
    "# 1. Preparations\n",
    "1. infer参数一是需要data，二是需要把p表示出来才能写出来loss func\n",
    "2. data来自`data_selected_path`\n",
    "\n",
    "## 1.1 全局设置\n",
    "1. 除了表示uniq auction的features，还引入了\n",
    "    - 'cnt_uniq':表示paper里的Loss function公式里的A"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# data path\n",
    "data_selected_path = \"../data/info_asymm/datawithnp_asc_symmetry_2_selected.csv\"\n",
    "# data_key path\n",
    "data_key_path = \"../data/SA_PT/data_key.csv\"\n",
    "# optimized parameters' saving path:\n",
    "params_opitim_path = \"../data/SA_PT/params_opitim.csv\"\n",
    "\n",
    "# for PT\n",
    "alpha = 1\n",
    "delta = 1\n",
    "labda = 2.25\n",
    "# features that GT need\n",
    "features_GT = ['product_id','bidincrement','bidfee','retail']\n",
    "features_GT_infer = ['cnt_uniq']\n",
    "\n",
    "# for SA\n",
    "# initial params\n",
    "table_5_M = [0.025,0.85,3.72]\n",
    "# lower/ upper bound\n",
    "lb = [-0.3,0.01,0.01]\n",
    "ub = [0.3, 2, 16]\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sympy\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from sko.SA import SABoltzmann\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from sko.tools import set_run_mode"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 data 读取\n",
    "1. 读取data以做SA\n",
    "2. 提取出来`data_key`，以及其他计算需要的features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For PT model, there are *1303* settings waiting to be inferred.\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(data_selected_path, encoding=\"utf-8\")\n",
    "data_key = data[features_GT].copy()\n",
    "data_key.drop_duplicates(inplace=True)\n",
    "data_key.to_csv(data_key_path,header=True, encoding=\"utf-8\",index=False)\n",
    "\n",
    "B = np.array(data.bidfee)               # bid fee (cent to dollar)\n",
    "D = np.array(data.bidincrement)         # bid increment (cent to dollar)\n",
    "V = np.array(data.retail)               # valuation\n",
    "# 需要计算`N_uniq_auction`组setting下的结果\n",
    "N_uniq_auction= data_key.shape[0]\n",
    "\n",
    "print(\"For PT model, there are *{}* settings waiting to be inferred.\".format(N_uniq_auction))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 functions about 'key'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# get key from i in 'data_key'\n",
    "def get_key_from_index(i,str=\"NotStr\"):\n",
    "    if(str==\"str\"):\n",
    "        key_i = list(data_key.iloc[i,:])\n",
    "        key_i_str = (str(key_i[0]),str(key_i[1]),str(key_i[2]))\n",
    "        return key_i_str\n",
    "    else:\n",
    "        key_i = data_key.iloc[i,:]\n",
    "        return key_i\n",
    "\n",
    "#features_GT = ['product_id','bidincrement','bidfee','retail']\n",
    "def select_data_fromkey(key_i_str):\n",
    "    return data[(data['product_id'] == key_i_str[0]) & (data['bidincrement'] == key_i_str[1]) & (data['bidfee'] == key_i_str[2]) & (data['retail'] == key_i_str[3])].copy()\n",
    "\n",
    "# def get_key_from_index(i):\n",
    "#     key_i ="
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. PT model\n",
    "## 2.1 prob. weighting func\n",
    "1. 根据Eq(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def OMEGA(p,delta):\n",
    "    return p**delta * ((p**delta + (1-p)**delta)**(-1/delta))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 C_{t-1}\n",
    "1. 根据5.1.2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def C(t,b):\n",
    "    return 0.2*t*b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 value functions\n",
    "1. 根据Eq(7)-(9)\n",
    "2. 注意这里把(-labda)(1-sympy.E**(alpha*x))/alpha的`labda`拿到外面去了，方便写"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# valuation function\n",
    "def f(x, alpha):\n",
    "    return (1-sympy.E**(-alpha*x))/alpha\n",
    "    # when x < 0, in fact, it shoule be : (-labda)*(1-sympy.E**(alpha*x))/alpha"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.4 Equi. condition\n",
    "1. 根据Eq(6)\n",
    "2. 注意分辨怎么代入上面的公式"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def f_Equi(t,v,d,b,alpha,labda,delta):\n",
    "    u = sympy.Symbol('u')\n",
    "\n",
    "    tmp = v-d*t-C(t-1,b) - b\n",
    "\n",
    "    func_1 = (labda * f(x=C(t-1, b), alpha=alpha) - labda * OMEGA(u, delta) * f(x=(C(t-1, b) + b), alpha=alpha) + OMEGA(1-u, delta) * f(tmp, alpha))\n",
    "    func_2 = (-f(x=C(t-1, b), alpha=alpha) + OMEGA(u, delta) * f(x=(C(t-1, b) + b), alpha=alpha) + (1 - OMEGA(u, delta)) * f(-tmp, alpha))\n",
    "\n",
    "    if(tmp >= 0):\n",
    "        return sympy.nsolve(func_1,(0,1),solver='bisect', verify=False)\n",
    "    else:\n",
    "        return sympy.nsolve(func_2,(0,1),solver='bisect', verify=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. SA\n",
    "## 3.1 define loss function\n",
    "1. loss function: NLL for auctions with same `features_GT`\n",
    "2."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def loss_func(params):\n",
    "    start_time = datetime.datetime.now()\n",
    "    alpha = params[0]\n",
    "    delta = params[1]\n",
    "    labda = params[2]\n",
    "\n",
    "    # solve for U from Equi. condt.\n",
    "    U_i = [0] * (max_T + 1)\n",
    "    U_i[0] = 1\n",
    "\n",
    "    for t in range(1,max_T+1):\n",
    "\n",
    "        U_i[t] = f_Equi(t, v, d, b, alpha, labda, delta)\n",
    "\n",
    "    # calculate NLL under this auction setting & PT params\n",
    "    nll = 0.0\n",
    "    if(U_i[0]==1):\n",
    "        U_i.pop(0)            # because U_i[0]=1\n",
    "    U_tmp_df = pd.DataFrame(U_i, index=np.arange(0, U_i.__len__()), columns=['U'], dtype=float)\n",
    "\n",
    "    for idx in range(0,data_i.shape[0]):\n",
    "        # sum up the log prob among all durations of this auction\n",
    "        nll += ( np.sum(U_tmp_df[0:(T_i[idx]-1)][:].apply(np.log,axis=1)) + np.log(1-U_tmp_df.iat[(T_i[idx]-1),0]) )* cnt_n_2_i[idx]\n",
    "\n",
    "    print('loss_func costs {time_costs}s \\n'.format(time_costs=(datetime.datetime.now() - start_time).total_seconds()))\n",
    "    return float(-nll)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 do SA\n",
    "1. 要对每一个setting做一次infer == 对每一个setting执行一次SA。\n",
    "    - 可以并行吗？YES\n",
    "2. 具体的：对每个setting `i`\n",
    "    - 每一个setting `i` 可以提取出来一个`data_i`，代表所有auction\n",
    "    - 每一个`data_i`中的`cnt_uniq`，也就是`A`，是相同的，表示setting `i` 进行的拍卖总次数.【但是这个`A`在计算loss的时候派不上用场】\n",
    "    - `N`表示duration，因此paper公式里的$T_a$即`N[a]`\n",
    "    - 因此有`A = sum(data_i['cnt_n_2'])`，其中的'cnt_n_2'表示了该行对应的`duration=N`发生的次数\n",
    "    - 按照上文，求解`U[i]_t` which is a array with shape of (max(N)),也就是求解paper里的`p_t`\n",
    "    - 求nll时，记得"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "params_opitim = pd.DataFrame(columns=['key_idx','alpha','delta','labda'])\n",
    "# best_x = [1,2,3]\n",
    "# i = 0\n",
    "# df_tmp = pd.DataFrame([[i,best_x]],columns=['key_idx','params'])\n",
    "# params_opitim = params_opitim.append(df_tmp,ignore_index=True)  # ignore_index=True could help in rearranging index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:17: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:17: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:17: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<ipython-input-41-951dd411281a>:17: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(A_i == sum(cnt_n_2_i),\"'cnt_uniq' does not match with sum of 'cnt_n_2'!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> For the 0th data_key, there is:\n",
      "> retail = 169.99,bidincrement = 0.15, bidfee = 0.75, infer PT's parameters\n",
      "> Initilizing SA....... \n",
      "\n",
      "loss_func costs 13.489796s \n",
      "\n",
      "> Now do SA....... \n",
      "\n",
      "-------------- 0_th iteration --------------\n",
      "\n",
      "---------- 0/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.21529555  1.17294108  3.86114926]\n",
      "loss_func costs 13.148291s \n",
      "\n",
      "y_new - y_current is 350.0177798587637: \n",
      "---------- 1/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.42162128  0.47455326  3.88987332]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pythorch\\lib\\site-packages\\ipykernel_launcher.py:23: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_func costs 13.181904s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "---------- 2/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.24328123  0.50608301  3.56372531]\n",
      "loss_func costs 13.06751s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "---------- 3/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.46444768  1.35331118  4.02348354]\n",
      "loss_func costs 12.937232s \n",
      "\n",
      "y_new - y_current is 153.61502365312094: \n",
      "---------- 4/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.17929591  1.40303568  3.83865529]\n",
      "loss_func costs 14.096783s \n",
      "\n",
      "y_new - y_current is -215.47230688803705: \n",
      "---------- 5/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.04843641  1.66570035  4.11464696]\n",
      "loss_func costs 14.507018s \n",
      "\n",
      "y_new - y_current is -196.4918562822508: \n",
      "---------- 6/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.15140529  1.4186307   3.99826694]\n",
      "loss_func costs 13.219885s \n",
      "\n",
      "y_new - y_current is 147.86575345195678: \n",
      "---------- 7/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.27290465  1.29480404  4.55534429]\n",
      "loss_func costs 13.437603s \n",
      "\n",
      "y_new - y_current is 370.59268842249946: \n",
      "---------- 8/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.07078277  1.29928901  4.4068023 ]\n",
      "loss_func costs 13.513663s \n",
      "\n",
      "y_new - y_current is -362.5312992967026: \n",
      "---------- 9/L ----------\n",
      "\n",
      "x_new before clipping:  [0.0296092  1.25169979 4.13831507]\n",
      "loss_func costs 14.082152s \n",
      "\n",
      "y_new - y_current is 53.58539078454726: \n",
      "---------- 10/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.17358318  1.87792921  3.88962842]\n",
      "loss_func costs 13.203247s \n",
      "\n",
      "y_new - y_current is 63.44590870300851: \n",
      "---------- 11/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.09241102  2.31484688  3.65381383]\n",
      "loss_func costs 9.958978s \n",
      "\n",
      "y_new - y_current is -112.90983912211016: \n",
      "---------- 12/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.35204399  2.03898053  3.3631971 ]\n",
      "loss_func costs 9.661094s \n",
      "\n",
      "y_new - y_current is 256.06576434023043: \n",
      "---------- 13/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.45972283  1.75336515  3.55580322]\n",
      "loss_func costs 13.291104s \n",
      "\n",
      "y_new - y_current is 61.89769162883613: \n",
      "---------- 14/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.27639803  1.88267056  3.51412523]\n",
      "loss_func costs 13.270276s \n",
      "\n",
      "y_new - y_current is -65.02780121394551: \n",
      "---------- 15/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.29793719  1.98203725  3.51408755]\n",
      "loss_func costs 13.590291s \n",
      "\n",
      "y_new - y_current is 5.26915720477723: \n",
      "---------- 16/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.30262135  1.95862274  3.44984197]\n",
      "loss_func costs 13.775006s \n",
      "\n",
      "y_new - y_current is 8.293982860255142: \n",
      "---------- 17/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.20562473  1.81170924  3.15561208]\n",
      "loss_func costs 12.910559s \n",
      "\n",
      "y_new - y_current is -100.06604643528192: \n",
      "---------- 18/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.41865011  1.90778209  3.46154523]\n",
      "loss_func costs 12.716612s \n",
      "\n",
      "y_new - y_current is 112.7310065140806: \n",
      "---------- 19/L ----------\n",
      "\n",
      "x_new before clipping:  [0.04799584 1.70289765 3.12207064]\n",
      "loss_func costs 13.650331s \n",
      "\n",
      "y_new - y_current is -195.4924030392608: \n",
      "-------------- 1_th iteration --------------\n",
      "\n",
      "---------- 0/L ----------\n",
      "\n",
      "x_new before clipping:  [0.24653422 1.81005126 3.74826848]\n",
      "loss_func costs 13.706773s \n",
      "\n",
      "y_new - y_current is 99.61768119628255: \n",
      "---------- 1/L ----------\n",
      "\n",
      "x_new before clipping:  [0.03948029 1.76991774 3.69759409]\n",
      "loss_func costs 13.078765s \n",
      "\n",
      "y_new - y_current is -93.68629954259097: \n",
      "---------- 2/L ----------\n",
      "\n",
      "x_new before clipping:  [0.19249174 1.84679944 3.62502766]\n",
      "loss_func costs 13.2275s \n",
      "\n",
      "y_new - y_current is 54.60209688734062: \n",
      "---------- 3/L ----------\n",
      "\n",
      "x_new before clipping:  [0.48245751 1.99902464 3.6663504 ]\n",
      "loss_func costs 13.021223s \n",
      "\n",
      "y_new - y_current is 93.51827699215107: \n",
      "---------- 4/L ----------\n",
      "\n",
      "x_new before clipping:  [0.7234809  2.28053161 2.90872171]\n",
      "loss_func costs 9.579323s \n",
      "\n",
      "y_new - y_current is -5.403418755036569: \n",
      "---------- 5/L ----------\n",
      "\n",
      "x_new before clipping:  [0.48473985 2.56756357 2.27864822]\n",
      "loss_func costs 10.136276s \n",
      "\n",
      "y_new - y_current is -5.8969908589537: \n",
      "---------- 6/L ----------\n",
      "\n",
      "x_new before clipping:  [0.38187437 1.47709931 2.71793835]\n",
      "loss_func costs 13.238115s \n",
      "\n",
      "y_new - y_current is 4.628169407785435: \n",
      "---------- 7/L ----------\n",
      "\n",
      "x_new before clipping:  [0.10663387 0.97703713 2.54291743]\n",
      "loss_func costs 13.051617s \n",
      "\n",
      "y_new - y_current is -239.20317331518822: \n",
      "---------- 8/L ----------\n",
      "\n",
      "x_new before clipping:  [0.21613969 1.43386811 2.16980007]\n",
      "loss_func costs 13.608636s \n",
      "\n",
      "y_new - y_current is 140.77926259475697: \n",
      "---------- 9/L ----------\n",
      "\n",
      "x_new before clipping:  [0.16647608 1.04710857 1.99053688]\n",
      "loss_func costs 13.361008s \n",
      "\n",
      "y_new - y_current is -95.28569861612647: \n",
      "---------- 10/L ----------\n",
      "\n",
      "x_new before clipping:  [0.0850174  0.64650398 2.43159408]\n",
      "loss_func costs 13.793469s \n",
      "\n",
      "y_new - y_current is -67.43462485813546: \n",
      "---------- 11/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.02694585  0.57881436  2.45842204]\n",
      "loss_func costs 13.420487s \n",
      "\n",
      "y_new - y_current is -87.04753737675799: \n",
      "---------- 12/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.23722785  1.65968386  3.18315962]\n",
      "loss_func costs 13.111055s \n",
      "\n",
      "y_new - y_current is 360.05865114183706: \n",
      "---------- 13/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.34612686  0.89574821  2.70849824]\n",
      "loss_func costs 13.265011s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "---------- 14/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.2746059   0.87220352  2.90561952]\n",
      "loss_func costs 13.309599s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "---------- 15/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.08397797  2.10811975  3.60638219]\n",
      "loss_func costs 9.887219s \n",
      "\n",
      "y_new - y_current is -250.0921919893176: \n",
      "---------- 16/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.2968095   2.51956195  3.42818209]\n",
      "loss_func costs 9.719712s \n",
      "\n",
      "y_new - y_current is 259.26170409660176: \n",
      "---------- 17/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.43178528  2.50207381  3.61682801]\n",
      "loss_func costs 9.722998s \n",
      "\n",
      "y_new - y_current is 264.01371972896266: \n",
      "---------- 18/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.15350271  1.97008913  3.76495384]\n",
      "loss_func costs 13.347271s \n",
      "\n",
      "y_new - y_current is -184.55576109417666: \n",
      "---------- 19/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.16219123  1.6354779   3.55982554]\n",
      "loss_func costs 13.191158s \n",
      "\n",
      "y_new - y_current is 59.15550363035493: \n",
      "-------------- 2_th iteration --------------\n",
      "\n",
      "---------- 0/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.43959944  0.71345494  3.93069301]\n",
      "loss_func costs 13.391306s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "---------- 1/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.20206456  1.22345356  3.60707983]\n",
      "loss_func costs 13.363145s \n",
      "\n",
      "y_new - y_current is 93.20578900777002: \n",
      "---------- 2/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.17255431  1.56753425  3.40222474]\n",
      "loss_func costs 13.053568s \n",
      "\n",
      "y_new - y_current is -68.75165295482708: \n",
      "---------- 3/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.18646599  1.79246641  3.63432642]\n",
      "loss_func costs 13.184994s \n",
      "\n",
      "y_new - y_current is -11.676591302851307: \n",
      "---------- 4/L ----------\n",
      "\n",
      "x_new before clipping:  [0.19631229 1.6935179  3.50217005]\n",
      "loss_func costs 12.739714s \n",
      "\n",
      "y_new - y_current is 5.607510912851865: \n",
      "---------- 5/L ----------\n",
      "\n",
      "x_new before clipping:  [0.13049613 2.05156149 3.06121431]\n",
      "loss_func costs 9.819591s \n",
      "\n",
      "y_new - y_current is -53.24020244540037: \n",
      "---------- 6/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.10360784  2.04144907  3.80725639]\n",
      "loss_func costs 9.686366s \n",
      "\n",
      "y_new - y_current is -85.23604670737302: \n",
      "---------- 7/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.21524853  1.99604569  4.12792209]\n",
      "loss_func costs 14.04211s \n",
      "\n",
      "y_new - y_current is 137.12831216301623: \n",
      "---------- 8/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.31617685  2.24901566  4.16055309]\n",
      "loss_func costs 10.065392s \n",
      "\n",
      "y_new - y_current is 109.86379418887589: \n",
      "---------- 9/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.3785699   2.00545552  4.03298781]\n",
      "loss_func costs 10.85043s \n",
      "\n",
      "y_new - y_current is -0.3349003271246147: \n",
      "---------- 10/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.03112497  2.11407408  4.44239382]\n",
      "loss_func costs 10.618916s \n",
      "\n",
      "y_new - y_current is -249.475959043787: \n",
      "---------- 11/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.08835758  1.98086371  4.649052  ]\n",
      "loss_func costs 13.019434s \n",
      "\n",
      "y_new - y_current is -6.092084573894681: \n",
      "---------- 12/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.27219133  2.36417813  4.83288945]\n",
      "loss_func costs 10.176955s \n",
      "\n",
      "y_new - y_current is 221.2522833988706: \n",
      "---------- 13/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.19770033  2.24016753  4.64086784]\n",
      "loss_func costs 9.881002s \n",
      "\n",
      "y_new - y_current is -96.97039562613242: \n",
      "---------- 14/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.22274655  2.16154993  4.80270924]\n",
      "loss_func costs 9.480341s \n",
      "\n",
      "y_new - y_current is 32.641876329215165: \n",
      "---------- 15/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.44385021  2.2152821   4.44921804]\n",
      "loss_func costs 9.592267s \n",
      "\n",
      "y_new - y_current is 99.70300845065339: \n",
      "---------- 16/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.42314026  2.21542728  4.29834536]\n",
      "loss_func costs 9.738035s \n",
      "\n",
      "y_new - y_current is -0.3726654419364195: \n",
      "---------- 17/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.14697532  1.73488087  4.21177978]\n",
      "loss_func costs 13.541333s \n",
      "\n",
      "y_new - y_current is -160.74037607603833: \n",
      "---------- 18/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.14287053  1.44522583  4.32122978]\n",
      "loss_func costs 13.370551s \n",
      "\n",
      "y_new - y_current is 23.266989671078477: \n",
      "---------- 19/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.25161497  1.45921924  4.23347684]\n",
      "loss_func costs 12.754819s \n",
      "\n",
      "y_new - y_current is 182.87123729470562: \n",
      "-------------- 3_th iteration --------------\n",
      "\n",
      "---------- 0/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.32239613  1.2454901   4.58766567]\n",
      "loss_func costs 13.107833s \n",
      "\n",
      "y_new - y_current is 105.67944289416016: \n",
      "---------- 1/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.23595171  1.38119899  4.62147844]\n",
      "loss_func costs 13.456705s \n",
      "\n",
      "y_new - y_current is -122.48653970190696: \n",
      "---------- 2/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.02085751  1.56508932  4.7818759 ]\n",
      "loss_func costs 13.722357s \n",
      "\n",
      "y_new - y_current is -272.77423469961946: \n",
      "---------- 3/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.39114068  1.44607212  4.85318161]\n",
      "loss_func costs 14.04608s \n",
      "\n",
      "y_new - y_current is 373.97980426904826: \n",
      "---------- 4/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.15058854  1.29257363  4.87559348]\n",
      "loss_func costs 13.571882s \n",
      "\n",
      "y_new - y_current is -251.13554042421146: \n",
      "---------- 5/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.20389348  1.49121455  5.23832596]\n",
      "loss_func costs 14.100308s \n",
      "\n",
      "y_new - y_current is 85.9922579108283: \n",
      "---------- 6/L ----------\n",
      "\n",
      "x_new before clipping:  [0.0639296  1.53698174 5.5723782 ]\n",
      "loss_func costs 14.104047s \n",
      "\n",
      "y_new - y_current is -102.76353050266152: \n",
      "---------- 7/L ----------\n",
      "\n",
      "x_new before clipping:  [0.24082619 1.26401502 5.8088694 ]\n",
      "loss_func costs 14.005863s \n",
      "\n",
      "y_new - y_current is 65.30426835672074: \n",
      "---------- 8/L ----------\n",
      "\n",
      "x_new before clipping:  [0.37124125 1.23948813 6.40603201]\n",
      "loss_func costs 13.630238s \n",
      "\n",
      "y_new - y_current is 66.30643514111091: \n",
      "---------- 9/L ----------\n",
      "\n",
      "x_new before clipping:  [0.3093982  1.12321163 6.24270372]\n",
      "loss_func costs 13.705735s \n",
      "\n",
      "y_new - y_current is -15.116644300557141: \n",
      "---------- 10/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.1670199   1.17793939  6.3394546 ]\n",
      "loss_func costs 13.870961s \n",
      "\n",
      "y_new - y_current is -76.45823274494603: \n",
      "---------- 11/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.21688142  1.59598504  6.49138936]\n",
      "loss_func costs 12.991417s \n",
      "\n",
      "y_new - y_current is 72.04523281461468: \n",
      "---------- 12/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.26536425  1.19502004  6.20058236]\n",
      "loss_func costs 12.691387s \n",
      "\n",
      "y_new - y_current is 187.63802715194151: \n",
      "---------- 13/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.19114539  1.36902028  6.33330019]\n",
      "loss_func costs 12.926146s \n",
      "\n",
      "y_new - y_current is -133.9356137081444: \n",
      "---------- 14/L ----------\n",
      "\n",
      "x_new before clipping:  [0.01337919 1.58468436 5.9987151 ]\n",
      "loss_func costs 13.005737s \n",
      "\n",
      "y_new - y_current is -103.70016734378686: \n",
      "---------- 15/L ----------\n",
      "\n",
      "x_new before clipping:  [0.06491913 1.7199832  5.5661275 ]\n",
      "loss_func costs 13.225599s \n",
      "\n",
      "y_new - y_current is 3.4984170049041836: \n",
      "---------- 16/L ----------\n",
      "\n",
      "x_new before clipping:  [-5.50595973e-03  1.66414423e+00  5.63872453e+00]\n",
      "loss_func costs 13.313899s \n",
      "\n",
      "y_new - y_current is -43.273993246889404: \n",
      "---------- 17/L ----------\n",
      "\n",
      "x_new before clipping:  [0.14187743 1.78786563 5.70015411]\n",
      "loss_func costs 13.295914s \n",
      "\n",
      "y_new - y_current is 62.8202485626029: \n",
      "---------- 18/L ----------\n",
      "\n",
      "x_new before clipping:  [0.16870593 1.82458356 5.76340779]\n",
      "loss_func costs 13.305307s \n",
      "\n",
      "y_new - y_current is 12.014635207565561: \n",
      "---------- 19/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.13776868  1.49672375  5.553668  ]\n",
      "loss_func costs 13.816533s \n",
      "\n",
      "y_new - y_current is -31.29687424720038: \n",
      "-------------- 4_th iteration --------------\n",
      "\n",
      "---------- 0/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.08487176  1.54922676  5.70533673]\n",
      "loss_func costs 13.916475s \n",
      "\n",
      "y_new - y_current is -83.75949928690397: \n",
      "---------- 1/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.02607102  1.28876744  5.87193615]\n",
      "loss_func costs 13.286237s \n",
      "\n",
      "y_new - y_current is -56.7593455834367: \n",
      "---------- 2/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.14632028  1.61311075  5.76934428]\n",
      "loss_func costs 13.876052s \n",
      "\n",
      "y_new - y_current is 144.05755345568298: \n",
      "---------- 3/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.31869538  1.68858924  5.94529673]\n",
      "loss_func costs 13.846088s \n",
      "\n",
      "y_new - y_current is 221.91964909684066: \n",
      "---------- 4/L ----------\n",
      "\n",
      "x_new before clipping:  [0.024299   1.76050008 5.02760806]\n",
      "loss_func costs 13.649384s \n",
      "\n",
      "y_new - y_current is -14.741018934793829: \n",
      "---------- 5/L ----------\n",
      "\n",
      "x_new before clipping:  [0.1647549  1.5393447  4.95936681]\n",
      "loss_func costs 13.05171s \n",
      "\n",
      "y_new - y_current is 44.52851331912268: \n",
      "---------- 6/L ----------\n",
      "\n",
      "x_new before clipping:  [0.00727831 1.46155841 4.94223713]\n",
      "loss_func costs 13.413168s \n",
      "\n",
      "y_new - y_current is -70.84979012276398: \n",
      "---------- 7/L ----------\n",
      "\n",
      "x_new before clipping:  [0.25795643 1.38004475 5.18372804]\n",
      "loss_func costs 13.161384s \n",
      "\n",
      "y_new - y_current is 126.14449759189876: \n",
      "---------- 8/L ----------\n",
      "\n",
      "x_new before clipping:  [0.43553149 1.16477033 4.95809552]\n",
      "loss_func costs 13.007589s \n",
      "\n",
      "y_new - y_current is 25.973326584189067: \n",
      "---------- 9/L ----------\n",
      "\n",
      "x_new before clipping:  [0.39339709 1.8241397  5.34852685]\n",
      "loss_func costs 13.650594s \n",
      "\n",
      "y_new - y_current is 50.7376550385128: \n",
      "---------- 10/L ----------\n",
      "\n",
      "x_new before clipping:  [0.55998795 1.77829798 5.49024504]\n",
      "loss_func costs 12.992278s \n",
      "\n",
      "y_new - y_current is 2.529513608292973: \n",
      "---------- 11/L ----------\n",
      "\n",
      "x_new before clipping:  [0.14007635 1.82393895 5.70363153]\n",
      "loss_func costs 14.023576s \n",
      "\n",
      "y_new - y_current is -125.6320723149613: \n",
      "---------- 12/L ----------\n",
      "\n",
      "x_new before clipping:  [0.22444522 1.71702391 5.9677353 ]\n",
      "loss_func costs 13.101367s \n",
      "\n",
      "y_new - y_current is 55.56383536521474: \n",
      "---------- 13/L ----------\n",
      "\n",
      "x_new before clipping:  [0.1506719  1.58204646 6.27385999]\n",
      "loss_func costs 12.8403s \n",
      "\n",
      "y_new - y_current is -35.316266189399244: \n",
      "---------- 14/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.25036758  1.6322273   6.44884332]\n",
      "loss_func costs 13.359045s \n",
      "\n",
      "y_new - y_current is 127.20601944044785: \n",
      "---------- 15/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.07058546  1.50783141  6.52285204]\n",
      "loss_func costs 14.135333s \n",
      "\n",
      "y_new - y_current is -134.16504907282302: \n",
      "---------- 16/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.06942017  1.94904851  6.50585241]\n",
      "loss_func costs 14.027338s \n",
      "\n",
      "y_new - y_current is -16.795977762891482: \n",
      "---------- 17/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.13494165  1.58681469  6.83420139]\n",
      "loss_func costs 14.086548s \n",
      "\n",
      "y_new - y_current is 106.429167556891: \n",
      "---------- 18/L ----------\n",
      "\n",
      "x_new before clipping:  [0.01426    1.8338835  6.70070925]\n",
      "loss_func costs 14.235815s \n",
      "\n",
      "y_new - y_current is 2.086552554302898: \n",
      "---------- 19/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.03455668  2.06144363  6.4351015 ]\n",
      "loss_func costs 10.59278s \n",
      "\n",
      "y_new - y_current is -91.46383161992304: \n",
      "-------------- 5_th iteration --------------\n",
      "\n",
      "---------- 0/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.10207749  1.96249901  6.4950116 ]\n",
      "loss_func costs 14.029565s \n",
      "\n",
      "y_new - y_current is 5.7595599821180485: \n",
      "---------- 1/L ----------\n",
      "\n",
      "x_new before clipping:  [0.0393412  2.33732976 6.63732106]\n",
      "loss_func costs 9.961181s \n",
      "\n",
      "y_new - y_current is 74.65901799755954: \n",
      "---------- 2/L ----------\n",
      "\n",
      "x_new before clipping:  [0.13557444 2.30625715 6.5009817 ]\n",
      "loss_func costs 10.089949s \n",
      "\n",
      "y_new - y_current is 22.743283175363615: \n",
      "---------- 3/L ----------\n",
      "\n",
      "x_new before clipping:  [0.23763977 2.19987873 6.66155719]\n",
      "loss_func costs 9.6052s \n",
      "\n",
      "y_new - y_current is 62.767202243407155: \n",
      "---------- 4/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.02696916  1.73304847  6.58621966]\n",
      "loss_func costs 12.86878s \n",
      "\n",
      "y_new - y_current is -91.62901754153228: \n",
      "---------- 5/L ----------\n",
      "\n",
      "x_new before clipping:  [0.54800956 1.97719176 6.38928233]\n",
      "loss_func costs 12.568779s \n",
      "\n",
      "y_new - y_current is 222.2255666347295: \n",
      "---------- 6/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.05890259  1.91753717  6.46014611]\n",
      "loss_func costs 13.629375s \n",
      "\n",
      "y_new - y_current is -29.443154828382717: \n",
      "---------- 7/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.02448377  1.91260259  6.47153193]\n",
      "loss_func costs 13.473977s \n",
      "\n",
      "y_new - y_current is 36.311703759040995: \n",
      "---------- 8/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.09621207  1.84016949  6.44096984]\n",
      "loss_func costs 13.335078s \n",
      "\n",
      "y_new - y_current is -8.057635006159217: \n",
      "---------- 9/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.11545769  1.5572874   6.48253903]\n",
      "loss_func costs 12.802338s \n",
      "\n",
      "y_new - y_current is 50.48319665464874: \n",
      "---------- 10/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.368754    1.57333475  6.07174454]\n",
      "loss_func costs 13.265931s \n",
      "\n",
      "y_new - y_current is 288.6941567548517: \n",
      "---------- 11/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.47013772  1.77539847  6.10108987]\n",
      "loss_func costs 14.047269s \n",
      "\n",
      "y_new - y_current is -47.64193256418696: \n",
      "---------- 12/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.31804071  1.69520468  6.23403587]\n",
      "loss_func costs 13.847822s \n",
      "\n",
      "y_new - y_current is 19.970012277019237: \n",
      "---------- 13/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.51393124  1.75378963  6.26355428]\n",
      "loss_func costs 13.35653s \n",
      "\n",
      "y_new - y_current is -14.280979402928779: \n",
      "---------- 14/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.63290017  2.02657902  6.09893725]\n",
      "loss_func costs 10.092784s \n",
      "\n",
      "y_new - y_current is -62.03145347318559: \n",
      "---------- 15/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.11606273  2.02483951  6.42722583]\n",
      "loss_func costs 10.652474s \n",
      "\n",
      "y_new - y_current is -229.18140030980237: \n",
      "---------- 16/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.10573205  1.79905179  6.32087972]\n",
      "loss_func costs 13.457513s \n",
      "\n",
      "y_new - y_current is 8.417969118301073: \n",
      "---------- 17/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.16736677  2.05827068  6.16443171]\n",
      "loss_func costs 10.690338s \n",
      "\n",
      "y_new - y_current is 50.840374970413336: \n",
      "---------- 18/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.00955844  1.45264327  6.39275851]\n",
      "loss_func costs 13.863881s \n",
      "\n",
      "y_new - y_current is -42.97574394021672: \n",
      "---------- 19/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.07456691  1.5392474   6.71033731]\n",
      "loss_func costs 13.670113s \n",
      "\n",
      "y_new - y_current is -27.731370425856426: \n",
      "> SA ENDS....... \n",
      "\n",
      "> For the 1th data_key, there is:\n",
      "> retail = 499.99,bidincrement = 0.15, bidfee = 0.75, infer PT's parameters\n",
      "> Initilizing SA....... \n",
      "\n",
      "loss_func costs 16.871095s \n",
      "\n",
      "> Now do SA....... \n",
      "\n",
      "-------------- 0_th iteration --------------\n",
      "\n",
      "---------- 0/L ----------\n",
      "\n",
      "x_new before clipping:  [0.11752332 0.88131856 4.01790083]\n",
      "loss_func costs 16.688709s \n",
      "\n",
      "y_new - y_current is 178.46753408105263: \n",
      "---------- 1/L ----------\n",
      "\n",
      "x_new before clipping:  [0.31243097 1.29716003 4.17111595]\n",
      "loss_func costs 15.721579s \n",
      "\n",
      "y_new - y_current is 343.62380609753376: \n",
      "---------- 2/L ----------\n",
      "\n",
      "x_new before clipping:  [0.18089274 1.46241399 3.80581689]\n",
      "loss_func costs 16.091691s \n",
      "\n",
      "y_new - y_current is -161.0942121443328: \n",
      "---------- 3/L ----------\n",
      "\n",
      "x_new before clipping:  [0.09453548 1.58629694 3.56555506]\n",
      "loss_func costs 17.190177s \n",
      "\n",
      "y_new - y_current is -64.75682097669687: \n",
      "---------- 4/L ----------\n",
      "\n",
      "x_new before clipping:  [0.11114706 1.23472907 3.65785682]\n",
      "loss_func costs 16.833676s \n",
      "\n",
      "y_new - y_current is -37.2135439871181: \n",
      "---------- 5/L ----------\n",
      "\n",
      "x_new before clipping:  [0.16647589 0.7388703  3.5704318 ]\n",
      "loss_func costs 16.180407s \n",
      "\n",
      "y_new - y_current is 5.60600942223698: \n",
      "---------- 6/L ----------\n",
      "\n",
      "x_new before clipping:  [0.03499667 0.49186954 3.6992758 ]\n",
      "loss_func costs 17.508452s \n",
      "\n",
      "y_new - y_current is -163.58211428538658: \n",
      "---------- 7/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.03187202  0.68412651  3.94462814]\n",
      "loss_func costs 16.747313s \n",
      "\n",
      "y_new - y_current is 284.3887441801398: \n",
      "---------- 8/L ----------\n",
      "\n",
      "x_new before clipping:  [0.08477523 0.78684471 3.57627412]\n",
      "loss_func costs 16.628665s \n",
      "\n",
      "y_new - y_current is -283.298741200082: \n",
      "---------- 9/L ----------\n",
      "\n",
      "x_new before clipping:  [0.15982013 1.05983929 3.01405791]\n",
      "loss_func costs 15.767251s \n",
      "\n",
      "y_new - y_current is 138.5558985946979: \n",
      "---------- 10/L ----------\n",
      "\n",
      "x_new before clipping:  [0.50990067 1.31348722 2.66118034]\n",
      "loss_func costs 14.352516s \n",
      "\n",
      "y_new - y_current is 253.86813992584234: \n",
      "---------- 11/L ----------\n",
      "\n",
      "x_new before clipping:  [0.02393398 1.11375631 2.87206219]\n",
      "loss_func costs 16.371719s \n",
      "\n",
      "y_new - y_current is -411.0606969551535: \n",
      "---------- 12/L ----------\n",
      "\n",
      "x_new before clipping:  [-1.50128046e-03  1.00383108e+00  2.59134338e+00]\n",
      "loss_func costs 16.32913s \n",
      "\n",
      "y_new - y_current is -146.62371219300567: \n",
      "---------- 13/L ----------\n",
      "\n",
      "x_new before clipping:  [0.28708016 0.62343166 2.53564794]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pythorch\\lib\\site-packages\\ipykernel_launcher.py:23: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_func costs 14.46125s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "---------- 14/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.1576722   0.79270988  2.92363523]\n",
      "loss_func costs 15.395767s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "---------- 15/L ----------\n",
      "\n",
      "x_new before clipping:  [0.45295668 0.7890455  2.47458977]\n",
      "loss_func costs 14.310181s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "---------- 16/L ----------\n",
      "\n",
      "x_new before clipping:  [0.04971027 1.72386744 3.28279542]\n",
      "loss_func costs 16.390574s \n",
      "\n",
      "y_new - y_current is 331.02493698789783: \n",
      "---------- 17/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.03264717  1.38449144  2.95628415]\n",
      "loss_func costs 16.180172s \n",
      "\n",
      "y_new - y_current is -205.97803758488794: \n",
      "---------- 18/L ----------\n",
      "\n",
      "x_new before clipping:  [0.17602576 0.79634101 2.55499843]\n",
      "loss_func costs 15.693403s \n",
      "\n",
      "y_new - y_current is 186.86834904110538: \n",
      "---------- 19/L ----------\n",
      "\n",
      "x_new before clipping:  [0.07771234 1.48847956 2.38443256]\n",
      "loss_func costs 16.050595s \n",
      "\n",
      "y_new - y_current is -4.938715784981355: \n",
      "-------------- 1_th iteration --------------\n",
      "\n",
      "---------- 0/L ----------\n",
      "\n",
      "x_new before clipping:  [0.3098911  1.6616187  2.21579878]\n",
      "loss_func costs 14.186788s \n",
      "\n",
      "y_new - y_current is 262.5343999340528: \n",
      "---------- 1/L ----------\n",
      "\n",
      "x_new before clipping:  [0.15148777 1.46447735 2.59118856]\n",
      "loss_func costs 16.026308s \n",
      "\n",
      "y_new - y_current is -203.23899934372764: \n",
      "---------- 2/L ----------\n",
      "\n",
      "x_new before clipping:  [0.09861917 1.26330991 2.56076448]\n",
      "loss_func costs 16.69899s \n",
      "\n",
      "y_new - y_current is -84.3759016841974: \n",
      "---------- 3/L ----------\n",
      "\n",
      "x_new before clipping:  [0.09605053 1.01367566 2.39906602]\n",
      "loss_func costs 16.359042s \n",
      "\n",
      "y_new - y_current is -87.05846629172305: \n",
      "---------- 4/L ----------\n",
      "\n",
      "x_new before clipping:  [0.04312794 1.43814335 1.81369289]\n",
      "loss_func costs 16.660983s \n",
      "\n",
      "y_new - y_current is 51.98938506222345: \n",
      "---------- 5/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.10799936  1.77835744  2.44019103]\n",
      "loss_func costs 16.060582s \n",
      "\n",
      "y_new - y_current is 285.97411526231326: \n",
      "---------- 6/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.42713386  1.63143802  1.97439368]\n",
      "loss_func costs 15.350576s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "---------- 7/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.16823709  1.6830492   2.99717109]\n",
      "loss_func costs 15.936651s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "---------- 8/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.13534034  1.20876754  2.23929788]\n",
      "loss_func costs 15.492928s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "---------- 9/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.47910884  2.0182015   2.76412031]\n",
      "loss_func costs 11.631324s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "---------- 10/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.05725993  1.12049242  2.63418436]\n",
      "loss_func costs 16.11424s \n",
      "\n",
      "y_new - y_current is -80.61111244068422: \n",
      "---------- 11/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.09865893  1.4337808   2.90811735]\n",
      "loss_func costs 15.765947s \n",
      "\n",
      "y_new - y_current is 182.0143028928942: \n",
      "---------- 12/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.35800353  1.51831604  3.2815384 ]\n",
      "loss_func costs 16.065622s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "---------- 13/L ----------\n",
      "\n",
      "x_new before clipping:  [1.02816689e-03 1.17221772e+00 3.12920836e+00]\n",
      "loss_func costs 16.359153s \n",
      "\n",
      "y_new - y_current is -595.7258522622035: \n",
      "---------- 14/L ----------\n",
      "\n",
      "x_new before clipping:  [0.35862284 1.9484842  3.46440343]\n",
      "loss_func costs 14.567478s \n",
      "\n",
      "y_new - y_current is 549.7008471771863: \n",
      "---------- 15/L ----------\n",
      "\n",
      "x_new before clipping:  [0.43436065 2.52362506 3.82546335]\n",
      "loss_func costs 10.796324s \n",
      "\n",
      "y_new - y_current is -2.5370479144481806: \n",
      "---------- 16/L ----------\n",
      "\n",
      "x_new before clipping:  [0.30698367 2.22659371 4.59412047]\n",
      "loss_func costs 10.717387s \n",
      "\n",
      "y_new - y_current is 6.28328959352848: \n",
      "---------- 17/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.04153966  1.90072082  4.79608898]\n",
      "loss_func costs 15.958974s \n",
      "\n",
      "y_new - y_current is -512.8872525945155: \n",
      "---------- 18/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.26459351  1.94306168  4.272269  ]\n",
      "loss_func costs 15.582934s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "---------- 19/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.27187575  2.44744429  4.1442881 ]\n",
      "loss_func costs 11.641825s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "-------------- 2_th iteration --------------\n",
      "\n",
      "---------- 0/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.22305754  1.34682289  4.88635138]\n",
      "loss_func costs 15.415302s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "---------- 1/L ----------\n",
      "\n",
      "x_new before clipping:  [0.12042544 2.24451577 5.1628759 ]\n",
      "loss_func costs 12.151279s \n",
      "\n",
      "y_new - y_current is 294.2627663131458: \n",
      "---------- 2/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.33927811  2.52993515  4.98567512]\n",
      "loss_func costs 11.529476s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "---------- 3/L ----------\n",
      "\n",
      "x_new before clipping:  [0.41789332 2.01266525 4.7060921 ]\n",
      "loss_func costs 10.693505s \n",
      "\n",
      "y_new - y_current is 513.7019907099058: \n",
      "---------- 4/L ----------\n",
      "\n",
      "x_new before clipping:  [0.33653154 1.73792957 5.11742243]\n",
      "loss_func costs 13.840456s \n",
      "\n",
      "y_new - y_current is 539.3208726105646: \n",
      "---------- 5/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.16453601  1.92795613  4.14426903]\n",
      "loss_func costs 15.307865s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "---------- 6/L ----------\n",
      "\n",
      "x_new before clipping:  [0.03380392 1.99106773 4.79586079]\n",
      "loss_func costs 16.123435s \n",
      "\n",
      "y_new - y_current is 251.59434697194865: \n",
      "---------- 7/L ----------\n",
      "\n",
      "x_new before clipping:  [0.00695011 2.43080092 4.63402488]\n",
      "loss_func costs 12.00418s \n",
      "\n",
      "y_new - y_current is -49.76817081527065: \n",
      "---------- 8/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.25603216  1.77686811  5.09643472]\n",
      "loss_func costs 15.90311s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "---------- 9/L ----------\n",
      "\n",
      "x_new before clipping:  [0.52307442 1.47003794 4.51573212]\n",
      "loss_func costs 15.523698s \n",
      "\n",
      "y_new - y_current is 326.8482780443254: \n",
      "---------- 10/L ----------\n",
      "\n",
      "x_new before clipping:  [0.41085975 1.32350375 4.61507669]\n",
      "loss_func costs 15.158185s \n",
      "\n",
      "y_new - y_current is -10.902380792645204: \n",
      "---------- 11/L ----------\n",
      "\n",
      "x_new before clipping:  [0.07236737 0.87078064 4.18787317]\n",
      "loss_func costs 16.461502s \n",
      "\n",
      "y_new - y_current is -429.5031561346904: \n",
      "---------- 12/L ----------\n",
      "\n",
      "x_new before clipping:  [0.11365242 0.38107636 4.13909819]\n",
      "loss_func costs 16.648784s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "---------- 13/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.18009205  1.14674788  3.90622745]\n",
      "loss_func costs 15.38945s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "---------- 14/L ----------\n",
      "\n",
      "x_new before clipping:  [0.22908303 1.49802964 3.82134878]\n",
      "loss_func costs 16.262481s \n",
      "\n",
      "y_new - y_current is 305.87935311193627: \n",
      "---------- 15/L ----------\n",
      "\n",
      "x_new before clipping:  [0.07571071 1.08291389 4.73130632]\n",
      "loss_func costs 16.50023s \n",
      "\n",
      "y_new - y_current is 98.35991329113813: \n",
      "---------- 16/L ----------\n",
      "\n",
      "x_new before clipping:  [0.06886039 0.79817031 4.84038823]\n",
      "loss_func costs 15.994248s \n",
      "\n",
      "y_new - y_current is -110.29000323263818: \n",
      "---------- 17/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.26576017  0.72847502  4.51739208]\n",
      "loss_func costs 16.143269s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "---------- 18/L ----------\n",
      "\n",
      "x_new before clipping:  [8.84299274e-04 3.80071797e-01 5.09647250e+00]\n",
      "loss_func costs 17.086406s \n",
      "\n",
      "y_new - y_current is 98.81115358853197: \n",
      "---------- 19/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.2437572   0.20114441  4.81560083]\n",
      "loss_func costs 16.101033s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "-------------- 3_th iteration --------------\n",
      "\n",
      "---------- 0/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.18359006  1.18141358  5.22783528]\n",
      "loss_func costs 15.895659s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "---------- 1/L ----------\n",
      "\n",
      "x_new before clipping:  [0.15861076 1.02739601 5.17083276]\n",
      "loss_func costs 16.628653s \n",
      "\n",
      "y_new - y_current is 95.47037895435483: \n",
      "---------- 2/L ----------\n",
      "\n",
      "x_new before clipping:  [0.47434589 0.93081612 5.02546658]\n",
      "loss_func costs 14.383893s \n",
      "\n",
      "y_new - y_current is 241.7820906676784: \n",
      "---------- 3/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.05003445  1.21265198  5.33462836]\n",
      "loss_func costs 16.509671s \n",
      "\n",
      "y_new - y_current is -34.677163870638765: \n",
      "---------- 4/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.03394531  1.24941203  5.77998741]\n",
      "loss_func costs 16.978368s \n",
      "\n",
      "y_new - y_current is -159.44653258528717: \n",
      "---------- 5/L ----------\n",
      "\n",
      "x_new before clipping:  [0.10012929 1.15865174 5.95885286]\n",
      "loss_func costs 17.584475s \n",
      "\n",
      "y_new - y_current is 187.6784973420954: \n",
      "---------- 6/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.01243461  1.043772    5.70828921]\n",
      "loss_func costs 17.141448s \n",
      "\n",
      "y_new - y_current is -131.41214610571924: \n",
      "---------- 7/L ----------\n",
      "\n",
      "x_new before clipping:  [0.08993135 0.82651827 5.47753417]\n",
      "loss_func costs 16.270057s \n",
      "\n",
      "y_new - y_current is 185.89437211343292: \n",
      "---------- 8/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.02518124  0.72488309  6.00423248]\n",
      "loss_func costs 16.278286s \n",
      "\n",
      "y_new - y_current is 83.21137037320415: \n",
      "---------- 9/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.19885561  0.60906773  5.80255614]\n",
      "loss_func costs 15.645313s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "---------- 10/L ----------\n",
      "\n",
      "x_new before clipping:  [0.07122294 0.65350434 6.25401578]\n",
      "loss_func costs 16.430996s \n",
      "\n",
      "y_new - y_current is -119.86343528622206: \n",
      "---------- 11/L ----------\n",
      "\n",
      "x_new before clipping:  [0.64361265 0.31009094 6.42490529]\n",
      "loss_func costs 14.294738s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "---------- 12/L ----------\n",
      "\n",
      "x_new before clipping:  [0.35338653 0.42475713 6.16744439]\n",
      "loss_func costs 14.014514s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "---------- 13/L ----------\n",
      "\n",
      "x_new before clipping:  [0.05443186 0.40522026 6.2628177 ]\n",
      "loss_func costs 16.23428s \n",
      "\n",
      "y_new - y_current is 128.6296030036375: \n",
      "---------- 14/L ----------\n",
      "\n",
      "x_new before clipping:  [0.0503835  0.53206941 6.29433275]\n",
      "loss_func costs 16.519626s \n",
      "\n",
      "y_new - y_current is -131.51645609966113: \n",
      "---------- 15/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.15081243  0.72806626  6.41184945]\n",
      "loss_func costs 17.182865s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "---------- 16/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.07490211  0.45729092  6.55097716]\n",
      "loss_func costs 16.045893s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "---------- 17/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.00827404  0.57410928  5.6516774 ]\n",
      "loss_func costs 16.591849s \n",
      "\n",
      "y_new - y_current is -16.150091935028456: \n",
      "---------- 18/L ----------\n",
      "\n",
      "x_new before clipping:  [0.23898286 0.36920029 5.019722  ]\n",
      "loss_func costs 15.760695s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "---------- 19/L ----------\n",
      "\n",
      "x_new before clipping:  [0.16446363 1.09630477 6.17243545]\n",
      "loss_func costs 15.656139s \n",
      "\n",
      "y_new - y_current is 233.1494208345302: \n",
      "-------------- 4_th iteration --------------\n",
      "\n",
      "---------- 0/L ----------\n",
      "\n",
      "x_new before clipping:  [0.22616375 0.8264911  6.31006266]\n",
      "loss_func costs 17.744068s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "---------- 1/L ----------\n",
      "\n",
      "x_new before clipping:  [0.42065264 1.08473103 6.26858852]\n",
      "loss_func costs 15.24472s \n",
      "\n",
      "y_new - y_current is 202.14164019340353: \n",
      "---------- 2/L ----------\n",
      "\n",
      "x_new before clipping:  [0.21295054 1.26271283 6.15321182]\n",
      "loss_func costs 15.801617s \n",
      "\n",
      "y_new - y_current is -120.58128552748724: \n",
      "---------- 3/L ----------\n",
      "\n",
      "x_new before clipping:  [0.10691478 1.28808835 5.96962936]\n",
      "loss_func costs 16.197782s \n",
      "\n",
      "y_new - y_current is -84.81303418177322: \n",
      "---------- 4/L ----------\n",
      "\n",
      "x_new before clipping:  [0.05888952 1.33592768 6.18200549]\n",
      "loss_func costs 17.054013s \n",
      "\n",
      "y_new - y_current is -11.79665446526019: \n",
      "---------- 5/L ----------\n",
      "\n",
      "x_new before clipping:  [0.16258932 1.20081368 6.15042002]\n",
      "loss_func costs 15.879882s \n",
      "\n",
      "y_new - y_current is 37.278542923304826: \n",
      "---------- 6/L ----------\n",
      "\n",
      "x_new before clipping:  [0.34107835 1.06630994 6.57190747]\n",
      "loss_func costs 14.796331s \n",
      "\n",
      "y_new - y_current is 184.58362174532908: \n",
      "---------- 7/L ----------\n",
      "\n",
      "x_new before clipping:  [0.48219654 1.11946937 6.65844287]\n",
      "loss_func costs 15.619842s \n",
      "\n",
      "y_new - y_current is 2.6573756166181965: \n",
      "---------- 8/L ----------\n",
      "\n",
      "x_new before clipping:  [0.40488357 0.98187619 6.60458949]\n",
      "loss_func costs 14.98456s \n",
      "\n",
      "y_new - y_current is 5.256297366364606: \n",
      "---------- 9/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.22771986  0.87911263  6.90623883]\n",
      "loss_func costs 15.921431s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "---------- 10/L ----------\n",
      "\n",
      "x_new before clipping:  [0.41699733 1.14764407 6.73617312]\n",
      "loss_func costs 14.834309s \n",
      "\n",
      "y_new - y_current is -0.6437869860633327: \n",
      "---------- 11/L ----------\n",
      "\n",
      "x_new before clipping:  [0.39039092 0.94297929 6.70127644]\n",
      "loss_func costs 15.593317s \n",
      "\n",
      "y_new - y_current is 9.677361440509799: \n",
      "---------- 12/L ----------\n",
      "\n",
      "x_new before clipping:  [5.48836556e-03 9.14109451e-01 6.69104654e+00]\n",
      "loss_func costs 18.025026s \n",
      "\n",
      "y_new - y_current is -570.9587422196639: \n",
      "---------- 13/L ----------\n",
      "\n",
      "x_new before clipping:  [0.07046526 0.62754359 6.65398337]\n",
      "loss_func costs 17.969624s \n",
      "\n",
      "y_new - y_current is 139.58209815262524: \n",
      "---------- 14/L ----------\n",
      "\n",
      "x_new before clipping:  [0.13385416 0.39777359 6.87096181]\n",
      "loss_func costs 16.209656s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "---------- 15/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.15346444  0.31956738  6.35966275]\n",
      "loss_func costs 16.101593s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "---------- 16/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.01380646  0.61809926  6.38749076]\n",
      "loss_func costs 17.784977s \n",
      "\n",
      "y_new - y_current is 15.448513996952215: \n",
      "---------- 17/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.11612416  0.88778793  6.43909907]\n",
      "loss_func costs 16.187919s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "---------- 18/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.04253878  0.63416806  6.47012809]\n",
      "loss_func costs 16.609101s \n",
      "\n",
      "y_new - y_current is 465.5553472210627: \n",
      "---------- 19/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.00811395  0.89069184  5.90869274]\n",
      "loss_func costs 17.566904s \n",
      "\n",
      "y_new - y_current is -172.7650629458718: \n",
      "> SA ENDS....... \n",
      "\n",
      "> For the 2th data_key, there is:\n",
      "> retail = 299.99,bidincrement = 0.15, bidfee = 0.75, infer PT's parameters\n",
      "> Initilizing SA....... \n",
      "\n",
      "loss_func costs 25.826222s \n",
      "\n",
      "> Now do SA....... \n",
      "\n",
      "-------------- 0_th iteration --------------\n",
      "\n",
      "---------- 0/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.03838665  0.66279276  4.02156801]\n",
      "loss_func costs 24.920372s \n",
      "\n",
      "y_new - y_current is -31.61481058395799: \n",
      "---------- 1/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.00983389  0.86787524  3.68372485]\n",
      "loss_func costs 25.67341s \n",
      "\n",
      "y_new - y_current is -104.42747506095247: \n",
      "---------- 2/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.40835313  0.10932428  3.35988251]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pythorch\\lib\\site-packages\\ipykernel_launcher.py:23: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_func costs 24.120219s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "---------- 3/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.10805746  1.27124567  3.70786189]\n",
      "loss_func costs 24.766562s \n",
      "\n",
      "y_new - y_current is 1091.610421953982: \n",
      "---------- 4/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.09987901  1.36116554  3.19414028]\n",
      "loss_func costs 23.464807s \n",
      "\n",
      "y_new - y_current is -78.45615520538081: \n",
      "---------- 5/L ----------\n",
      "\n",
      "x_new before clipping:  [-0.35668208  0.87940527  3.07819679]\n",
      "loss_func costs 23.785717s \n",
      "\n",
      "y_new - y_current is inf: \n",
      "---------- 6/L ----------\n",
      "\n",
      "x_new before clipping:  [0.07088674 1.46189232 3.23685744]\n",
      "loss_func costs 24.019397s \n",
      "\n",
      "y_new - y_current is -384.0747633150263: \n",
      "---------- 7/L ----------\n",
      "\n",
      "x_new before clipping:  [0.2128657  1.25283484 3.1910049 ]\n",
      "loss_func costs 20.95828s \n",
      "\n",
      "y_new - y_current is 1355.360766756385: \n",
      "---------- 8/L ----------\n",
      "\n",
      "x_new before clipping:  [0.24081031 1.21552811 2.63195992]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'pop from an empty set'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-41-951dd411281a>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     33\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"> Now do SA....... \\n\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 34\u001B[1;33m     \u001B[0mbest_x\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbest_y\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msa_boltzmann\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     35\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     36\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"> SA ENDS....... \\n\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\envs\\pythorch\\lib\\site-packages\\sko\\SA.py\u001B[0m in \u001B[0;36mrun\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     78\u001B[0m                 \u001B[0mx_new\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_new_x\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_current\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     79\u001B[0m                 \u001B[1;31m# print(\"x_new after clipping: \", x_new)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 80\u001B[1;33m                 \u001B[0my_new\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_new\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     81\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     82\u001B[0m                 \u001B[1;31m# Metropolis\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-37-58e73e9aad6a>\u001B[0m in \u001B[0;36mloss_func\u001B[1;34m(params)\u001B[0m\n\u001B[0;32m     11\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mt\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mmax_T\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 13\u001B[1;33m         \u001B[0mU_i\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mt\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mf_Equi\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mt\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mv\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0md\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mb\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0malpha\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabda\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdelta\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     14\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     15\u001B[0m     \u001B[1;31m# calculate NLL under this auction setting & PT params\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-36-061909f34b23>\u001B[0m in \u001B[0;36mf_Equi\u001B[1;34m(t, v, d, b, alpha, labda, delta)\u001B[0m\n\u001B[0;32m     10\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0msympy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnsolve\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfunc_1\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0msolver\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'bisect'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mverify\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 12\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0msympy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnsolve\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfunc_2\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0msolver\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'bisect'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mverify\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     13\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\envs\\pythorch\\lib\\site-packages\\sympy\\utilities\\decorator.py\u001B[0m in \u001B[0;36mfunc_wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     86\u001B[0m         \u001B[0mdps\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmpmath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdps\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     87\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 88\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     89\u001B[0m         \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     90\u001B[0m             \u001B[0mmpmath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdps\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdps\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\envs\\pythorch\\lib\\site-packages\\sympy\\solvers\\solvers.py\u001B[0m in \u001B[0;36mnsolve\u001B[1;34m(dict, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2924\u001B[0m         \u001B[0msyms\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfree_symbols\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2925\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mfargs\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2926\u001B[1;33m             \u001B[0mfargs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msyms\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpop\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2927\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msyms\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m1\u001B[0m \u001B[1;32mand\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mfargs\u001B[0m \u001B[1;32min\u001B[0m \u001B[0msyms\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mfargs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32min\u001B[0m \u001B[0msyms\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2928\u001B[0m             raise ValueError(filldedent('''\n",
      "\u001B[1;31mKeyError\u001B[0m: 'pop from an empty set'"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Perform SA respectively for all settings\n",
    "# for i in range(0,N_uniq_auction):\n",
    "for i in range(0,3):\n",
    "\n",
    "    # get i_th data_key\n",
    "    key_i = get_key_from_index(i)\n",
    "    # extract data with same `key_i` into a table\n",
    "    data_i = select_data_fromkey(key_i)\n",
    "    data_i.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    T_i = data_i['N'].astype(int)          # auction duration sequence\n",
    "    max_T = int(max(T_i))                  # max duration value\n",
    "\n",
    "    cnt_n_2_i = data_i['cnt_n_2'].astype(int)       # Number of occurrences of different durations\n",
    "    # for a certain auction(like 'data_i'), 'cnt_uniq' should be all the same\n",
    "    A_i = int(data_i['cnt_uniq'].unique())\n",
    "    assert(A_i == sum(cnt_n_2_i),\"'cnt_uniq' does not match with sum of 'cnt_n_2'!\")\n",
    "\n",
    "    v = float(data_i['retail'].unique())\n",
    "    d = float(data_i['bidincrement'].unique())\n",
    "    b = float(data_i['bidfee'].unique())\n",
    "\n",
    "    # calculate NLL\n",
    "    print(\"> For the {}th data_key, there is:\".format(i))\n",
    "    print(\"> retail = {0},bidincrement = {1}, bidfee = {2}, infer PT's parameters\".format(v,d,b))\n",
    "    print(\"> Initilizing SA....... \\n\")\n",
    "    # L=50, max_stay_counter=50\n",
    "    set_run_mode(loss_func, 'cached')\n",
    "    set_run_mode(loss_func, 'multithreading')\n",
    "    sa_boltzmann = SABoltzmann(func=loss_func, x0=table_5_M, T_max=1000, T_min=1e-5, learn_rate=0.01, L=20, max_stay_counter=3,\n",
    "                            lb=lb, ub=ub)\n",
    "\n",
    "    print(\"> Now do SA....... \\n\")\n",
    "    best_x, best_y = sa_boltzmann.run()\n",
    "\n",
    "    print(\"> SA ENDS....... \\n\")\n",
    "\n",
    "    # draw the pic of NLL Loss in SA\n",
    "    plt.title(\"In {0} iteration of {1}, the T(NLL) in SA\".format(i,N_uniq_auction))\n",
    "    plt.xlabel(\"iter_cycle\")\n",
    "    plt.ylabel(\"T of SA\")\n",
    "    sns.lineplot(x = np.arange(0,sa_boltzmann.iter_cycle+1),y=np.array(sa_boltzmann.generation_best_Y))\n",
    "\n",
    "    # append the opitimized params into the df\n",
    "    df_tmp = pd.DataFrame([[i,best_x[0],best_x[1],best_x[2]]],columns=['key_idx','alpha','delta','labda'])\n",
    "    params_opitim = params_opitim.append(df_tmp,ignore_index=True)  # ignore_index=True could help in rearranging index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# reset the type\n",
    "params_opitim.key_idx = params_opitim.key_idx.astype(int)\n",
    "params_opitim.alpha = params_opitim.alpha.astype(float)\n",
    "params_opitim.delta = params_opitim.delta.astype(float)\n",
    "params_opitim.labda = params_opitim.labda.astype(float)\n",
    "# save 'params_opitim' for later check\n",
    "params_opitim.to_csv(params_opitim_path, header=True, encoding=\"utf-8\",index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4 P\n",
    "1. 得到`params_opitim`之后，可以对不同的auction settings做generate了\n",
    "2. generate过程无非是求u-->p，u的代码在上面loss func里写过了。然后把P存到dict里"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# solve for U&P respectively for all settings\n",
    "P = {}\n",
    "for i in range(0,N_uniq_auction):\n",
    "\n",
    "    # get optimized params\n",
    "    alpha, labda, delta = params_opitim.iloc[i][0],params_opitim.iloc[i][1],params_opitim.iloc[i][2]\n",
    "\n",
    "    # get i_th data_key\n",
    "    key_i = get_key_from_index(i)\n",
    "    # extract data with same `key_i` into a table\n",
    "    data_i = select_data_fromkey(key_i)\n",
    "    data_i.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    T_i = data_i['N'].astype(int)          # auction duration sequence\n",
    "    max_T = int(max(T_i))                  # max duration value\n",
    "\n",
    "    cnt_n_2_i = data_i['cnt_n_2'].astype(int)       # Number of occurrences of different durations\n",
    "\n",
    "    v = float(data_i['retail'].unique())\n",
    "    d = float(data_i['bidincrement'].unique())\n",
    "    b = float(data_i['bidfee'].unique())\n",
    "\n",
    "    U_i = [0] * (max_T + 1)\n",
    "    U_i[0] = 1\n",
    "    key_i_str = get_key_from_index\n",
    "    P[key_i_str] = np.array([0.0]*(max_T+1))\n",
    "    P_tmp = [0.0]*(max_T+1)   # P is what we want to generate\n",
    "    P_tmp[0] = 1\n",
    "    tmp = 1\n",
    "\n",
    "    # solve for U\n",
    "    for t in range(1,max_T+1):\n",
    "        U_i[t] = f_Equi(t, v, d, b, alpha, labda, delta)\n",
    "        P_tmp[t] = (1- U_i[t])*tmp\n",
    "        tmp = tmp*U_i[t]\n",
    "    # solve for P\n",
    "    for j in range()\n",
    "    # P[key_i][j] = 1.0-b[i]/(v[i]-s[i]*(j-1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "def test_func(i,str=\"NotStr\"):\n",
    "    if(str == \"Str\"):\n",
    "        return i+10\n",
    "    return i+1\n",
    "\n",
    "print(test_func(1))\n",
    "print(test_func(1,\"Str\"))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [
     "%load_ext autoreload\n",
     "%autoreload 2\n",
     "%matplotlib notebook\n",
     "%matplotlib inline"
    ],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}