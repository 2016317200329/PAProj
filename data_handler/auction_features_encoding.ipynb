{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2022/11/21 10:06\n",
    "# @Author  : Wang Yujia\n",
    "# @File    : auction_features_encoding.ipynb\n",
    "# @Description : 为auction从'desc'列提取contextual features。目前采用的是`SentenceTransformer`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0. what for\n",
    "1. auction的features/ setting info需要encoding\n",
    "2. 只对`desc`进行encoding\n",
    "\n",
    "# 1. preparations\n",
    "## 1.1 全局设置"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "data_key_path = \"../data/target_datakey.csv\"\n",
    "prod_id_path = \"../data/prod_id.csv\"\n",
    "\n",
    "# large data\n",
    "settings_np_path = r'E:\\DATA\\large_dta\\settings_np.csv'\n",
    "large_prod_path = r'E:\\DATA\\large_dta\\prod.csv'\n",
    "\n",
    "# output path\n",
    "# prod_embedding_output_path = \"../data/prod_embedding.csv\"\n",
    "# prod_embedding_output_path = \"../data/prod_embedding_300.csv\"\n",
    "prod_embedding_output_path = \"../data/prod_embedding_60.csv\"\n",
    "\n",
    "large_prod_embedding_output_path = r'E:\\DATA\\large_dta\\prod_embedding_300.csv'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# 希望得到的encoding维度\n",
    "new_dimension = 300\n",
    "\n",
    "# 聚类\n",
    "num_clusters = 4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D:\\\\PyCharm\\\\plugins\\\\python\\\\helpers-pro\\\\jupyter_debug', 'D:\\\\PyCharm\\\\plugins\\\\python\\\\helpers\\\\pydev', 'D:\\\\Desktop\\\\PROJ\\\\PAProj\\\\data_handler', 'D:\\\\Desktop\\\\PROJ\\\\PAProj', 'D:\\\\Anaconda\\\\python39.zip', 'D:\\\\Anaconda\\\\DLLs', 'D:\\\\Anaconda\\\\lib', 'D:\\\\Anaconda', '', 'D:\\\\Anaconda\\\\Lib\\\\site-packages', 'D:\\\\Anaconda\\\\Lib\\\\site-packages\\\\win32', 'D:\\\\Anaconda\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'D:\\\\Anaconda\\\\Lib\\\\site-packages\\\\Pythonwin', 'D:\\\\Anaconda\\\\Lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\Administrator\\\\.ipython']\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import torch\n",
    "from transformers import BertModel\n",
    "from visdom import Visdom\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "import gzip\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "from sklearn.decomposition import PCA\n",
    "from sentence_transformers import SentenceTransformer, util, InputExample,evaluation,models\n",
    "\n",
    "print(sys.path)\n",
    "device = 'cuda'\n",
    "print(torch.cuda.is_available())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 读取data\n",
    "### 1.2.1 读取id和对应的desc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                           desc\n",
      "0   0  Sony Ericsson S500i Unlocked Mysterious Green\n",
      "1   1               PSP Slim & Lite Sony Piano Black\n",
      "2   2     iPod Touch Apple 8GB with Software Upgrade\n",
      "3   3      Logitech Cordless Wave Keyboard and Mouse\n",
      "4   4   Apple Macbook Air 1.6GHz Core 2 Duo Notebook\n",
      "5   5                     SanDisk Cruzer Contour 4GB\n",
      "6   6           Mario Kart with Wheel (Nintendo Wii)\n",
      "7   7      PS3 | Playstation 3 Sony Console 40GB HDD\n",
      "8   8                    DS | Nintendo DS Lite White\n",
      "9  11            Corsair Voyager Mini 4 GB USB Flash\n",
      "(907, 2)\n"
     ]
    }
   ],
   "source": [
    "data_key = pd.read_csv(data_key_path,encoding=\"utf-8\")\n",
    "prod_id_all = pd.read_csv(prod_id_path,encoding=\"utf-8\")\n",
    "large_prod_all = pd.read_csv(large_prod_path,encoding=\"utf-8\")\n",
    "\n",
    "prod_id = prod_id_all[ prod_id_all['id'].isin(data_key['id']) ]\n",
    "prod_id.reset_index(drop=True,inplace=True)\n",
    "print(prod_id.head(10))\n",
    "print(prod_id.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2.2 SentenceTransformer微调用到的data\n",
    "1. training：AllNLI（2w个samples）\n",
    "2. evaluating：STS benchmark"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Use AllNLI as a source of sentences to compute PCA\n",
    "nli_dataset_path = 'datasets/AllNLI.tsv.gz'\n",
    "\n",
    "# Use the STS benchmark dataset to see how much performance we lose by the dimensionality reduction\n",
    "sts_dataset_path = 'datasets/stsbenchmark.tsv.gz'\n",
    "\n",
    "if not os.path.exists(nli_dataset_path):\n",
    "    util.http_get('https://sbert.net/datasets/AllNLI.tsv.gz', nli_dataset_path)\n",
    "\n",
    "if not os.path.exists(sts_dataset_path):\n",
    "    util.http_get('https://sbert.net/datasets/stsbenchmark.tsv.gz', sts_dataset_path)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "1379"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the benchmark dataset\n",
    "eval_examples = []\n",
    "with gzip.open(sts_dataset_path, 'rt', encoding='utf8') as f:\n",
    "    reader = csv.DictReader(f, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        if row['split'] == 'test':\n",
    "            score = float(row['score']) / 5.0 #Normalize score to range 0 ... 1\n",
    "            eval_examples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=score))\n",
    "eval_examples.__len__()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "1196755"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read sentences from NLI dataset\n",
    "nli_sentences = set()\n",
    "with gzip.open(nli_dataset_path, 'rt', encoding='utf8') as f:\n",
    "    reader = csv.DictReader(f, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        nli_sentences.add(row['sentence1'])\n",
    "        nli_sentences.add(row['sentence2'])\n",
    "nli_sentences = list(nli_sentences)\n",
    "random.shuffle(nli_sentences)\n",
    "nli_sentences.__len__()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 读取model\n",
    "1. 读取的model包括\n",
    "    - SentenceTransformer\n",
    "    - PCA：用来降维SentenceTransformer的输出维度\n",
    "    - 聚类用的kmeans和tSNE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "model_2 = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "pca = PCA(n_components=new_dimension)\n",
    "\n",
    "tsne_model = TSNE(perplexity=10, n_components=2, init='pca', n_iter=250, random_state=23)\n",
    "clustering_model = KMeans(n_clusters=num_clusters)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Encoding\n",
    "\n",
    "## 2.1 全连接层（PCA）降维\n",
    "1. 计算加这个FC层之前的模型表现"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8203247283076371"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Measure the performance of the original model\n",
    "# Evaluate the original model on the STS benchmark dataset\n",
    "stsb_evaluator = evaluation.EmbeddingSimilarityEvaluator.from_input_examples(eval_examples, name='sts-benchmark-test')\n",
    "\n",
    "stsb_evaluator(model_2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. 用training data计算PCA特征矩阵"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 384)\n",
      "4.03432559967041\n"
     ]
    }
   ],
   "source": [
    "#To determine the PCA matrix, we need some example sentence embeddings.\n",
    "#Here, we compute the embeddings for 20k random sentences from the AllNLI dataset\n",
    "\n",
    "time_start = time.time()\n",
    "pca_train_sentences = nli_sentences[0:20000]\n",
    "train_embeddings = model_2.encode(pca_train_sentences, convert_to_numpy=True)\n",
    "\n",
    "#Compute PCA on the training embeddings matrix\n",
    "pca.fit(train_embeddings)\n",
    "pca_comp = np.asarray(pca.components_)\n",
    "\n",
    "time_end = time.time()  # 记录开始时间\n",
    "\n",
    "print(pca_comp.shape)\n",
    "print(time_end-time_start)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. 用PCA矩阵当做全连接层的权重"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Add a dense layer to the model, so that it will produce directly embeddings with the new size\n",
    "dense = models.Dense(in_features=model_2.get_sentence_embedding_dimension(), out_features=new_dimension, bias=False, activation_function=torch.nn.Identity())\n",
    "dense.linear.weight = torch.nn.Parameter(torch.tensor(pca_comp))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. 加入现有的pretrained model中，并且计算现在的evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8234310619689741"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.add_module('dense', dense)\n",
    "\n",
    "# Evaluate the model with the reduce embedding size\n",
    "stsb_evaluator(model_2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Generate and save"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "   index                                item\n0      0                     20 BIDS VOUCHER\n1    791                     50 BIDS VOUCHER\n2    802                50 FREEBIDS VOUCHER!\n3   2069  MAKITA CORDLESS ARTICULATED WRENCH\n4   2094                     50.- EURO CASH!",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>item</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>20 BIDS VOUCHER</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>791</td>\n      <td>50 BIDS VOUCHER</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>802</td>\n      <td>50 FREEBIDS VOUCHER!</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2069</td>\n      <td>MAKITA CORDLESS ARTICULATED WRENCH</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2094</td>\n      <td>50.- EURO CASH!</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_prod_all.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12979984283447266\n"
     ]
    }
   ],
   "source": [
    "### model generate and save\n",
    "time_start = time.time()\n",
    "# prod_embedding = model_2.encode(list(prod_id.loc[:,'desc']), convert_to_numpy=True,device=device)\n",
    "prod_embedding = model_2.encode(list(large_prod_all.loc[:,'item']), convert_to_numpy=True,device=device)\n",
    "assert prod_embedding.shape[0] == large_prod_all.shape[0], \"Wrong!\"\n",
    "time_end = time.time()  # 记录开始时间\n",
    "print(time_end - time_start)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "(          0         1         2         3         4         5         6  \\\n 0 -0.094135  0.040026 -0.041996  0.008506 -0.018257  0.183759 -0.123493   \n 1 -0.088604  0.030237 -0.059413 -0.029522 -0.007331  0.160427 -0.100010   \n 2 -0.080406 -0.036259 -0.038542 -0.077779  0.040293  0.126037 -0.083026   \n 3  0.085327 -0.059333  0.059414 -0.017793 -0.010209 -0.043411  0.080159   \n 4 -0.020600 -0.058047 -0.025251 -0.058453 -0.021372  0.113071 -0.086308   \n 5 -0.022841  0.054777  0.005317  0.121434 -0.058431 -0.014406 -0.034863   \n 6  0.112447 -0.014499  0.008788 -0.098316 -0.132019  0.021437  0.000178   \n 7 -0.085164  0.024830 -0.047954 -0.029983 -0.007887  0.171949 -0.123925   \n 8 -0.084975 -0.034312 -0.031602 -0.071158  0.048506  0.148114 -0.092877   \n 9 -0.044354  0.018795 -0.042867 -0.029346 -0.000366  0.131457 -0.002735   \n \n           7         8         9  ...       291       292       293       294  \\\n 0 -0.094562 -0.005950  0.014576  ... -0.007971 -0.042334  0.013645 -0.016216   \n 1 -0.104681  0.003096  0.025216  ... -0.011346 -0.033330  0.006432 -0.027616   \n 2 -0.090534 -0.025556 -0.033241  ... -0.032506 -0.027266  0.023854 -0.007530   \n 3  0.069116 -0.003544 -0.133923  ...  0.030676  0.039983 -0.012218  0.047222   \n 4 -0.055351 -0.071047 -0.009101  ... -0.016375 -0.005070  0.025948 -0.049492   \n 5 -0.062797 -0.084532  0.063852  ...  0.033520  0.022545 -0.032396 -0.031791   \n 6 -0.077823 -0.029858  0.076752  ... -0.000899 -0.003558 -0.038055 -0.010180   \n 7 -0.129564 -0.019722  0.044844  ...  0.002852 -0.038382  0.002210 -0.007770   \n 8 -0.112787 -0.044255 -0.030085  ... -0.024091 -0.023250  0.018899  0.005868   \n 9 -0.060672  0.094602  0.024305  ... -0.001356 -0.032025 -0.027411  0.025610   \n \n         295       296       297       298       299  \\\n 0 -0.016310  0.026330  0.002612  0.044714 -0.034248   \n 1 -0.027945  0.017044  0.005199  0.042737 -0.026931   \n 2 -0.000481  0.003881 -0.015918  0.003363 -0.023020   \n 3  0.004253  0.002394 -0.016213 -0.006022 -0.012896   \n 4  0.038268 -0.029127 -0.000649 -0.018977 -0.036579   \n 5  0.006981  0.030773 -0.004838  0.026906 -0.005004   \n 6 -0.049231 -0.005540 -0.030303  0.006839 -0.017359   \n 7 -0.037707  0.005633 -0.017188  0.021443 -0.055301   \n 8 -0.012219 -0.006303 -0.032714 -0.014007 -0.048132   \n 9 -0.038988  0.087359  0.027036  0.010532 -0.027675   \n \n                                             desc  \n 0  Sony Ericsson S500i Unlocked Mysterious Green  \n 1               PSP Slim & Lite Sony Piano Black  \n 2     iPod Touch Apple 8GB with Software Upgrade  \n 3      Logitech Cordless Wave Keyboard and Mouse  \n 4   Apple Macbook Air 1.6GHz Core 2 Duo Notebook  \n 5                     SanDisk Cruzer Contour 4GB  \n 6           Mario Kart with Wheel (Nintendo Wii)  \n 7      PS3 | Playstation 3 Sony Console 40GB HDD  \n 8                    DS | Nintendo DS Lite White  \n 9            Corsair Voyager Mini 4 GB USB Flash  \n \n [10 rows x 301 columns],\n (77, 301))"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod_embedding_df = pd.DataFrame(prod_embedding)\n",
    "# prod_embedding_df['id'] = prod_id['id']\n",
    "prod_embedding_df['desc'] = prod_id['desc']     # Add this new column\n",
    "\n",
    "# prod_embedding_df.to_csv(prod_embedding_output_path,header=True,index=False,encoding=\"utf-8\")\n",
    "prod_embedding_df.to_csv(large_prod_embedding_output_path,header=True,index=False,encoding=\"utf-8\")\n",
    "prod_embedding_df.head(10),prod_embedding_df.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Test: clustering\n",
    "做一下clustering来判断encoding效果\n",
    "see `auction_features_encoding_demo.ipynb`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
