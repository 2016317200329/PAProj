{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2022/11/21 10:06\n",
    "# @Author  : Wang Yujia\n",
    "# @File    : auction_features_encoding.ipynb\n",
    "# @Description : 为auction从'desc'列提取contextual features。目前采用的是`SentenceTransformer`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0. what for\n",
    "1. auction的features/ setting info需要encoding\n",
    "2. 只对`desc`进行encoding\n",
    "\n",
    "# 1. preparations\n",
    "## 1.1 全局设置"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "'../data/small_prod_embedding_4.csv'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LENTH=4      # 望得到的encoding维度=300 or 4? 取决于用途\n",
    "\n",
    "# small data\n",
    "settings_small_NN_path = r\"../data/small_settings_NN.csv\"\n",
    "\n",
    "# large data\n",
    "settings_large_NN_path = r'E:\\DATA\\large_dta\\large_settings_NN.csv'\n",
    "\n",
    "# output path\n",
    "prod_embedding_small_path = \"../data/small_prod_embedding_\"+str(LENTH)+\".csv\"\n",
    "prod_embedding_large_path = \"E:\\DATA\\large_dta\\large_prod_embedding_\"+str(LENTH)+\".csv\"\n",
    "prod_embedding_small_path"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# 希望得到的encoding维度\n",
    "new_dimension = LENTH\n",
    "\n",
    "# 聚类数量\n",
    "num_clusters = 4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D:\\\\PyCharm\\\\plugins\\\\python\\\\helpers-pro\\\\jupyter_debug', 'D:\\\\PyCharm\\\\plugins\\\\python\\\\helpers\\\\pydev', 'D:\\\\Desktop\\\\PROJ\\\\PAProj\\\\data_handler', 'D:\\\\Desktop\\\\PROJ\\\\PAProj', 'D:\\\\Anaconda\\\\python39.zip', 'D:\\\\Anaconda\\\\DLLs', 'D:\\\\Anaconda\\\\lib', 'D:\\\\Anaconda', '', 'D:\\\\Anaconda\\\\Lib\\\\site-packages', 'D:\\\\Anaconda\\\\Lib\\\\site-packages\\\\win32', 'D:\\\\Anaconda\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'D:\\\\Anaconda\\\\Lib\\\\site-packages\\\\Pythonwin', 'D:\\\\Anaconda\\\\Lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\Administrator\\\\.ipython']\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import torch\n",
    "from transformers import BertModel\n",
    "from visdom import Visdom\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "import gzip\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "from sklearn.decomposition import PCA\n",
    "from sentence_transformers import SentenceTransformer, util, InputExample,evaluation,models\n",
    "\n",
    "print(sys.path)\n",
    "device = 'cuda'\n",
    "print(torch.cuda.is_available())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 读取data\n",
    "### 1.2.1 读取desc，不去重"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一共有 *1276* 个商品item\n"
     ]
    }
   ],
   "source": [
    "small_data_key = pd.read_csv(settings_small_NN_path,encoding=\"utf-8\")\n",
    "large_data_key = pd.read_csv(settings_large_NN_path,encoding=\"utf-8\")\n",
    "\n",
    "data = pd.concat([small_data_key,large_data_key],axis=0,ignore_index=True)\n",
    "prod = data.desc\n",
    "print(f\"一共有 *{prod.shape[0]}* 个商品item\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2.2 SentenceTransformer微调用到的data\n",
    "1. training：AllNLI（2w个samples）\n",
    "2. evaluating：STS benchmark"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# Use AllNLI as a source of sentences to compute PCA\n",
    "nli_dataset_path = 'datasets/AllNLI.tsv.gz'\n",
    "\n",
    "# Use the STS benchmark dataset to see how much performance we lose by the dimensionality reduction\n",
    "sts_dataset_path = 'datasets/stsbenchmark.tsv.gz'\n",
    "\n",
    "if not os.path.exists(nli_dataset_path):\n",
    "    util.http_get('https://sbert.net/datasets/AllNLI.tsv.gz', nli_dataset_path)\n",
    "\n",
    "if not os.path.exists(sts_dataset_path):\n",
    "    util.http_get('https://sbert.net/datasets/stsbenchmark.tsv.gz', sts_dataset_path)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "1379"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the benchmark dataset\n",
    "eval_examples = []\n",
    "with gzip.open(sts_dataset_path, 'rt', encoding='utf8') as f:\n",
    "    reader = csv.DictReader(f, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        if row['split'] == 'test':\n",
    "            score = float(row['score']) / 5.0 #Normalize score to range 0 ... 1\n",
    "            eval_examples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=score))\n",
    "eval_examples.__len__()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "1196755"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read sentences from NLI dataset\n",
    "nli_sentences = set()\n",
    "with gzip.open(nli_dataset_path, 'rt', encoding='utf8') as f:\n",
    "    reader = csv.DictReader(f, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        nli_sentences.add(row['sentence1'])\n",
    "        nli_sentences.add(row['sentence2'])\n",
    "nli_sentences = list(nli_sentences)\n",
    "random.shuffle(nli_sentences)\n",
    "nli_sentences.__len__()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 读取model\n",
    "1. 读取的model包括\n",
    "    - SentenceTransformer\n",
    "    - PCA：用来降维SentenceTransformer的输出维度\n",
    "    - 聚类用的kmeans和tSNE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "model_2 = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "pca = PCA(n_components=new_dimension)\n",
    "\n",
    "tsne_model = TSNE(perplexity=10, n_components=2, init='pca', n_iter=250, random_state=23)\n",
    "clustering_model = KMeans(n_clusters=num_clusters)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Encoding\n",
    "\n",
    "## 2.1 全连接层（PCA）降维\n",
    "1. 计算加这个FC层之前的模型表现"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8203247283076371"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Measure the performance of the original model\n",
    "# Evaluate the original model on the STS benchmark dataset\n",
    "stsb_evaluator = evaluation.EmbeddingSimilarityEvaluator.from_input_examples(eval_examples, name='sts-benchmark-test')\n",
    "\n",
    "stsb_evaluator(model_2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. 用training data计算PCA特征矩阵"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 384)\n",
      "3.08321475982666\n"
     ]
    }
   ],
   "source": [
    "#To determine the PCA matrix, we need some example sentence embeddings.\n",
    "#Here, we compute the embeddings for 20k random sentences from the AllNLI dataset\n",
    "\n",
    "time_start = time.time()\n",
    "pca_train_sentences = nli_sentences[0:20000]\n",
    "train_embeddings = model_2.encode(pca_train_sentences, convert_to_numpy=True)\n",
    "\n",
    "#Compute PCA on the training embeddings matrix\n",
    "pca.fit(train_embeddings)\n",
    "pca_comp = np.asarray(pca.components_)\n",
    "\n",
    "time_end = time.time()  # 记录开始时间\n",
    "\n",
    "print(pca_comp.shape)\n",
    "print(time_end-time_start)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. 用PCA矩阵当做全连接层的权重"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# Add a dense layer to the model, so that it will produce directly embeddings with the new size\n",
    "dense = models.Dense(in_features=model_2.get_sentence_embedding_dimension(), out_features=new_dimension, bias=False, activation_function=torch.nn.Identity())\n",
    "dense.linear.weight = torch.nn.Parameter(torch.tensor(pca_comp))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. 加入现有的pretrained model中，并且计算现在的evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "0.49181313689977346"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.add_module('dense', dense)\n",
    "\n",
    "# Evaluate the model with the reduce embedding size\n",
    "stsb_evaluator(model_2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Generate and save"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用时： 0.46291613578796387\n"
     ]
    }
   ],
   "source": [
    "### model generate and save\n",
    "time_start = time.time()\n",
    "# prod_embedding = model_2.encode(list(prod_id.loc[:,'desc']), convert_to_numpy=True,device=device)\n",
    "prod_embedding = model_2.encode(list(prod), convert_to_numpy=True,device=device)\n",
    "assert prod_embedding.shape[0] == prod.shape[0], \"Wrong!\"\n",
    "time_end = time.time()  # 记录开始时间\n",
    "print(\"用时：\",time_end - time_start)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- save: 分别保存到两个csv中"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "prod_embedding_df = pd.DataFrame(prod_embedding)\n",
    "# prod_embedding_df['id'] = prod_id['id']\n",
    "prod_embedding_df['desc'] = pd.DataFrame(prod)['desc']     # Add this new column\n",
    "\n",
    "small_prod = prod_embedding_df.iloc[0:small_data_key.shape[0],:]\n",
    "small_prod.to_csv(prod_embedding_small_path,header=True,index=False,encoding=\"utf-8\")\n",
    "\n",
    "large_prod = prod_embedding_df.iloc[small_data_key.shape[0]:,:]\n",
    "large_prod.to_csv(prod_embedding_large_path,header=True,index=False,encoding=\"utf-8\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Test: clustering\n",
    "做一下clustering来判断encoding效果\n",
    "see `auction_features_encoding_demo.ipynb`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
