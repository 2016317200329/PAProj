{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2022/11/22 10:36\n",
    "# @Author  : Wang Yujia\n",
    "# @File    : auction_features_encoding_demo.ipynb\n",
    "# @Description : Demo 为auction提取contextual features"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. preparations\n",
    "## 1.1 全局设置\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D:\\\\PyCharm\\\\plugins\\\\python\\\\helpers-pro\\\\jupyter_debug', 'D:\\\\PyCharm\\\\plugins\\\\python\\\\helpers\\\\pydev', 'D:\\\\Desktop\\\\PROJ\\\\PAProj\\\\data_handler', 'D:\\\\Desktop\\\\PROJ\\\\PAProj', 'D:\\\\Anaconda\\\\python39.zip', 'D:\\\\Anaconda\\\\DLLs', 'D:\\\\Anaconda\\\\lib', 'D:\\\\Anaconda', '', 'D:\\\\Anaconda\\\\Lib\\\\site-packages', 'D:\\\\Anaconda\\\\Lib\\\\site-packages\\\\win32', 'D:\\\\Anaconda\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'D:\\\\Anaconda\\\\Lib\\\\site-packages\\\\Pythonwin', 'D:\\\\Anaconda\\\\Lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\Administrator\\\\.ipython']\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_key_path = \"../data/targets/target_datakey.csv\"\n",
    "prod_id_path = \"../data/prod_id.csv\"\n",
    "\n",
    "# output path\n",
    "prod_embedding_output_path = \"../data/prod_embedding.csv\"\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import torch\n",
    "from transformers import BertModel\n",
    "from tokenizers import Tokenizer\n",
    "from visdom import Visdom\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "import gzip\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "from sklearn.decomposition import PCA\n",
    "from sentence_transformers import SentenceTransformer, util, InputExample,evaluation,models\n",
    "\n",
    "from tokenizers.models import BPE\n",
    "import tokenizers\n",
    "# from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "print(sys.path)\n",
    "device = 'cuda'\n",
    "print(torch.cuda.is_available())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 读取data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                           desc\n",
      "0   0  Sony Ericsson S500i Unlocked Mysterious Green\n",
      "1   1               PSP Slim & Lite Sony Piano Black\n",
      "2   2     iPod Touch Apple 8GB with Software Upgrade\n",
      "3   3      Logitech Cordless Wave Keyboard and Mouse\n",
      "4   4   Apple Macbook Air 1.6GHz Core 2 Duo Notebook\n",
      "5   5                     SanDisk Cruzer Contour 4GB\n",
      "6   6           Mario Kart with Wheel (Nintendo Wii)\n",
      "7   7      PS3 | Playstation 3 Sony Console 40GB HDD\n",
      "8   8                    DS | Nintendo DS Lite White\n",
      "9  11            Corsair Voyager Mini 4 GB USB Flash\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "data_key = pd.read_csv(data_key_path,encoding=\"utf-8\")\n",
    "prod_id_all = pd.read_csv(prod_id_path,encoding=\"utf-8\")\n",
    "\n",
    "prod_id = prod_id_all[ prod_id_all['id'].isin(data_key['id']) ]\n",
    "prod_id.reset_index(drop=True,inplace=True)\n",
    "print(prod_id.head(10))\n",
    "print(prod_id.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 读取model\n",
    "1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[CLS] 标志放在第一个句子的首位，经过 BERT 得到的的表征向量 C 可以用于后续的分类任务。\n",
    "[SEP] 标志用于分开两个输入句子，例如输入句子 A 和 B，要在句子 A，B 后面增加 [SEP] 标志。\n",
    "[UNK] 标志指的是未知字符\n",
    "[MASK] 标志用于遮盖句子中的一些单词，将单词用 [MASK] 遮盖之后，再利用 BERT 输出的 [MASK] 向量预测单词是什么。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Tokenizer\n",
    "## 2.1 使用预训练的分词器\n",
    "> http://t.csdn.cn/nKArk\n",
    "1. 英文是subword的概念，例如将\"unwanted\"分解成[“un”, “##want”, “##ed”]\n",
    "2. 只要有 tokenizer.json 文件就能直接用 `from_pretrained` 加载。\n",
    "3. 输出包括：\n",
    "    - ‘input_ids’：顾名思义，是单词在词典中的编码\n",
    "    - ‘token_type_ids’， 区分两个句子的编码\n",
    "    - ’overflowing_tokens’, 当指定最大长度时，溢出的单词\n",
    "    - ‘num_truncated_tokens’, 溢出的token数量\n",
    "    - ‘return_special_tokens_mask’，如果添加特殊token，则这是[0，1]的列表，其中0指定特殊添加的token，而1指定序列标记\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "('DS | Nintendo DS Lite (Crimson Red and Black)',\n 'DS | Nintendo DS Lite (Cobalt Blue and Black)')"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc_str = prod_id.loc[21,'desc']\n",
    "desc_str_2 = prod_id.loc[22,'desc']\n",
    "desc_str,desc_str_2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词结果：\n",
      " ['[CLS]', 'sony', 'eric', '##sson', 's', '##500', '##i', 'unlocked', 'mysterious', 'green', '[SEP]']\n",
      "分词id：\n",
      " [101, 8412, 4388, 7092, 1055, 29345, 2072, 14058, 8075, 2665, 102]\n",
      "词典大小：\n",
      " 30522\n"
     ]
    }
   ],
   "source": [
    "tokenizer_1 = Tokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "output = tokenizer_1.encode(desc_str)\n",
    "\n",
    "print(\"分词结果：\\n\",output.tokens)\n",
    "print(\"分词id：\\n\",output.ids)\n",
    "print(\"词典大小：\\n\",tokenizer_1.get_vocab_size())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. encoding\n",
    "## 3.1 句子相似度: SentenceTransformer\n",
    "1. 可以选的models 见：https://www.sbert.net/docs/pretrained_models.html\n",
    "2. The `all-*` models where trained on all available training data (more than 1 billion training pairs) and are designed as general purpose models.\n",
    "    - all-mpnet-base-v2最好，all-MiniLM-L6-v2比较快（5倍）\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "model_2 = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# util.pytorch_cos_sim(embedding_1, embedding_2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### PCA降维\n",
    "\n",
    "1. https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/distillation/dimensionality_reduction.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "new_dimension = 303\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8203247283076371"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######## Reduce the embedding dimensions ########\n",
    "\n",
    "#We use AllNLI as a source of sentences to compute PCA\n",
    "nli_dataset_path = 'datasets/AllNLI.tsv.gz'\n",
    "\n",
    "#We use the STS benchmark dataset to see how much performance we loose by using the dimensionality reduction\n",
    "sts_dataset_path = 'datasets/stsbenchmark.tsv.gz'\n",
    "\n",
    "if not os.path.exists(nli_dataset_path):\n",
    "    util.http_get('https://sbert.net/datasets/AllNLI.tsv.gz', nli_dataset_path)\n",
    "\n",
    "if not os.path.exists(sts_dataset_path):\n",
    "    util.http_get('https://sbert.net/datasets/stsbenchmark.tsv.gz', sts_dataset_path)\n",
    "\n",
    "# We measure the performance of the original model\n",
    "# and later we will measure the performance with the reduces dimension size\n",
    "logger.info(\"Read STSbenchmark test dataset\")\n",
    "eval_examples = []\n",
    "with gzip.open(sts_dataset_path, 'rt', encoding='utf8') as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        if row['split'] == 'test':\n",
    "            score = float(row['score']) / 5.0 #Normalize score to range 0 ... 1\n",
    "            eval_examples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=score))\n",
    "\n",
    "# Evaluate the original model on the STS benchmark dataset\n",
    "stsb_evaluator = evaluation.EmbeddingSimilarityEvaluator.from_input_examples(eval_examples, name='sts-benchmark-test')\n",
    "\n",
    "logger.info(\"Original model performance:\")\n",
    "stsb_evaluator(model_2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "#Read sentences from NLI dataset\n",
    "nli_sentences = set()\n",
    "with gzip.open(nli_dataset_path, 'rt', encoding='utf8') as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        nli_sentences.add(row['sentence1'])\n",
    "        nli_sentences.add(row['sentence2'])\n",
    "\n",
    "nli_sentences = list(nli_sentences)\n",
    "random.shuffle(nli_sentences)\n",
    "nli_sentences.__len__()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "(303, 384)"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "(303, 384)"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To determine the PCA matrix, we need some example sentence embeddings.\n",
    "#Here, we compute the embeddings for 20k random sentences from the AllNLI dataset\n",
    "pca_train_sentences = nli_sentences[0:20000]\n",
    "train_embeddings = model_2.encode(pca_train_sentences, convert_to_numpy=True)\n",
    "\n",
    "#Compute PCA on the train embeddings matrix\n",
    "pca = PCA(n_components=new_dimension)\n",
    "pca.fit(train_embeddings)\n",
    "pca_comp = np.asarray(pca.components_)\n",
    "pca_comp.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8234394986740291"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a dense layer to the model, so that it will produce directly embeddings with the new size\n",
    "dense = models.Dense(in_features=model_2.get_sentence_embedding_dimension(), out_features=new_dimension, bias=False, activation_function=torch.nn.Identity())\n",
    "dense.linear.weight = torch.nn.Parameter(torch.tensor(pca_comp))\n",
    "model_2.add_module('dense', dense)\n",
    "\n",
    "# Evaluate the model with the reduce embedding size\n",
    "logger.info(\"Model with {} dimensions:\".format(new_dimension))\n",
    "stsb_evaluator(model_2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### model generate and save"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "prod_embedding = model_2.encode(list(prod_id.loc[:,'desc']), convert_to_numpy=True,device=device)\n",
    "assert prod_embedding.shape[0] == prod_id.shape[0], \"Wrong!\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "data": {
      "text/plain": "          0         1         2         3         4         5         6  \\\n0  0.116034  0.020119 -0.021310 -0.075286  0.016879  0.018455 -0.034276   \n1  0.061373  0.032199  0.011518  0.034196 -0.077273  0.059780  0.041093   \n2 -0.051620 -0.072511  0.111702  0.046107 -0.020478  0.150707  0.056321   \n3 -0.000960  0.025139  0.003293  0.072950 -0.010923 -0.078227  0.151397   \n4  0.053038  0.071515  0.017683 -0.006201 -0.022760 -0.008116  0.085413   \n5  0.012295 -0.014781  0.060551 -0.009491  0.094940  0.034042 -0.050308   \n6  0.090535  0.037109  0.043333  0.141303  0.034144  0.002742  0.017636   \n7 -0.011322  0.008502  0.006862 -0.057471  0.035309  0.021279  0.013266   \n8  0.003335  0.086421  0.009364  0.058529 -0.030463 -0.028964 -0.044886   \n9 -0.034880  0.081387  0.151240  0.008757 -0.003367 -0.042134  0.031805   \n\n          7         8         9  ...       295       296       297       298  \\\n0  0.031793 -0.057861 -0.049602  ... -0.077679 -0.027134  0.007310 -0.076596   \n1  0.074851  0.053483  0.116248  ... -0.049425  0.017499  0.006069  0.010160   \n2  0.018349  0.011857 -0.039793  ... -0.043603  0.015063 -0.011231  0.017059   \n3  0.171092 -0.007707  0.072944  ... -0.026399 -0.049374 -0.024551  0.010284   \n4  0.048034  0.087793 -0.058904  ... -0.075914 -0.008973  0.043354  0.005005   \n5 -0.018858 -0.017407  0.004311  ... -0.053434  0.036242  0.020129  0.005436   \n6 -0.103622 -0.102304 -0.016737  ... -0.000215  0.024440 -0.007869  0.026648   \n7 -0.028090  0.056078 -0.001076  ... -0.013573  0.027576  0.004757 -0.026771   \n8 -0.060023 -0.061517 -0.016009  ... -0.045395 -0.008474  0.006412  0.007268   \n9  0.012283 -0.030045 -0.035092  ... -0.006119  0.043008  0.000976 -0.034087   \n\n        299       300       301       302  id  \\\n0 -0.025088  0.018122 -0.032752  0.014568   0   \n1  0.003023  0.031978 -0.038282  0.011311   1   \n2 -0.039363 -0.002873 -0.015840 -0.004983   2   \n3  0.009512  0.006033  0.008652  0.042826   3   \n4  0.021273  0.029586  0.014097  0.034332   4   \n5 -0.030297  0.013453 -0.025003 -0.004230   5   \n6 -0.056101 -0.030093  0.004264 -0.000276   6   \n7 -0.033586 -0.014026  0.036300  0.019918   7   \n8 -0.034747  0.006816 -0.021698 -0.010173   8   \n9 -0.017426  0.016793 -0.006562 -0.004813  11   \n\n                                            desc  \n0  Sony Ericsson S500i Unlocked Mysterious Green  \n1               PSP Slim & Lite Sony Piano Black  \n2     iPod Touch Apple 8GB with Software Upgrade  \n3      Logitech Cordless Wave Keyboard and Mouse  \n4   Apple Macbook Air 1.6GHz Core 2 Duo Notebook  \n5                     SanDisk Cruzer Contour 4GB  \n6           Mario Kart with Wheel (Nintendo Wii)  \n7      PS3 | Playstation 3 Sony Console 40GB HDD  \n8                    DS | Nintendo DS Lite White  \n9            Corsair Voyager Mini 4 GB USB Flash  \n\n[10 rows x 305 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>295</th>\n      <th>296</th>\n      <th>297</th>\n      <th>298</th>\n      <th>299</th>\n      <th>300</th>\n      <th>301</th>\n      <th>302</th>\n      <th>id</th>\n      <th>desc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.116034</td>\n      <td>0.020119</td>\n      <td>-0.021310</td>\n      <td>-0.075286</td>\n      <td>0.016879</td>\n      <td>0.018455</td>\n      <td>-0.034276</td>\n      <td>0.031793</td>\n      <td>-0.057861</td>\n      <td>-0.049602</td>\n      <td>...</td>\n      <td>-0.077679</td>\n      <td>-0.027134</td>\n      <td>0.007310</td>\n      <td>-0.076596</td>\n      <td>-0.025088</td>\n      <td>0.018122</td>\n      <td>-0.032752</td>\n      <td>0.014568</td>\n      <td>0</td>\n      <td>Sony Ericsson S500i Unlocked Mysterious Green</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.061373</td>\n      <td>0.032199</td>\n      <td>0.011518</td>\n      <td>0.034196</td>\n      <td>-0.077273</td>\n      <td>0.059780</td>\n      <td>0.041093</td>\n      <td>0.074851</td>\n      <td>0.053483</td>\n      <td>0.116248</td>\n      <td>...</td>\n      <td>-0.049425</td>\n      <td>0.017499</td>\n      <td>0.006069</td>\n      <td>0.010160</td>\n      <td>0.003023</td>\n      <td>0.031978</td>\n      <td>-0.038282</td>\n      <td>0.011311</td>\n      <td>1</td>\n      <td>PSP Slim &amp; Lite Sony Piano Black</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.051620</td>\n      <td>-0.072511</td>\n      <td>0.111702</td>\n      <td>0.046107</td>\n      <td>-0.020478</td>\n      <td>0.150707</td>\n      <td>0.056321</td>\n      <td>0.018349</td>\n      <td>0.011857</td>\n      <td>-0.039793</td>\n      <td>...</td>\n      <td>-0.043603</td>\n      <td>0.015063</td>\n      <td>-0.011231</td>\n      <td>0.017059</td>\n      <td>-0.039363</td>\n      <td>-0.002873</td>\n      <td>-0.015840</td>\n      <td>-0.004983</td>\n      <td>2</td>\n      <td>iPod Touch Apple 8GB with Software Upgrade</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.000960</td>\n      <td>0.025139</td>\n      <td>0.003293</td>\n      <td>0.072950</td>\n      <td>-0.010923</td>\n      <td>-0.078227</td>\n      <td>0.151397</td>\n      <td>0.171092</td>\n      <td>-0.007707</td>\n      <td>0.072944</td>\n      <td>...</td>\n      <td>-0.026399</td>\n      <td>-0.049374</td>\n      <td>-0.024551</td>\n      <td>0.010284</td>\n      <td>0.009512</td>\n      <td>0.006033</td>\n      <td>0.008652</td>\n      <td>0.042826</td>\n      <td>3</td>\n      <td>Logitech Cordless Wave Keyboard and Mouse</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.053038</td>\n      <td>0.071515</td>\n      <td>0.017683</td>\n      <td>-0.006201</td>\n      <td>-0.022760</td>\n      <td>-0.008116</td>\n      <td>0.085413</td>\n      <td>0.048034</td>\n      <td>0.087793</td>\n      <td>-0.058904</td>\n      <td>...</td>\n      <td>-0.075914</td>\n      <td>-0.008973</td>\n      <td>0.043354</td>\n      <td>0.005005</td>\n      <td>0.021273</td>\n      <td>0.029586</td>\n      <td>0.014097</td>\n      <td>0.034332</td>\n      <td>4</td>\n      <td>Apple Macbook Air 1.6GHz Core 2 Duo Notebook</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.012295</td>\n      <td>-0.014781</td>\n      <td>0.060551</td>\n      <td>-0.009491</td>\n      <td>0.094940</td>\n      <td>0.034042</td>\n      <td>-0.050308</td>\n      <td>-0.018858</td>\n      <td>-0.017407</td>\n      <td>0.004311</td>\n      <td>...</td>\n      <td>-0.053434</td>\n      <td>0.036242</td>\n      <td>0.020129</td>\n      <td>0.005436</td>\n      <td>-0.030297</td>\n      <td>0.013453</td>\n      <td>-0.025003</td>\n      <td>-0.004230</td>\n      <td>5</td>\n      <td>SanDisk Cruzer Contour 4GB</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.090535</td>\n      <td>0.037109</td>\n      <td>0.043333</td>\n      <td>0.141303</td>\n      <td>0.034144</td>\n      <td>0.002742</td>\n      <td>0.017636</td>\n      <td>-0.103622</td>\n      <td>-0.102304</td>\n      <td>-0.016737</td>\n      <td>...</td>\n      <td>-0.000215</td>\n      <td>0.024440</td>\n      <td>-0.007869</td>\n      <td>0.026648</td>\n      <td>-0.056101</td>\n      <td>-0.030093</td>\n      <td>0.004264</td>\n      <td>-0.000276</td>\n      <td>6</td>\n      <td>Mario Kart with Wheel (Nintendo Wii)</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>-0.011322</td>\n      <td>0.008502</td>\n      <td>0.006862</td>\n      <td>-0.057471</td>\n      <td>0.035309</td>\n      <td>0.021279</td>\n      <td>0.013266</td>\n      <td>-0.028090</td>\n      <td>0.056078</td>\n      <td>-0.001076</td>\n      <td>...</td>\n      <td>-0.013573</td>\n      <td>0.027576</td>\n      <td>0.004757</td>\n      <td>-0.026771</td>\n      <td>-0.033586</td>\n      <td>-0.014026</td>\n      <td>0.036300</td>\n      <td>0.019918</td>\n      <td>7</td>\n      <td>PS3 | Playstation 3 Sony Console 40GB HDD</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.003335</td>\n      <td>0.086421</td>\n      <td>0.009364</td>\n      <td>0.058529</td>\n      <td>-0.030463</td>\n      <td>-0.028964</td>\n      <td>-0.044886</td>\n      <td>-0.060023</td>\n      <td>-0.061517</td>\n      <td>-0.016009</td>\n      <td>...</td>\n      <td>-0.045395</td>\n      <td>-0.008474</td>\n      <td>0.006412</td>\n      <td>0.007268</td>\n      <td>-0.034747</td>\n      <td>0.006816</td>\n      <td>-0.021698</td>\n      <td>-0.010173</td>\n      <td>8</td>\n      <td>DS | Nintendo DS Lite White</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>-0.034880</td>\n      <td>0.081387</td>\n      <td>0.151240</td>\n      <td>0.008757</td>\n      <td>-0.003367</td>\n      <td>-0.042134</td>\n      <td>0.031805</td>\n      <td>0.012283</td>\n      <td>-0.030045</td>\n      <td>-0.035092</td>\n      <td>...</td>\n      <td>-0.006119</td>\n      <td>0.043008</td>\n      <td>0.000976</td>\n      <td>-0.034087</td>\n      <td>-0.017426</td>\n      <td>0.016793</td>\n      <td>-0.006562</td>\n      <td>-0.004813</td>\n      <td>11</td>\n      <td>Corsair Voyager Mini 4 GB USB Flash</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 305 columns</p>\n</div>"
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod_embedding_df = pd.DataFrame(prod_embedding)\n",
    "prod_embedding_df['id'] = prod_id['id']\n",
    "prod_embedding_df['desc'] = prod_id['desc']\n",
    "\n",
    "prod_embedding_df.to_csv(prod_embedding_output_path,header=True,index=False,encoding=\"utf-8\")\n",
    "prod_embedding_df.head(10),prod_embedding_df.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### clustering\n",
    "1. 先用tsne降维"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "LENTH=300      # 望得到的encoding维度=300 or 4? 取决于用途\n",
    "prod_embedding_small_path = \"../data/small_prod_embedding_\"+str(LENTH)+\".csv\"\n",
    "prod_embedding_large_path = \"E:\\DATA\\large_dta\\large_prod_embedding_\"+str(LENTH)+\".csv\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "prod_embedding_small = pd.read_csv(prod_embedding_small_path,encoding=\"utf-8\")\n",
    "prod_embedding_large = pd.read_csv(prod_embedding_large_path,encoding=\"utf-8\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "num_clusters = 4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "(slice(None, None, None), slice(None, 100, None))",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32mD:\\Anaconda\\Lib\\site-packages\\pandas\\core\\indexes\\base.py\u001B[0m in \u001B[0;36mget_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3628\u001B[0m             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3629\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engine\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcasted_key\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3630\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\Lib\\site-packages\\pandas\\_libs\\index.pyx\u001B[0m in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\Lib\\site-packages\\pandas\\_libs\\index.pyx\u001B[0m in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: '(slice(None, None, None), slice(None, 100, None))' is an invalid key",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mInvalidIndexError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_17208\\1568065722.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mprod_embedding_path\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"../data/prod_embedding_100.csv\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0mprod_embedding_100\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprod_embedding_path\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mencoding\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"utf-8\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[0mprod_embedding\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mprod_embedding_100\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;36m100\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[0mnum_clusters\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m4\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\Lib\\site-packages\\pandas\\core\\frame.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3503\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnlevels\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3504\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_getitem_multilevel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3505\u001B[1;33m             \u001B[0mindexer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3506\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mis_integer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindexer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3507\u001B[0m                 \u001B[0mindexer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mindexer\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\Lib\\site-packages\\pandas\\core\\indexes\\base.py\u001B[0m in \u001B[0;36mget_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3634\u001B[0m                 \u001B[1;31m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3635\u001B[0m                 \u001B[1;31m#  the TypeError.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3636\u001B[1;33m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_check_indexing_error\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3637\u001B[0m                 \u001B[1;32mraise\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3638\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\Lib\\site-packages\\pandas\\core\\indexes\\base.py\u001B[0m in \u001B[0;36m_check_indexing_error\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   5649\u001B[0m             \u001B[1;31m# if key is not a scalar, directly raise an error (the code below\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   5650\u001B[0m             \u001B[1;31m# would convert to numpy arrays and raise later any way) - GH29926\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 5651\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mInvalidIndexError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   5652\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   5653\u001B[0m     \u001B[1;33m@\u001B[0m\u001B[0mcache_readonly\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mInvalidIndexError\u001B[0m: (slice(None, None, None), slice(None, 100, None))"
     ]
    }
   ],
   "source": [
    "# prod_embedding_path = \"../data/prod_embedding_100.csv\"\n",
    "# prod_embedding_100 = pd.read_csv(prod_embedding_path,encoding=\"utf-8\")\n",
    "# prod_embedding = prod_embedding_100[:,:100]\n",
    "\n",
    "# perplexity: 默认为30，数据集越大，需要参数值越大，建议值位5-50 , n_components=2 默认为2，嵌入空间的维度（嵌入空间的意思就是结果空间）\n",
    "tsne_model = TSNE(perplexity=10, n_components=2, init='pca', n_iter=250, random_state=23)\n",
    "new_values = tsne_model.fit_transform(prod_embedding)  # 新坐标\n",
    "x = []\n",
    "y = []\n",
    "for value in new_values:\n",
    "    x.append(value[0])\n",
    "    y.append(value[1])\n",
    "\n",
    "\n",
    "clustering_model = KMeans(n_clusters=num_clusters)\n",
    "clustering_model.fit(prod_embedding)\n",
    "cluster_assignment = clustering_model.labels_   # cluster 归属\n",
    "\n",
    "# 长度为num_clusters的list，每个元素也是一个list\n",
    "clustered_sentences = [[] for i in range(num_clusters)]\n",
    "\n",
    "# cluster_assignment是array\n",
    "# enumerate可以同时获得索引和值\n",
    "# clustered_sentences记录了num_clusters个cluster里面的desc\n",
    "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "    clustered_sentences[cluster_id].append( list(prod_id.loc[:,'desc'])[sentence_id] )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "# np.save(\"new_value\",new_values)\n",
    "np.save(\"cluster_assignment\",clustering_model.labels_)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "data": {
      "text/plain": "'window_3b4d30aea14cdc'"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viz = Visdom()\n",
    "viz.scatter(X=new_values, Y=cluster_assignment+1,\n",
    "        opts=dict(\n",
    "            colormap='Jet',\n",
    "            markersymbol=\"dot\",  # \"x\"\n",
    "            markersize=10,\n",
    "            legend = ['0','1','2','3','4'],\n",
    "            textlabels= list(prod_id.loc[:,'desc'])\n",
    "        ))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "data": {
      "text/plain": "'[\"main\"]'"
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viz.save([\"main\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 使用bert的encoder\n",
    "1. 直接把分词后的`id`当做input，然后得到输出。\n",
    "2."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "BertModel(\n  (embeddings): BertEmbeddings(\n    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (token_type_embeddings): Embedding(2, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): BertEncoder(\n    (layer): ModuleList(\n      (0): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval() # 将模型设为验证模式\n",
    "input_ids = torch.tensor([output.ids]) # 添加batch维度并转化为tensor\n",
    "# token_type_ids = torch.tensor([output['token_type_ids']])\n",
    "\n",
    "tokens_tensor = input_ids.to(device)\n",
    "# segments_tensors = token_type_ids.to(device)\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bert的输出：\n",
    "    - sequence_output：输出序列,大小为`(1,11,768)`其中11是因为`desc`长度为11\n",
    "    - pooler_output,对输出序列进行pool操作的结果\n",
    "    - (hidden_states),隐藏层状态(包括Embedding层)，取决于modelconfig中output_hidden_states\n",
    "    - (attentions), 注意力层，取决于参数中output_attentions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.4594, -0.2017, -0.0968,  ..., -0.2735,  0.3161,  0.3787],\n",
      "         [-0.3372,  0.0911,  0.1787,  ...,  0.0088,  1.0508, -0.3864],\n",
      "         [-0.0296,  0.5422,  0.9316,  ...,  0.4370,  0.8096, -0.7587],\n",
      "         ...,\n",
      "         [-0.1785, -0.2206,  0.1787,  ..., -0.0660, -0.0233,  0.2102],\n",
      "         [-0.3025, -0.4273,  0.1322,  ...,  0.0447,  0.2715, -0.3983],\n",
      "         [ 0.7027, -0.0926, -0.2899,  ...,  0.1964, -0.6339, -0.2348]]],\n",
      "       device='cuda:0'), pooler_output=tensor([[-7.8197e-01, -3.8046e-01, -4.4998e-01,  7.0813e-01,  2.3026e-01,\n",
      "         -1.4321e-01,  7.7712e-01,  3.1890e-01, -1.0563e-01, -9.9996e-01,\n",
      "          9.4048e-02,  4.0133e-01,  9.4646e-01,  1.0262e-01,  8.2900e-01,\n",
      "         -3.7846e-01,  1.5961e-01, -4.5966e-01,  3.4073e-01, -3.7634e-01,\n",
      "          6.4000e-01,  9.9932e-01,  2.1653e-01,  2.0881e-01,  4.3850e-01,\n",
      "          6.2764e-01, -4.1283e-01,  8.4835e-01,  9.3581e-01,  7.0680e-01,\n",
      "         -4.8501e-01,  2.5973e-01, -9.7900e-01, -3.0109e-01, -7.5960e-01,\n",
      "         -9.8199e-01,  3.3668e-01, -6.1143e-01, -2.7709e-02, -3.7268e-03,\n",
      "         -8.2798e-01,  3.3813e-01,  9.9975e-01, -3.2959e-03,  3.8334e-01,\n",
      "         -1.8382e-01, -9.9998e-01,  2.3796e-01, -8.0581e-01,  3.6930e-01,\n",
      "         -6.1730e-02,  3.5255e-01,  1.8665e-01,  3.3083e-01,  3.6171e-01,\n",
      "          1.5901e-01,  1.0582e-02,  1.8568e-01, -2.4317e-01, -5.3865e-01,\n",
      "         -5.5671e-01,  3.2399e-01, -4.3667e-01, -8.5920e-01,  1.2795e-01,\n",
      "          1.8920e-01, -1.9110e-01, -2.9440e-01, -5.2140e-02, -4.6873e-02,\n",
      "          7.2705e-01,  1.6541e-01,  2.6213e-01, -7.3556e-01, -1.4639e-01,\n",
      "          3.0823e-01, -6.4388e-01,  1.0000e+00, -3.5940e-01, -9.6130e-01,\n",
      "          3.5496e-01,  1.8954e-01,  5.7087e-01,  1.7109e-01,  1.4959e-01,\n",
      "         -1.0000e+00,  3.3448e-01, -1.7109e-01, -9.7395e-01,  2.9573e-01,\n",
      "          4.4766e-01, -2.6568e-02, -1.2925e-01,  5.4714e-01, -3.4085e-01,\n",
      "         -3.5587e-01, -3.1325e-01, -3.6365e-01, -3.5361e-01, -1.9740e-01,\n",
      "          2.4326e-01, -2.9800e-01, -9.7704e-02, -2.5533e-01,  2.7840e-01,\n",
      "         -4.4848e-01, -3.9217e-01,  4.3499e-01, -1.9868e-01,  5.9724e-01,\n",
      "          4.9771e-01, -4.3484e-01,  2.8446e-01, -9.0113e-01,  5.7038e-01,\n",
      "         -3.2775e-01, -9.7119e-01, -6.1035e-01, -9.8107e-01,  7.0181e-01,\n",
      "         -9.2713e-02, -3.0889e-01,  9.0379e-01,  2.2440e-01,  2.1944e-01,\n",
      "          1.7196e-02, -3.3635e-01, -1.0000e+00, -2.0940e-01, -2.6604e-01,\n",
      "         -8.8515e-02, -2.2720e-01, -9.5811e-01, -9.3257e-01,  6.1060e-01,\n",
      "          9.0446e-01,  1.8442e-01,  9.9930e-01, -3.7363e-01,  9.0569e-01,\n",
      "          3.8336e-04, -3.8424e-01, -3.4191e-02, -3.9177e-01,  5.4594e-01,\n",
      "          4.9610e-02, -5.0973e-01,  1.7740e-01, -2.2952e-01, -6.5278e-02,\n",
      "         -4.6184e-01, -2.2931e-01, -3.2288e-02, -8.0074e-01, -3.1637e-01,\n",
      "          8.6585e-01, -1.7211e-02, -4.2691e-01,  3.5079e-01, -1.9237e-01,\n",
      "         -3.5621e-01,  8.5017e-01,  2.3292e-01,  3.0025e-01, -3.2909e-02,\n",
      "          5.1866e-01, -3.7323e-02,  4.3555e-01, -7.6277e-01,  9.4210e-02,\n",
      "          3.4720e-01, -2.8495e-01, -5.3883e-01, -9.5478e-01, -2.8536e-01,\n",
      "          4.1308e-01,  9.7167e-01,  7.1283e-01,  2.8919e-01,  1.3543e-01,\n",
      "         -3.2490e-01,  1.8003e-01, -9.5308e-01,  9.5697e-01, -1.4499e-01,\n",
      "          2.2876e-01,  1.1940e-01,  2.7641e-01, -7.3679e-01, -2.2074e-01,\n",
      "          7.0372e-01, -2.8502e-01, -6.4245e-01, -1.1961e-01, -4.5626e-01,\n",
      "         -4.5635e-01, -3.1674e-01,  4.9823e-01, -1.7354e-01, -4.1250e-01,\n",
      "         -6.7986e-02,  8.4074e-01,  9.0758e-01,  5.7097e-01, -2.1675e-01,\n",
      "          4.4999e-01, -8.1540e-01, -3.1844e-01,  7.3833e-02,  3.7810e-03,\n",
      "          2.5925e-01,  9.8641e-01, -2.6089e-01, -1.3214e-01, -8.4826e-01,\n",
      "         -9.6713e-01, -3.9811e-04, -8.2215e-01, -5.2020e-02, -7.2475e-01,\n",
      "          3.9845e-01,  2.7697e-02, -1.7342e-01,  3.0613e-01, -9.4895e-01,\n",
      "         -7.4671e-01,  2.5765e-01, -4.0260e-01,  4.0356e-01, -1.5663e-01,\n",
      "          8.0784e-01,  6.2728e-01, -6.5361e-01,  2.3986e-01,  9.1624e-01,\n",
      "         -2.5541e-01, -6.8702e-01,  7.0681e-01, -2.0063e-01,  8.2782e-01,\n",
      "         -5.9363e-01,  9.7790e-01,  7.2987e-01,  1.5213e-01, -7.9828e-01,\n",
      "         -3.1346e-01, -7.5280e-01, -1.3149e-01, -2.3941e-02, -2.7787e-01,\n",
      "          2.6582e-01,  5.7655e-01,  2.9761e-01,  4.8585e-01, -4.5098e-01,\n",
      "          9.7902e-01, -8.0935e-01, -9.2299e-01, -5.8276e-01, -1.8904e-01,\n",
      "         -9.7524e-01,  1.4498e-01,  2.5049e-01,  3.1068e-01, -4.8709e-01,\n",
      "         -3.9397e-01, -9.1711e-01,  7.4920e-01,  6.8751e-02,  9.7199e-01,\n",
      "          6.5567e-02, -8.0759e-01, -3.4005e-01, -8.7954e-01, -4.2504e-02,\n",
      "         -1.1304e-01,  2.7107e-01, -1.6855e-01, -9.0076e-01,  4.4388e-01,\n",
      "          3.7458e-01,  3.7799e-01, -2.1097e-01,  9.8923e-01,  9.9999e-01,\n",
      "          9.4925e-01,  7.8986e-01,  7.3141e-01, -9.9645e-01, -6.6163e-01,\n",
      "          9.9998e-01, -8.3336e-01, -1.0000e+00, -8.4555e-01, -5.8495e-01,\n",
      "          2.8988e-01, -1.0000e+00, -2.4866e-02,  5.0911e-02, -8.1330e-01,\n",
      "         -3.9433e-02,  9.6274e-01,  9.3729e-01, -1.0000e+00,  7.4886e-01,\n",
      "          8.8210e-01, -7.2963e-01,  4.8072e-01, -4.0289e-01,  9.2193e-01,\n",
      "          2.5918e-01,  4.5992e-01, -2.3033e-01,  3.4297e-01, -6.3496e-01,\n",
      "         -7.9600e-01,  9.9291e-03, -2.2250e-02,  9.7167e-01,  1.5963e-01,\n",
      "         -7.9383e-01, -8.1899e-01,  3.1446e-01, -1.0767e-01, -1.7407e-01,\n",
      "         -9.0594e-01, -2.8214e-01, -1.0979e-01,  5.5013e-01,  1.4702e-01,\n",
      "          2.5268e-01, -6.3600e-01,  2.2643e-01, -2.5962e-01,  4.9079e-02,\n",
      "          6.6481e-01, -8.8866e-01, -4.0324e-01, -2.4461e-01, -3.0751e-01,\n",
      "         -1.1911e-01, -9.2954e-01,  9.4104e-01, -5.0556e-01,  4.4943e-01,\n",
      "          1.0000e+00,  5.1536e-01, -7.9523e-01,  3.3935e-01,  2.0892e-01,\n",
      "         -1.7428e-01,  1.0000e+00,  6.0461e-01, -9.6860e-01, -6.3611e-01,\n",
      "          5.0878e-01, -4.7727e-01, -5.1140e-01,  9.9868e-01, -2.6447e-01,\n",
      "         -2.1123e-02,  5.1110e-01,  9.6476e-01, -9.8562e-01,  9.4271e-01,\n",
      "         -8.3593e-01, -9.2860e-01,  9.1951e-01,  8.5940e-01, -1.0025e-01,\n",
      "         -6.1524e-01,  1.1107e-01, -4.5101e-01,  3.3126e-01, -8.4981e-01,\n",
      "          5.9705e-01,  3.5178e-01, -1.8928e-01,  8.2014e-01, -7.4152e-01,\n",
      "         -5.8932e-01,  3.5041e-01, -3.3087e-01,  1.7674e-01,  6.4439e-01,\n",
      "          5.4334e-01, -2.7107e-01,  6.7964e-03, -3.0711e-01, -5.0727e-01,\n",
      "         -9.1962e-01,  1.7126e-01,  1.0000e+00, -7.8415e-03,  3.8000e-01,\n",
      "         -7.7224e-02, -1.5873e-01, -3.3974e-02,  3.1473e-01,  3.5010e-01,\n",
      "         -2.9617e-01, -7.7152e-01,  6.0488e-02, -7.0828e-01, -9.7543e-01,\n",
      "          5.3105e-01,  2.0708e-01, -3.0917e-01,  9.9976e-01,  1.2621e-01,\n",
      "          3.7341e-01, -7.0652e-02,  6.6622e-01,  5.1725e-02,  3.6669e-01,\n",
      "          8.6541e-02,  9.5173e-01, -2.1904e-01,  6.7276e-01,  6.8342e-01,\n",
      "         -1.8652e-01, -2.6336e-01, -5.6965e-01,  1.3421e-01, -9.0775e-01,\n",
      "         -1.1663e-01, -9.1779e-01,  9.3186e-01,  6.9447e-01,  4.3323e-01,\n",
      "          2.8149e-01,  3.5105e-01,  1.0000e+00, -6.7053e-01,  4.8448e-01,\n",
      "         -6.3625e-02,  6.0814e-01, -9.9408e-01, -4.8246e-01, -3.9940e-01,\n",
      "         -1.3247e-02, -1.8665e-01, -2.1281e-01,  2.4372e-01, -9.3213e-01,\n",
      "          5.1771e-02,  4.2058e-01, -8.7162e-01, -9.7479e-01,  1.2871e-01,\n",
      "          5.7036e-01,  1.0316e-01, -9.1020e-01, -5.9648e-01, -5.1608e-01,\n",
      "         -4.1965e-03, -2.1313e-01, -8.6281e-01,  2.3216e-01, -1.9846e-01,\n",
      "          5.4248e-01, -2.6390e-01,  6.2197e-01,  3.1104e-01,  7.5961e-01,\n",
      "         -4.2030e-01,  9.2332e-02, -1.7885e-01, -8.0919e-01,  5.8131e-01,\n",
      "         -7.2746e-01, -4.8350e-01, -8.3283e-02,  1.0000e+00, -3.1355e-01,\n",
      "          3.5801e-02,  5.8137e-01,  5.6138e-01, -2.6701e-01,  2.8581e-01,\n",
      "          7.6240e-01,  2.9337e-01, -1.7033e-01,  6.3693e-02, -2.4736e-01,\n",
      "         -4.1517e-01,  4.6583e-01, -1.4638e-01,  2.0561e-01,  6.5572e-01,\n",
      "          5.4623e-01,  2.2197e-01, -8.3951e-02,  1.0656e-01,  9.9251e-01,\n",
      "         -2.8099e-01, -9.8017e-02, -4.8219e-01, -2.0395e-01, -3.8921e-01,\n",
      "         -2.1027e-01,  1.0000e+00,  3.1200e-01,  5.6281e-02, -9.8324e-01,\n",
      "         -1.1895e-01, -8.0060e-01,  9.9997e-01,  7.7179e-01, -7.0284e-01,\n",
      "          6.1574e-01,  4.7780e-01, -1.5522e-01,  3.2233e-01, -1.7998e-01,\n",
      "         -1.6987e-01,  2.3033e-01,  2.0524e-01,  8.9394e-01, -3.5584e-01,\n",
      "         -9.3914e-01, -5.2182e-01,  3.0314e-01, -8.6989e-01,  9.9741e-01,\n",
      "         -5.1020e-01, -1.1693e-01, -2.9396e-01, -1.1076e-01,  2.4763e-01,\n",
      "          1.0528e-01, -9.4327e-01, -2.9759e-01,  7.6443e-02,  9.0992e-01,\n",
      "          1.9596e-01, -6.4310e-01, -8.6959e-01,  4.4695e-01,  2.0191e-01,\n",
      "         -1.6359e-01, -8.9597e-01,  9.3293e-01, -9.4851e-01,  3.6273e-01,\n",
      "          1.0000e+00,  3.5279e-01, -3.3564e-01,  8.9708e-02, -3.4487e-01,\n",
      "          2.9959e-01, -1.5020e-01,  4.1102e-01, -9.0102e-01, -1.6636e-01,\n",
      "         -2.4640e-01,  3.9919e-01, -1.6423e-01, -5.3846e-01,  5.4574e-01,\n",
      "          2.0050e-01, -5.7221e-01, -6.2287e-01,  3.8392e-03,  3.4642e-01,\n",
      "          7.2583e-01, -3.9414e-01, -1.2365e-01,  1.5474e-01, -1.1860e-01,\n",
      "         -8.2439e-01, -1.5187e-01, -2.8876e-01, -9.9967e-01,  6.4988e-01,\n",
      "         -1.0000e+00,  2.6699e-01, -2.0660e-01, -2.1092e-01,  6.7963e-01,\n",
      "          4.0802e-01,  1.8402e-01, -5.4020e-01, -2.4337e-01,  6.6390e-01,\n",
      "          6.4754e-01, -2.3543e-01, -8.2334e-02, -5.3939e-01,  1.8656e-01,\n",
      "          7.5977e-03,  2.3439e-01, -3.9060e-01,  6.3922e-01, -1.1468e-01,\n",
      "          1.0000e+00,  6.1142e-02, -2.9414e-01, -9.2441e-01,  3.1449e-01,\n",
      "         -2.9037e-01,  9.9999e-01, -6.6022e-01, -9.3375e-01,  3.6666e-01,\n",
      "         -6.2131e-01, -7.9190e-01,  3.1855e-01,  3.8869e-03, -6.9532e-01,\n",
      "         -5.2366e-01,  8.2357e-01,  6.0592e-01, -5.6998e-01,  3.7354e-01,\n",
      "         -3.3947e-01, -4.5216e-01,  2.2210e-01,  4.1690e-01,  9.7257e-01,\n",
      "          3.8685e-01,  7.8174e-01, -2.4316e-01, -4.0003e-02,  9.1636e-01,\n",
      "          3.7522e-01,  2.3303e-01,  2.3207e-01,  1.0000e+00,  3.8342e-01,\n",
      "         -8.9542e-01,  1.4437e-01, -9.3943e-01, -3.0455e-01, -8.9569e-01,\n",
      "          2.2897e-01,  2.4344e-01,  8.5072e-01, -1.1185e-01,  8.9899e-01,\n",
      "         -6.0742e-02,  8.4629e-02,  8.6321e-02, -5.5627e-02,  3.9973e-01,\n",
      "         -8.8395e-01, -9.6706e-01, -9.7041e-01,  4.4060e-01, -3.5619e-01,\n",
      "         -1.4838e-01,  3.5052e-01,  2.8556e-01,  3.3370e-01,  3.9427e-01,\n",
      "         -1.0000e+00,  9.0720e-01,  3.2253e-01,  2.7344e-01,  9.4329e-01,\n",
      "          4.6376e-01,  4.0308e-01,  3.5297e-01, -9.6796e-01, -8.1368e-01,\n",
      "         -3.0618e-01, -2.2704e-01,  7.8742e-01,  7.0435e-01,  7.3239e-01,\n",
      "          3.1799e-01, -4.6970e-01, -3.7943e-01,  6.0121e-02, -7.1038e-01,\n",
      "         -9.8318e-01,  4.3608e-01,  5.9405e-02, -9.0149e-01,  9.4973e-01,\n",
      "         -3.4714e-01, -1.7108e-01,  1.9208e-01, -4.8804e-01,  8.4681e-01,\n",
      "          7.3009e-01,  5.7059e-01,  3.8783e-02,  5.2998e-01,  8.1547e-01,\n",
      "          9.0040e-01,  9.6950e-01, -4.8827e-01,  5.5980e-01, -2.0126e-01,\n",
      "          3.5890e-01,  7.6645e-01, -9.2126e-01,  1.9883e-01,  2.5969e-01,\n",
      "         -4.2861e-01,  3.3764e-01, -2.6329e-01, -8.9375e-01,  5.6364e-01,\n",
      "         -3.6954e-01,  4.1469e-01, -3.9184e-01,  3.3620e-02, -4.0980e-01,\n",
      "         -3.2261e-01, -6.9106e-01, -5.9527e-01,  6.9803e-01, -8.4093e-02,\n",
      "          8.5874e-01,  5.2123e-01, -1.4157e-01, -5.0782e-01, -1.9387e-01,\n",
      "         -1.1042e-01, -8.7762e-01,  6.1774e-01, -8.9236e-02,  6.7835e-02,\n",
      "          1.5023e-01, -7.9077e-02,  7.8476e-01, -2.2737e-01, -3.1432e-01,\n",
      "         -2.8923e-01, -6.9565e-01,  7.0413e-01, -4.0469e-01, -4.5042e-01,\n",
      "         -5.2964e-01,  7.0183e-01,  2.7695e-01,  9.9953e-01, -1.7686e-01,\n",
      "         -2.3242e-01, -1.1111e-01, -3.0451e-01,  3.2108e-01, -3.3228e-01,\n",
      "         -1.0000e+00,  3.2046e-01, -3.0636e-01,  3.1098e-01, -5.5715e-02,\n",
      "          4.7212e-01, -1.3289e-01, -9.2625e-01, -2.2466e-01,  9.0366e-02,\n",
      "          2.3764e-01, -3.9679e-01, -4.7791e-01,  6.2784e-01, -1.3738e-01,\n",
      "          6.3795e-01,  8.4325e-01,  4.2161e-01,  4.7932e-01,  6.9914e-01,\n",
      "         -3.2041e-01, -5.8808e-01,  8.3933e-01]], device='cuda:0'), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": "'pooler_output'"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 进行编码\n",
    "\n",
    "with torch.no_grad():\n",
    "    # See the models docstrings for the detail of the inputs\n",
    "    # , token_type_ids=segments_tensors\n",
    "    sequence_output= model(tokens_tensor)\n",
    "    # Transformers models always output tuples.\n",
    "    # See the models docstrings for the detail of all the outputs\n",
    "    # In our case, the first element is the hidden state of the last layer of the Bert model\n",
    "    print(sequence_output)\n",
    "    sequence_output\n",
    "# 得到最终的编码结果encoded_layers\n",
    "pooled_output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
