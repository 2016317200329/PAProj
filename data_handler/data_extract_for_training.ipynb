{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2022/11/25 14:22\n",
    "# @Author  : Wang Yujia\n",
    "# @File    : data_extract_for_training.ipynb\n",
    "# @Description : 为了training data分别提取csv并保存"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Preparations\n",
    "## 1.1 全局设置"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# 人工数据集or真实数据集\n",
    "ARTIFICIAL = True\n",
    "# 是否使用GT3\n",
    "W_GT3 = True\n",
    "# seed = [3,31,62,204,223,407,508,626], [4,31,35,204,407,66,508]\n",
    "seed=35\n",
    "noise_pct = 0.05                # 噪音占比:我们希望生成的data总体上最多浮动的百分比noise_pct\n",
    "\n",
    "# Small dataset\n",
    "settings_small_NN_path = r\"../data/small_settings_NN.csv\"\n",
    "prod_embedding_small_path = \"../data/small_prod_embedding_300.csv\"\n",
    "\n",
    "# Large data\n",
    "settings_large_NN_path = r'E:\\DATA\\large_dta\\large_settings_NN.csv'\n",
    "prod_embedding_large_path = \"E:\\DATA\\large_dta\\large_prod_embedding_300.csv\"\n",
    "\n",
    "if ARTIFICIAL:\n",
    "    GT_1_path = r\"../data/info_asymm/results/GT_1_artificial_LEN=300.csv\"\n",
    "    GT_2_path = r\"../data/SA_PT/results/GT_2_artificial_SA_LEN=300_noise=\"+str(noise_pct) +\"_seed=\"+str(seed)+ \".csv\"\n",
    "    GT_3_path = r\"../data/SA_PT/results/GT_3_artificial_SA_LEN=300_noise=\"+str(noise_pct) +\"_seed=\"+str(seed)+ \".csv\"\n",
    "else:\n",
    "    GT_1_path = r\"../data/info_asymm/results/GT_1_LEN=300.csv\"\n",
    "    GT_2_path = r\"../data/SA_PT/results/GT_2_NN_LEN=300_seed=\"+str(seed)+ \".csv\"\n",
    "    GT_3_path = r\"../data/SA_PT/results/GT_3_NN_LEN=300_seed=\"+str(seed)+ \".csv\"\n",
    "\n",
    "# output paths\n",
    "if ARTIFICIAL:\n",
    "    if not W_GT3:\n",
    "        train_root_path= \"../data/artificial_train_v2_\"+\"noise=\"+str(noise_pct)+\"_seed=\"+str(seed)+\"/\"\n",
    "    else:\n",
    "        train_root_path= \"../data/artificial_train_v3_\"+\"noise=\"+str(noise_pct)+\"_seed=\"+str(seed)+\"/\"\n",
    "else:\n",
    "    if not W_GT3:\n",
    "        train_root_path= \"../data/train_300_uniq_all_seed=\"+str(seed)+\"/\"\n",
    "    else:\n",
    "        train_root_path= \"../data/train_300_uniq_all_v3_seed=\"+str(seed)+\"/\"\n",
    "\n",
    "train_file_head = \"train_data_NP_\"\n",
    "train_file_tail= \".csv\"\n",
    "\n",
    "unique_setting_NN = ['desc','bidincrement','bidfee','retail','flg_endprice']\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.2 data读取与合并"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# 检查目录是否存在，如果不存在则创建\n",
    "if not os.path.exists(train_root_path):\n",
    "    os.makedirs(train_root_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小数据集有 *1196* 场auction，大数据集有 *80* 场auction，一共*1276*\n"
     ]
    }
   ],
   "source": [
    "GT_1 = pd.read_csv(GT_1_path,encoding=\"utf-8\")\n",
    "\n",
    "GT_2 = pd.read_csv(GT_2_path,encoding=\"utf-8\")\n",
    "\n",
    "if W_GT3:\n",
    "\n",
    "    GT_3 = pd.read_csv(GT_3_path,encoding=\"utf-8\")\n",
    "\n",
    "prod_embedding_small = pd.read_csv(prod_embedding_small_path,encoding=\"utf-8\")\n",
    "prod_embedding_large = pd.read_csv(prod_embedding_large_path,encoding=\"utf-8\")\n",
    "\n",
    "data_key_small = pd.read_csv(settings_small_NN_path,encoding=\"utf-8\")\n",
    "data_key_large = pd.read_csv(settings_large_NN_path,encoding=\"utf-8\")\n",
    "\n",
    "len_small = data_key_small.shape[0]\n",
    "len_large = data_key_large.shape[0]\n",
    "print(f\"小数据集有 *{len_small}* 场auction，大数据集有 *{len_large}* 场auction，一共*{len_small+len_large}*\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 检查"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "assert prod_embedding_small.shape[0] == len_small,\"5\"\n",
    "assert prod_embedding_large.shape[0] == len_large,\"6\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- prod_embedding去掉desc列"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "prod_embedding_small.drop(columns=['desc'],inplace=True)\n",
    "prod_embedding_large.drop(columns=['desc'],inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. 按照data key拼接\n",
    "1. 拼接GT1+GT2+embedding\n",
    "2. 先拼小数据集，再拼大数据集"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1196/1196 [00:02<00:00, 415.53it/s]\n"
     ]
    }
   ],
   "source": [
    "LEN = 300\n",
    "train_col = [str(i) for i in range(0,LEN)]\n",
    "if not W_GT3:\n",
    "    for i in tqdm(range(len_small)):\n",
    "        train_tmp = pd.concat([pd.DataFrame(GT_1.iloc[i,:]).T,\n",
    "                                pd.DataFrame(GT_2.iloc[i,:]).T,\n",
    "                                pd.DataFrame(prod_embedding_small.iloc[i,:]).T],\n",
    "                                ignore_index=True)\n",
    "        # 重命名，避免合并出问题\n",
    "        train_tmp.columns = train_col\n",
    "        # Save\n",
    "        output_path = train_root_path + train_file_head + str(i).zfill(4) + train_file_tail\n",
    "        train_tmp.to_csv(output_path,header=True,index=False,encoding=\"utf-8\")\n",
    "else:\n",
    "    for i in tqdm(range(len_small)):\n",
    "        train_tmp = pd.concat([pd.DataFrame(GT_1.iloc[i,:]).T,\n",
    "                                pd.DataFrame(GT_2.iloc[i,:]).T,\n",
    "                                pd.DataFrame(GT_3.iloc[i,:]).T,\n",
    "                                pd.DataFrame(prod_embedding_small.iloc[i,:]).T],\n",
    "                                ignore_index=True)\n",
    "        # 重命名，避免合并出问题\n",
    "        train_tmp.columns = train_col\n",
    "        # Save\n",
    "        output_path = train_root_path + train_file_head + str(i).zfill(4) + train_file_tail\n",
    "        train_tmp.to_csv(output_path,header=True,index=False,encoding=\"utf-8\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:00<00:00, 396.56it/s]\n"
     ]
    }
   ],
   "source": [
    "LEN = 300\n",
    "train_col = [str(i) for i in range(0,LEN)]\n",
    "if not W_GT3:\n",
    "    for i in tqdm(range(len_large)):\n",
    "        train_tmp = pd.concat([pd.DataFrame(GT_1.iloc[i+len_small,:]).T,\n",
    "                                pd.DataFrame(GT_2.iloc[i+len_small,:]).T,\n",
    "                                pd.DataFrame(prod_embedding_large.iloc[i,:]).T],\n",
    "                                ignore_index=True)\n",
    "        # 重命名，避免合并出问题\n",
    "        train_tmp.columns = train_col\n",
    "        # Save\n",
    "        output_path = train_root_path + train_file_head + str(i+len_small).zfill(4) + train_file_tail\n",
    "        train_tmp.to_csv(output_path,header=True,index=False,encoding=\"utf-8\")\n",
    "else:\n",
    "    for i in tqdm(range(len_large)):\n",
    "        train_tmp = pd.concat([pd.DataFrame(GT_1.iloc[i+len_small,:]).T,\n",
    "                                pd.DataFrame(GT_2.iloc[i+len_small,:]).T,\n",
    "                                pd.DataFrame(GT_3.iloc[i+len_small,:]).T,\n",
    "                                pd.DataFrame(prod_embedding_large.iloc[i,:]).T],\n",
    "                                ignore_index=True)\n",
    "        # 重命名，避免合并出问题\n",
    "        train_tmp.columns = train_col\n",
    "        # Save\n",
    "        output_path = train_root_path + train_file_head + str(i+len_small).zfill(4) + train_file_tail\n",
    "        train_tmp.to_csv(output_path,header=True,index=False,encoding=\"utf-8\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. 拆分小表与保存【LEN<300】(用不到了..)\n",
    "1. 缩小training data的粒度\n",
    "    - scale=3时相当于LEN=100\n",
    "2. **是在原LEN=300的基础上进行的，必须读入GT长度为300的那2个表+长度为新粒度的embedding表**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "def rescale(data,SCALE):\n",
    "    assert data.shape[1]==300,\"!=300\"\n",
    "    data_new = pd.DataFrame()\n",
    "    for i in range(0,data.shape[1],SCALE):\n",
    "        if(i+SCALE<=data.shape[1]):  # 说明可以add SCALE个值\n",
    "            tmp = np.sum(data.iloc[:,i:i+SCALE],axis=1)\n",
    "        else:  # 加不够SCALE个值\n",
    "            tmp = np.sum(data.iloc[:,i:],axis=1)\n",
    "        # 拼接起来\n",
    "        data_new = pd.concat([data_new,tmp],axis=1)\n",
    "    assert data_new.shape[0] == data.shape[0],\"Shape不等\"\n",
    "\n",
    "    return data_new"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCALE=5, LEN=60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1196/1196 [00:35<00:00, 33.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "../data/train_60/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "SCALE = 5\n",
    "LEN = np.ceil(300/SCALE).astype(int)\n",
    "\n",
    "print(f\"SCALE={SCALE}, LEN={LEN}\")\n",
    "\n",
    "# 以SCALE为间隔生成col name\n",
    "# train_col = [str(i+1) for i in range(0,LEN,SCALE)]\n",
    "train_col = [str(i) for i in range(0,LEN)]\n",
    "\n",
    "for i in tqdm(range(0,GT_1_withid.shape[0])):\n",
    "\n",
    "    # 保存'id'\n",
    "    id = GT_1_withid.loc[i,'id']\n",
    "    # 先合并GT data\n",
    "    train_tmp = pd.concat([pd.DataFrame(GT_1_withid.iloc[i,:]).T,\n",
    "                            pd.DataFrame(GT_2_withid.iloc[i,:]).T],\n",
    "                            ignore_index=True)\n",
    "\n",
    "    # drop一些列，保持长度相等（LEN）\n",
    "    train_tmp.drop(['id','bidincrement','bidfee','retail'],axis=1,inplace=True)\n",
    "    # 执行Rescale\n",
    "    scaled_data = rescale(train_tmp,SCALE)\n",
    "\n",
    "    # 按照'id'列找到embedding信息\n",
    "    embedding = prod_embedding[prod_embedding['id'] == id].copy()\n",
    "    # drop一些列，保持长度相等(LEN)\n",
    "    embedding.drop(['id','desc'],axis=1,inplace=True)\n",
    "    # 重命名，避免合并出问题\n",
    "    scaled_data.columns = train_col\n",
    "    # 合并\n",
    "    train_pd = pd.concat([scaled_data,pd.DataFrame(embedding)], ignore_index=True)\n",
    "    # save\n",
    "    output_path = train_root_path+train_file_head+ str(i).zfill(4) + train_file_tail\n",
    "    train_pd.to_csv(output_path,header=True,index=False,encoding=\"utf-8\")\n",
    "\n",
    "print(\"Done\")\n",
    "print(train_root_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
