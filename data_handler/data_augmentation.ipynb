{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 0. Intro\n",
    "\n",
    "# 1.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# oversample数据的比例\n",
    "P = 0.5\n",
    "\n",
    "N_cluster = 3\n",
    "\n",
    "\n",
    "# Target data\n",
    "target_path = r\"../data/targets\"\n",
    "# target_path = r\"../data/targets_5\"\n",
    "\n",
    "# Target data after DA\n",
    "target_path_DA = r\"../data/targets_DA\"\n",
    "\n",
    "# data keys\n",
    "data_key_path = \"../data/target_datakey.csv\"\n",
    "\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import random\n",
    "import collections\n",
    "from scipy.spatial.distance import cdist\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# all_path里有全部的data地址作为list\n",
    "target_all_path = os.listdir(target_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "Total_data_records = 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1196/1196 [00:32<00:00, 36.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(target_all_path))):\n",
    "\n",
    "    # Read in the data\n",
    "    data_path = os.path.join(target_path,target_all_path[i])\n",
    "    data = pd.read_csv(data_path,encoding=\"utf-8\")\n",
    "    data = np.array(data.iloc[:,0]).reshape(-1, 1)\n",
    "\n",
    "    ##################### KMeans and compute dist and prob  #####################\n",
    "    # Do the KMeans\n",
    "    kmeans = KMeans(n_clusters=N_cluster, random_state=0).fit(data)\n",
    "    centers = np.array(kmeans.cluster_centers_, dtype=int)  # 返回center\n",
    "    labels =  pd.DataFrame(kmeans.labels_)                  # 返回每一点的cluster标签\n",
    "\n",
    "    # Compute the distance from data to their center\n",
    "    dist_all = cdist(data, centers, metric='cityblock')\n",
    "    dist = []\n",
    "\n",
    "    for j in range(len(dist_all)):\n",
    "        dist.append(dist_all[j,kmeans.labels_[j]])\n",
    "    dist = pd.DataFrame(dist)\n",
    "\n",
    "    # Combine into one table\n",
    "    dist_pd = pd.concat([pd.DataFrame(data),dist,labels],axis = 1)\n",
    "    dist_pd.columns = [\"N\",\"dist\",\"cluster\"]\n",
    "\n",
    "    # Put the 'centers position' and 'max_dist' in to the table\n",
    "    centers_pd = pd.DataFrame(centers)\n",
    "    centers_pd.reset_index(drop=False,inplace=True)\n",
    "    centers_pd.columns = ['cluster','center']\n",
    "\n",
    "    max_dist = pd.DataFrame(dist_pd.groupby(\"cluster\")['dist'].max())  # 每个cluster中dist的最大值\n",
    "    max_dist.reset_index(drop=False,inplace=True)\n",
    "    max_dist.columns = ['cluster','max_dist']\n",
    "\n",
    "    # Combine\n",
    "    centers_pd = pd.merge(centers_pd,max_dist,on='cluster')\n",
    "    data_pd = pd.merge(dist_pd,centers_pd,on='cluster',how=\"left\")\n",
    "\n",
    "    # Redefine the 'dist'\n",
    "    data_pd['new_dist'] = data_pd['max_dist'] - data_pd['dist']+1\n",
    "\n",
    "    # Compute the dist sum\n",
    "    new_sum_dist = pd.DataFrame(data_pd.groupby(\"cluster\")['new_dist'].sum())\n",
    "    new_sum_dist.reset_index(drop=False,inplace=True)\n",
    "    new_sum_dist.columns = ['cluster','new_sum_dist']\n",
    "\n",
    "    # Combine\n",
    "    data_pd = pd.merge(data_pd,new_sum_dist,on='cluster',how=\"left\")\n",
    "\n",
    "    # Compute the prob\n",
    "    data_pd['prob'] = data_pd['new_dist']/data_pd['new_sum_dist']\n",
    "\n",
    "    # N_news = int(P*len(data))\n",
    "    # Get the amount of data to be oversampled (in each cluster)\n",
    "    cluster_size = np.array(data_pd.groupby(\"cluster\").count().iloc[:,0])\n",
    "    N_news_ls = np.array(cluster_size*P,dtype=int) # 根据cluster大小分配抽样规模\n",
    "\n",
    "    ##################### Oversample #####################\n",
    "\n",
    "    new_data_i = pd.DataFrame()       # 新生成的data‘N’\n",
    "\n",
    "    for j in range(kmeans.n_clusters):\n",
    "        # Get all data in cluster j\n",
    "\n",
    "        data_j = data_pd[data_pd.loc[:,'cluster'] == j]\n",
    "        data_j = data_j.reset_index(drop=True)\n",
    "\n",
    "        data_j.sort_values(by='prob',ascending = False,inplace = True,ignore_index = True) # 降序排序,并且重新index一下\n",
    "\n",
    "        # Sample according to the prob\n",
    "        new_idx = random.choice(a = data_j.shape[0], p = data_j.prob,size=N_news_ls[j])\n",
    "        new_data_j = data_j.loc[new_idx,'N'].reset_index(drop=True)\n",
    "\n",
    "        new_data_i = pd.concat([new_data_i,pd.DataFrame(new_data_j)],axis=0)\n",
    "\n",
    "    new_data_i.reset_index(drop=True, inplace=True)\n",
    "    assert new_data_i.shape[0] == N_news_ls.sum(),\"Did not oversample enough data\"\n",
    "\n",
    "    ##################### Redesign the data table (Compute the 'p' value)#####################\n",
    "\n",
    "    # Concat the old with the new\n",
    "    N_data = pd.concat([pd.DataFrame(data,columns=['N']),new_data_i],axis=0,ignore_index=True)\n",
    "    # Count 'cnt' to compute P\n",
    "    pcount = collections.Counter(N_data.iloc[:,0])\n",
    "    tmp = pd.DataFrame.from_dict(pcount,orient='index').reset_index()\n",
    "    tmp.columns = ['N','cnt']\n",
    "    # Compute P\n",
    "    tmp['P'] = tmp.cnt/ tmp.shape[0]\n",
    "    N_data = pd.merge(N_data,tmp,how='left',on=['N'])\n",
    "    # Repeat data records according to 'cnt'\n",
    "    output_data = N_data.loc[N_data.index.repeat(N_data['cnt'])]\n",
    "    # output_data.drop(columns=['cnt'],inplace=True)\n",
    "\n",
    "    output_path = os.path.join(target_path_DA,target_all_path[i])\n",
    "    output_data.to_csv(output_path,header=True,index=False,encoding=\"utf-8\")\n",
    "\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
