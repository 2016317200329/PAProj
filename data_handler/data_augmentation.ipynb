{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 0. Intro\n",
    "\n",
    "# 1.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/targets_5_DA_P=0.5_N_c=3\n"
     ]
    }
   ],
   "source": [
    "P = 0.5         # oversample数据的比例\n",
    "N_cluster = 3   # cluster 类型\n",
    "RESCALE = True  # 是否需要重新整理粒度？\n",
    "\n",
    "\n",
    "# Target data\n",
    "target_path = r\"../data/targets\"\n",
    "arr_path_root = r\"../data/arr_selected\"\n",
    "\n",
    "if RESCALE:\n",
    "    SCALE = 5   # 粒度整理为5\n",
    "    col_names_2 = ['N','P'] # 保留列名\n",
    "    target_path_DA = r\"../data/targets_\"+str(SCALE)+\"_DA_P=\"+str(P)+r\"_N_c=\"+str(N_cluster) # Target data after DA\n",
    "\n",
    "else:\n",
    "    target_path_DA = r\"../data/targets_DA_P=\"+str(P)+r\"_N_c=\"+str(N_cluster)\n",
    "print(target_path_DA)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import random\n",
    "import collections\n",
    "from scipy.spatial.distance import cdist\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "\n",
    "# all_path里有全部的data地址作为list\n",
    "target_all_path = os.listdir(target_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. DA\n",
    "1. 根据RESCALE参数决定要不要整理粒度"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "Total_data_records = 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1196/1196 [01:04<00:00, 18.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(target_all_path))):\n",
    "\n",
    "    # Read in the data\n",
    "    data_path = os.path.join(target_path,target_all_path[i])\n",
    "    data = pd.read_csv(data_path,encoding=\"utf-8\")\n",
    "    data = np.array(data.iloc[:,0]).reshape(-1, 1)\n",
    "\n",
    "    ##################### KMeans and compute dist and prob  #####################\n",
    "    # Do the KMeans\n",
    "    n_cluster = min(N_cluster,len(np.unique(data))) # 有可能data的nunique还没有N_cluster大\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_cluster, random_state=0).fit(data)\n",
    "    centers = np.array(kmeans.cluster_centers_, dtype=int)  # 返回center\n",
    "    labels =  pd.DataFrame(kmeans.labels_)                  # 返回每一点的cluster标签\n",
    "\n",
    "    # Compute the distance from data to their center\n",
    "    dist_all = cdist(data, centers, metric='cityblock')\n",
    "    dist = []\n",
    "\n",
    "    for j in range(len(dist_all)):\n",
    "        dist.append(dist_all[j,kmeans.labels_[j]])\n",
    "    dist = pd.DataFrame(dist)\n",
    "\n",
    "    # Combine into one table\n",
    "    dist_pd = pd.concat([pd.DataFrame(data),dist,labels],axis = 1)\n",
    "    dist_pd.columns = [\"N\",\"dist\",\"cluster\"]\n",
    "\n",
    "    # Put the 'centers position' and 'max_dist' in to the table\n",
    "    centers_pd = pd.DataFrame(centers)\n",
    "    centers_pd.reset_index(drop=False,inplace=True)\n",
    "    centers_pd.columns = ['cluster','center']\n",
    "\n",
    "    max_dist = pd.DataFrame(dist_pd.groupby(\"cluster\")['dist'].max())  # 每个cluster中dist的最大值\n",
    "    max_dist.reset_index(drop=False,inplace=True)\n",
    "    max_dist.columns = ['cluster','max_dist']\n",
    "\n",
    "    # Combine\n",
    "    centers_pd = pd.merge(centers_pd,max_dist,on='cluster')\n",
    "    data_pd = pd.merge(dist_pd,centers_pd,on='cluster',how=\"left\")\n",
    "\n",
    "    # Redefine the 'dist'\n",
    "    data_pd['new_dist'] = data_pd['max_dist'] - data_pd['dist']+1\n",
    "\n",
    "    # Compute the dist sum\n",
    "    new_sum_dist = pd.DataFrame(data_pd.groupby(\"cluster\")['new_dist'].sum())\n",
    "    new_sum_dist.reset_index(drop=False,inplace=True)\n",
    "    new_sum_dist.columns = ['cluster','new_sum_dist']\n",
    "\n",
    "    # Combine\n",
    "    data_pd = pd.merge(data_pd,new_sum_dist,on='cluster',how=\"left\")\n",
    "\n",
    "    # Compute the prob\n",
    "    data_pd['prob'] = data_pd['new_dist']/data_pd['new_sum_dist']\n",
    "\n",
    "    # N_news = int(P*len(data))\n",
    "    # Get the amount of data to be oversampled (in each cluster)\n",
    "    cluster_size = np.array(data_pd.groupby(\"cluster\").count().iloc[:,0])\n",
    "    N_news_ls = np.array(cluster_size*P,dtype=int) # 根据cluster大小分配抽样规模\n",
    "\n",
    "    ##################### Oversample #####################\n",
    "\n",
    "    new_data_i = pd.DataFrame()       # 新生成的data‘N’\n",
    "\n",
    "    for j in range(kmeans.n_clusters):\n",
    "        # Get all data in cluster j\n",
    "\n",
    "        data_j = data_pd[data_pd.loc[:,'cluster'] == j]\n",
    "        data_j = data_j.reset_index(drop=True)\n",
    "\n",
    "        data_j.sort_values(by='prob',ascending = False,inplace = True,ignore_index = True) # 降序排序,并且重新index一下。降序是为了下面抽取idx\n",
    "\n",
    "        # Sample according to the prob\n",
    "        assert kmeans.n_clusters == n_cluster,f\"kmeans.n_cluster = {kmeans.n_clusters}!!\"\n",
    "        assert N_news_ls.__len__() == n_cluster,f\"N_news_ls len = {N_news_ls.__len__()}!!\"\n",
    "        new_idx = random.choice(a = data_j.shape[0], p = data_j.prob,size=N_news_ls[j])\n",
    "        new_data_j = data_j.loc[new_idx,'N'].reset_index(drop=True)\n",
    "\n",
    "        new_data_i = pd.concat([new_data_i,pd.DataFrame(new_data_j)],axis=0)\n",
    "\n",
    "    new_data_i.reset_index(drop=True, inplace=True)\n",
    "    assert new_data_i.shape[0] == N_news_ls.sum(),\"Did not oversample enough data\"\n",
    "\n",
    "    ##################### Redesign the data table (Compute the 'p' value)#####################\n",
    "\n",
    "    # Concat the old with the new\n",
    "    N_data = pd.concat([pd.DataFrame(data,columns=['N']),new_data_i],axis=0,ignore_index=True)\n",
    "    # Count 'cnt' to compute P\n",
    "    pcount = collections.Counter(N_data.iloc[:,0])\n",
    "    tmp = pd.DataFrame.from_dict(pcount,orient='index').reset_index()\n",
    "    tmp.columns = ['N','cnt']\n",
    "    # Compute P\n",
    "    tmp['P'] = tmp.cnt/ tmp.shape[0]\n",
    "    N_data = pd.merge(N_data,tmp,how='left',on=['N'])\n",
    "    # Repeat data records according to 'cnt'\n",
    "    output_data = N_data.loc[N_data.index.repeat(N_data['cnt'])]\n",
    "\n",
    "    ##################### 如果需要重新整理粒度，就不要drop'cnt'这一列\n",
    "    if not RESCALE:\n",
    "        output_data.drop(columns=['cnt'],inplace=True)\n",
    "        output_path = os.path.join(target_path_DA,target_all_path[i])\n",
    "        output_data.to_csv(output_path,header=True,index=False,encoding=\"utf-8\")\n",
    "    elif RESCALE:\n",
    "        output_data.sort_values(by='N',ignore_index=True,inplace=True)\n",
    "        # Drop duplicates\n",
    "        data_i_df = output_data.drop_duplicates(inplace = False,ignore_index=True)\n",
    "\n",
    "        # 现在数据是无重复版本的，'cnt'记录了这个record重复出现的次数\n",
    "        # New added: Rearrange the data according to their length\n",
    "        data_lenth = data_i_df.shape[0]         # Data length\n",
    "        dele_idx = []                           # Idx to be deleted\n",
    "\n",
    "        if(data_lenth >= 0):\n",
    "            j = 0\n",
    "            while (j < data_lenth):\n",
    "                # N值-1恰好是SCALE倍\n",
    "                if((data_i_df.iloc[j,0]-1)%SCALE == 0):\n",
    "                    k = j+1    # 用k记录位置\n",
    "                    # k不是data最后一个值 and [k]在+SCALE的范围内\n",
    "                    while((k < data_lenth) and (data_i_df.iloc[j,0]+SCALE > data_i_df.iloc[k,0])):\n",
    "                        # 更新[j]的prob值和cnt值：叠加\n",
    "                        data_i_df.iloc[j,1] += data_i_df.iloc[k,1]\n",
    "                        data_i_df.iloc[j,2] += data_i_df.iloc[k,2]\n",
    "                        # 删除[k]\n",
    "                        dele_idx.append(k)\n",
    "                        k += 1\n",
    "                    # j 从 k（其实是k+1）的位置继续\n",
    "                    j = k\n",
    "                # N值不是SCALE倍且未被并入任何一个已存在的开头\n",
    "                else:\n",
    "                    # 原地修改（减小）成一个新的区间的开头,\n",
    "                    while((data_i_df.iloc[j,0]-1) %SCALE != 0):\n",
    "                        data_i_df.iloc[j,0] -= 1\n",
    "                    # 注意不需要j+1，下一次循环从当前开始，check后面的n需不需要并进来\n",
    "                    # j += 1\n",
    "\n",
    "        ######### 如果只想保存改动的data，把下面都拿到上面的if里面来\n",
    "        # Save i_th training data file\n",
    "        # save_i_trainfile(i)\n",
    "        # Dele\n",
    "        data_i_df = data_i_df.drop(dele_idx,axis = 0).copy()\n",
    "\n",
    "        # Reconstruct and repeat data according to 'cnt_n_2'\n",
    "        data_i_df = data_i_df.loc[data_i_df.index.repeat(data_i_df['cnt'])]\n",
    "        data_i_df = data_i_df[col_names_2]\n",
    "\n",
    "        # Get the output path\n",
    "        output_path = os.path.join(target_path_DA,target_all_path[i])\n",
    "        data_i_df.to_csv(output_path,header=True,index=False,encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. drop uniform auctions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1196/1196 [00:00<00:00, 1627.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target data近似为均匀分布的数量: 16, 占比: 0.013377926421404682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "target_ls = []\n",
    "arr = []\n",
    "K = 2\n",
    "\n",
    "# target_path_DA = r\"../data/targets_5\"\n",
    "for i in tqdm(range(len(target_all_path))):\n",
    "    target_path_i_path = os.path.join(target_path_DA,target_all_path[i])\n",
    "    target_df = pd.read_csv(target_path_i_path,encoding=\"utf-8\")\n",
    "\n",
    "    p = target_df.P.nunique()\n",
    "    if p <= K:\n",
    "        target_ls.append(i)\n",
    "    else:\n",
    "        arr.append(i)\n",
    "\n",
    "print(f\"target data近似为均匀分布的数量: {len(target_ls)}, 占比: {len(target_ls)/len(target_all_path)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "# 保存idx for training\n",
    "\n",
    "if RESCALE:\n",
    "    arr_path_DA = r\"arr_targets_\"+str(SCALE)+\"_DA_P=\"+str(P)+r\"_N_c=\"+str(N_cluster)+r\"_K=\"+str(K)\n",
    "else:\n",
    "    arr_path_DA = r\"arr_targets_DA_P=\"+str(P)+r\"_N_c=\"+str(N_cluster)+r\"_K=\"+str(K)\n",
    "\n",
    "arr_path = os.path.join(arr_path_root,arr_path_DA)\n",
    "arr = np.array(arr)\n",
    "np.save(arr_path,arr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
