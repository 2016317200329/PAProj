{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd.function import Function\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. 自己编写function以探究autograd"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.7183], grad_fn=<ExpBackward>)\n",
      "tensor([0.2718])\n"
     ]
    }
   ],
   "source": [
    "class Exp(Function):                    # 此层计算e^x\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, coeff):                # 模型前向\n",
    "        result = x.exp()\n",
    "        # 保存backward需要的内容，正向计算的结果被保存在saved_tensors元组中，注意此处仅能保存tensor类型变量，\n",
    "        ctx.save_for_backward(result)\n",
    "        # 其余类型变量（Int等），可直接赋予ctx作为成员变量，也可以达到保存效果\n",
    "        ctx.coeff = coeff\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):     # 梯度反传\n",
    "        result, = ctx.saved_tensors     # 取出forward中保存的result\n",
    "        # 计算梯度并返回，注意乘系数\n",
    "        # 返回值是2个：因为要和forward的输入个数和顺序保持一致\n",
    "        return grad_output * result*ctx.coeff, None\n",
    "\n",
    "# 尝试使用\n",
    "x = torch.tensor([1.], requires_grad=True)  # 需要设置tensor的requires_grad属性为True，才会进行梯度反传\n",
    "ret = Exp.apply(x,0.1)                          # 使用apply方法调用自定义autograd function\n",
    "print(ret)                                  # tensor([2.7183], grad_fn=<ExpBackward>)\n",
    "ret.backward()                              # 反传梯度，得到乘了系数的结果\n",
    "print(x.grad)                               # tensor([0.27183])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. mask的用法\n",
    "Here's a simple example of a custom loss function that computes the mean squared error (MSE) between two tensors, **but only for the elements that are greater than a certain threshold**:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0100, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0100, grad_fn=<MeanBackward0>)\n",
      "tensor(1.0000, grad_fn=<MeanBackward0>)\n",
      "tensor([-0.1333, -0.1333, -1.3333])\n"
     ]
    }
   ],
   "source": [
    "# 原始版本\n",
    "def custom_loss_fn_1(x, y, threshold=0.5):\n",
    "    loss = torch.tensor([0.],requires_grad=True)\n",
    "    mask = (y > threshold).int()\n",
    "    mse = (diff*diff).mean()\n",
    "    for i in range(len(x)):\n",
    "        diff = x[i] - y[i]\n",
    "        mse = (diff * diff).mean()\n",
    "        print(mse)\n",
    "        loss = loss + mse\n",
    "\n",
    "    masked_mse = loss/len(x) * mask         # 作mask\n",
    "    return masked_mse\n",
    "\n",
    "x = torch.tensor([0.2, 0.6, 0.8], requires_grad=True)\n",
    "y = torch.tensor([0.3, 0.7, 1.8])\n",
    "\n",
    "loss = custom_loss_fn_1(x, y, threshold=0.5)\n",
    "loss.backward(torch.ones_like(loss))        # torch.ones_like(loss)相当于sum\n",
    "\n",
    "print(x.grad)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "zero-dimensional tensor (at position 1) cannot be concatenated",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_21768\\2239366841.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[0my\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0.3\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0.7\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0.9\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrequires_grad\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 17\u001B[1;33m \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcustom_loss_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mthreshold\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.5\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     18\u001B[0m \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mones_like\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m        \u001B[1;31m# torch.ones_like(loss)相当于sum\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_21768\\2239366841.py\u001B[0m in \u001B[0;36mcustom_loss_fn\u001B[1;34m(x, y, threshold)\u001B[0m\n\u001B[0;32m      6\u001B[0m         \u001B[0mdiff\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m         \u001B[0mmse\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mdiff\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mdiff\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m         \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mloss\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mmse\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m         \u001B[1;31m# loss = loss + mse\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: zero-dimensional tensor (at position 1) cannot be concatenated"
     ]
    }
   ],
   "source": [
    "# loss?\n",
    "def custom_loss_fn(x, y, threshold=0.5):\n",
    "    loss = torch.tensor([])\n",
    "    mask = (x > threshold).int()\n",
    "    for i in range(len(x)):\n",
    "        diff = x[i] - y[i]\n",
    "        mse = (diff * diff).mean()\n",
    "        loss = torch.cat([loss,mse])\n",
    "        # loss = loss + mse\n",
    "\n",
    "    masked_mse = loss/len(x) * mask         # 作mask\n",
    "    return masked_mse\n",
    "\n",
    "x = torch.tensor([0.2, 0.6, 0.8], requires_grad=True)\n",
    "y = torch.tensor([0.3, 0.7, 0.9], requires_grad=True)\n",
    "\n",
    "loss = custom_loss_fn(x, y, threshold=0.5)\n",
    "loss.backward(torch.ones_like(loss))        # torch.ones_like(loss)相当于sum\n",
    "\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.0100, 0.0100], grad_fn=<MulBackward0>)\n",
      "tensor([-0.1333, -0.1333, -0.1333])\n",
      "tensor([0.1333, 0.1333, 0.1333])\n"
     ]
    }
   ],
   "source": [
    "def custom_loss_fn(x, y, threshold=0.5):\n",
    "        diff = x - y\n",
    "        mask = (x > threshold).int()\n",
    "        mse = (diff * diff).mean()\n",
    "        # mask的作用是：只保留部分值的loss贡献，不是所有的\n",
    "        # 用mask？mask = (x > T).int()\n",
    "        masked_mse = mse * mask         # 作mask\n",
    "        print(masked_mse)\n",
    "        return masked_mse\n",
    "\n",
    "x = torch.tensor([0.2, 0.6, 0.8], requires_grad=True)\n",
    "y = torch.tensor([0.3, 0.7, 0.9], requires_grad=True)\n",
    "\n",
    "loss = custom_loss_fn(x, y, threshold=0.5)\n",
    "loss.backward(torch.ones_like(loss))        # torch.ones_like(loss)相当于sum\n",
    "\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. logistic_function的两个写法？"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def logistic_function(x):\n",
    "#     x = np.float64(x)\n",
    "    return 1.0 / (1.0 + torch.exp(-x))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "def logistic_function_2(x):\n",
    "    return .5 * (1 + torch.tanh(.5 * x))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "input = torch.tensor([1.,2.,-100.,-200.,-500.,-10000.])\n",
    "ans1 = [logistic_function(i) for i in input]\n",
    "ans2 = [logistic_function_2(i) for i in input]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.7311), tensor(0.8808), tensor(0.), tensor(0.), tensor(0.), tensor(0.)]\n",
      "[tensor(0.7311), tensor(0.8808), tensor(0.), tensor(0.), tensor(0.), tensor(0.)]\n"
     ]
    }
   ],
   "source": [
    "print(ans1)\n",
    "print(ans2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Standardize with sklearn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([[-1.22474487],\n        [ 0.        ],\n        [ 1.22474487]]),\n array([-1.22474487,  0.        ,  1.22474487]))"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "data = np.array([1.,2.,3.])\n",
    "data_st = scaler.fit_transform(data.reshape(-1, 1))\n",
    "\n",
    "data_st,(data-data.mean())/data.std()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1其他标准化方式"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min_Max标准化:[0.     0.0012 0.0025 0.0037 0.0049 0.0062 0.0075 0.0087 0.01   0.0113\n",
      " 0.0126 0.0139 0.0152 0.0165 0.0179 0.0192 0.0205 0.0219 0.0232 0.0246\n",
      " 0.026  0.0274 0.0287 0.0301 0.0316 0.033  0.0344 0.0358 0.0373 0.0387\n",
      " 0.0402 0.0416 0.0431 0.0446 0.0461 0.0476 0.0491 0.0506 0.0522 0.0537\n",
      " 0.0553 0.0568 0.0584 0.06   0.0616 0.0631 0.0648 0.0664 0.068  0.0696\n",
      " 0.0713 0.0729 0.0746 0.0763 0.078  0.0797 0.0814 0.0831 0.0848 0.0865\n",
      " 0.0883 0.09   0.0918 0.0936 0.0954 0.0972 0.099  0.1008 0.1026 0.1045\n",
      " 0.1063 0.1082 0.1101 0.112  0.1139 0.1158 0.1177 0.1196 0.1216 0.1235\n",
      " 0.1255 0.1275 0.1295 0.1315 0.1335 0.1355 0.1375 0.1396 0.1417 0.1437\n",
      " 0.1458 0.1479 0.15   0.1522 0.1543 0.1564 0.1586 0.1608 0.163  0.1652\n",
      " 0.1674 0.1696 0.1719 0.1741 0.1764 0.1787 0.181  0.1833 0.1856 0.1879\n",
      " 0.1903 0.1927 0.195  0.1974 0.1998 0.2023 0.2047 0.2072 0.2096 0.2121\n",
      " 0.2146 0.2171 0.2196 0.2222 0.2247 0.2273 0.2299 0.2325 0.2351 0.2377\n",
      " 0.2404 0.243  0.2457 0.2484 0.2511 0.2538 0.2566 0.2593 0.2621 0.2649\n",
      " 0.2677 0.2705 0.2734 0.2762 0.2791 0.282  0.2849 0.2878 0.2908 0.2937\n",
      " 0.2967 0.2997 0.3027 0.3057 0.3088 0.3118 0.3149 0.318  0.3211 0.3243\n",
      " 0.3274 0.3306 0.3338 0.337  0.3402 0.3435 0.3468 0.35   0.3533 0.3567\n",
      " 0.36   0.3634 0.3668 0.3702 0.3736 0.377  0.3805 0.384  0.3875 0.391\n",
      " 0.3946 0.3981 0.4017 0.4053 0.4089 0.4126 0.4163 0.42   0.4237 0.4274\n",
      " 0.4312 0.4349 0.4387 0.4426 0.4464 0.4503 0.4542 0.4581 0.462  0.466\n",
      " 0.47   0.474  0.478  0.482  0.4861 0.4902 0.4943 0.4985 0.5026 0.5068\n",
      " 0.511  0.5153 0.5195 0.5238 0.5282 0.5325 0.5369 0.5412 0.5457 0.5501\n",
      " 0.5546 0.559  0.5636 0.5681 0.5727 0.5773 0.5819 0.5865 0.5912 0.5959\n",
      " 0.6006 0.6054 0.6102 0.615  0.6198 0.6247 0.6296 0.6345 0.6394 0.6444\n",
      " 0.6494 0.6544 0.6595 0.6646 0.6697 0.6748 0.68   0.6852 0.6904 0.6957\n",
      " 0.701  0.7063 0.7117 0.7171 0.7225 0.7279 0.7334 0.7389 0.7444 0.75\n",
      " 0.7556 0.7612 0.7669 0.7726 0.7783 0.7841 0.7898 0.7957 0.8015 0.8074\n",
      " 0.8133 0.8193 0.8253 0.8313 0.8373 0.8434 0.8495 0.8557 0.8619 0.8681\n",
      " 0.8744 0.8806 0.887  0.8933 0.8997 0.9062 0.9126 0.9191 0.9257 0.9322\n",
      " 0.9389 0.9455 0.9522 0.9589 0.9657 0.9725 0.9793 0.9862 0.9931 1.    ]\n",
      "Z_Score标准化:[-1.2782 -1.2738 -1.2695 -1.2651 -1.2607 -1.2563 -1.2518 -1.2474 -1.2429\n",
      " -1.2383 -1.2338 -1.2292 -1.2246 -1.2199 -1.2152 -1.2105 -1.2058 -1.2011\n",
      " -1.1963 -1.1915 -1.1866 -1.1817 -1.1768 -1.1719 -1.1669 -1.162  -1.1569\n",
      " -1.1519 -1.1468 -1.1417 -1.1365 -1.1314 -1.1262 -1.1209 -1.1157 -1.1104\n",
      " -1.105  -1.0997 -1.0943 -1.0888 -1.0834 -1.0779 -1.0724 -1.0668 -1.0612\n",
      " -1.0556 -1.0499 -1.0442 -1.0385 -1.0327 -1.0269 -1.0211 -1.0152 -1.0093\n",
      " -1.0034 -0.9974 -0.9914 -0.9853 -0.9793 -0.9731 -0.967  -0.9608 -0.9546\n",
      " -0.9483 -0.942  -0.9357 -0.9293 -0.9229 -0.9164 -0.9099 -0.9034 -0.8968\n",
      " -0.8902 -0.8835 -0.8768 -0.8701 -0.8633 -0.8565 -0.8497 -0.8428 -0.8358\n",
      " -0.8289 -0.8218 -0.8148 -0.8077 -0.8005 -0.7934 -0.7861 -0.7788 -0.7715\n",
      " -0.7642 -0.7568 -0.7493 -0.7418 -0.7343 -0.7267 -0.7191 -0.7114 -0.7037\n",
      " -0.6959 -0.6881 -0.6803 -0.6723 -0.6644 -0.6564 -0.6484 -0.6403 -0.6321\n",
      " -0.6239 -0.6157 -0.6074 -0.5991 -0.5907 -0.5822 -0.5738 -0.5652 -0.5566\n",
      " -0.548  -0.5393 -0.5306 -0.5218 -0.5129 -0.504  -0.4951 -0.4861 -0.477\n",
      " -0.4679 -0.4587 -0.4495 -0.4403 -0.4309 -0.4215 -0.4121 -0.4026 -0.3931\n",
      " -0.3835 -0.3738 -0.3641 -0.3543 -0.3445 -0.3346 -0.3246 -0.3146 -0.3045\n",
      " -0.2944 -0.2842 -0.274  -0.2637 -0.2533 -0.2429 -0.2324 -0.2218 -0.2112\n",
      " -0.2005 -0.1898 -0.179  -0.1681 -0.1572 -0.1462 -0.1352 -0.124  -0.1129\n",
      " -0.1016 -0.0903 -0.0789 -0.0674 -0.0559 -0.0443 -0.0327 -0.021  -0.0092\n",
      "  0.0027  0.0146  0.0266  0.0387  0.0508  0.063   0.0753  0.0877  0.1001\n",
      "  0.1126  0.1252  0.1378  0.1505  0.1633  0.1762  0.1891  0.2021  0.2152\n",
      "  0.2284  0.2416  0.2549  0.2683  0.2818  0.2954  0.309   0.3227  0.3365\n",
      "  0.3504  0.3643  0.3783  0.3925  0.4067  0.4209  0.4353  0.4497  0.4642\n",
      "  0.4789  0.4935  0.5083  0.5232  0.5381  0.5532  0.5683  0.5835  0.5988\n",
      "  0.6142  0.6296  0.6452  0.6608  0.6766  0.6924  0.7083  0.7243  0.7404\n",
      "  0.7566  0.7729  0.7893  0.8057  0.8223  0.839   0.8557  0.8726  0.8895\n",
      "  0.9065  0.9237  0.9409  0.9583  0.9757  0.9932  1.0109  1.0286  1.0464\n",
      "  1.0644  1.0824  1.1005  1.1188  1.1371  1.1556  1.1741  1.1928  1.2115\n",
      "  1.2304  1.2494  1.2684  1.2876  1.3069  1.3263  1.3458  1.3655  1.3852\n",
      "  1.405   1.425   1.445   1.4652  1.4855  1.5059  1.5264  1.5471  1.5678\n",
      "  1.5887  1.6097  1.6308  1.652   1.6733  1.6948  1.7163  1.738   1.7598\n",
      "  1.7818  1.8038  1.826   1.8483  1.8707  1.8932  1.9159  1.9387  1.9616\n",
      "  1.9847  2.0078  2.0311  2.0546  2.0781  2.1018  2.1256  2.1496  2.1736\n",
      "  2.1979  2.2222  2.2467]\n",
      "均值归一法：\n",
      "公式一:[-0.3626 -0.3614 -0.3602 -0.3589 -0.3577 -0.3564 -0.3552 -0.3539 -0.3526\n",
      " -0.3513 -0.35   -0.3487 -0.3474 -0.3461 -0.3448 -0.3434 -0.3421 -0.3407\n",
      " -0.3394 -0.338  -0.3366 -0.3353 -0.3339 -0.3325 -0.3311 -0.3296 -0.3282\n",
      " -0.3268 -0.3253 -0.3239 -0.3224 -0.321  -0.3195 -0.318  -0.3165 -0.315\n",
      " -0.3135 -0.312  -0.3104 -0.3089 -0.3074 -0.3058 -0.3042 -0.3027 -0.3011\n",
      " -0.2995 -0.2979 -0.2962 -0.2946 -0.293  -0.2913 -0.2897 -0.288  -0.2863\n",
      " -0.2847 -0.283  -0.2813 -0.2795 -0.2778 -0.2761 -0.2743 -0.2726 -0.2708\n",
      " -0.269  -0.2672 -0.2654 -0.2636 -0.2618 -0.26   -0.2581 -0.2563 -0.2544\n",
      " -0.2525 -0.2507 -0.2488 -0.2469 -0.2449 -0.243  -0.2411 -0.2391 -0.2371\n",
      " -0.2352 -0.2332 -0.2312 -0.2291 -0.2271 -0.2251 -0.223  -0.221  -0.2189\n",
      " -0.2168 -0.2147 -0.2126 -0.2105 -0.2083 -0.2062 -0.204  -0.2018 -0.1996\n",
      " -0.1974 -0.1952 -0.193  -0.1907 -0.1885 -0.1862 -0.1839 -0.1816 -0.1793\n",
      " -0.177  -0.1747 -0.1723 -0.17   -0.1676 -0.1652 -0.1628 -0.1604 -0.1579\n",
      " -0.1555 -0.153  -0.1505 -0.148  -0.1455 -0.143  -0.1405 -0.1379 -0.1353\n",
      " -0.1327 -0.1301 -0.1275 -0.1249 -0.1223 -0.1196 -0.1169 -0.1142 -0.1115\n",
      " -0.1088 -0.106  -0.1033 -0.1005 -0.0977 -0.0949 -0.0921 -0.0893 -0.0864\n",
      " -0.0835 -0.0806 -0.0777 -0.0748 -0.0719 -0.0689 -0.0659 -0.0629 -0.0599\n",
      " -0.0569 -0.0538 -0.0508 -0.0477 -0.0446 -0.0415 -0.0383 -0.0352 -0.032\n",
      " -0.0288 -0.0256 -0.0224 -0.0191 -0.0159 -0.0126 -0.0093 -0.0059 -0.0026\n",
      "  0.0008  0.0041  0.0076  0.011   0.0144  0.0179  0.0214  0.0249  0.0284\n",
      "  0.0319  0.0355  0.0391  0.0427  0.0463  0.05    0.0537  0.0573  0.0611\n",
      "  0.0648  0.0686  0.0723  0.0761  0.08    0.0838  0.0877  0.0916  0.0955\n",
      "  0.0994  0.1034  0.1073  0.1113  0.1154  0.1194  0.1235  0.1276  0.1317\n",
      "  0.1359  0.14    0.1442  0.1484  0.1527  0.1569  0.1612  0.1655  0.1699\n",
      "  0.1742  0.1786  0.183   0.1875  0.1919  0.1964  0.2009  0.2055  0.2101\n",
      "  0.2146  0.2193  0.2239  0.2286  0.2333  0.238   0.2428  0.2475  0.2524\n",
      "  0.2572  0.2621  0.2669  0.2719  0.2768  0.2818  0.2868  0.2918  0.2969\n",
      "  0.302   0.3071  0.3122  0.3174  0.3226  0.3278  0.3331  0.3384  0.3437\n",
      "  0.3491  0.3544  0.3599  0.3653  0.3708  0.3763  0.3818  0.3874  0.393\n",
      "  0.3986  0.4043  0.41    0.4157  0.4214  0.4272  0.4331  0.4389  0.4448\n",
      "  0.4507  0.4567  0.4626  0.4687  0.4747  0.4808  0.4869  0.4931  0.4993\n",
      "  0.5055  0.5117  0.518   0.5244  0.5307  0.5371  0.5435  0.55    0.5565\n",
      "  0.5631  0.5696  0.5762  0.5829  0.5896  0.5963  0.603   0.6098  0.6167\n",
      "  0.6235  0.6304  0.6374]\n",
      "公式二:[-0.3018 -0.3008 -0.2998 -0.2988 -0.2977 -0.2967 -0.2956 -0.2946 -0.2935\n",
      " -0.2924 -0.2914 -0.2903 -0.2892 -0.2881 -0.287  -0.2859 -0.2848 -0.2836\n",
      " -0.2825 -0.2814 -0.2802 -0.2791 -0.2779 -0.2767 -0.2756 -0.2744 -0.2732\n",
      " -0.272  -0.2708 -0.2696 -0.2684 -0.2672 -0.2659 -0.2647 -0.2635 -0.2622\n",
      " -0.261  -0.2597 -0.2584 -0.2571 -0.2558 -0.2545 -0.2532 -0.2519 -0.2506\n",
      " -0.2493 -0.2479 -0.2466 -0.2452 -0.2439 -0.2425 -0.2411 -0.2397 -0.2384\n",
      " -0.2369 -0.2355 -0.2341 -0.2327 -0.2313 -0.2298 -0.2284 -0.2269 -0.2254\n",
      " -0.2239 -0.2225 -0.221  -0.2194 -0.2179 -0.2164 -0.2149 -0.2133 -0.2118\n",
      " -0.2102 -0.2086 -0.2071 -0.2055 -0.2039 -0.2023 -0.2007 -0.199  -0.1974\n",
      " -0.1957 -0.1941 -0.1924 -0.1907 -0.189  -0.1874 -0.1856 -0.1839 -0.1822\n",
      " -0.1805 -0.1787 -0.177  -0.1752 -0.1734 -0.1716 -0.1698 -0.168  -0.1662\n",
      " -0.1643 -0.1625 -0.1606 -0.1588 -0.1569 -0.155  -0.1531 -0.1512 -0.1493\n",
      " -0.1473 -0.1454 -0.1434 -0.1415 -0.1395 -0.1375 -0.1355 -0.1335 -0.1314\n",
      " -0.1294 -0.1274 -0.1253 -0.1232 -0.1211 -0.119  -0.1169 -0.1148 -0.1126\n",
      " -0.1105 -0.1083 -0.1062 -0.104  -0.1018 -0.0995 -0.0973 -0.0951 -0.0928\n",
      " -0.0906 -0.0883 -0.086  -0.0837 -0.0813 -0.079  -0.0767 -0.0743 -0.0719\n",
      " -0.0695 -0.0671 -0.0647 -0.0623 -0.0598 -0.0574 -0.0549 -0.0524 -0.0499\n",
      " -0.0474 -0.0448 -0.0423 -0.0397 -0.0371 -0.0345 -0.0319 -0.0293 -0.0267\n",
      " -0.024  -0.0213 -0.0186 -0.0159 -0.0132 -0.0105 -0.0077 -0.0049 -0.0022\n",
      "  0.0006  0.0035  0.0063  0.0091  0.012   0.0149  0.0178  0.0207  0.0236\n",
      "  0.0266  0.0296  0.0325  0.0355  0.0386  0.0416  0.0447  0.0477  0.0508\n",
      "  0.0539  0.0571  0.0602  0.0634  0.0666  0.0698  0.073   0.0762  0.0795\n",
      "  0.0827  0.086   0.0893  0.0927  0.096   0.0994  0.1028  0.1062  0.1096\n",
      "  0.1131  0.1166  0.12    0.1235  0.1271  0.1306  0.1342  0.1378  0.1414\n",
      "  0.145   0.1487  0.1524  0.1561  0.1598  0.1635  0.1673  0.171   0.1748\n",
      "  0.1787  0.1825  0.1864  0.1903  0.1942  0.1981  0.2021  0.2061  0.2101\n",
      "  0.2141  0.2181  0.2222  0.2263  0.2304  0.2346  0.2387  0.2429  0.2471\n",
      "  0.2513  0.2556  0.2599  0.2642  0.2685  0.2729  0.2773  0.2817  0.2861\n",
      "  0.2906  0.295   0.2995  0.3041  0.3086  0.3132  0.3178  0.3225  0.3271\n",
      "  0.3318  0.3365  0.3412  0.346   0.3508  0.3556  0.3605  0.3653  0.3702\n",
      "  0.3752  0.3801  0.3851  0.3901  0.3952  0.4002  0.4053  0.4104  0.4156\n",
      "  0.4208  0.426   0.4312  0.4365  0.4418  0.4471  0.4524  0.4578  0.4632\n",
      "  0.4687  0.4742  0.4797  0.4852  0.4908  0.4963  0.502   0.5076  0.5133\n",
      "  0.519   0.5248  0.5306]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "'''\n",
    "around(arr,decimals=?)？表示保留多少位小数\n",
    "'''\n",
    "\n",
    "\n",
    "class Datanorm:\n",
    "    def __init__(self):\n",
    "        self.arr = np.array([3.8880657774004e-05,3.911735146961352e-05,3.935547427214615e-05,3.9595034658223214e-05,3.983604115299071e-05,4.0078502330276094e-05,4.032242681230033e-05,4.056782327105409e-05,4.08147004273422e-05,4.106306705171421e-05,4.131293196472811e-05,4.156430403710224e-05,4.181719218953348e-05,4.2071605393624246e-05,4.232755267180993e-05,4.258504309750736e-05,4.284408579615e-05,4.310468994411414e-05,4.336686477008516e-05,4.3630619554980406e-05,4.389596363209313e-05,4.416290638734643e-05,4.4431457259435226e-05,4.470162574152024e-05,4.49734213784841e-05,4.524685377039804e-05,4.552193257077433e-05,4.579866748825603e-05,4.607706828453511e-05,4.6357144778146974e-05,4.663890684105586e-05,4.6922364402114375e-05,4.720752744453417e-05,4.74944060086777e-05,4.77830101906356e-05,4.807335014335304e-05,4.836543607587029e-05,4.865927825533354e-05,4.895488700578967e-05,4.9252272708642935e-05,4.9551445803886e-05,4.985241678966738e-05,5.015519622241176e-05,5.045979471716097e-05,5.076622294868913e-05,5.107449165095444e-05,5.13846116168823e-05,5.1696593700473794e-05,5.201044881558893e-05,5.23261879363906e-05,5.264382209800877e-05,5.296336239676013e-05,5.328481999058783e-05,5.360820609861397e-05,5.3933532002462666e-05,5.426080904625261e-05,5.4590048636366816e-05,5.492126224210687e-05,5.52544613956814e-05,5.558965769407513e-05,5.592686279637704e-05,5.626608842697511e-05,5.660734637409948e-05,5.695064849135307e-05,5.729600669647328e-05,5.764343297330226e-05,5.799293937087752e-05,5.834453800440261e-05,5.869824105510951e-05,5.905406077078314e-05,5.941200946628446e-05,5.977209952340767e-05,6.013434339195295e-05,6.049875358847408e-05,6.08653426986752e-05,6.123412337604427e-05,6.16051083430293e-05,6.197831039088511e-05,6.235374238062464e-05,6.273141724230907e-05,6.311134797654816e-05,6.349354765389741e-05,6.387802941535917e-05,6.426480647354526e-05,6.465389211129438e-05,6.504529968382584e-05,6.543904261790584e-05,6.583513441244929e-05,6.62335886385666e-05,6.663441894093485e-05,6.703763903673512e-05,6.744326271646695e-05,6.785130384520212e-05,6.826177636118454e-05,6.867469427774272e-05,6.909007168243771e-05,6.95079227381976e-05,6.992826168323431e-05,7.035110283117927e-05,7.077646057221066e-05,7.120434937274237e-05,7.16347837750005e-05,7.206777839935804e-05,7.250334794269167e-05,7.294150717960724e-05,7.338227096322074e-05,7.38256542243914e-05,7.427167197216658e-05,7.4720339295659e-05,7.517167136227857e-05,7.562568341905265e-05,7.608239079339123e-05,7.654180889263553e-05,7.700395320470764e-05,7.746883929776427e-05,7.793648282227482e-05,7.840689950934554e-05,7.888010517224037e-05,7.935611570602337e-05,7.98349470890732e-05,8.031661538172729e-05,8.080113672734937e-05,8.128852735350396e-05,8.177880357081213e-05,8.227198177423027e-05,8.276807844322277e-05,8.326711014182155e-05,8.376909351912329e-05,8.427404530967306e-05,8.478198233351428e-05,8.529292149645568e-05,8.580687979165691e-05,8.632387429790562e-05,8.684392218119638e-05,8.736704069465353e-05,8.789324717944183e-05,8.842255906446199e-05,8.895499386648338e-05,8.949056919115397e-05,9.002930273290542e-05,9.057121227518492e-05,9.11163156911234e-05,9.166463094364986e-05,9.221617608549162e-05,9.277096926005128e-05,9.332902870128916e-05,9.38903727344823e-05,9.4455019775659e-05,9.502298833311972e-05,9.55942970065337e-05,9.616896448845128e-05,9.674700956317252e-05,9.732845110891091e-05,9.791330809566393e-05,9.850159958802754e-05,9.909334474382817e-05,9.968856281494916e-05,0.0001002872731477,0.000100889495183,0.0001014952484567,0.0001021045525997,0.0001027174273385,0.0001033338924955,0.0001039539679888,0.0001045776738334,0.0001052050301402,0.0001058360571181,0.0001064707750718,0.0001071092044048,0.0001077513656168,0.0001083972793075,0.0001090469661728,0.0001097004470087,0.0001103577427091,0.0001110188742675,0.0001116838627763,0.0001123527294278,0.0001130254955142,0.0001137021824271,0.0001143828116596,0.0001150674048043,0.0001157559835551,0.000116448569707,0.0001171451851564,0.000117845851901,0.0001185505920403,0.0001192594277762,0.0001199723814122,0.000120689475355,0.0001214107321131,0.0001221361742989,0.0001228658246271,0.0001235997059163,0.0001243378410881,0.0001250802531684,0.0001258269652865,0.0001265780006761,0.0001273333826754,0.0001280931347269,0.0001288572803776,0.0001296258432799,0.0001303988471908,0.0001311763159728,0.0001319582735935,0.0001327447441263,0.0001335357517505,0.0001343313207507,0.0001351314755178,0.0001359362405493,0.0001367456404485,0.0001375596999249,0.0001383784437956,0.0001392018969834,0.0001400300845179,0.0001408630315366,0.0001417007632832,0.0001425433051086,0.0001433906824714,0.0001442429209374,0.0001451000461793,0.0001459620839781,0.0001468290602218,0.0001477010009062,0.0001485779321349,0.0001494598801193,0.0001503468711787,0.0001512389317399,0.0001521360883383,0.0001530383676163,0.0001539457963253,0.0001548584013237,0.0001557762095784,0.0001566992481644,0.0001576275442649,0.0001585611251698,0.0001595000182788,0.0001604442510983,0.0001613938512428,0.0001623488464349,0.0001633092645052,0.0001642751333915,0.0001652464811397,0.0001662233359031,0.0001672057259428,0.0001681936796273,0.0001691872254317,0.0001701863919399,0.0001711912078415,0.0001722017019333,0.00017321790312,0.0001742398404117,0.0001752675429259,0.0001763010398859,0.0001773403606222,0.0001783855345699,0.0001794365912716,0.0001804935603739,0.0001815564716301,0.0001826253548981,0.0001837002401408,0.0001847811574259,0.0001858681369256,0.0001869612089165,0.0001880604037784,0.0001891657519955,0.0001902772841549,0.0001913950309471,0.0001925190231647,0.0001936492917028,0.0001947858675591,0.0001959287818319,0.0001970780657217,0.0001982337505295,0.0001993958676562,0.000200564448604,0.0002017395249733,0.0002029211284646,0.000204109290877,0.0002053040441076,0.0002065054201513,0.0002077134511005,0.0002089281691444,0.0002101496065676,0.0002113777957514,0.0002126127691721,0.0002138545593997,0.000215103199099,0.0002163587210283,0.0002176211580382,0.0002188905430711,0.0002201669091617,0.0002214502894352,0.0002227407171065,0.00022403822548,0.0002253428479496,0.0002266546179965,0.0002279735691891,0.0002292997351824,0.0002306331497171,0.0002319738466188\n",
    "])\n",
    "        self.x_max = self.arr.max()\n",
    "        self.x_min = self.arr.min()\n",
    "        self.x_mean = self.arr.mean()\n",
    "        self.x_std = self.arr.std()\n",
    "\n",
    "    def Min_MaxNorm(self):\n",
    "        arr = np.around(((self.arr - self.x_min) / (self.x_max - self.x_min)), decimals=4)\n",
    "        print(\"Min_Max标准化:{}\".format(arr))\n",
    "\n",
    "    def Z_ScoreNorm(self):\n",
    "        arr = np.around((self.arr - self.x_mean) / self.x_std, decimals=4)\n",
    "        print(\"Z_Score标准化:{}\".format(arr))\n",
    "\n",
    "    def Decimal_ScalingNorm(self):\n",
    "        power = 1\n",
    "        maxValue = self.x_max\n",
    "        while maxValue / 10 >= 1.0:\n",
    "            power += 1\n",
    "            maxValue /= 10\n",
    "        arr = np.around((self.arr / pow(10, power)), decimals=4)\n",
    "        print(f\"小数定标,power = {power},标准化:{arr}\")\n",
    "\n",
    "    def MeanNorm(self):\n",
    "        first_arr = np.around((self.arr-self.x_mean) / (self.x_max - self.x_min), decimals=4)\n",
    "        second_arr = np.around((self.arr - self.x_mean)/self.x_max, decimals=4)\n",
    "        print(\"均值归一法：\\n公式一:{}\\n公式二:{}\".format(first_arr, second_arr))\n",
    "\n",
    "    def Vector(self):\n",
    "        arr = np.around((self.arr/self.arr.sum()), decimals=4)\n",
    "        print(\"向量归一法:{}\".format(arr))\n",
    "\n",
    "    def exponeential(self):\n",
    "\n",
    "        first_arr = np.around(np.log10(self.arr) / np.log10(self.x_max), decimals=4)\n",
    "        #second_arr = np.around(np.exp(self.arr)/sum(np.exp(self.arr)), decimals=4)\n",
    "        #three_arr = np.around(1/(1+np.exp(-1*self.arr)), decimals=4)\n",
    "        # print(\"lg函数:{}\\nSoftmax函数:{}\\nSigmoid函数:{}\\n\".format(first_arr,second_arr,three_arr))\n",
    "        print(\"lg函数:{}\\n\".format(first_arr))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    a = Datanorm()\n",
    "    a.Min_MaxNorm()\n",
    "    a.Z_ScoreNorm()     # seems the best\n",
    "    #a.Decimal_ScalingNorm()\n",
    "    a.MeanNorm()\n",
    "    #a.Vector()\n",
    "    # a.exponeential()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. bp hook"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import torch  # torch version is 1.11.0\n",
    "def save_grad(name):\n",
    "    def hook(grad):\n",
    "        print(f\"name={name}, grad={grad}\")\n",
    "    return hook\n",
    "\n",
    "def bp_hook(module, grad_input, grad_output):\n",
    "    param_grad = list(module.parameters())[0].grad\n",
    "    print(f'gradient of the module inside bp_hook: {param_grad}')\n",
    "\n",
    "# net = torch.nn.Linear(4, 1, bias=False)\n",
    "# # net.register_full_backward_hook(bp_hook)\n",
    "# data = torch.ones(1, 4)\n",
    "# # net.register_full_backward_hook(bp_hook)\n",
    "# output = net(data)\n",
    "# # net.register_full_backward_hook(bp_hook)\n",
    "# net.register_hook(save_grad(\"net\"))\n",
    "# output.backward(retain_graph=True)\n",
    "# # net.register_full_backward_hook(bp_hook)\n",
    "\n",
    "# print(f'gradient of the module outside bp_hook: {list(net.parameters())[0].grad}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name=labda, grad=tensor([1., 0., 0.])\n",
      "name=alpha, grad=tensor([5., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "alpha = torch.tensor([1.,2.,3.],requires_grad=True)\n",
    "labda = torch.tensor([5.,6.,7.],requires_grad=True)\n",
    "condtn = torch.tensor([1.,1.,0.])\n",
    "\n",
    "\n",
    "def loss_fn(condtn,alpha,labda):\n",
    "    if condtn>0:\n",
    "        return torch.min(alpha*labda,torch.tensor([10.]))\n",
    "        # 被torch.min截断的grad=0(常数的导数当然是0，得不到更新)\n",
    "        # 因此没有必要计算？alpha*x的\n",
    "    else:\n",
    "        return alpha\n",
    "\n",
    "loss = torch.tensor([0.],requires_grad=True)\n",
    "for i in range(3):\n",
    "    loss = loss_fn(condtn[i],alpha[i],labda[i]) + loss\n",
    "\n",
    "alpha.register_hook(save_grad(\"alpha\"))\n",
    "labda.register_hook(save_grad(\"labda\"))\n",
    "\n",
    "loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. del对于bp的影响"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse:tensor([-0.1000, -0.1000, -0.1000])\n",
      "mse:tensor([0.5000, 1.7000, 2.3000])\n",
      "tensor([4., 4., 4.])\n"
     ]
    }
   ],
   "source": [
    "def custom_loss_fn(x, y):\n",
    "        diff = x - y\n",
    "        # mse = (diff * diff).mean()\n",
    "        mse = diff\n",
    "        # 设置中间变量，与x有关\n",
    "        print(f\"mse:{mse.detach()}\")\n",
    "        middle = x*3\n",
    "        mse = middle + mse\n",
    "        print(f\"mse:{mse.detach()}\")\n",
    "        # 以下del都不影响\n",
    "        del middle\n",
    "        del diff\n",
    "        del y\n",
    "        del x\n",
    "        return mse\n",
    "\n",
    "x = torch.tensor([0.2, 0.6, 0.8], requires_grad=True)\n",
    "y = torch.tensor([0.3, 0.7, 0.9], requires_grad=True)       # target data\n",
    "\n",
    "loss = custom_loss_fn(x, y)\n",
    "loss.backward(torch.ones_like(loss))        # torch.ones_like(loss)相当于sum\n",
    "print(x.grad)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
