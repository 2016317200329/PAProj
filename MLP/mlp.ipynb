{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2022/6/7 15:17\n",
    "# @Author  : Wang Yujia\n",
    "# @File    : mlp.ipynb\n",
    "# @Description : 搭一个基本的mlp，用于测试思路\n",
    "# @TODO: 画图细化一下training的流程：怎么用target data/ NN的规模/ batch size等\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0. what for\n",
    "1. 搭一个基本的mlp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Preparations\n",
    "## 1.1 global settings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "# nums of Gaussian kernels\n",
    "N_gaussians = 3\n",
    "\n",
    "# dataset划分\n",
    "batch_size = 10\n",
    "train_pct = 0.7\n",
    "vali_pct = 0.2\n",
    "test_pct = 0.1\n",
    "\n",
    "# train and optim.\n",
    "learning_rate = 0.01\n",
    "total_train_step = 0\n",
    "total_test_step = 0\n",
    "epoch = 5\n",
    "\n",
    "# from torchsummary import summary\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.utils.data\n",
    "from mydataset import *\n",
    "# from tensorboardX import SummaryWriter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 the data path\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# training data\n",
    "train_path = r\"../data/train\"\n",
    "# target data\n",
    "target_path = r\"../data/targets\"\n",
    "# data keys\n",
    "data_key_path = \"../data/target_datakey.csv\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Dataloader and Split\n",
    "1. DataLoader中的shuffer=True表示在每一次epoch中都打乱所有数据的顺序，然后以batch为单位从头到尾按顺序取用数据。这样的结果就是不同epoch中的数据都是乱序的,设置随机种子的作用就是让你的每一次训练都乱的一样，\n",
    "2. Split\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 设置随机数种子"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup_seed(7)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 读取data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = myDataset(train_path, target_path, data_key_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 产生index的乱序排列"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shuffled_indices = np.random.permutation(dataset.__len__())\n",
    "train_idx = shuffled_indices[:int(train_pct*dataset.__len__())]\n",
    "val_idx = shulled_indices[int(0.8*len(data)):]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "   product_id  bidincrement  bidfee  retail\n0    10009881          0.15    0.75  169.99\n1    10006115          0.15    0.75  499.99\n2    10007148          0.15    0.75  299.99\n3    10007263          0.15    0.75   89.99\n4    10010575          0.15    0.75   59.99",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>product_id</th>\n      <th>bidincrement</th>\n      <th>bidfee</th>\n      <th>retail</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10009881</td>\n      <td>0.15</td>\n      <td>0.75</td>\n      <td>169.99</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10006115</td>\n      <td>0.15</td>\n      <td>0.75</td>\n      <td>499.99</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10007148</td>\n      <td>0.15</td>\n      <td>0.75</td>\n      <td>299.99</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10007263</td>\n      <td>0.15</td>\n      <td>0.75</td>\n      <td>89.99</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10010575</td>\n      <td>0.15</td>\n      <td>0.75</td>\n      <td>59.99</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = myDataset(train_path, target_path, data_key_path)\n",
    "dataloader = DataLoader(dataset = dataset,batch_size = batch_size, shuffle=True, num_workers=0, drop_last=False, sampler=SubsetRandomSampler())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 Dataloader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "data = myDataset([GT_1_data_path, GT_2_data_path, target_output_head, data_key_path])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "train_size = int(train_percentage * data.__len__())\n",
    "vali_size = data.__len__() - train_size\n",
    "train_data, vali_data= torch.utils.data.random_split(data, [train_size, vali_size])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Desktop\\PROJ\\PAProj\\MLP\\mydataset.py:120: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # select target data with the key\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot change data-type for object array.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_17336\\630742423.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__getitem__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mD:\\Desktop\\PROJ\\PAProj\\MLP\\mydataset.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m    135\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    136\u001B[0m         \u001B[1;31m# transform P into array vector\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 137\u001B[1;33m         \u001B[0mp_1_i\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransform_P\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_data_i_1_df\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'P'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    138\u001B[0m         \u001B[0mp_2_i\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransform_P\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_data_i_2_df\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'P'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    139\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Desktop\\PROJ\\PAProj\\MLP\\mydataset.py\u001B[0m in \u001B[0;36mtransform_P\u001B[1;34m(self, str_p)\u001B[0m\n\u001B[0;32m     99\u001B[0m         \"\"\"\n\u001B[0;32m    100\u001B[0m         \u001B[0ma\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstr_p\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 101\u001B[1;33m         \u001B[0ma\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdtype\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfloat64\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    102\u001B[0m         \u001B[0ma_vec\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0ma\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mflatten\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    103\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0ma_vec\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\Lib\\site-packages\\numpy\\core\\_internal.py\u001B[0m in \u001B[0;36m_view_is_safe\u001B[1;34m(oldtype, newtype)\u001B[0m\n\u001B[0;32m    492\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    493\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mnewtype\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhasobject\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0moldtype\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhasobject\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 494\u001B[1;33m         \u001B[1;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Cannot change data-type for object array.\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    495\u001B[0m     \u001B[1;32mreturn\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    496\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: Cannot change data-type for object array."
     ]
    }
   ],
   "source": [
    "data.__getitem__(0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1.2 加载dataloader\n",
    "train_iter = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=False)\n",
    "vali_iter = DataLoader(vali_data, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# data keys (for target)\n",
    "import pandas as pd\n",
    "data_key_path = \"../data/targets/target_datakey.csv\"\n",
    "data_key_all = pd.read_csv(data_key_path)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "data_key_all = data_key_all.copy()\n",
    "data_key_all.loc[:,'product_id'] = data_key_all['product_id'].values.astype(\"int64\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10009881\n",
      "<class 'numpy.float64'>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "data_key_i = data_key_all.iloc[0,:]\n",
    "data_key_i = data_key_i.copy()\n",
    "data_key_i[0] = data_key_i[0].astype(\"int64\")\n",
    "print(int(data_key_i[0]))\n",
    "print(type(data_key_i[0]))\n",
    "print(data_key_i[0] == 10009881)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
