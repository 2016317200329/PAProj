{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2023/12/7\n",
    "# @Author  : Wang Yujia\n",
    "# @File    : mlp.ipynb\n",
    "# @Description : 无Conv层，只有MLP\n",
    "# @TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0. what for\n",
    "1. loss部分逻辑重写，添加`no grad`\n",
    "2. 添加hooks\n",
    "3. 使用pad_seq函数取代自己写的pad函数\n",
    "4. 效果：可以train虽然loss不下降，不会出现NaN的问题"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Preparations\n",
    "## 1.1 global settings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# nums of Gaussian kernels\n",
    "N_gaussians = 3\n",
    "\n",
    "# dataset划分\n",
    "batch_size = 10\n",
    "train_pct = 0.7\n",
    "vali_pct = 0.2\n",
    "test_pct = 0.1\n",
    "\n",
    "# train and optim.\n",
    "learning_rate = 1e-4\n",
    "lr_for_mu = 1e-2\n",
    "total_train_step = 0\n",
    "total_test_step = 0\n",
    "EPOCH_NUM = 5\n",
    "MIN_LOSS = 1e-7\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.utils.data\n",
    "from mydataset import *\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from visdom import Visdom\n",
    "from torchviz import make_dot\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import math\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 the data path\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# training data\n",
    "train_path = r\"../data/train\"\n",
    "# target data\n",
    "target_path = r\"../data/targets_5\"\n",
    "# data keys\n",
    "data_key_path = \"../data/target_datakey.csv\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Dataset and Dataloader\n",
    "1. DataLoader中的shuffer=True表示在每一次epoch中都打乱所有数据的顺序，然后以batch为单位从头到尾按顺序取用数据。这样的结果就是不同epoch中的数据都是乱序的,设置随机种子的作用就是让你的每一次训练都乱的一样，\n",
    "\n",
    "## 2.1 Dataset and spliting\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 设置随机数种子"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup_seed(7)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 读取data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "dataset = myDataset(train_path, target_path, data_key_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 产生index的乱序排列"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 807  305  455  939  508  594  835 1082  598 1102   46  761  841  141\n",
      "  407  334  253  500  734  936  698  446  907 1087 1009  140  463  547\n",
      " 1155  856   34 1156  703 1121  751  587 1100  509  473  128  788   97\n",
      "  471  385 1085  525  679 1135  284 1146  186  318 1088 1113    9  457\n",
      " 1047  451  569 1120 1074  326  377  809  109   98  620 1194  825  828\n",
      "   83  113  556  674  568  853  351  558   54  656  804 1149  101  344\n",
      "  851  544  955   40 1021  489  626  664  657  868 1169  151  179 1130\n",
      "  564  713  743 1014  966  861 1132  146  610  408  662  551  172 1020\n",
      "  982 1148  431  517  270  858  170  374  816  618  205   17   53 1003\n",
      "  263  857  716  843  498  228  339  725  752  278  649 1017  108  642\n",
      " 1195 1174   99  530  632  888  189  961  358 1078  663  757 1051  204\n",
      "  409  283  562   23  619  216  474  921  950 1162  123  785  769  621\n",
      "  262  586 1178  541  795   70 1189  396  171  845  168 1125  361  224\n",
      "  125  706  164  231  264   42  746  872  998  132 1035  430   38  522\n",
      "  880  325  285  367  870  418   13  166  513  901  617  740  680 1126\n",
      "   20  372   71 1038  355   60  220  110    4 1115  572  415  336  316\n",
      "  449  412 1066  768 1055  704  869   94   95 1163 1145  648  884  885\n",
      " 1170  848   58  207  306 1067  729   24  644  633  252  832  244   25\n",
      " 1094   63  222 1070  996 1063  130  153 1180  922  399  997   75  894\n",
      " 1048  308  280  934  721  199  820  134  289  259 1159    3  510  492\n",
      "  196  565  563   26  913  483 1141  653   61  623  984  652  609  739\n",
      "  824 1018  891  467  842    8  503  714 1015  447  783  992   74 1184\n",
      "   57  978  484  297 1039  217  681 1027  960  350 1129 1045 1183  906\n",
      "  813  660  266    6  379  526  611  138  163  701 1031 1050   43  733\n",
      "  720  319 1158 1153  877  294  432  531  578  523   19  426  830  927\n",
      "  781 1033 1013  420  137  488  185  920  702 1134 1124  454  327  317\n",
      "  799  655  878  699  981  806  953  750  148  591  915  731  937  365\n",
      "  557  397 1173  779 1185  459  232  389 1147 1011  158  315  256  863\n",
      "   96 1157  445  384  511  817  328  314  357  347  452  298  899  887\n",
      " 1034  839  778  747  249  766   15  159  696  585  360   48  478  330\n",
      "  893  112  338  307  528  422 1142  971  506  504 1049  791  395  669\n",
      "  540  127 1160  902  507  980  237  167  602  983   80  414  640  300\n",
      "  977  324 1117  215 1058   30  242  697  860  951  209  671  742 1072\n",
      "  435   27 1060  518  122 1086  462 1028 1118  603  929  732  722  493\n",
      "  202  403 1089 1179  362  271  466  641 1061  744  272  277 1177   12\n",
      "  597  668  210  651  952  147  442  299  115  375  635  767  748 1080\n",
      "  755  177  472  685   51  810 1079  571  388   22  650  965  423  938\n",
      " 1112  643 1193  142   50  666 1143   81  482  321  948  687 1138   66\n",
      "  909  534  738  417 1005   64  895  886  918  491  700  790   65  968\n",
      "  627  999  296  419  438  261  837  281  631  677  188  881  251  694\n",
      "  514  590  487  273  943 1073  448  411 1131  933 1191  935  149  218\n",
      "  239  718  192  191  945  754  665  705  595  926  708  582 1046  323\n",
      " 1026 1071  658  437  770  833  341    2  229 1098 1152   76 1103  371\n",
      "  180  889  577  736  758 1019 1105 1010  589 1167  812  956  882   37\n",
      "  353 1095  236  928  499  553   55  302  286  800  219  789  387  737\n",
      "  390  352  193  235 1110  480  567  116 1057  957 1106  890  777  120\n",
      "  815  990  773   62  667  453  515  570  223  613  221  673  794  465\n",
      "   52 1093  485 1140 1053  155  912  600 1023  248  441  975 1164  542\n",
      "  117  724  628  712  916  291 1041  381  954  200  596  976 1137 1187\n",
      "  691  505  348 1190  340  615  154 1104  879  995  287  322  819  818\n",
      "  796   82  793  464  797  756 1168  771  333   10  470   32  782  413\n",
      "  580  382  930  760 1133  840 1139  378   89  792  601  213  332  133\n",
      "  346  855    7  941  320 1044  187  972  552  304   31  831  684 1065\n",
      "  240  993  174  245   56  630  583 1025  145   90 1114  392  682  310\n",
      "  359   93  947 1024  376  243 1076   87  214  444  173  425 1040  118\n",
      "  119 1151  905    0  394  401  404  836 1091  162  114  670  150  798\n",
      "  875 1064  368  543  477 1128  532  625  967  512  801   72  258  624\n",
      "  234  135  616  516  866  802  592 1056  501  538  897  728  581  343\n",
      "  925 1002  293  364  883  659   14  686   86  370  165  131  335   59\n",
      "  709  676  354  959  424  292   41  329  614  429   79]\n"
     ]
    }
   ],
   "source": [
    "shuffled_indices = np.random.permutation(dataset.__len__())\n",
    "# shuffled_indices = np.arange(0,dataset.__len__())\n",
    "train_idx = shuffled_indices[:int(train_pct*dataset.__len__())]\n",
    "# train_idx = shuffled_indices\n",
    "tmp = int((train_pct+vali_pct)*dataset.__len__())\n",
    "val_idx = shuffled_indices[int(train_pct*dataset.__len__()):tmp]\n",
    "\n",
    "test_idx = shuffled_indices[tmp:]\n",
    "print(train_idx)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 根据这个乱序排列抽取dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Dataloader and collating\n",
    "1. 主要是对label数据进行collate\n",
    "    - 按照batch中的最大target data长度进行padding，padding with 0\n",
    "2. 返回的结果多一个batch dim,比如下面的`5`\n",
    "    - After collating:\n",
    "        - `torch.Size([5, 3, 300]),torch.Size([5, 87, 2])`\n",
    "        - `87`是最长的targets data长度"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def test_pad_sequence():\n",
    "    seq1 = torch.tensor([[ 2., 0.04761905], [3., 0.14285714], [4., 0.04761905]])\n",
    "    seq2 = torch.tensor([[ 1., 0.04761905]])\n",
    "    seq3 = torch.tensor([[ 3., 0.14285714], [4., 0.04761905]])\n",
    "    ls = list((seq1,seq2,seq3))\n",
    "    ls_length = torch.tensor([3,1,2])\n",
    "    ans = pad_sequence(ls,batch_first=True)\n",
    "    print(ans)\n",
    "    # seq_pak = pack_padded_sequence(ans,ls_length,batch_first=True,enforce_sorted=False)\n",
    "    # seq_unpacked, lens_unpacked = pad_packed_sequence(seq_pak, batch_first=True)\n",
    "    # lens_unpacked"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def my_collate_fn(data):\n",
    "# 这里的data是一个list， list的元素是元组: (self.data, self.label)\n",
    "# collate_fn的作用是把[(data, label),(data, label)...]转化成([data, data...],[label,label...])\n",
    "# 假设self.data的一个data的shape为(channels, length), 每一个channel的length相等,\n",
    "# data[索引到index(batch)][索引到data或者label][索引到channel]\n",
    "\n",
    "    data_list = []\n",
    "    target_list = []\n",
    "    data_len = len(data)      # 读进来的data batch的大小\n",
    "\n",
    "    batch = 0\n",
    "    while batch < data_len:\n",
    "        data_list.append(torch.tensor(data[batch][0]))\n",
    "        target_list.append(torch.tensor(data[batch][1]))\n",
    "        batch += 1\n",
    "\n",
    "    # pad with zeros\n",
    "    target_padded = pad_sequence(target_list,batch_first=True)\n",
    "\n",
    "    data_tensor = torch.stack(data_list).float()\n",
    "    target_tensor = target_padded.float()\n",
    "\n",
    "    return data_tensor, target_tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = dataset,batch_size = batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(train_idx), collate_fn = my_collate_fn)\n",
    "\n",
    "val_loader = DataLoader(dataset = dataset,batch_size = batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(val_idx),collate_fn = my_collate_fn)\n",
    "\n",
    "test_loader = DataLoader(dataset = dataset,batch_size = batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(test_idx),collate_fn = my_collate_fn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. The Net\n",
    "1. BatchNorm1d: The mean and std are calculated per-dimension over the mini-batches\n",
    "2."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "### BatchNorm2d测试\n",
    "def test_BN():\n",
    "    m = nn.BatchNorm2d(3, affine=False)  # affine: With Learnable Parameters or not\n",
    "    print('m:', m)\n",
    "    # The mean and std are calculated per-dimension over the mini-batches\n",
    "    input = torch.tensor([\n",
    "        [[1.,2.,3.,4.],[1.,2.,3.,4.],[-1.,-2.,-3.,-4.]],\n",
    "        [[0.,0.,0.,0.],[0.,0.,0.,0.],[0.,0.,0.,0.]]\n",
    "    ], requires_grad=True)\n",
    "\n",
    "    print('input:', input.shape)\n",
    "    input = input.unsqueeze(dim=2)\n",
    "    print('input:', input.shape)\n",
    "    output = m(input) # 归一化\n",
    "    print('output:', output.shape)\n",
    "    print('output:', output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-0.1690,  0.5071,  1.1832,  1.8593]],\n\n        [[-0.8452, -0.8452, -0.8452, -0.8452]]])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 复现batchNorm2d在input shape为3维的情况\n",
    "input = torch.tensor([[[1.,2.,3.,4.]],[[0.,0.,0.,0.]]])\n",
    "# print(input.shape)\n",
    "# torch.mean(input),torch.var(input,unbiased = False)\n",
    "(input-torch.mean(input))/ torch.sqrt(torch.var(input,unbiased = False))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.1 Init the weight\n",
    "- 设置网络初始权重: 不太work"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class model_param_init(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        assert isinstance(model, nn.Module), 'model not a class nn.Module'\n",
    "        self.net = model\n",
    "        self.initParam()\n",
    "\n",
    "    def initParam(self):\n",
    "        for param in self.net.parameters():\n",
    "            # nn.init.zeros_(param)\n",
    "            # nn.init.ones_(param)\n",
    "            # nn.init.normal_(param, mean=0, std=1)\n",
    "            # nn.init.uniform_(param, a=0, b=1)\n",
    "            # nn.init.constant_(param, val=1)   # 将所有权重初始化为1\n",
    "            # nn.init.eye_(param)  # 只能将二维的tensor初始化为单位矩阵\n",
    "            # nn.init.xavier_uniform_(param, gain=1)  # Glorot初始化  得到的张量是从-a——a中采用的\n",
    "            # nn.init.xavier_normal_(param, gain=1)   # 得到的张量是从0-std采样的\n",
    "            nn.init.kaiming_normal_(param, a=0, mode='fan_in', nonlinearity='relu') # he初始化方法\n",
    "            # nn.init.kaiming_uniform_(param)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- print网络每层结构"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "class PrintLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if(DEBUG):\n",
    "            print(\"This layer: \")\n",
    "            print(x)      #print(x.shape)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Sequential结构\n",
    "- 最后输出mu的时候求了mean，不太好？"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- hook_backward_fn: 输入端的grad，输出端的grad，这里称呼的**输入与输出是站在前向传播的角度的**。如果模块有多个输入与输出的话， 其`grad_input`和`grad_output`可以是tuple类型。\n",
    "- 与forward不同的是，backward传播的时候，**不仅反向传递input和output的grad，还会传递模块Parameter的grad**：\n",
    "    - 比如fc模块，其`grad_input`是一个三元组的tuple，（对bias的梯度，对输入的梯度，对w的梯度）；\n",
    "    - conv模块`grad_input`也是一个三元组tuple，为（对输入的梯度，对w的梯度，对bias的梯度"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# hook functions have to take these 3 input\n",
    "def hook_forward_fn(module, input, output):\n",
    "    print(\"It's forward: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"input: {input}\")\n",
    "    print(f\"output: {output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn(module, grad_input, grad_output):\n",
    "    print(\"It's backward: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn_pi(module, grad_input, grad_output):\n",
    "    print(\"It's backward in pi: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn_sigma(module, grad_input, grad_output):\n",
    "    print(\"It's backward in sigma: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn_mu(module, grad_input, grad_output):\n",
    "    print(\"It's backward in mu: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 MLP结构\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# Not Sequential\n",
    "class MLP(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "        self.BN = nn.BatchNorm1d(num_features=3,affine=False)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.ac_func = nn.Softplus()\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "\n",
    "        self.linear00 = nn.Linear(900,300)\n",
    "        self.linear01 = nn.Linear(300,30)\n",
    "        self.linear1 = nn.Linear(30, 9)\n",
    "\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(9, n_gaussians),  # 30个params要learn\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(9, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(9, n_gaussians)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 加一个height维度\n",
    "        # x.unsqueeze_(dim=2)\n",
    "        x = self.BN(x)\n",
    "        # x.squeeze_()\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear00(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        x = self.linear01(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "        # print(\"after linear1, the output shape is: \",x.shape)\n",
    "\n",
    "        pi = self.z_pi(x)\n",
    "        mu = self.z_mu(x)\n",
    "        sigma = torch.exp(self.z_sigma(x))\n",
    "\n",
    "        return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input's shape is torch.Size([2, 3, 1, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": "torch.Size([2, 12])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([\n",
    "    [[1.,2.,3.,4.],[1.,2.,3.,4.],[-1.,-2.,-3.,-4.]],\n",
    "    [[0.,0.,0.,0.],[0.,0.,0.,0.],[0.,0.,0.,0.]]\n",
    "], requires_grad=True)\n",
    "input = input.unsqueeze(dim=2)\n",
    "print(f\"input's shape is {input.shape}\")\n",
    "flt = nn.Flatten(start_dim=1)\n",
    "flt(input).shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. The Loss\n",
    "- `loss_preparation`用来做loss的前期data准备：\n",
    "    - 计算混合模型的分布`m`以及target data中的`duration`\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# 当input的shape是[50,3]时，输出应该是50个GMM\n",
    "def loss_preparation(pi, mu, sigma, target):\n",
    "\n",
    "    m=[]\n",
    "    for i in range(pi.shape[0]):\n",
    "        # m.append(torch.distributions.Normal(loc=mu[i,:].T, scale=sigma[i,:].T))\n",
    "        m.append(torch.distributions.Normal(loc=mu[i,:], scale=sigma[i,:]))\n",
    "\n",
    "    # target_packed = pack_padded_sequence(target,target_len,batch_first=True,enforce_sorted=False)  # 去掉padded 0并拉成一个vector\n",
    "    # seq_unpacked, lens_unpacked = pad_packed_sequence(target_packed, batch_first=True)             # 和上面互为逆运算\n",
    "\n",
    "    duration = target[:,:,0]\n",
    "\n",
    "    return duration,m"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# # 当input的shape是[50,3]时，输出应该是50个GMM\n",
    "# # 对这50个GMM看能生成什么output\n",
    "\n",
    "def loss_fn(Pi,duration,m,N_gaussians):\n",
    "    loss_list = []\n",
    "    with torch.no_grad():\n",
    "        # for each GMM\n",
    "        # 后期肯定要矩阵化这个计算！\n",
    "        for i in range(len(m)):\n",
    "            target = duration[i,:]\n",
    "            pi = Pi[i,:]\n",
    "\n",
    "            # Drop all zero data and Expanded to the same dim\n",
    "            target_nonzero = target[torch.nonzero(target)].squeeze_()\n",
    "            target_nonzero = torch.repeat_interleave(target_nonzero.unsqueeze(dim=1), repeats=N_gaussians, dim=1).to(device)\n",
    "            len_target = len(target_nonzero)       # The length of target data\n",
    "\n",
    "            # loss_1 是高斯分布的概率密度value\n",
    "            loss_1 = torch.exp(m[i].log_prob(target_nonzero))\n",
    "\n",
    "            # loss_2 是MDN的概率密度value\n",
    "            loss_2 = torch.sum(loss_1 * pi, dim=1)\n",
    "\n",
    "            # loss_3 是值非0的概率密度value\n",
    "            loss_3 = loss_2[torch.nonzero(loss_2)].view(-1,1)                # 再去掉所有的log(0)\n",
    "\n",
    "            # 如果loss_2全是0则赋值为1e-40，否则赋值为loss的最小值\n",
    "            MIN_LOSS = torch.min(loss_3) if torch.min(loss_2)>0 else 1e-40\n",
    "            loss_padded = [MIN_LOSS]* (loss_2.shape[0]-loss_3.shape[0])\n",
    "            loss_padded_ts = torch.tensor(loss_padded,device=device)\n",
    "\n",
    "            # loss_4是用MIN_LOSS填充后的loss_3，和loss_2等长\n",
    "            loss_4 = torch.cat((loss_3[:,0],loss_padded_ts))\n",
    "            torch._assert(len(loss_4) == len(loss_2),\"cat不正确\")\n",
    "\n",
    "            # loss_5是log likelihood loss\n",
    "            loss_5 = torch.log(loss_4)\n",
    "            #loss_5 = torch.log(loss_4*len_target)\n",
    "            #loss_5 = torch.log(loss_4)*len_target   # loss值会比较大，不建议用\n",
    "            loss_list.append(-torch.mean((loss_5)).item())\n",
    "\n",
    "        # 最后处理loss\n",
    "        loss = np.sum(loss_list)/ len(m)\n",
    "\n",
    "    loss_ts = torch.tensor(loss,device=device,requires_grad=True)\n",
    "    return loss_ts,loss_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def loss_test():\n",
    "    loss_2 = torch.tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "    loss_3 = loss_2[torch.nonzero(loss_2)].view(-1,1)         # 去掉所有的log(0)\n",
    "    print(\"loss_3:\",loss_3.shape)\n",
    "\n",
    "    MIN_LOSS = torch.min(loss_3) if torch.min(loss_2)>0 else 1e-20\n",
    "\n",
    "    print(\"MIN_LOSS：\",MIN_LOSS)\n",
    "    loss_padded = [MIN_LOSS]* (loss_2.shape[0]-loss_3.shape[0])\n",
    "    loss_padded_ts = torch.tensor(loss_padded) # ,device=device\n",
    "    loss_3 = torch.cat((loss_3[:,0],loss_padded_ts))\n",
    "    loss_3.shape, torch.log(loss_3)\n",
    "    print(\"data: \",loss_3.data)\n",
    "    print(\"data: \",type(loss_3))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Training\n",
    "## 5.1 preparations\n",
    "1. 初始化Visdom环境\n",
    "2.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# writer = SummaryWriter(\"logs-MLP\")\n",
    "viz = Visdom(env=\"001\")\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.2 Plot\n",
    "1. draw:\n",
    "    - loss\n",
    "    - MLP的网络结构（.png）\n",
    "    - target distrb.和pred. distrb.\n",
    "    - 所有target data的分布图"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def draw_all_target_data():\n",
    "    # target data\n",
    "    target_path = r\"../data/targets\"\n",
    "    # target_path里有全部的target data地址\n",
    "    target_all_path = os.listdir(target_path)\n",
    "    len_target = len(target_all_path)\n",
    "    print(f\"一共有 *{len_target}* 组 target data\")\n",
    "\n",
    "    # Init\n",
    "    # viz_env_str = \"target_bar\"\n",
    "    # viz_env_str = \"target_distrb\"\n",
    "    # viz_env_str = \"target_hist\"\n",
    "    viz_env_str = \"target_scatter\"\n",
    "    viz = Visdom(env = viz_env_str)\n",
    "\n",
    "    for i in range(len_target):\n",
    "        target_path_i_path = os.path.join(target_path,target_all_path[i])\n",
    "        target_df = pd.read_csv(target_path_i_path,encoding=\"utf-8\")\n",
    "\n",
    "        # Init\n",
    "        win_str = str(target_all_path[i])\n",
    "        title_str = \"Target Distrb. of \"+win_str\n",
    "\n",
    "        if(i < 100):\n",
    "            # Hist plot which is not that great\n",
    "            # viz.histogram(X = np.array(target_df.N), env=viz_env_str, win=win_str, opts= dict(title=title_str))\n",
    "\n",
    "            # Line plot\n",
    "            # target_df.drop_duplicates(inplace=True)\n",
    "            # viz.line(X = np.array(target_df.N),Y= np.array(target_df.P), env=viz_env_str, win=win_str,opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "            # Bar plot. 这个看起来是最准确的一个\n",
    "            # target_df.drop_duplicates(inplace=True)\n",
    "            # arr_str = [str(i) for i in np.array(target_df.N)]\n",
    "            # viz.bar(X=np.array(target_df.P), env=viz_env_str, win=win_str,\n",
    "            #         opts=dict(title=title_str,rownames=arr_str))\n",
    "\n",
    "            # Scatter plot\n",
    "            target_df.drop_duplicates(inplace=True)\n",
    "            viz.scatter(X=np.array(target_df),env=viz_env_str, win=win_str,opts=dict(title=title_str,markersize = 3))\n",
    "    print(\"Done\")\n",
    "\n",
    "\n",
    "# draw_all_target_data()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "#### Test for drawing\n",
    "def test_draw():\n",
    "    viz = Visdom(env=\"001\")\n",
    "\n",
    "    mu = torch.tensor([0,10,20])\n",
    "    sigma = torch.tensor([1,1,1])\n",
    "    duration = torch.tensor([0,1,2,0])\n",
    "    duration = torch.repeat_interleave(duration.unsqueeze(dim=1), repeats=3, dim=1)\n",
    "    m = torch.distributions.Normal(loc=mu, scale=sigma)\n",
    "    pi = torch.tensor([0.2,0.3,0.5])\n",
    "\n",
    "    # draw\n",
    "    x_0 = torch.tensor(np.arange(0,1000))\n",
    "    x = torch.repeat_interleave(x_0.unsqueeze(dim=1), repeats=3, dim=1)\n",
    "    y = torch.exp(m.log_prob(x))\n",
    "    y_sum = torch.unsqueeze(torch.sum(pi*y,dim=1),dim=1)\n",
    "    viz.line(X = x_0,Y= torch.cat([y,y_sum],dim = 1), env=\"001\", win=\"test_draw_2\",\n",
    "            opts= dict(title='test_draw', legend=['N1', 'N2', 'N3','NNN']))\n",
    "# test_draw()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def draw_mdn(pi, m, target, total_train_step,N_gaussians):\n",
    "    # 只画一个batch中的第一个\n",
    "\n",
    "    # The target distrb.\n",
    "    n_target = target[:,0]\n",
    "    n = n_target[torch.nonzero(n_target)]\n",
    "    # Auctions amount\n",
    "    auction_num = len(n_target)\n",
    "    p_target = target[:,1]\n",
    "    p = p_target[torch.nonzero(p_target)]*auction_num\n",
    "    max_n = max(n).item()           # 横轴长度\n",
    "\n",
    "    # The predicted distrb.\n",
    "    x_0 = torch.arange(0,max_n).to(device=device)\n",
    "    x = torch.repeat_interleave(x_0.unsqueeze(dim=1), repeats=N_gaussians, dim=1)\n",
    "    y = torch.exp(m.log_prob(x)).to(device=device)                       # y:多条高斯曲线; y_pred: 一条GMM曲线\n",
    "    # y_pred = torch.unsqueeze(torch.sum(pi*y,dim=1),dim=1)                # 维度相等才能cat\n",
    "    y_pred = torch.sum(pi*y,dim=1)\n",
    "    y_pred = y_pred*auction_num    # 乘auction_num\n",
    "\n",
    "    # Init\n",
    "    win_str = \"total_train_step = \"+str(total_train_step)\n",
    "    title_str = \"Distrb. in \"+win_str\n",
    "    viz.line(X = [0.],Y = [0.], env=\"001\", win=win_str, opts= dict(title=title_str))\n",
    "\n",
    "    # Visdom本身不能把hist和line画在一个window中\n",
    "    # 如果想画一起只能是两条line\n",
    "    # Plot y_target\n",
    "    # viz.histogram(X = n, env=\"001\", win=win_str,\n",
    "    #         opts= dict(title=title_str))\n",
    "    viz.line(X = n,Y= p, env=\"001\", win=win_str, update=\"append\", name='target',\n",
    "            opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "    # Plot y_pred\n",
    "    viz.line(X = x_0,Y= y_pred, env=\"001\", win=win_str, update=\"append\", name='pred',\n",
    "            opts= dict(title=title_str))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def draw_the_net_png():\n",
    "\n",
    "    x = torch.randn([5, 3, 300])  # 定义网络的输入值\n",
    "    mlp = MLP(N_gaussians)\n",
    "    y = mlp(x)                    # 获取网络的预测值\n",
    "\n",
    "    MyConvNetVis = make_dot(y, params=dict(list(mlp.named_parameters()) + [('x', x)]))\n",
    "    MyConvNetVis.format = \"png\"\n",
    "    # 指定文件生成的文件夹\n",
    "    MyConvNetVis.directory = \"data_pic\"\n",
    "    # 生成文件\n",
    "    MyConvNetVis.view()\n",
    "# draw_the_net_png()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "win_train_loss_str = \"The Loss of BATCH in the Training Data\"\n",
    "win_vali_loss_str = \"The Loss in the Vali Data\"\n",
    "win_train_epoch_loss_str = \"The Loss of EPOCH in the Training Data\"\n",
    "\n",
    "def draw_loss(X_step, loss, win_str):\n",
    "    viz.line(X = [X_step], Y = [loss],win=win_str, update=\"append\",\n",
    "        opts= dict(title=win_str))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.3 Training\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "       BatchNorm1d-1               [-1, 3, 300]               0\n",
      "           Flatten-2                  [-1, 900]               0\n",
      "            Linear-3                  [-1, 300]         270,300\n",
      "           Dropout-4                  [-1, 300]               0\n",
      "          Softplus-5                  [-1, 300]               0\n",
      "            Linear-6                   [-1, 30]           9,030\n",
      "           Dropout-7                   [-1, 30]               0\n",
      "          Softplus-8                   [-1, 30]               0\n",
      "            Linear-9                    [-1, 9]             279\n",
      "          Dropout-10                    [-1, 9]               0\n",
      "         Softplus-11                    [-1, 9]               0\n",
      "           Linear-12                    [-1, 3]              30\n",
      "          Softmax-13                    [-1, 3]               0\n",
      "           Linear-14                    [-1, 3]              30\n",
      "           Linear-15                    [-1, 3]              30\n",
      "================================================================\n",
      "Total params: 279,699\n",
      "Trainable params: 279,699\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 1.07\n",
      "Estimated Total Size (MB): 1.09\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP(N_gaussians)\n",
    "\n",
    "# Init the params\n",
    "# # mlp = model_param_init(mlp)\n",
    "# Init the vis\n",
    "viz.line(X = [0.],Y = [0.], env=\"001\", win=win_train_loss_str, opts= dict(title=win_train_loss_str))\n",
    "viz.line(X = [0.],Y = [0.], env=\"001\", win=win_vali_loss_str, opts= dict(title=win_vali_loss_str))\n",
    "viz.line(X = [0.],Y = [0.], env=\"001\", win=win_train_epoch_loss_str, opts= dict(title=win_train_epoch_loss_str))\n",
    "\n",
    "# Save the init params\n",
    "torch.save(mlp.state_dict(), 'mlp_init.pth')\n",
    "\n",
    "# Read the saved model\n",
    "# model_data = torch.load('mlp_init_loss_17.pth')\n",
    "# mlp.load_state_dict(model_data)\n",
    "\n",
    "mlp = mlp.to(device=device)\n",
    "summary(mlp, (3,300))\n",
    "# optimizer = torch.optim.Adagrad(mlp.parameters(),lr=learning_rate, lr_decay=learning_rate, weight_decay=learning_rate)\n",
    "\n",
    "# Set different lr for params\n",
    "# id: id() 函数返回对象的唯一标识符，标识符是一个整数。返回对象的内存地址。\n",
    "mu_params = list(map(id, mlp.z_mu.parameters()))\n",
    "base_params = filter(lambda p: id(p) not in mu_params, mlp.parameters())\n",
    "params = [{'params': base_params},         # 如果对某个参数不指定学习率，就使用最外层的默认学习率\n",
    "        {'params': mlp.z_mu.parameters(), 'lr': learning_rate * 100}]\n",
    "# optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "optimizer = torch.optim.Adagrad(params,lr=learning_rate, lr_decay=learning_rate, weight_decay=learning_rate)\n",
    "\n",
    "# # hooks\n",
    "# #mlp.conv1.register_forward_hook(hook_forward_fn)\n",
    "# mlp.conv1.register_full_backward_hook(hook_backward_fn)\n",
    "#\n",
    "# #mlp.linear1.register_forward_hook(hook_forward_fn)\n",
    "# #mlp.linear1.register_full_backward_hook(hook_backward_fn)\n",
    "#\n",
    "# #mlp.z_pi.register_forward_hook(hook_forward_fn)\n",
    "# mlp.z_pi.register_full_backward_hook(hook_backward_fn_pi)\n",
    "#\n",
    "# #mlp.z_mu.register_forward_hook(hook_forward_fn)\n",
    "# mlp.z_mu.register_full_backward_hook(hook_backward_fn_mu)\n",
    "#\n",
    "# #mlp.z_sigma.register_forward_hook(hook_forward_fn)\n",
    "# mlp.z_sigma.register_full_backward_hook(hook_backward_fn_sigma)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- 0 batch----\n",
      "---- 1 batch----\n",
      "---- 2 batch----\n",
      "---- 3 batch----\n",
      "---- 4 batch----\n",
      "---- 5 batch----\n",
      "---- 6 batch----\n",
      "---- 7 batch----\n",
      "---- 8 batch----\n",
      "---- 9 batch----\n",
      "---- 10 batch----\n",
      "---- 11 batch----\n",
      "---- 12 batch----\n",
      "---- 13 batch----\n",
      "---- 14 batch----\n",
      "---- 15 batch----\n",
      "---- 16 batch----\n",
      "---- 17 batch----\n",
      "---- 18 batch----\n",
      "---- 19 batch----\n",
      "---- 20 batch----\n",
      "---- 21 batch----\n",
      "---- 22 batch----\n",
      "---- 23 batch----\n",
      "---- 24 batch----\n",
      "---- 25 batch----\n",
      "---- 26 batch----\n",
      "---- 27 batch----\n",
      "---- 28 batch----\n",
      "---- 29 batch----\n",
      "---- 30 batch----\n",
      "---- 31 batch----\n",
      "---- 32 batch----\n",
      "---- 33 batch----\n",
      "---- 34 batch----\n",
      "---- 35 batch----\n",
      "---- 36 batch----\n",
      "---- 37 batch----\n",
      "---- 38 batch----\n",
      "---- 39 batch----\n",
      "---- 40 batch----\n",
      "---- 41 batch----\n",
      "---- 42 batch----\n",
      "---- 43 batch----\n",
      "---- 44 batch----\n",
      "---- 45 batch----\n",
      "---- 46 batch----\n",
      "---- 47 batch----\n",
      "---- 48 batch----\n",
      "---- 49 batch----\n",
      "---- 50 batch----\n",
      "---- 51 batch----\n",
      "---- 52 batch----\n",
      "---- 53 batch----\n",
      "---- 54 batch----\n",
      "---- 55 batch----\n",
      "---- 56 batch----\n",
      "---- 57 batch----\n",
      "---- 58 batch----\n",
      "---- 59 batch----\n",
      "---- 60 batch----\n",
      "---- 61 batch----\n",
      "---- 62 batch----\n",
      "---- 63 batch----\n",
      "---- 64 batch----\n",
      "---- 65 batch----\n",
      "---- 66 batch----\n",
      "---- 67 batch----\n",
      "---- 68 batch----\n",
      "---- 69 batch----\n",
      "---- 70 batch----\n",
      "---- 71 batch----\n",
      "---- 72 batch----\n",
      "---- 73 batch----\n",
      "---- 74 batch----\n",
      "---- 75 batch----\n",
      "---- 76 batch----\n",
      "---- 77 batch----\n",
      "---- 78 batch----\n",
      "---- 79 batch----\n",
      "---- 80 batch----\n",
      "---- 81 batch----\n",
      "---- 82 batch----\n",
      "---- 83 batch----\n",
      "========== IN EPOCH 0 the total loss is 6645.3306920869 ==========\n",
      "---- 0 batch----\n",
      "---- 1 batch----\n",
      "---- 2 batch----\n",
      "---- 3 batch----\n",
      "---- 4 batch----\n",
      "---- 5 batch----\n",
      "---- 6 batch----\n",
      "---- 7 batch----\n",
      "---- 8 batch----\n",
      "---- 9 batch----\n",
      "---- 10 batch----\n",
      "---- 11 batch----\n",
      "---- 12 batch----\n",
      "---- 13 batch----\n",
      "---- 14 batch----\n",
      "---- 15 batch----\n",
      "---- 16 batch----\n",
      "---- 17 batch----\n",
      "---- 18 batch----\n",
      "---- 19 batch----\n",
      "---- 20 batch----\n",
      "---- 21 batch----\n",
      "---- 22 batch----\n",
      "---- 23 batch----\n",
      "---- 24 batch----\n",
      "---- 25 batch----\n",
      "---- 26 batch----\n",
      "---- 27 batch----\n",
      "---- 28 batch----\n",
      "---- 29 batch----\n",
      "---- 30 batch----\n",
      "---- 31 batch----\n",
      "---- 32 batch----\n",
      "---- 33 batch----\n",
      "---- 34 batch----\n",
      "---- 35 batch----\n",
      "---- 36 batch----\n",
      "---- 37 batch----\n",
      "---- 38 batch----\n",
      "---- 39 batch----\n",
      "---- 40 batch----\n",
      "---- 41 batch----\n",
      "---- 42 batch----\n",
      "---- 43 batch----\n",
      "---- 44 batch----\n",
      "---- 45 batch----\n",
      "---- 46 batch----\n",
      "---- 47 batch----\n",
      "---- 48 batch----\n",
      "---- 49 batch----\n",
      "---- 50 batch----\n",
      "---- 51 batch----\n",
      "---- 52 batch----\n",
      "---- 53 batch----\n",
      "---- 54 batch----\n",
      "---- 55 batch----\n",
      "---- 56 batch----\n",
      "---- 57 batch----\n",
      "---- 58 batch----\n",
      "---- 59 batch----\n",
      "---- 60 batch----\n",
      "---- 61 batch----\n",
      "---- 62 batch----\n",
      "---- 63 batch----\n",
      "---- 64 batch----\n",
      "---- 65 batch----\n",
      "---- 66 batch----\n",
      "---- 67 batch----\n",
      "---- 68 batch----\n",
      "---- 69 batch----\n",
      "---- 70 batch----\n",
      "---- 71 batch----\n",
      "---- 72 batch----\n",
      "---- 73 batch----\n",
      "---- 74 batch----\n",
      "---- 75 batch----\n",
      "---- 76 batch----\n",
      "---- 77 batch----\n",
      "---- 78 batch----\n",
      "---- 79 batch----\n",
      "---- 80 batch----\n",
      "---- 81 batch----\n",
      "---- 82 batch----\n",
      "---- 83 batch----\n",
      "========== IN EPOCH 1 the total loss is 6641.416961819783 ==========\n",
      "---- 0 batch----\n",
      "---- 1 batch----\n",
      "---- 2 batch----\n",
      "---- 3 batch----\n",
      "---- 4 batch----\n",
      "---- 5 batch----\n",
      "---- 6 batch----\n",
      "---- 7 batch----\n",
      "---- 8 batch----\n",
      "---- 9 batch----\n",
      "---- 10 batch----\n",
      "---- 11 batch----\n",
      "---- 12 batch----\n",
      "---- 13 batch----\n",
      "---- 14 batch----\n",
      "---- 15 batch----\n",
      "---- 16 batch----\n",
      "---- 17 batch----\n",
      "---- 18 batch----\n",
      "---- 19 batch----\n",
      "---- 20 batch----\n",
      "---- 21 batch----\n",
      "---- 22 batch----\n",
      "---- 23 batch----\n",
      "---- 24 batch----\n",
      "---- 25 batch----\n",
      "---- 26 batch----\n",
      "---- 27 batch----\n",
      "---- 28 batch----\n",
      "---- 29 batch----\n",
      "---- 30 batch----\n",
      "---- 31 batch----\n",
      "---- 32 batch----\n",
      "---- 33 batch----\n",
      "---- 34 batch----\n",
      "---- 35 batch----\n",
      "---- 36 batch----\n",
      "---- 37 batch----\n",
      "---- 38 batch----\n",
      "---- 39 batch----\n",
      "---- 40 batch----\n",
      "---- 41 batch----\n",
      "---- 42 batch----\n",
      "---- 43 batch----\n",
      "---- 44 batch----\n",
      "---- 45 batch----\n",
      "---- 46 batch----\n",
      "---- 47 batch----\n",
      "---- 48 batch----\n",
      "---- 49 batch----\n",
      "---- 50 batch----\n",
      "---- 51 batch----\n",
      "---- 52 batch----\n",
      "---- 53 batch----\n",
      "---- 54 batch----\n",
      "---- 55 batch----\n",
      "---- 56 batch----\n",
      "---- 57 batch----\n",
      "---- 58 batch----\n",
      "---- 59 batch----\n",
      "---- 60 batch----\n",
      "---- 61 batch----\n",
      "---- 62 batch----\n",
      "---- 63 batch----\n",
      "---- 64 batch----\n",
      "---- 65 batch----\n",
      "---- 66 batch----\n",
      "---- 67 batch----\n",
      "---- 68 batch----\n",
      "---- 69 batch----\n",
      "---- 70 batch----\n",
      "---- 71 batch----\n",
      "---- 72 batch----\n",
      "---- 73 batch----\n",
      "---- 74 batch----\n",
      "---- 75 batch----\n",
      "---- 76 batch----\n",
      "---- 77 batch----\n",
      "---- 78 batch----\n",
      "---- 79 batch----\n",
      "---- 80 batch----\n",
      "---- 81 batch----\n",
      "---- 82 batch----\n",
      "---- 83 batch----\n",
      "========== IN EPOCH 2 the total loss is 6651.101894075527 ==========\n",
      "---- 0 batch----\n",
      "---- 1 batch----\n",
      "---- 2 batch----\n",
      "---- 3 batch----\n",
      "---- 4 batch----\n",
      "---- 5 batch----\n",
      "---- 6 batch----\n",
      "---- 7 batch----\n",
      "---- 8 batch----\n",
      "---- 9 batch----\n",
      "---- 10 batch----\n",
      "---- 11 batch----\n",
      "---- 12 batch----\n",
      "---- 13 batch----\n",
      "---- 14 batch----\n",
      "---- 15 batch----\n",
      "---- 16 batch----\n",
      "---- 17 batch----\n",
      "---- 18 batch----\n",
      "---- 19 batch----\n",
      "---- 20 batch----\n",
      "---- 21 batch----\n",
      "---- 22 batch----\n",
      "---- 23 batch----\n",
      "---- 24 batch----\n",
      "---- 25 batch----\n",
      "---- 26 batch----\n",
      "---- 27 batch----\n",
      "---- 28 batch----\n",
      "---- 29 batch----\n",
      "---- 30 batch----\n",
      "---- 31 batch----\n",
      "---- 32 batch----\n",
      "---- 33 batch----\n",
      "---- 34 batch----\n",
      "---- 35 batch----\n",
      "---- 36 batch----\n",
      "---- 37 batch----\n",
      "---- 38 batch----\n",
      "---- 39 batch----\n",
      "---- 40 batch----\n",
      "---- 41 batch----\n",
      "---- 42 batch----\n",
      "---- 43 batch----\n",
      "---- 44 batch----\n",
      "---- 45 batch----\n",
      "---- 46 batch----\n",
      "---- 47 batch----\n",
      "---- 48 batch----\n",
      "---- 49 batch----\n",
      "---- 50 batch----\n",
      "---- 51 batch----\n",
      "---- 52 batch----\n",
      "---- 53 batch----\n",
      "---- 54 batch----\n",
      "---- 55 batch----\n",
      "---- 56 batch----\n",
      "---- 57 batch----\n",
      "---- 58 batch----\n",
      "---- 59 batch----\n",
      "---- 60 batch----\n",
      "---- 61 batch----\n",
      "---- 62 batch----\n",
      "---- 63 batch----\n",
      "---- 64 batch----\n",
      "---- 65 batch----\n",
      "---- 66 batch----\n",
      "---- 67 batch----\n",
      "---- 68 batch----\n",
      "---- 69 batch----\n",
      "---- 70 batch----\n",
      "---- 71 batch----\n",
      "---- 72 batch----\n",
      "---- 73 batch----\n",
      "---- 74 batch----\n",
      "---- 75 batch----\n",
      "---- 76 batch----\n",
      "---- 77 batch----\n",
      "---- 78 batch----\n",
      "---- 79 batch----\n",
      "---- 80 batch----\n",
      "---- 81 batch----\n",
      "---- 82 batch----\n",
      "---- 83 batch----\n",
      "========== IN EPOCH 3 the total loss is 6641.330016211102 ==========\n",
      "---- 0 batch----\n",
      "---- 1 batch----\n",
      "---- 2 batch----\n",
      "---- 3 batch----\n",
      "---- 4 batch----\n",
      "---- 5 batch----\n",
      "---- 6 batch----\n",
      "---- 7 batch----\n",
      "---- 8 batch----\n",
      "---- 9 batch----\n",
      "---- 10 batch----\n",
      "---- 11 batch----\n",
      "---- 12 batch----\n",
      "---- 13 batch----\n",
      "---- 14 batch----\n",
      "---- 15 batch----\n",
      "---- 16 batch----\n",
      "---- 17 batch----\n",
      "---- 18 batch----\n",
      "---- 19 batch----\n",
      "---- 20 batch----\n",
      "---- 21 batch----\n",
      "---- 22 batch----\n",
      "---- 23 batch----\n",
      "---- 24 batch----\n",
      "---- 25 batch----\n",
      "---- 26 batch----\n",
      "---- 27 batch----\n",
      "---- 28 batch----\n",
      "---- 29 batch----\n",
      "---- 30 batch----\n",
      "---- 31 batch----\n",
      "---- 32 batch----\n",
      "---- 33 batch----\n",
      "---- 34 batch----\n",
      "---- 35 batch----\n",
      "---- 36 batch----\n",
      "---- 37 batch----\n",
      "---- 38 batch----\n",
      "---- 39 batch----\n",
      "---- 40 batch----\n",
      "---- 41 batch----\n",
      "---- 42 batch----\n",
      "---- 43 batch----\n",
      "---- 44 batch----\n",
      "---- 45 batch----\n",
      "---- 46 batch----\n",
      "---- 47 batch----\n",
      "---- 48 batch----\n",
      "---- 49 batch----\n",
      "---- 50 batch----\n",
      "---- 51 batch----\n",
      "---- 52 batch----\n",
      "---- 53 batch----\n",
      "---- 54 batch----\n",
      "---- 55 batch----\n",
      "---- 56 batch----\n",
      "---- 57 batch----\n",
      "---- 58 batch----\n",
      "---- 59 batch----\n",
      "---- 60 batch----\n",
      "---- 61 batch----\n",
      "---- 62 batch----\n",
      "---- 63 batch----\n",
      "---- 64 batch----\n",
      "---- 65 batch----\n",
      "---- 66 batch----\n",
      "---- 67 batch----\n",
      "---- 68 batch----\n",
      "---- 69 batch----\n",
      "---- 70 batch----\n",
      "---- 71 batch----\n",
      "---- 72 batch----\n",
      "---- 73 batch----\n",
      "---- 74 batch----\n",
      "---- 75 batch----\n",
      "---- 76 batch----\n",
      "---- 77 batch----\n",
      "---- 78 batch----\n",
      "---- 79 batch----\n",
      "---- 80 batch----\n",
      "---- 81 batch----\n",
      "---- 82 batch----\n",
      "---- 83 batch----\n",
      "========== IN EPOCH 4 the total loss is 6638.650526765415 ==========\n",
      "---- 0 batch----\n",
      "---- 1 batch----\n",
      "---- 2 batch----\n",
      "---- 3 batch----\n",
      "---- 4 batch----\n",
      "---- 5 batch----\n",
      "---- 6 batch----\n",
      "---- 7 batch----\n",
      "---- 8 batch----\n",
      "---- 9 batch----\n",
      "---- 10 batch----\n",
      "---- 11 batch----\n",
      "---- 12 batch----\n",
      "---- 13 batch----\n",
      "---- 14 batch----\n",
      "---- 15 batch----\n",
      "---- 16 batch----\n",
      "---- 17 batch----\n",
      "---- 18 batch----\n",
      "---- 19 batch----\n",
      "---- 20 batch----\n",
      "---- 21 batch----\n",
      "---- 22 batch----\n",
      "---- 23 batch----\n",
      "---- 24 batch----\n",
      "---- 25 batch----\n",
      "---- 26 batch----\n",
      "---- 27 batch----\n",
      "---- 28 batch----\n",
      "---- 29 batch----\n",
      "---- 30 batch----\n",
      "---- 31 batch----\n",
      "---- 32 batch----\n",
      "---- 33 batch----\n",
      "---- 34 batch----\n",
      "---- 35 batch----\n",
      "---- 36 batch----\n",
      "---- 37 batch----\n",
      "---- 38 batch----\n",
      "---- 39 batch----\n",
      "---- 40 batch----\n",
      "---- 41 batch----\n",
      "---- 42 batch----\n",
      "---- 43 batch----\n",
      "---- 44 batch----\n",
      "---- 45 batch----\n",
      "---- 46 batch----\n",
      "---- 47 batch----\n",
      "---- 48 batch----\n",
      "---- 49 batch----\n",
      "---- 50 batch----\n",
      "---- 51 batch----\n",
      "---- 52 batch----\n",
      "---- 53 batch----\n",
      "---- 54 batch----\n",
      "---- 55 batch----\n",
      "---- 56 batch----\n",
      "---- 57 batch----\n",
      "---- 58 batch----\n",
      "---- 59 batch----\n",
      "---- 60 batch----\n",
      "---- 61 batch----\n",
      "---- 62 batch----\n",
      "---- 63 batch----\n",
      "---- 64 batch----\n",
      "---- 65 batch----\n",
      "---- 66 batch----\n",
      "---- 67 batch----\n",
      "---- 68 batch----\n",
      "---- 69 batch----\n",
      "---- 70 batch----\n",
      "---- 71 batch----\n",
      "---- 72 batch----\n",
      "---- 73 batch----\n",
      "---- 74 batch----\n",
      "---- 75 batch----\n",
      "---- 76 batch----\n",
      "---- 77 batch----\n",
      "---- 78 batch----\n",
      "---- 79 batch----\n",
      "---- 80 batch----\n",
      "---- 81 batch----\n",
      "---- 82 batch----\n",
      "---- 83 batch----\n",
      "========== IN EPOCH 5 the total loss is 6649.738702058792 ==========\n",
      "---- 0 batch----\n",
      "---- 1 batch----\n",
      "---- 2 batch----\n",
      "---- 3 batch----\n",
      "---- 4 batch----\n",
      "---- 5 batch----\n",
      "---- 6 batch----\n",
      "---- 7 batch----\n",
      "---- 8 batch----\n",
      "---- 9 batch----\n",
      "---- 10 batch----\n",
      "---- 11 batch----\n",
      "---- 12 batch----\n",
      "---- 13 batch----\n",
      "---- 14 batch----\n",
      "---- 15 batch----\n",
      "---- 16 batch----\n",
      "---- 17 batch----\n",
      "---- 18 batch----\n",
      "---- 19 batch----\n",
      "---- 20 batch----\n",
      "---- 21 batch----\n",
      "---- 22 batch----\n",
      "---- 23 batch----\n",
      "---- 24 batch----\n",
      "---- 25 batch----\n",
      "---- 26 batch----\n",
      "---- 27 batch----\n",
      "---- 28 batch----\n",
      "---- 29 batch----\n",
      "---- 30 batch----\n",
      "---- 31 batch----\n",
      "---- 32 batch----\n",
      "---- 33 batch----\n",
      "---- 34 batch----\n",
      "---- 35 batch----\n",
      "---- 36 batch----\n",
      "---- 37 batch----\n",
      "---- 38 batch----\n",
      "---- 39 batch----\n",
      "---- 40 batch----\n",
      "---- 41 batch----\n",
      "---- 42 batch----\n",
      "---- 43 batch----\n",
      "---- 44 batch----\n",
      "---- 45 batch----\n",
      "---- 46 batch----\n",
      "---- 47 batch----\n",
      "---- 48 batch----\n",
      "---- 49 batch----\n",
      "---- 50 batch----\n",
      "---- 51 batch----\n",
      "---- 52 batch----\n",
      "---- 53 batch----\n",
      "---- 54 batch----\n",
      "---- 55 batch----\n",
      "---- 56 batch----\n",
      "---- 57 batch----\n",
      "---- 58 batch----\n",
      "---- 59 batch----\n",
      "---- 60 batch----\n",
      "---- 61 batch----\n",
      "---- 62 batch----\n",
      "---- 63 batch----\n",
      "---- 64 batch----\n",
      "---- 65 batch----\n",
      "---- 66 batch----\n",
      "---- 67 batch----\n",
      "---- 68 batch----\n",
      "---- 69 batch----\n",
      "---- 70 batch----\n",
      "---- 71 batch----\n",
      "---- 72 batch----\n",
      "---- 73 batch----\n",
      "---- 74 batch----\n",
      "---- 75 batch----\n",
      "---- 76 batch----\n",
      "---- 77 batch----\n",
      "---- 78 batch----\n",
      "---- 79 batch----\n",
      "---- 80 batch----\n",
      "---- 81 batch----\n",
      "---- 82 batch----\n",
      "---- 83 batch----\n",
      "========== IN EPOCH 6 the total loss is 6644.683889801158 ==========\n",
      "---- 0 batch----\n",
      "---- 1 batch----\n",
      "---- 2 batch----\n",
      "---- 3 batch----\n",
      "---- 4 batch----\n",
      "---- 5 batch----\n",
      "---- 6 batch----\n",
      "---- 7 batch----\n",
      "---- 8 batch----\n",
      "---- 9 batch----\n",
      "---- 10 batch----\n",
      "---- 11 batch----\n",
      "---- 12 batch----\n",
      "---- 13 batch----\n",
      "---- 14 batch----\n",
      "---- 15 batch----\n",
      "---- 16 batch----\n",
      "---- 17 batch----\n",
      "---- 18 batch----\n",
      "---- 19 batch----\n",
      "---- 20 batch----\n",
      "---- 21 batch----\n",
      "---- 22 batch----\n",
      "---- 23 batch----\n",
      "---- 24 batch----\n",
      "---- 25 batch----\n",
      "---- 26 batch----\n",
      "---- 27 batch----\n",
      "---- 28 batch----\n",
      "---- 29 batch----\n",
      "---- 30 batch----\n",
      "---- 31 batch----\n",
      "---- 32 batch----\n",
      "---- 33 batch----\n",
      "---- 34 batch----\n",
      "---- 35 batch----\n",
      "---- 36 batch----\n",
      "---- 37 batch----\n",
      "---- 38 batch----\n",
      "---- 39 batch----\n",
      "---- 40 batch----\n",
      "---- 41 batch----\n",
      "---- 42 batch----\n",
      "---- 43 batch----\n",
      "---- 44 batch----\n",
      "---- 45 batch----\n",
      "---- 46 batch----\n",
      "---- 47 batch----\n",
      "---- 48 batch----\n",
      "---- 49 batch----\n",
      "---- 50 batch----\n",
      "---- 51 batch----\n",
      "---- 52 batch----\n",
      "---- 53 batch----\n",
      "---- 54 batch----\n",
      "---- 55 batch----\n",
      "---- 56 batch----\n",
      "---- 57 batch----\n",
      "---- 58 batch----\n",
      "---- 59 batch----\n",
      "---- 60 batch----\n",
      "---- 61 batch----\n",
      "---- 62 batch----\n",
      "---- 63 batch----\n",
      "---- 64 batch----\n",
      "---- 65 batch----\n",
      "---- 66 batch----\n",
      "---- 67 batch----\n",
      "---- 68 batch----\n",
      "---- 69 batch----\n",
      "---- 70 batch----\n",
      "---- 71 batch----\n",
      "---- 72 batch----\n",
      "---- 73 batch----\n",
      "---- 74 batch----\n",
      "---- 75 batch----\n",
      "---- 76 batch----\n",
      "---- 77 batch----\n",
      "---- 78 batch----\n",
      "---- 79 batch----\n",
      "---- 80 batch----\n",
      "---- 81 batch----\n",
      "---- 82 batch----\n",
      "---- 83 batch----\n",
      "========== IN EPOCH 7 the total loss is 6648.060533441816 ==========\n",
      "---- 0 batch----\n",
      "---- 1 batch----\n",
      "---- 2 batch----\n",
      "---- 3 batch----\n",
      "---- 4 batch----\n",
      "---- 5 batch----\n",
      "---- 6 batch----\n",
      "---- 7 batch----\n",
      "---- 8 batch----\n",
      "---- 9 batch----\n",
      "---- 10 batch----\n",
      "---- 11 batch----\n",
      "---- 12 batch----\n",
      "---- 13 batch----\n",
      "---- 14 batch----\n",
      "---- 15 batch----\n",
      "---- 16 batch----\n",
      "---- 17 batch----\n",
      "---- 18 batch----\n",
      "---- 19 batch----\n",
      "---- 20 batch----\n",
      "---- 21 batch----\n",
      "---- 22 batch----\n",
      "---- 23 batch----\n",
      "---- 24 batch----\n",
      "---- 25 batch----\n",
      "---- 26 batch----\n",
      "---- 27 batch----\n",
      "---- 28 batch----\n",
      "---- 29 batch----\n",
      "---- 30 batch----\n",
      "---- 31 batch----\n",
      "---- 32 batch----\n",
      "---- 33 batch----\n",
      "---- 34 batch----\n",
      "---- 35 batch----\n",
      "---- 36 batch----\n",
      "---- 37 batch----\n",
      "---- 38 batch----\n",
      "---- 39 batch----\n",
      "---- 40 batch----\n",
      "---- 41 batch----\n",
      "---- 42 batch----\n",
      "---- 43 batch----\n",
      "---- 44 batch----\n",
      "---- 45 batch----\n",
      "---- 46 batch----\n",
      "---- 47 batch----\n",
      "---- 48 batch----\n",
      "---- 49 batch----\n",
      "---- 50 batch----\n",
      "---- 51 batch----\n",
      "---- 52 batch----\n",
      "---- 53 batch----\n",
      "---- 54 batch----\n",
      "---- 55 batch----\n",
      "---- 56 batch----\n",
      "---- 57 batch----\n",
      "---- 58 batch----\n",
      "---- 59 batch----\n",
      "---- 60 batch----\n",
      "---- 61 batch----\n",
      "---- 62 batch----\n",
      "---- 63 batch----\n",
      "---- 64 batch----\n",
      "---- 65 batch----\n",
      "---- 66 batch----\n",
      "---- 67 batch----\n",
      "---- 68 batch----\n",
      "---- 69 batch----\n",
      "---- 70 batch----\n",
      "---- 71 batch----\n",
      "---- 72 batch----\n",
      "---- 73 batch----\n",
      "---- 74 batch----\n",
      "---- 75 batch----\n",
      "---- 76 batch----\n",
      "---- 77 batch----\n",
      "---- 78 batch----\n",
      "---- 79 batch----\n",
      "---- 80 batch----\n",
      "---- 81 batch----\n",
      "---- 82 batch----\n",
      "---- 83 batch----\n",
      "========== IN EPOCH 8 the total loss is 6646.870398545266 ==========\n",
      "---- 0 batch----\n",
      "---- 1 batch----\n",
      "---- 2 batch----\n",
      "---- 3 batch----\n",
      "---- 4 batch----\n",
      "---- 5 batch----\n",
      "---- 6 batch----\n",
      "---- 7 batch----\n",
      "---- 8 batch----\n",
      "---- 9 batch----\n",
      "---- 10 batch----\n",
      "---- 11 batch----\n",
      "---- 12 batch----\n",
      "---- 13 batch----\n",
      "---- 14 batch----\n",
      "---- 15 batch----\n",
      "---- 16 batch----\n",
      "---- 17 batch----\n",
      "---- 18 batch----\n",
      "---- 19 batch----\n",
      "---- 20 batch----\n",
      "---- 21 batch----\n",
      "---- 22 batch----\n",
      "---- 23 batch----\n",
      "---- 24 batch----\n",
      "---- 25 batch----\n",
      "---- 26 batch----\n",
      "---- 27 batch----\n",
      "---- 28 batch----\n",
      "---- 29 batch----\n",
      "---- 30 batch----\n",
      "---- 31 batch----\n",
      "---- 32 batch----\n",
      "---- 33 batch----\n",
      "---- 34 batch----\n",
      "---- 35 batch----\n",
      "---- 36 batch----\n",
      "---- 37 batch----\n",
      "---- 38 batch----\n",
      "---- 39 batch----\n",
      "---- 40 batch----\n",
      "---- 41 batch----\n",
      "---- 42 batch----\n",
      "---- 43 batch----\n",
      "---- 44 batch----\n",
      "---- 45 batch----\n",
      "---- 46 batch----\n",
      "---- 47 batch----\n",
      "---- 48 batch----\n",
      "---- 49 batch----\n",
      "---- 50 batch----\n",
      "---- 51 batch----\n",
      "---- 52 batch----\n",
      "---- 53 batch----\n",
      "---- 54 batch----\n",
      "---- 55 batch----\n",
      "---- 56 batch----\n",
      "---- 57 batch----\n",
      "---- 58 batch----\n",
      "---- 59 batch----\n",
      "---- 60 batch----\n",
      "---- 61 batch----\n",
      "---- 62 batch----\n",
      "---- 63 batch----\n",
      "---- 64 batch----\n",
      "---- 65 batch----\n",
      "---- 66 batch----\n",
      "---- 67 batch----\n",
      "---- 68 batch----\n",
      "---- 69 batch----\n",
      "---- 70 batch----\n",
      "---- 71 batch----\n",
      "---- 72 batch----\n",
      "---- 73 batch----\n",
      "---- 74 batch----\n",
      "---- 75 batch----\n",
      "---- 76 batch----\n",
      "---- 77 batch----\n",
      "---- 78 batch----\n",
      "---- 79 batch----\n",
      "---- 80 batch----\n",
      "---- 81 batch----\n",
      "---- 82 batch----\n",
      "---- 83 batch----\n",
      "========== IN EPOCH 9 the total loss is 6647.341547908101 ==========\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# filename = \"../log_file.txt\"\n",
    "# f = open(filename,'w')\n",
    "total_train_step = 0\n",
    "mlp.train()\n",
    "for epoch in range(10):\n",
    "    total_train_loss = 0\n",
    "    for batch_id,data in enumerate(train_loader):\n",
    "\n",
    "        input_data, target = data\n",
    "        print(f\"---- {batch_id} batch----\")\n",
    "        # Do the inference\n",
    "        input_data = input_data.to(device)\n",
    "        pi, mu, sigma = mlp(input_data)\n",
    "\n",
    "        # Save the params\n",
    "        # params = list(mlp.named_parameters())\n",
    "\n",
    "        # Cal the loss and draw the distrb.\n",
    "        duration,m  = loss_preparation(pi.detach(), mu.detach(), sigma.detach(), target.detach())\n",
    "        loss,loss_list = loss_fn(pi,duration,m,N_gaussians)\n",
    "        total_train_loss += loss.item()\n",
    "        draw_loss(total_train_step, loss.item(),win_train_loss_str)\n",
    "        # print(\"训练次数：{}，Loss：{}\".format(total_train_step, loss.item()))\n",
    "\n",
    "        ########### before step() ###############\n",
    "\n",
    "        # for name, parms in mlp.named_parameters():\n",
    "        #     print(\"=====before step()=====\")\n",
    "        #     print('-->name:', name)\n",
    "        #     print('-->para:', parms)\n",
    "        #     print('-->grad_requirs:',parms.requires_grad)\n",
    "        #     print('-->grad_value:',parms.grad)\n",
    "        #     print(\"===\")\n",
    "\n",
    "        # Optim\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # for name, parms in mlp.named_parameters():\n",
    "        #     print(\"=====After step()=====\")\n",
    "        #     print('-->name:', name)\n",
    "        #     print('-->para:', parms)\n",
    "        #     print('-->grad_requirs:',parms.requires_grad)\n",
    "        #     print('-->grad_value:',parms.grad)\n",
    "        #     print(\"===\")\n",
    "\n",
    "        # print(\"训练次数：{}，Loss：{}, Loss's grad: {}\".format(total_train_step, loss.item(), loss.grad))\n",
    "\n",
    "        if total_train_step % 10 == 0:\n",
    "            # print(pi,\"\\n\",mu,\"\\n\",sigma)\n",
    "            # Only draw the 1st result in a batch (5 in total)\n",
    "            draw_mdn(pi[0,:].detach(), m[0], target[0].detach(), total_train_step,N_gaussians)\n",
    "\n",
    "        # # Do validation\n",
    "        # total_vali_loss = 0\n",
    "        # for vali_batch_id, vali_data in enumerate(val_loader):\n",
    "        #     vali_input_data, vali_target = vali_data\n",
    "        #     vali_input_data = vali_input_data.to(device)\n",
    "        #     vali_pi, vali_mu, vali_sigma = mlp(vali_input_data)\n",
    "        #\n",
    "        #     # Cal the sum of vali loss instead of vali loss in a batch\n",
    "        #     vali_duration,vali_m  = loss_preparation(vali_pi.detach(), vali_mu.detach(), vali_sigma.detach(), vali_target.detach())\n",
    "        #     vali_loss, _ = loss_fn(vali_pi,vali_duration,vali_m,N_gaussians)\n",
    "        #     total_vali_loss += vali_loss\n",
    "        # # Plot the vali loss\n",
    "        # # total_vali_loss.item()/83.\n",
    "        # draw_loss(total_train_step, (total_vali_loss.item()), win_vali_loss_str)\n",
    "\n",
    "        total_train_step += 1\n",
    "       # Plot the loss in this EPOCH\n",
    "    print(f\"========== IN EPOCH {epoch} the total loss is {total_train_loss} ==========\")\n",
    "    draw_loss(epoch, total_train_loss,win_train_epoch_loss_str)\n",
    "\n",
    "# f.close()\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "$\\mathcal{L}(y \\vert x) = - \\log\\bigg\\{\\sum_{k=1}^K \\pi_k(x)  \\mathcal{N}\\big(y \\vert \\mu_k(x), \\Sigma_k(x)\\big)\\bigg\\}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [],
   "source": [
    "# torch.save(mlp.state_dict(), 'mlp_init_loss_17.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
