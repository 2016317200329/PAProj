{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2022/6/7 15:17\n",
    "# @Author  : Wang Yujia\n",
    "# @File    : mlp.ipynb\n",
    "# @Description : 非Sequential结构\n",
    "# @TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0. what for\n",
    "1. loss部分逻辑重写，添加`no grad`\n",
    "2. 添加hooks\n",
    "3. 使用pad_seq函数取代自己写的pad函数\n",
    "4. 效果：可以train虽然loss不下降，不会出现NaN的问题"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Preparations\n",
    "## 1.1 global settings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# nums of Gaussian kernels\n",
    "N_gaussians = 3\n",
    "\n",
    "# dataset划分\n",
    "batch_size = 5\n",
    "train_pct = 0.7\n",
    "vali_pct = 0.2\n",
    "test_pct = 0.1\n",
    "\n",
    "# train and optim.\n",
    "learning_rate = 0.0001\n",
    "total_train_step = 0\n",
    "total_test_step = 0\n",
    "EPOCH_NUM = 5\n",
    "MIN_LOSS = 1e-7\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.utils.data\n",
    "from mydataset import *\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from tensorboardX import SummaryWriter\n",
    "from visdom import Visdom\n",
    "from torchviz import make_dot\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import math\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 the data path\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# training data\n",
    "train_path = r\"../data/train\"\n",
    "# target data\n",
    "target_path = r\"../data/targets\"\n",
    "# data keys\n",
    "data_key_path = \"../data/target_datakey.csv\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Dataset and Dataloader\n",
    "1. DataLoader中的shuffer=True表示在每一次epoch中都打乱所有数据的顺序，然后以batch为单位从头到尾按顺序取用数据。这样的结果就是不同epoch中的数据都是乱序的,设置随机种子的作用就是让你的每一次训练都乱的一样，\n",
    "\n",
    "## 2.1 Dataset and spliting\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 设置随机数种子"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup_seed(7)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 读取data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "dataset = myDataset(train_path, target_path, data_key_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 产生index的乱序排列"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 807  305  455  939  508  594  835 1082  598 1102   46  761  841  141\n",
      "  407  334  253  500  734  936  698  446  907 1087 1009  140  463  547\n",
      " 1155  856   34 1156  703 1121  751  587 1100  509  473  128  788   97\n",
      "  471  385 1085  525  679 1135  284 1146  186  318 1088 1113    9  457\n",
      " 1047  451  569 1120 1074  326  377  809  109   98  620 1194  825  828\n",
      "   83  113  556  674  568  853  351  558   54  656  804 1149  101  344\n",
      "  851  544  955   40 1021  489  626  664  657  868 1169  151  179 1130\n",
      "  564  713  743 1014  966  861 1132  146  610  408  662  551  172 1020\n",
      "  982 1148  431  517  270  858  170  374  816  618  205   17   53 1003\n",
      "  263  857  716  843  498  228  339  725  752  278  649 1017  108  642\n",
      " 1195 1174   99  530  632  888  189  961  358 1078  663  757 1051  204\n",
      "  409  283  562   23  619  216  474  921  950 1162  123  785  769  621\n",
      "  262  586 1178  541  795   70 1189  396  171  845  168 1125  361  224\n",
      "  125  706  164  231  264   42  746  872  998  132 1035  430   38  522\n",
      "  880  325  285  367  870  418   13  166  513  901  617  740  680 1126\n",
      "   20  372   71 1038  355   60  220  110    4 1115  572  415  336  316\n",
      "  449  412 1066  768 1055  704  869   94   95 1163 1145  648  884  885\n",
      " 1170  848   58  207  306 1067  729   24  644  633  252  832  244   25\n",
      " 1094   63  222 1070  996 1063  130  153 1180  922  399  997   75  894\n",
      " 1048  308  280  934  721  199  820  134  289  259 1159    3  510  492\n",
      "  196  565  563   26  913  483 1141  653   61  623  984  652  609  739\n",
      "  824 1018  891  467  842    8  503  714 1015  447  783  992   74 1184\n",
      "   57  978  484  297 1039  217  681 1027  960  350 1129 1045 1183  906\n",
      "  813  660  266    6  379  526  611  138  163  701 1031 1050   43  733\n",
      "  720  319 1158 1153  877  294  432  531  578  523   19  426  830  927\n",
      "  781 1033 1013  420  137  488  185  920  702 1134 1124  454  327  317\n",
      "  799  655  878  699  981  806  953  750  148  591  915  731  937  365\n",
      "  557  397 1173  779 1185  459  232  389 1147 1011  158  315  256  863\n",
      "   96 1157  445  384  511  817  328  314  357  347  452  298  899  887\n",
      " 1034  839  778  747  249  766   15  159  696  585  360   48  478  330\n",
      "  893  112  338  307  528  422 1142  971  506  504 1049  791  395  669\n",
      "  540  127 1160  902  507  980  237  167  602  983   80  414  640  300\n",
      "  977  324 1117  215 1058   30  242  697  860  951  209  671  742 1072\n",
      "  435   27 1060  518  122 1086  462 1028 1118  603  929  732  722  493\n",
      "  202  403 1089 1179  362  271  466  641 1061  744  272  277 1177   12\n",
      "  597  668  210  651  952  147  442  299  115  375  635  767  748 1080\n",
      "  755  177  472  685   51  810 1079  571  388   22  650  965  423  938\n",
      " 1112  643 1193  142   50  666 1143   81  482  321  948  687 1138   66\n",
      "  909  534  738  417 1005   64  895  886  918  491  700  790   65  968\n",
      "  627  999  296  419  438  261  837  281  631  677  188  881  251  694\n",
      "  514  590  487  273  943 1073  448  411 1131  933 1191  935  149  218\n",
      "  239  718  192  191  945  754  665  705  595  926  708  582 1046  323\n",
      " 1026 1071  658  437  770  833  341    2  229 1098 1152   76 1103  371\n",
      "  180  889  577  736  758 1019 1105 1010  589 1167  812  956  882   37\n",
      "  353 1095  236  928  499  553   55  302  286  800  219  789  387  737\n",
      "  390  352  193  235 1110  480  567  116 1057  957 1106  890  777  120\n",
      "  815  990  773   62  667  453  515  570  223  613  221  673  794  465\n",
      "   52 1093  485 1140 1053  155  912  600 1023  248  441  975 1164  542\n",
      "  117  724  628  712  916  291 1041  381  954  200  596  976 1137 1187\n",
      "  691  505  348 1190  340  615  154 1104  879  995  287  322  819  818\n",
      "  796   82  793  464  797  756 1168  771  333   10  470   32  782  413\n",
      "  580  382  930  760 1133  840 1139  378   89  792  601  213  332  133\n",
      "  346  855    7  941  320 1044  187  972  552  304   31  831  684 1065\n",
      "  240  993  174  245   56  630  583 1025  145   90 1114  392  682  310\n",
      "  359   93  947 1024  376  243 1076   87  214  444  173  425 1040  118\n",
      "  119 1151  905    0  394  401  404  836 1091  162  114  670  150  798\n",
      "  875 1064  368  543  477 1128  532  625  967  512  801   72  258  624\n",
      "  234  135  616  516  866  802  592 1056  501  538  897  728  581  343\n",
      "  925 1002  293  364  883  659   14  686   86  370  165  131  335   59\n",
      "  709  676  354  959  424  292   41  329  614  429   79]\n"
     ]
    }
   ],
   "source": [
    "shuffled_indices = np.random.permutation(dataset.__len__())\n",
    "# shuffled_indices = np.arange(0,dataset.__len__())\n",
    "train_idx = shuffled_indices[:int(train_pct*dataset.__len__())]\n",
    "# train_idx = shuffled_indices\n",
    "tmp = int((train_pct+vali_pct)*dataset.__len__())\n",
    "val_idx = shuffled_indices[int(train_pct*dataset.__len__()):tmp]\n",
    "\n",
    "test_idx = shuffled_indices[tmp:]\n",
    "print(train_idx)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 根据这个乱序排列抽取dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Dataloader and collating\n",
    "1. 主要是对label数据进行collate\n",
    "    - 按照batch中的最大target data长度进行padding，padding with 0\n",
    "2. 返回的结果多一个batch dim,比如下面的`5`\n",
    "    - After collating:\n",
    "        - `torch.Size([5, 3, 300]),torch.Size([5, 87, 2])`\n",
    "        - `87`是最长的targets data长度"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[2.0000, 0.0476],\n         [3.0000, 0.1429],\n         [4.0000, 0.0476]],\n\n        [[1.0000, 0.0476],\n         [0.0000, 0.0000],\n         [0.0000, 0.0000]],\n\n        [[3.0000, 0.1429],\n         [4.0000, 0.0476],\n         [0.0000, 0.0000]]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq1 = torch.tensor([[ 2., 0.04761905], [3., 0.14285714], [4., 0.04761905]])\n",
    "seq2 = torch.tensor([[ 1., 0.04761905]])\n",
    "seq3 = torch.tensor([[ 3., 0.14285714], [4., 0.04761905]])\n",
    "ls = list((seq1,seq2,seq3))\n",
    "ls_length = torch.tensor([3,1,2])\n",
    "ans = pad_sequence(ls,batch_first=True)\n",
    "ans\n",
    "# seq_pak = pack_padded_sequence(ans,ls_length,batch_first=True,enforce_sorted=False)\n",
    "# seq_unpacked, lens_unpacked = pad_packed_sequence(seq_pak, batch_first=True)\n",
    "# lens_unpacked"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def my_collate_fn(data):\n",
    "# 这里的data是一个list， list的元素是元组: (self.data, self.label)\n",
    "# collate_fn的作用是把[(data, label),(data, label)...]转化成([data, data...],[label,label...])\n",
    "# 假设self.data的一个data的shape为(channels, length), 每一个channel的length相等,\n",
    "# data[索引到index(batch)][索引到data或者label][索引到channel]\n",
    "\n",
    "    data_list = []\n",
    "    target_list = []\n",
    "    data_len = len(data)      # 读进来的data batch的大小\n",
    "\n",
    "    batch = 0\n",
    "    while batch < data_len:\n",
    "        data_list.append(torch.tensor(data[batch][0]))\n",
    "        target_list.append(torch.tensor(data[batch][1]))\n",
    "        batch += 1\n",
    "\n",
    "    # pad with zeros\n",
    "    target_padded = pad_sequence(target_list,batch_first=True)\n",
    "\n",
    "    data_tensor = torch.stack(data_list).float()\n",
    "    target_tensor = target_padded.float()\n",
    "\n",
    "    return data_tensor, target_tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = dataset,batch_size = batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(train_idx), collate_fn = my_collate_fn)\n",
    "\n",
    "val_loader = DataLoader(dataset = dataset,batch_size = batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(val_idx),collate_fn = my_collate_fn)\n",
    "\n",
    "test_loader = DataLoader(dataset = dataset,batch_size = batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(test_idx),collate_fn = my_collate_fn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. The Net and Init\n",
    "1. BatchNorm1d: The mean and std are calculated per-dimension over the mini-batches\n",
    "2."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "### BatchNorm2d测试\n",
    "def test_BN():\n",
    "    m = nn.BatchNorm2d(3, affine=False)  # affine: With Learnable Parameters or not\n",
    "    print('m:', m)\n",
    "    # The mean and std are calculated per-dimension over the mini-batches\n",
    "    input = torch.tensor([\n",
    "        [[1.,2.,3.,4.],[1.,2.,3.,4.],[-1.,-2.,-3.,-4.]],\n",
    "        [[0.,0.,0.,0.],[0.,0.,0.,0.],[0.,0.,0.,0.]]\n",
    "    ], requires_grad=True)\n",
    "\n",
    "    print('input:', input.shape)\n",
    "    input = input.unsqueeze(dim=2)\n",
    "    print('input:', input.shape)\n",
    "    output = m(input) # 归一化\n",
    "    print('output:', output.shape)\n",
    "    print('output:', output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-0.1690,  0.5071,  1.1832,  1.8593]],\n\n        [[-0.8452, -0.8452, -0.8452, -0.8452]]])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 复现batchNorm2d在input shape为3维的情况\n",
    "input = torch.tensor([[[1.,2.,3.,4.]],[[0.,0.,0.,0.]]])\n",
    "# print(input.shape)\n",
    "# torch.mean(input),torch.var(input,unbiased = False)\n",
    "(input-torch.mean(input))/ torch.sqrt(torch.var(input,unbiased = False))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 设置网络初始权重: 不太work"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class model_param_init(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        assert isinstance(model, nn.Module), 'model not a class nn.Module'\n",
    "        self.net = model\n",
    "        self.initParam()\n",
    "\n",
    "    def initParam(self):\n",
    "        for param in self.net.parameters():\n",
    "            # nn.init.zeros_(param)\n",
    "            # nn.init.ones_(param)\n",
    "            # nn.init.normal_(param, mean=0, std=1)\n",
    "            # nn.init.uniform_(param, a=0, b=1)\n",
    "            # nn.init.constant_(param, val=1)   # 将所有权重初始化为1\n",
    "            # nn.init.eye_(param)  # 只能将二维的tensor初始化为单位矩阵\n",
    "            # nn.init.xavier_uniform_(param, gain=1)  # Glorot初始化  得到的张量是从-a——a中采用的\n",
    "            # nn.init.xavier_normal_(param, gain=1)   # 得到的张量是从0-std采样的\n",
    "            nn.init.kaiming_normal_(param, a=0, mode='fan_in', nonlinearity='relu') # he初始化方法\n",
    "            # nn.init.kaiming_uniform_(param)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- print网络每层结构"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "class PrintLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if(DEBUG):\n",
    "            print(\"This layer: \")\n",
    "            print(x)      #print(x.shape)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Sequential结构\n",
    "- 最后输出mu的时候求了mean，不太好？"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- hook_backward_fn: 输入端的grad，输出端的grad，这里称呼的**输入与输出是站在前向传播的角度的**。如果模块有多个输入与输出的话， 其`grad_input`和`grad_output`可以是tuple类型。\n",
    "- 与forward不同的是，backward传播的时候，**不仅反向传递input和output的grad，还会传递模块Parameter的grad**：\n",
    "    - 比如fc模块，其`grad_input`是一个三元组的tuple，（对bias的梯度，对输入的梯度，对w的梯度）；\n",
    "    - conv模块`grad_input`也是一个三元组tuple，为（对输入的梯度，对w的梯度，对bias的梯度"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# hook functions have to take these 3 input\n",
    "def hook_forward_fn(module, input, output):\n",
    "    print(\"It's forward: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"input: {input}\")\n",
    "    print(f\"output: {output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn(module, grad_input, grad_output):\n",
    "    print(\"It's backward: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn_pi(module, grad_input, grad_output):\n",
    "    print(\"It's backward in pi: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn_sigma(module, grad_input, grad_output):\n",
    "    print(\"It's backward in sigma: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn_mu(module, grad_input, grad_output):\n",
    "    print(\"It's backward in mu: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Not Sequential\n",
    "class MLP(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "        self.BN = nn.BatchNorm1d(num_features=3,affine=False)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear00 = nn.Linear(900,300)\n",
    "        self.linear01 = nn.Linear(300,30)\n",
    "        self.linear1 = nn.Linear(30, 9)\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(9, n_gaussians),  # 30个params要learn\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(9, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(9, n_gaussians)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 加一个height维度\n",
    "        # x.unsqueeze_(dim=2)\n",
    "        x = self.BN(x)\n",
    "        # x.squeeze_()\n",
    "        x = self.flatten(x)\n",
    "        print(\"After flatten, x's shape: \",x.shape)\n",
    "        x = self.linear00(x)\n",
    "        x = self.linear01(x)\n",
    "        x = self.linear1(x)\n",
    "        # print(\"after linear1, the output shape is: \",x.shape)\n",
    "\n",
    "        # pi = torch.mean(self.z_pi(x))\n",
    "        pi = self.z_pi(x)\n",
    "        print(\"pi's shape: \",pi.shape)\n",
    "        mu = self.z_mu(x)\n",
    "        print(\"mu's shape: \",mu.shape)\n",
    "        sigma = torch.exp(self.z_sigma(x))\n",
    "        print(\"sigma's shape: \",sigma.shape)\n",
    "\n",
    "        return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input's shape is torch.Size([2, 3, 1, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": "torch.Size([2, 12])"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([\n",
    "    [[1.,2.,3.,4.],[1.,2.,3.,4.],[-1.,-2.,-3.,-4.]],\n",
    "    [[0.,0.,0.,0.],[0.,0.,0.,0.],[0.,0.,0.,0.]]\n",
    "], requires_grad=True)\n",
    "input = input.unsqueeze(dim=2)\n",
    "print(f\"input's shape is {input.shape}\")\n",
    "flt = nn.Flatten(start_dim=1)\n",
    "flt(input).shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. The Loss\n",
    "- `loss_preparation`用来做loss的前期data准备：\n",
    "    - 计算混合模型的分布`m`以及target data中的`duration`\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# 当input的shape是[50,3]时，输出应该是50个GMM\n",
    "def loss_preparation(pi, mu, sigma, target):\n",
    "\n",
    "    m=[]\n",
    "    for i in range(pi.shape[0]):\n",
    "        m.append(torch.distributions.Normal(loc=mu[i,:].T, scale=sigma[i,:].T))\n",
    "\n",
    "    # target_packed = pack_padded_sequence(target,target_len,batch_first=True,enforce_sorted=False)  # 去掉padded 0并拉成一个vector\n",
    "    # seq_unpacked, lens_unpacked = pad_packed_sequence(target_packed, batch_first=True)             # 和上面互为逆运算\n",
    "\n",
    "    duration = target[:,:,0]\n",
    "\n",
    "    return duration,m"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# # 当input的shape是[50,3]时，输出应该是50个GMM\n",
    "# # 对这50个GMM看能生成什么output\n",
    "\n",
    "def loss_fn(Pi,duration,m):\n",
    "    loss_list = []\n",
    "    with torch.no_grad():\n",
    "        # for each GMM\n",
    "        # 后期肯定要矩阵化这个计算！\n",
    "        for i in range(len(m)):\n",
    "            target = duration[i,:]\n",
    "            pi = Pi[i,:]\n",
    "\n",
    "            # repeat and copy target data\n",
    "            target_nonzero = target[torch.nonzero(target)].squeeze_()\n",
    "            target_nonzero = torch.repeat_interleave(target_nonzero.unsqueeze(dim=1), repeats=3, dim=1).to(device)\n",
    "\n",
    "            # loss_1 是高斯分布的概率密度value\n",
    "            loss_1 = torch.exp(m[i].log_prob(target_nonzero))\n",
    "\n",
    "            # loss_2是MDN的概率密度value\n",
    "            loss_2 = torch.sum(loss_1 * pi, dim=1)\n",
    "\n",
    "            # loss_3是非0的prob value\n",
    "            loss_3 = loss_2[torch.nonzero(loss_2)].view(-1,1)                # 再去掉所有的log(0)\n",
    "\n",
    "            MIN_LOSS = torch.min(loss_3) if torch.min(loss_2)>0 else 1e-20   # 如果loss_2全是0则赋值为1e-20，否则赋值为loss的最小值\n",
    "            loss_padded = [MIN_LOSS]* (loss_2.shape[0]-loss_3.shape[0])\n",
    "            loss_padded_ts = torch.tensor(loss_padded,device=device)\n",
    "            loss_3 = torch.cat((loss_3[:,0],loss_padded_ts))\n",
    "            torch._assert(len(loss_3) == len(loss_2),\"cat不正确\")\n",
    "\n",
    "            loss_4 = torch.log(loss_3)\n",
    "            loss_list.append(-torch.mean((loss_4)).item())\n",
    "\n",
    "        # 最后处理loss\n",
    "        loss = np.sum(loss_list)\n",
    "\n",
    "    loss_ts = torch.tensor(loss,device=device,requires_grad=True)\n",
    "    return loss_ts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def loss_test():\n",
    "    loss_2 = torch.tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "    loss_3 = loss_2[torch.nonzero(loss_2)].view(-1,1)         # 去掉所有的log(0)\n",
    "    print(\"loss_3:\",loss_3.shape)\n",
    "\n",
    "    MIN_LOSS = torch.min(loss_3) if torch.min(loss_2)>0 else 1e-20\n",
    "\n",
    "    print(\"MIN_LOSS：\",MIN_LOSS)\n",
    "    loss_padded = [MIN_LOSS]* (loss_2.shape[0]-loss_3.shape[0])\n",
    "    loss_padded_ts = torch.tensor(loss_padded) # ,device=device\n",
    "    loss_3 = torch.cat((loss_3[:,0],loss_padded_ts))\n",
    "    loss_3.shape, torch.log(loss_3)\n",
    "    print(\"data: \",loss_3.data)\n",
    "    print(\"data: \",type(loss_3))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Training\n",
    "## 5.1 preparations\n",
    "1. 初始化Visdom环境\n",
    "2.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# writer = SummaryWriter(\"logs-MLP\")\n",
    "viz = Visdom(env=\"001\")\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.2 Draw\n",
    "1. draw:\n",
    "    - mdn的图（visdom）以及mdn的test draw\n",
    "    - loss图以及初始化（visdom）\n",
    "    - MLP的网络结构（.png）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "#### Test for drawing\n",
    "def test_draw():\n",
    "    viz = Visdom(env=\"001\")\n",
    "\n",
    "    mu = torch.tensor([0,10,20])\n",
    "    sigma = torch.tensor([1,1,1])\n",
    "    duration = torch.tensor([0,1,2,0])\n",
    "    duration = torch.repeat_interleave(duration.unsqueeze(dim=1), repeats=3, dim=1)\n",
    "    m = torch.distributions.Normal(loc=mu, scale=sigma)\n",
    "    pi = torch.tensor([0.2,0.3,0.5])\n",
    "\n",
    "    # draw\n",
    "    x_0 = torch.tensor(np.arange(0,1000))\n",
    "    x = torch.repeat_interleave(x_0.unsqueeze(dim=1), repeats=3, dim=1)\n",
    "    y = torch.exp(m.log_prob(x))\n",
    "    y_sum = torch.unsqueeze(torch.sum(pi*y,dim=1),dim=1)\n",
    "    viz.line(X = x_0,Y= torch.cat([y,y_sum],dim = 1), env=\"001\", win=\"test_draw_2\",\n",
    "            opts= dict(title='test_draw', legend=['N1', 'N2', 'N3','NNN']))\n",
    "# test_draw()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def draw_mdn(pi,duration,m,total_train_step):\n",
    "    # draw the distrb.\n",
    "    x_0 = torch.arange(0,torch.max(duration).item()).to(device)\n",
    "    x = torch.repeat_interleave(x_0.unsqueeze(dim=1), repeats=3, dim=1)\n",
    "    y = torch.exp(m.log_prob(x)).to(device)\n",
    "    y_sum = torch.unsqueeze(torch.sum(pi*y,dim=1),dim=1)   # 维度相等才能cat\n",
    "    win_str = \"total_train_step-\"+str(total_train_step)\n",
    "    viz.line(X = x_0,Y= torch.cat([y,y_sum],dim = 1), env=\"001\", win=win_str,\n",
    "        opts= dict(title=win_str, legend=['N1', 'N2', 'N3','NNN']))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def draw_the_net():\n",
    "\n",
    "    x = torch.randn([5, 3, 300])  # 定义网络的输入值\n",
    "    mlp = MLP(N_gaussians)\n",
    "    y = mlp(x)                    # 获取网络的预测值\n",
    "\n",
    "    MyConvNetVis = make_dot(y, params=dict(list(mlp.named_parameters()) + [('x', x)]))\n",
    "    MyConvNetVis.format = \"png\"\n",
    "    # 指定文件生成的文件夹\n",
    "    MyConvNetVis.directory = \"data_pic\"\n",
    "    # 生成文件\n",
    "    MyConvNetVis.view()\n",
    "# draw_the_net()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "viz.line(X = [0.],Y = [0.], env=\"001\", win=\"The Loss\", opts= dict(title=\"The Loss\"))\n",
    "def draw_loss(total_train_step, loss):\n",
    "    viz.line(X = [total_train_step], Y = [loss],win=\"The Loss\", update=\"append\",\n",
    "        opts= dict(title=\"The Loss\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.3 Training\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After flatten, x's shape:  torch.Size([2, 900])\n",
      "pi's shape:  torch.Size([2, 3])\n",
      "mu's shape:  torch.Size([2, 3])\n",
      "sigma's shape:  torch.Size([2, 3])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "       BatchNorm1d-1               [-1, 3, 300]               0\n",
      "           Flatten-2                  [-1, 900]               0\n",
      "            Linear-3                  [-1, 300]         270,300\n",
      "            Linear-4                   [-1, 30]           9,030\n",
      "            Linear-5                    [-1, 9]             279\n",
      "            Linear-6                    [-1, 3]              30\n",
      "           Softmax-7                    [-1, 3]               0\n",
      "            Linear-8                    [-1, 3]              30\n",
      "            Linear-9                    [-1, 3]              30\n",
      "================================================================\n",
      "Total params: 279,699\n",
      "Trainable params: 279,699\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 1.07\n",
      "Estimated Total Size (MB): 1.09\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP(N_gaussians)\n",
    "\n",
    "# Init the params\n",
    "# # mlp = model_param_init(mlp)\n",
    "\n",
    "# save the init params\n",
    "torch.save(mlp.state_dict(), 'mlp_init.pth')\n",
    "\n",
    "# read the saved model\n",
    "# model_data = torch.load('mlp_init_loss_17.pth')\n",
    "# mlp.load_state_dict(model_data)\n",
    "\n",
    "mlp = mlp.to(device=device)\n",
    "summary(mlp, (3,300))\n",
    "# optimizer = torch.optim.Adagrad(mlp.parameters(),lr=learning_rate, lr_decay=learning_rate, weight_decay=learning_rate)\n",
    "optimizer = torch.optim.Adam(mlp.parameters(),lr=learning_rate)\n",
    "\n",
    "# # hooks\n",
    "# #mlp.conv1.register_forward_hook(hook_forward_fn)\n",
    "# mlp.conv1.register_full_backward_hook(hook_backward_fn)\n",
    "#\n",
    "# #mlp.linear1.register_forward_hook(hook_forward_fn)\n",
    "# #mlp.linear1.register_full_backward_hook(hook_backward_fn)\n",
    "#\n",
    "# #mlp.z_pi.register_forward_hook(hook_forward_fn)\n",
    "# mlp.z_pi.register_full_backward_hook(hook_backward_fn_pi)\n",
    "#\n",
    "# #mlp.z_mu.register_forward_hook(hook_forward_fn)\n",
    "# mlp.z_mu.register_full_backward_hook(hook_backward_fn_mu)\n",
    "#\n",
    "# #mlp.z_sigma.register_forward_hook(hook_forward_fn)\n",
    "# mlp.z_sigma.register_full_backward_hook(hook_backward_fn_sigma)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- 0 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：0，Loss：217.36496353149414\n",
      "tensor([[0.4036, 0.2888, 0.3075],\n",
      "        [0.4037, 0.3014, 0.2948],\n",
      "        [0.4697, 0.2739, 0.2564],\n",
      "        [0.4044, 0.3052, 0.2904],\n",
      "        [0.4222, 0.3087, 0.2691]], device='cuda:0', grad_fn=<SoftmaxBackward0>) \n",
      " tensor([[0.0827, 0.0569, 0.1485],\n",
      "        [0.1076, 0.0064, 0.1203],\n",
      "        [0.2006, 0.0450, 0.1254],\n",
      "        [0.0416, 0.0587, 0.1051],\n",
      "        [0.1153, 0.0832, 0.0176]], device='cuda:0', grad_fn=<AddmmBackward0>) \n",
      " tensor([[1.1567, 1.2674, 1.0327],\n",
      "        [1.2220, 1.3521, 0.9603],\n",
      "        [1.3439, 1.4645, 0.9011],\n",
      "        [1.2246, 1.3610, 0.9231],\n",
      "        [1.3254, 1.4244, 0.8782]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "---- 1 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：1，Loss：199.9262180328369\n",
      "---- 2 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：2，Loss：222.27778244018555\n",
      "---- 3 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：3，Loss：196.5947322845459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wang Yujia\\AppData\\Local\\Temp\\ipykernel_19780\\2256927416.py:6: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3281.)\n",
      "  m.append(torch.distributions.Normal(loc=mu[i,:].T, scale=sigma[i,:].T))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- 4 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：4，Loss：224.54161071777344\n",
      "---- 5 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：5，Loss：220.69069290161133\n",
      "---- 6 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：6，Loss：231.06779098510742\n",
      "---- 7 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：7，Loss：218.30096435546875\n",
      "---- 8 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：8，Loss：221.3379249572754\n",
      "---- 9 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：9，Loss：216.8359489440918\n",
      "---- 10 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：10，Loss：224.05221557617188\n",
      "tensor([[0.4578, 0.2909, 0.2512],\n",
      "        [0.4007, 0.3096, 0.2896],\n",
      "        [0.4509, 0.2977, 0.2514],\n",
      "        [0.4600, 0.2672, 0.2729],\n",
      "        [0.3710, 0.3336, 0.2954]], device='cuda:0', grad_fn=<SoftmaxBackward0>) \n",
      " tensor([[ 0.0437,  0.0903,  0.0372],\n",
      "        [ 0.1738,  0.0970, -0.0224],\n",
      "        [ 0.2198,  0.0954,  0.0541],\n",
      "        [ 0.2323,  0.0137,  0.1100],\n",
      "        [ 0.0636,  0.0983,  0.1311]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) \n",
      " tensor([[1.3255, 1.2227, 0.8471],\n",
      "        [1.4007, 1.2368, 0.8277],\n",
      "        [1.4145, 1.5785, 0.8077],\n",
      "        [1.3283, 1.4118, 0.8973],\n",
      "        [1.1039, 1.3182, 1.0542]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "---- 11 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：11，Loss：187.1232395172119\n",
      "---- 12 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：12，Loss：227.29106521606445\n",
      "---- 13 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：13，Loss：219.2221450805664\n",
      "---- 14 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：14，Loss：214.89582061767578\n",
      "---- 15 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：15，Loss：216.15591430664062\n",
      "---- 16 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：16，Loss：224.24431991577148\n",
      "---- 17 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：17，Loss：211.98651695251465\n",
      "---- 18 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：18，Loss：230.91053009033203\n",
      "---- 19 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：19，Loss：230.37067794799805\n",
      "---- 20 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：20，Loss：227.6229248046875\n",
      "tensor([[0.4660, 0.2817, 0.2523],\n",
      "        [0.4483, 0.2700, 0.2816],\n",
      "        [0.4312, 0.2873, 0.2814],\n",
      "        [0.4223, 0.3041, 0.2737],\n",
      "        [0.4583, 0.2756, 0.2661]], device='cuda:0', grad_fn=<SoftmaxBackward0>) \n",
      " tensor([[ 0.1242,  0.0763,  0.0344],\n",
      "        [ 0.2726, -0.0029,  0.0900],\n",
      "        [ 0.1446,  0.0557,  0.1067],\n",
      "        [ 0.0710,  0.0500,  0.1053],\n",
      "        [ 0.1012,  0.1043,  0.0699]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) \n",
      " tensor([[1.3955, 1.5046, 0.8075],\n",
      "        [1.3822, 1.3650, 0.9065],\n",
      "        [1.4225, 1.4239, 0.7788],\n",
      "        [1.1977, 1.4176, 0.9610],\n",
      "        [1.4479, 1.3776, 0.7622]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "---- 21 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：21，Loss：199.12927055358887\n",
      "---- 22 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：22，Loss：229.1031837463379\n",
      "---- 23 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：23，Loss：233.2494888305664\n",
      "---- 24 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：24，Loss：224.097900390625\n",
      "---- 25 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：25，Loss：210.0471534729004\n",
      "---- 26 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：26，Loss：206.25478744506836\n",
      "---- 27 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：27，Loss：214.39093399047852\n",
      "---- 28 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：28，Loss：228.24794006347656\n",
      "---- 29 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：29，Loss：219.80841445922852\n",
      "---- 30 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：30，Loss：225.0253791809082\n",
      "tensor([[0.4542, 0.2725, 0.2732],\n",
      "        [0.3989, 0.3102, 0.2910],\n",
      "        [0.4666, 0.2815, 0.2519],\n",
      "        [0.4440, 0.2710, 0.2850],\n",
      "        [0.4624, 0.2833, 0.2543]], device='cuda:0', grad_fn=<SoftmaxBackward0>) \n",
      " tensor([[ 0.1318,  0.0704,  0.1017],\n",
      "        [ 0.0394,  0.0812,  0.1163],\n",
      "        [ 0.1000,  0.0583,  0.1093],\n",
      "        [ 0.0294,  0.0744,  0.1593],\n",
      "        [ 0.2801, -0.0172,  0.0999]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) \n",
      " tensor([[1.3813, 1.3121, 0.8463],\n",
      "        [1.1909, 1.3556, 0.9587],\n",
      "        [1.2645, 1.5314, 0.9134],\n",
      "        [1.3059, 1.3149, 0.8376],\n",
      "        [1.2314, 1.5348, 1.0456]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "---- 31 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：31，Loss：200.56144428253174\n",
      "---- 32 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：32，Loss：213.2611789703369\n",
      "---- 33 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：33，Loss：225.51422119140625\n",
      "---- 34 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：34，Loss：214.05668258666992\n",
      "---- 35 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：35，Loss：194.6441307067871\n",
      "---- 36 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：36，Loss：223.01802825927734\n",
      "---- 37 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：37，Loss：227.8685417175293\n",
      "---- 38 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：38，Loss：227.6341781616211\n",
      "---- 39 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：39，Loss：201.2316493988037\n",
      "---- 40 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：40，Loss：208.4307041168213\n",
      "tensor([[0.4325, 0.2975, 0.2699],\n",
      "        [0.4358, 0.2975, 0.2668],\n",
      "        [0.4297, 0.2750, 0.2953],\n",
      "        [0.4385, 0.2797, 0.2818],\n",
      "        [0.4218, 0.3017, 0.2765]], device='cuda:0', grad_fn=<SoftmaxBackward0>) \n",
      " tensor([[ 0.2120,  0.0723,  0.1142],\n",
      "        [ 0.0513,  0.1120,  0.0107],\n",
      "        [ 0.2425, -0.0029,  0.1640],\n",
      "        [ 0.0369,  0.0427,  0.1922],\n",
      "        [ 0.1898,  0.0417,  0.1297]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) \n",
      " tensor([[1.2974, 1.4602, 0.8664],\n",
      "        [1.2925, 1.2297, 0.8908],\n",
      "        [1.3490, 1.2816, 0.8830],\n",
      "        [1.0472, 1.5162, 1.1159],\n",
      "        [1.2406, 1.4837, 0.9658]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "---- 41 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：41，Loss：224.94987106323242\n",
      "---- 42 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：42，Loss：229.81848907470703\n",
      "---- 43 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：43，Loss：234.7306365966797\n",
      "---- 44 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：44，Loss：228.62687301635742\n",
      "---- 45 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：45，Loss：230.2480354309082\n",
      "---- 46 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：46，Loss：231.3477020263672\n",
      "---- 47 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：47，Loss：224.49777603149414\n",
      "---- 48 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：48，Loss：230.84906768798828\n",
      "---- 49 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：49，Loss：228.1921844482422\n",
      "---- 50 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：50，Loss：204.1091022491455\n",
      "tensor([[0.4500, 0.2873, 0.2627],\n",
      "        [0.4217, 0.3055, 0.2728],\n",
      "        [0.4038, 0.3036, 0.2926],\n",
      "        [0.4169, 0.3135, 0.2696],\n",
      "        [0.4354, 0.3006, 0.2640]], device='cuda:0', grad_fn=<SoftmaxBackward0>) \n",
      " tensor([[0.2076, 0.0533, 0.0644],\n",
      "        [0.1031, 0.0841, 0.0097],\n",
      "        [0.0055, 0.0538, 0.1271],\n",
      "        [0.0774, 0.1220, 0.0280],\n",
      "        [0.2581, 0.0317, 0.0711]], device='cuda:0', grad_fn=<AddmmBackward0>) \n",
      " tensor([[1.3991, 1.3761, 0.8092],\n",
      "        [1.2711, 1.3486, 0.9208],\n",
      "        [1.1946, 1.2081, 0.9480],\n",
      "        [1.1958, 1.3720, 0.9789],\n",
      "        [1.4032, 1.3921, 0.8486]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "---- 51 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：51，Loss：218.47055435180664\n",
      "---- 52 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：52，Loss：227.60088348388672\n",
      "---- 53 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：53，Loss：219.62572479248047\n",
      "---- 54 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：54，Loss：232.1849365234375\n",
      "---- 55 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：55，Loss：222.44219589233398\n",
      "---- 56 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：56，Loss：176.42920303344727\n",
      "---- 57 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：57，Loss：209.9135627746582\n",
      "---- 58 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：58，Loss：231.02275848388672\n",
      "---- 59 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：59，Loss：232.78338623046875\n",
      "---- 60 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：60，Loss：222.74608993530273\n",
      "tensor([[0.4228, 0.2868, 0.2905],\n",
      "        [0.4478, 0.2928, 0.2594],\n",
      "        [0.4412, 0.2721, 0.2867],\n",
      "        [0.4605, 0.2933, 0.2462],\n",
      "        [0.3535, 0.3260, 0.3205]], device='cuda:0', grad_fn=<SoftmaxBackward0>) \n",
      " tensor([[0.0501, 0.0969, 0.1194],\n",
      "        [0.3268, 0.0268, 0.0225],\n",
      "        [0.0038, 0.0584, 0.1303],\n",
      "        [0.0845, 0.0505, 0.0487],\n",
      "        [0.0114, 0.0782, 0.1683]], device='cuda:0', grad_fn=<AddmmBackward0>) \n",
      " tensor([[1.3923, 1.3782, 0.7615],\n",
      "        [1.4814, 1.4547, 0.8503],\n",
      "        [1.2386, 1.2951, 0.9198],\n",
      "        [1.3062, 1.4256, 0.8661],\n",
      "        [1.1193, 1.2197, 0.9872]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "---- 61 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：61，Loss：224.51481246948242\n",
      "---- 62 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：62，Loss：230.6790428161621\n",
      "---- 63 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：63，Loss：221.72759628295898\n",
      "---- 64 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：64，Loss：198.12216186523438\n",
      "---- 65 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：65，Loss：226.37158203125\n",
      "---- 66 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：66，Loss：214.07273864746094\n",
      "---- 67 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：67，Loss：218.14868545532227\n",
      "---- 68 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：68，Loss：226.05680465698242\n",
      "---- 69 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：69，Loss：200.59400749206543\n",
      "---- 70 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：70，Loss：214.46406173706055\n",
      "tensor([[0.4476, 0.2804, 0.2719],\n",
      "        [0.4157, 0.2901, 0.2943],\n",
      "        [0.4148, 0.2949, 0.2903],\n",
      "        [0.3825, 0.3208, 0.2967],\n",
      "        [0.3904, 0.3127, 0.2969]], device='cuda:0', grad_fn=<SoftmaxBackward0>) \n",
      " tensor([[ 0.1426,  0.0893,  0.1377],\n",
      "        [ 0.1068,  0.1258,  0.1349],\n",
      "        [-0.0697,  0.0683,  0.1572],\n",
      "        [-0.0193,  0.1184,  0.0211],\n",
      "        [ 0.1452,  0.0477,  0.1071]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) \n",
      " tensor([[1.3278, 1.3723, 0.8268],\n",
      "        [1.1618, 1.3022, 1.0569],\n",
      "        [1.1745, 1.3648, 0.9017],\n",
      "        [1.2889, 1.2346, 0.8532],\n",
      "        [1.2737, 1.2379, 0.9229]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "---- 71 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：71，Loss：224.93438339233398\n",
      "---- 72 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：72，Loss：227.55535125732422\n",
      "---- 73 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：73，Loss：230.79494857788086\n",
      "---- 74 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：74，Loss：207.48429679870605\n",
      "---- 75 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：75，Loss：225.40002822875977\n",
      "---- 76 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：76，Loss：200.7053394317627\n",
      "---- 77 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：77，Loss：237.30739212036133\n",
      "---- 78 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：78，Loss：224.3636817932129\n",
      "---- 79 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：79，Loss：227.52517318725586\n",
      "---- 80 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：80，Loss：231.4684181213379\n",
      "tensor([[0.4333, 0.2920, 0.2747],\n",
      "        [0.4893, 0.2565, 0.2542],\n",
      "        [0.4541, 0.2717, 0.2742],\n",
      "        [0.3936, 0.3071, 0.2993],\n",
      "        [0.4239, 0.3045, 0.2716]], device='cuda:0', grad_fn=<SoftmaxBackward0>) \n",
      " tensor([[0.0948, 0.0790, 0.0452],\n",
      "        [0.2003, 0.0221, 0.1830],\n",
      "        [0.1138, 0.0359, 0.1788],\n",
      "        [0.0196, 0.1095, 0.0654],\n",
      "        [0.2090, 0.0420, 0.0612]], device='cuda:0', grad_fn=<AddmmBackward0>) \n",
      " tensor([[1.2739, 1.2474, 0.9507],\n",
      "        [1.3118, 1.5962, 0.9133],\n",
      "        [1.2637, 1.3520, 0.8968],\n",
      "        [1.1837, 1.2628, 0.9797],\n",
      "        [1.4424, 1.3811, 0.7957]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "---- 81 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：81，Loss：223.82802963256836\n",
      "---- 82 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：82，Loss：203.50681114196777\n",
      "---- 83 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：83，Loss：214.6356315612793\n",
      "---- 84 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：84，Loss：200.30301094055176\n",
      "---- 85 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：85，Loss：221.38182830810547\n",
      "---- 86 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：86，Loss：228.3304786682129\n",
      "---- 87 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：87，Loss：217.4179401397705\n",
      "---- 88 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：88，Loss：226.9830665588379\n",
      "---- 89 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：89，Loss：220.9624252319336\n",
      "---- 90 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：90，Loss：230.13837814331055\n",
      "tensor([[0.4467, 0.2904, 0.2629],\n",
      "        [0.4139, 0.2947, 0.2914],\n",
      "        [0.4575, 0.2820, 0.2605],\n",
      "        [0.4851, 0.2634, 0.2515],\n",
      "        [0.4129, 0.3187, 0.2685]], device='cuda:0', grad_fn=<SoftmaxBackward0>) \n",
      " tensor([[ 0.1112,  0.0588,  0.0730],\n",
      "        [ 0.1883,  0.0646,  0.1533],\n",
      "        [ 0.1264,  0.0724,  0.0939],\n",
      "        [ 0.1781,  0.0331,  0.0662],\n",
      "        [ 0.1024,  0.0996, -0.0320]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) \n",
      " tensor([[1.2708, 1.4651, 0.9028],\n",
      "        [1.3992, 1.4274, 0.8041],\n",
      "        [1.3286, 1.3937, 0.8802],\n",
      "        [1.3532, 1.5523, 0.8925],\n",
      "        [1.4294, 1.2704, 0.7779]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "---- 91 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：91，Loss：227.00064086914062\n",
      "---- 92 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：92，Loss：222.79491806030273\n",
      "---- 93 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：93，Loss：216.17182540893555\n",
      "---- 94 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：94，Loss：217.98608779907227\n",
      "---- 95 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：95，Loss：221.13225936889648\n",
      "---- 96 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：96，Loss：197.99715995788574\n",
      "---- 97 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：97，Loss：225.48351669311523\n",
      "---- 98 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：98，Loss：230.8142967224121\n",
      "---- 99 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：99，Loss：227.34999084472656\n",
      "---- 100 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：100，Loss：231.10811614990234\n",
      "tensor([[0.4193, 0.2823, 0.2984],\n",
      "        [0.4279, 0.2829, 0.2892],\n",
      "        [0.4620, 0.2732, 0.2648],\n",
      "        [0.4425, 0.2936, 0.2640],\n",
      "        [0.4177, 0.2804, 0.3019]], device='cuda:0', grad_fn=<SoftmaxBackward0>) \n",
      " tensor([[0.0623, 0.0097, 0.2392],\n",
      "        [0.1053, 0.0849, 0.1675],\n",
      "        [0.2234, 0.0655, 0.1025],\n",
      "        [0.0897, 0.0853, 0.0745],\n",
      "        [0.0824, 0.0872, 0.1682]], device='cuda:0', grad_fn=<AddmmBackward0>) \n",
      " tensor([[1.0838, 1.4543, 1.1085],\n",
      "        [1.3389, 1.4016, 0.7989],\n",
      "        [1.4220, 1.3971, 0.8271],\n",
      "        [1.1450, 1.4408, 1.0791],\n",
      "        [1.3767, 1.3211, 0.7978]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "---- 101 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：101，Loss：217.43379974365234\n",
      "---- 102 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：102，Loss：219.27700805664062\n",
      "---- 103 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：103，Loss：222.99763107299805\n",
      "---- 104 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：104，Loss：229.88565826416016\n",
      "---- 105 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：105，Loss：222.61191940307617\n",
      "---- 106 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：106，Loss：224.8405876159668\n",
      "---- 107 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：107，Loss：228.32724380493164\n",
      "---- 108 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：108，Loss：222.9049186706543\n",
      "---- 109 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：109，Loss：227.60348892211914\n",
      "---- 110 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：110，Loss：221.01714706420898\n",
      "tensor([[0.4260, 0.2920, 0.2820],\n",
      "        [0.4205, 0.3127, 0.2669],\n",
      "        [0.3876, 0.3027, 0.3096],\n",
      "        [0.4524, 0.2819, 0.2657],\n",
      "        [0.3976, 0.3120, 0.2903]], device='cuda:0', grad_fn=<SoftmaxBackward0>) \n",
      " tensor([[-0.0115,  0.1028,  0.1232],\n",
      "        [ 0.2705,  0.0312, -0.0083],\n",
      "        [ 0.0161,  0.0738,  0.1712],\n",
      "        [ 0.2044,  0.0822,  0.1493],\n",
      "        [ 0.0263,  0.1190,  0.1706]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) \n",
      " tensor([[1.1203, 1.2688, 1.0213],\n",
      "        [1.4849, 1.3290, 0.8117],\n",
      "        [1.1325, 1.3211, 0.9808],\n",
      "        [1.3074, 1.4789, 0.8812],\n",
      "        [1.1518, 1.4465, 0.9772]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "---- 111 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：111，Loss：227.76195526123047\n",
      "---- 112 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：112，Loss：194.52091598510742\n",
      "---- 113 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：113，Loss：216.83034133911133\n",
      "---- 114 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：114，Loss：218.26353073120117\n",
      "---- 115 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：115，Loss：232.42748260498047\n",
      "---- 116 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：116，Loss：218.08347702026367\n",
      "---- 117 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：117，Loss：228.1233024597168\n",
      "---- 118 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：118，Loss：232.6814422607422\n",
      "---- 119 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：119，Loss：231.6604995727539\n",
      "---- 120 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：120，Loss：214.67132186889648\n",
      "tensor([[0.4372, 0.2989, 0.2639],\n",
      "        [0.4431, 0.2837, 0.2732],\n",
      "        [0.4271, 0.2795, 0.2934],\n",
      "        [0.4155, 0.3129, 0.2716],\n",
      "        [0.4371, 0.2840, 0.2789]], device='cuda:0', grad_fn=<SoftmaxBackward0>) \n",
      " tensor([[ 0.1467,  0.0649,  0.0771],\n",
      "        [ 0.2263,  0.0008,  0.1594],\n",
      "        [-0.0662,  0.0654,  0.1245],\n",
      "        [ 0.1055,  0.0670, -0.0062],\n",
      "        [ 0.1662,  0.0719,  0.1384]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) \n",
      " tensor([[1.2924, 1.4423, 0.9087],\n",
      "        [1.1399, 1.3197, 1.1091],\n",
      "        [1.1853, 1.1693, 0.9334],\n",
      "        [1.3067, 1.3763, 0.8990],\n",
      "        [1.2861, 1.3875, 0.8584]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "---- 121 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：121，Loss：206.11503410339355\n",
      "---- 122 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：122，Loss：227.97663116455078\n",
      "---- 123 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：123，Loss：219.2752914428711\n",
      "---- 124 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：124，Loss：218.62987518310547\n",
      "---- 125 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：125，Loss：201.12070274353027\n",
      "---- 126 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：126，Loss：225.16361618041992\n",
      "---- 127 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：127，Loss：204.5639305114746\n",
      "---- 128 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：128，Loss：221.42591857910156\n",
      "---- 129 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：129，Loss：223.2321434020996\n",
      "---- 130 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：130，Loss：226.06893157958984\n",
      "tensor([[0.4273, 0.2983, 0.2743],\n",
      "        [0.4460, 0.2826, 0.2715],\n",
      "        [0.4468, 0.2803, 0.2729],\n",
      "        [0.4220, 0.3110, 0.2670],\n",
      "        [0.4461, 0.2797, 0.2742]], device='cuda:0', grad_fn=<SoftmaxBackward0>) \n",
      " tensor([[ 0.1749,  0.0350,  0.0815],\n",
      "        [-0.0261,  0.1095,  0.1003],\n",
      "        [ 0.1155,  0.0454,  0.0628],\n",
      "        [ 0.1217,  0.0084,  0.1117],\n",
      "        [ 0.0746,  0.0327,  0.1609]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) \n",
      " tensor([[1.2869, 1.3859, 0.9453],\n",
      "        [1.2402, 1.2828, 0.8711],\n",
      "        [1.3702, 1.5234, 0.8410],\n",
      "        [1.3118, 1.3954, 0.8583],\n",
      "        [1.2252, 1.4238, 0.9401]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "---- 131 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：131，Loss：225.50367736816406\n",
      "---- 132 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：132，Loss：227.4811019897461\n",
      "---- 133 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：133，Loss：233.3986701965332\n",
      "---- 134 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：134，Loss：232.45145797729492\n",
      "---- 135 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：135，Loss：221.4384880065918\n",
      "---- 136 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：136，Loss：200.14491271972656\n",
      "---- 137 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：137，Loss：227.8125\n",
      "---- 138 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：138，Loss：219.52249145507812\n",
      "---- 139 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：139，Loss：212.69874954223633\n",
      "---- 140 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：140，Loss：218.4019317626953\n",
      "tensor([[0.4598, 0.2742, 0.2660],\n",
      "        [0.4177, 0.2952, 0.2872],\n",
      "        [0.4276, 0.3037, 0.2686],\n",
      "        [0.4347, 0.2877, 0.2776],\n",
      "        [0.4212, 0.3013, 0.2774]], device='cuda:0', grad_fn=<SoftmaxBackward0>) \n",
      " tensor([[0.2320, 0.0378, 0.1371],\n",
      "        [0.1712, 0.0544, 0.1388],\n",
      "        [0.1383, 0.0596, 0.0730],\n",
      "        [0.1011, 0.0169, 0.1573],\n",
      "        [0.1923, 0.0727, 0.0256]], device='cuda:0', grad_fn=<AddmmBackward0>) \n",
      " tensor([[1.3672, 1.4465, 0.8724],\n",
      "        [1.3826, 1.3430, 0.7965],\n",
      "        [1.3077, 1.3709, 0.8857],\n",
      "        [1.1556, 1.4172, 1.0159],\n",
      "        [1.4363, 1.2093, 0.7996]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "---- 141 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：141，Loss：211.70358276367188\n",
      "---- 142 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：142，Loss：227.4460906982422\n",
      "---- 143 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：143，Loss：216.64828872680664\n",
      "---- 144 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：144，Loss：232.63018798828125\n",
      "---- 145 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：145，Loss：213.6152057647705\n",
      "---- 146 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：146，Loss：229.73216247558594\n",
      "---- 147 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：147，Loss：231.7825813293457\n",
      "---- 148 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：148，Loss：217.0200023651123\n",
      "---- 149 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：149，Loss：244.85379791259766\n",
      "---- 150 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：150，Loss：210.77415084838867\n",
      "tensor([[0.4162, 0.3005, 0.2833],\n",
      "        [0.4511, 0.2880, 0.2609],\n",
      "        [0.4312, 0.3012, 0.2676],\n",
      "        [0.4484, 0.2821, 0.2694],\n",
      "        [0.4439, 0.2773, 0.2788]], device='cuda:0', grad_fn=<SoftmaxBackward0>) \n",
      " tensor([[-0.0125,  0.1068,  0.1632],\n",
      "        [ 0.1686,  0.0712,  0.0358],\n",
      "        [ 0.2332,  0.0111,  0.0676],\n",
      "        [ 0.2339,  0.0022,  0.1007],\n",
      "        [ 0.1602,  0.0301,  0.1621]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) \n",
      " tensor([[1.2355, 1.4931, 0.8684],\n",
      "        [1.3386, 1.3402, 0.8855],\n",
      "        [1.3626, 1.3290, 0.8631],\n",
      "        [1.5059, 1.4775, 0.7627],\n",
      "        [1.1955, 1.3407, 1.0036]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "---- 151 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：151，Loss：220.10761642456055\n",
      "---- 152 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：152，Loss：199.57176971435547\n",
      "---- 153 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：153，Loss：230.50635528564453\n",
      "---- 154 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：154，Loss：226.03534317016602\n",
      "---- 155 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：155，Loss：214.02636337280273\n",
      "---- 156 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：156，Loss：214.89928817749023\n",
      "---- 157 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：157，Loss：236.76710510253906\n",
      "---- 158 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：158，Loss：225.41960906982422\n",
      "---- 159 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：159，Loss：229.9753074645996\n",
      "---- 160 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：160，Loss：232.3493537902832\n",
      "tensor([[0.4217, 0.2919, 0.2864],\n",
      "        [0.4247, 0.2749, 0.3005],\n",
      "        [0.4120, 0.2938, 0.2942],\n",
      "        [0.4408, 0.3002, 0.2590],\n",
      "        [0.4334, 0.2822, 0.2844]], device='cuda:0', grad_fn=<SoftmaxBackward0>) \n",
      " tensor([[0.2593, 0.0297, 0.0662],\n",
      "        [0.1819, 0.0558, 0.0790],\n",
      "        [0.0636, 0.1073, 0.0125],\n",
      "        [0.0466, 0.1072, 0.0719],\n",
      "        [0.1812, 0.0724, 0.1214]], device='cuda:0', grad_fn=<AddmmBackward0>) \n",
      " tensor([[1.4186, 1.2802, 0.8454],\n",
      "        [1.4639, 1.2554, 0.8170],\n",
      "        [1.5203, 1.2826, 0.7289],\n",
      "        [1.1862, 1.4426, 0.9708],\n",
      "        [1.3795, 1.2972, 0.8247]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "---- 161 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：161，Loss：209.73997116088867\n",
      "---- 162 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：162，Loss：224.65900421142578\n",
      "---- 163 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：163，Loss：226.92814254760742\n",
      "---- 164 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：164，Loss：226.77428436279297\n",
      "---- 165 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：165，Loss：230.02362060546875\n",
      "---- 166 batch----\n",
      "After flatten, x's shape:  torch.Size([5, 900])\n",
      "pi's shape:  torch.Size([5, 3])\n",
      "mu's shape:  torch.Size([5, 3])\n",
      "sigma's shape:  torch.Size([5, 3])\n",
      "训练次数：166，Loss：230.29153442382812\n",
      "---- 167 batch----\n",
      "After flatten, x's shape:  torch.Size([2, 900])\n",
      "pi's shape:  torch.Size([2, 3])\n",
      "mu's shape:  torch.Size([2, 3])\n",
      "sigma's shape:  torch.Size([2, 3])\n",
      "训练次数：167，Loss：93.75151443481445\n"
     ]
    }
   ],
   "source": [
    "# filename = \"../log_file.txt\"\n",
    "# f = open(filename,'w')\n",
    "total_train_step = 0\n",
    "mlp.train()\n",
    "for epoch in range(0,1):\n",
    "    for batch_id,data in enumerate(train_loader):\n",
    "\n",
    "        input_data, target = data\n",
    "        print(f\"---- {batch_id} batch----\")\n",
    "\n",
    "        # do the inference\n",
    "        input_data = input_data.to(device)\n",
    "        pi, mu, sigma = mlp(input_data)\n",
    "        # print(f\"The [pi,mu,sigma] is : \\n\")\n",
    "        # print(pi,\"\\n\",mu,\"\\n\",sigma)\n",
    "\n",
    "        # save the params\n",
    "        # params = list(mlp.named_parameters())\n",
    "\n",
    "        # cal the loss and draw the MDN\n",
    "        duration,m  = loss_preparation(pi.detach(), mu.detach(), sigma.detach(), target)\n",
    "        # draw_mdn(pi,duration,m,total_train_step)\n",
    "        loss = loss_fn(pi,duration,m)\n",
    "        draw_loss(total_train_step, loss.item())\n",
    "        print(\"训练次数：{}，Loss：{}\".format(total_train_step, loss.item()))\n",
    "\n",
    "\n",
    "        ########### before step() ###############\n",
    "\n",
    "        # for name, parms in mlp.named_parameters():\n",
    "        #     print(\"=====before step()=====\")\n",
    "        #     print('-->name:', name)\n",
    "        #     print('-->para:', parms)\n",
    "        #     print('-->grad_requirs:',parms.requires_grad)\n",
    "        #     print('-->grad_value:',parms.grad)\n",
    "        #     print(\"===\")\n",
    "\n",
    "        # optim\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        # for name, parms in mlp.named_parameters():\n",
    "        #     print(\"=====After step()=====\")\n",
    "        #     print('-->name:', name)\n",
    "        #     print('-->para:', parms)\n",
    "        #     print('-->grad_requirs:',parms.requires_grad)\n",
    "        #     print('-->grad_value:',parms.grad)\n",
    "        #     print(\"===\")\n",
    "\n",
    "        # print(\"训练次数：{}，Loss：{}, Loss's grad: {}\".format(total_train_step, loss.item(), loss.grad))\n",
    "\n",
    "        if total_train_step % 10 == 0:\n",
    "            print(pi,\"\\n\",mu,\"\\n\",sigma)\n",
    "\n",
    "        total_train_step += 1\n",
    "\n",
    "# f.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [],
   "source": [
    "# torch.save(mlp.state_dict(), 'mlp_init_loss_17.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "$\\mathcal{L}(y \\vert x) = - \\log\\bigg\\{\\sum_{k=1}^K \\pi_k(x)  \\mathcal{N}\\big(y \\vert \\mu_k(x), \\Sigma_k(x)\\big)\\bigg\\}$"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
