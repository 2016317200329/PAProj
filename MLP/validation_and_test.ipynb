{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 0. Intro\n",
    "\n",
    "# 1. Preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from common import *\n",
    "from utils import *\n",
    "from mydataset import *\n",
    "from my_collate_fn import my_collate_fn_3\n",
    "\n",
    "from Config.config_MDN import DefaultConfig\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. 执行validation\n",
    "- 注意使用的validation function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "MODEL_LIST = [\"GT1(MDN)\",\"GT2(MDN)\",\"GT3\",\"EMD\"]\n",
    "class Conv_block_4(nn.Module):\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def __init__(self, ch_out=1,kernel_size=9, stride=3, init_weight=True) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.kernel_size = (1,kernel_size)\n",
    "        self.stride = (1,stride)\n",
    "        self.ln_in = int((300-kernel_size)/stride+1)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=ch_out, kernel_size=self.kernel_size, stride=self.stride, padding=0, dilation=(1,1))\n",
    "\n",
    "        self.BN_aff2 = nn.BatchNorm1d(num_features=self.ln_in,affine=True)      # works better\n",
    "\n",
    "        if init_weight:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Conv=>BN=>AC\n",
    "        x = self.conv(x)\n",
    "        # print(x.shape)\n",
    "        # 方法一：\n",
    "        # x = torch.squeeze(x,dim=2)\n",
    "        # x = self.ac_func(self.BN_aff1(x))\n",
    "\n",
    "        # 方法二：works better\n",
    "        x = torch.flatten(x,start_dim=1)\n",
    "        x = self.ac_func(self.BN_aff2(x))\n",
    "        # x = self.ac_func(self.BN_aff1(x))\n",
    "        return x\n",
    "\n",
    "class Conv_1_4(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians, ch_out=1, kernel_size=9, stride=3) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln_in = int((300-kernel_size)/stride+1)\n",
    "\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=1,affine=True)\n",
    "        # self.IN1 = nn.InstanceNorm1d(num_features=3,affine=True)\n",
    "        self.layer_pi = Conv_block_4(ch_out=1,kernel_size=kernel_size,stride=stride)\n",
    "        self.layer_scale = Conv_block_4(ch_out=1,kernel_size=kernel_size,stride=stride)\n",
    "        self.layer_shape = Conv_block_4(ch_out=1,kernel_size=kernel_size,stride=stride)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(self.ln_in, n_gaussians),\n",
    "            nn.Softmax(dim=1)           # dim=0是B, dim=1才是feature\n",
    "        )\n",
    "        self.z_scale = nn.Linear(self.ln_in, n_gaussians)\n",
    "        self.z_shape = nn.Linear(self.ln_in, n_gaussians)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.BN1(x)\n",
    "        # x = self.IN1(x)\n",
    "        x = torch.unsqueeze(x,dim=1)                     # torch.Size([B, 1, 3, 300])\n",
    "\n",
    "        x_pi = self.layer_pi(x)\n",
    "        x_scale = self.layer_scale(x)\n",
    "        x_shape = self.layer_shape(x)\n",
    "\n",
    "        pi = self.z_pi(x_pi)\n",
    "        scale = torch.exp(self.z_scale(x_scale))\n",
    "        scale = torch.clamp(scale,1e-4)\n",
    "        shape = torch.exp(self.z_shape(x_shape))\n",
    "        shape = torch.clamp(shape,1e-4)\n",
    "\n",
    "        return pi,scale,shape\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "seed = 62\n",
    "setup_seed(seed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 98])",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_23424\\263983507.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     26\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mno_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 27\u001B[1;33m         \u001B[0mtotal_test_metric\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mGT_metric\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mvalidate_KL\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmlp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_loader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mopt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mN_gaussians\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mopt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mMIN_LOSS\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     28\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     29\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"========== seed = {seed} ==========\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Desktop\\PROJ\\PAProj\\MLP\\loss.py\u001B[0m in \u001B[0;36mvalidate_KL\u001B[1;34m(mlp, data_loader, N_gaussians, MIN_LOSS, device, THRESHOLD)\u001B[0m\n\u001B[0;32m    298\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m   \u001B[1;31m# 不计算少于THRESHOLD的auction\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    299\u001B[0m             \u001B[1;32mcontinue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 300\u001B[1;33m         \u001B[0mpi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmu\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msigma\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmlp\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput_data\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    301\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    302\u001B[0m         \u001B[1;31m# Compute the error/ metric\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1188\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1191\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1192\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_23424\\2119455924.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     67\u001B[0m         \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munsqueeze\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mdim\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m                     \u001B[1;31m# torch.Size([B, 1, 3, 300])\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     68\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 69\u001B[1;33m         \u001B[0mx_pi\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayer_pi\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     70\u001B[0m         \u001B[0mx_scale\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayer_scale\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     71\u001B[0m         \u001B[0mx_shape\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayer_shape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1188\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1191\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1192\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_23424\\2119455924.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     35\u001B[0m         \u001B[1;31m# 方法二：works better\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     36\u001B[0m         \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mflatten\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mstart_dim\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 37\u001B[1;33m         \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mac_func\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mBN_aff2\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     38\u001B[0m         \u001B[1;31m# x = self.ac_func(self.BN_aff1(x))\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     39\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1188\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1191\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1192\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    169\u001B[0m         \u001B[0mused\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mnormalization\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m \u001B[1;32min\u001B[0m \u001B[0meval\u001B[0m \u001B[0mmode\u001B[0m \u001B[0mwhen\u001B[0m \u001B[0mbuffers\u001B[0m \u001B[0mare\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    170\u001B[0m         \"\"\"\n\u001B[1;32m--> 171\u001B[1;33m         return F.batch_norm(\n\u001B[0m\u001B[0;32m    172\u001B[0m             \u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    173\u001B[0m             \u001B[1;31m# If buffers are not to be tracked, ensure that they won't be updated\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\Lib\\site-packages\\torch\\nn\\functional.py\u001B[0m in \u001B[0;36mbatch_norm\u001B[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001B[0m\n\u001B[0;32m   2446\u001B[0m         )\n\u001B[0;32m   2447\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mtraining\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2448\u001B[1;33m         \u001B[0m_verify_batch_size\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2449\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2450\u001B[0m     return torch.batch_norm(\n",
      "\u001B[1;32mD:\\Anaconda\\Lib\\site-packages\\torch\\nn\\functional.py\u001B[0m in \u001B[0;36m_verify_batch_size\u001B[1;34m(size)\u001B[0m\n\u001B[0;32m   2414\u001B[0m         \u001B[0msize_prods\u001B[0m \u001B[1;33m*=\u001B[0m \u001B[0msize\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2415\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0msize_prods\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2416\u001B[1;33m         \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Expected more than 1 value per channel when training, got input size {}\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2417\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2418\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 98])"
     ]
    }
   ],
   "source": [
    "for MODEL_NAME in MODEL_LIST:\n",
    "    opt = DefaultConfig(MODEL_NAME=MODEL_NAME)\n",
    "    mlp = Conv_1_4(opt.N_gaussians).to(device)\n",
    "\n",
    "    model_path = get_MDN_save_path(opt.ARTIFICIAL, seed, opt.net_root_path, opt.noise_pct, MODEL_NAME)\n",
    "    mlp, hyperparameters = load_checkpoint(model_path, mlp)\n",
    "\n",
    "    dataset = myDataset(opt.train_path, opt.target_path_metric, opt.target_path_loss, opt.data_key_path, opt.NLL_metric_path)\n",
    "    shuffled_indices = save_data_idx(dataset, opt.arr_flag)\n",
    "    train_idx, val_idx, test_idx = get_data_idx(shuffled_indices, opt.train_pct, opt.vali_pct,opt.SET_VAL)\n",
    "\n",
    "    if MODEL_NAME == \"GT1(MDN)\":\n",
    "        INPUT_LIST = [1]\n",
    "    elif MODEL_NAME == \"GT2(MDN)\":\n",
    "        INPUT_LIST=[2]\n",
    "    elif MODEL_NAME == \"GT3\":\n",
    "        INPUT_LIST=[3]\n",
    "    elif MODEL_NAME == \"EMD\":\n",
    "        INPUT_LIST=[4]\n",
    "    else:\n",
    "        assert f\"Wrong Model Name! The Name Has to be One of {MODEL_LIST}\"\n",
    "\n",
    "    my_collate_fn = functools.partial(my_collate_fn_3, INPUT_LIST=INPUT_LIST)\n",
    "    _,_,test_loader = get_data_loader(dataset, opt.batch_size, train_idx, val_idx, test_idx, my_collate_fn)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_test_metric, GT_metric = validate_KL(mlp, test_loader, opt.N_gaussians, opt.MIN_LOSS, device)\n",
    "\n",
    "    print(f\"========== seed = {seed} ==========\")\n",
    "    print(f\"========== MODEL_NAME = {MODEL_NAME} ==========\")\n",
    "\n",
    "    print(f\"========== total_test_metric: {total_test_metric} ==========\")\n",
    "    print(f\"========== GT_metric: {GT_metric} ==========\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "for epoch in range(0):\n",
    "    print(f\"aaa\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
