{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 0.\n",
    "\n",
    "1. 0个GT，只有context features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Preparations\n",
    "## 1.1 global settings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import torch.utils.data\n",
    "from mydataset import *\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from tensorboardX import SummaryWriter\n",
    "from visdom import Visdom\n",
    "from graphviz import Digraph\n",
    "from torchviz import make_dot\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pygraphviz as pgv\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import geomloss\n",
    "import time\n",
    "\n",
    "import scipy\n",
    "from torch.nn import KLDivLoss\n",
    "from torch.autograd.gradcheck import gradcheck\n",
    "from config import bcolors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from config import DefaultConfig\n",
    "opt = DefaultConfig()\n",
    "\n",
    "# nums of Gaussian kernels\n",
    "N_gaussians = opt.N_gaussians\n",
    "\n",
    "# dataset划分\n",
    "batch_size = opt.batch_size\n",
    "train_pct = opt.train_pct\n",
    "vali_pct = opt.vali_pct\n",
    "test_pct = opt.test_pct\n",
    "\n",
    "# train and optim.\n",
    "learning_rate = opt.learning_rate\n",
    "lr_for_mu = opt.lr_for_mu   # 给mu单独设置learning rate\n",
    "\n",
    "total_train_step = 0\n",
    "total_test_step = 0\n",
    "\n",
    "MIN_LOSS = opt.MIN_LOSS\n",
    "SAFETY = opt.SAFETY\n",
    "\n",
    "# Range【一般用不到】\n",
    "a = 0\n",
    "b = 255\n",
    "\n",
    "# Training data的粒度，画图会使用到\n",
    "SCALE = opt.SCALE\n",
    "# Target data是target_5时\n",
    "TARGET = opt.TARGET\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 the data path\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Training data\n",
    "train_path = opt.train_path\n",
    "\n",
    "# Target data\n",
    "target_path = opt.target_path\n",
    "\n",
    "# data keys\n",
    "data_key_path = opt.data_key_path\n",
    "\n",
    "# NLL metric\n",
    "NLL_metric_path = opt.NLL_metric_path\n",
    "\n",
    "# Net path\n",
    "net_root_path = opt.net_root_path"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Dataset and Dataloader\n",
    "1. DataLoader中的`shuffer=True`表示在每一次epoch中都打乱所有数据的顺序，然后以batch为单位从头到尾按顺序取用数据。这样的结果就是不同epoch中的数据都是乱序的,设置随机种子的作用就是让你的每一次训练都乱的一样，\n",
    "\n",
    "## 2.1 Dataset and spliting\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 设置随机数种子"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup_seed(7)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "dataset = myDataset(train_path, target_path, data_key_path, NLL_metric_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 807  305  455  939  508  594  835 1082  598 1102   46  761  841  141\n",
      "  407  334  253  500  734  936  698  446  907 1087 1009  140  463  547\n",
      " 1155  856   34 1156  703 1121  751  587 1100  509  473  128  788   97\n",
      "  471  385 1085  525  679 1135  284 1146  186  318 1088 1113    9  457\n",
      " 1047  451  569 1120 1074  326  377  809  109   98  620 1194  825  828\n",
      "   83  113  556  674  568  853  351  558   54  656  804 1149  101  344\n",
      "  851  544  955   40 1021  489  626  664  657  868 1169  151  179 1130\n",
      "  564  713  743 1014  966  861 1132  146  610  408  662  551  172 1020\n",
      "  982 1148  431  517  270  858  170  374  816  618  205   17   53 1003\n",
      "  263  857  716  843  498  228  339  725  752  278  649 1017  108  642\n",
      " 1195 1174   99  530  632  888  189  961  358 1078  663  757 1051  204\n",
      "  409  283  562   23  619  216  474  921  950 1162  123  785  769  621\n",
      "  262  586 1178  541  795   70 1189  396  171  845  168 1125  361  224\n",
      "  125  706  164  231  264   42  746  872  998  132 1035  430   38  522\n",
      "  880  325  285  367  870  418   13  166  513  901  617  740  680 1126\n",
      "   20  372   71 1038  355   60  220  110    4 1115  572  415  336  316\n",
      "  449  412 1066  768 1055  704  869   94   95 1163 1145  648  884  885\n",
      " 1170  848   58  207  306 1067  729   24  644  633  252  832  244   25\n",
      " 1094   63  222 1070  996 1063  130  153 1180  922  399  997   75  894\n",
      " 1048  308  280  934  721  199  820  134  289  259 1159    3  510  492\n",
      "  196  565  563   26  913  483 1141  653   61  623  984  652  609  739\n",
      "  824 1018  891  467  842    8  503  714 1015  447  783  992   74 1184\n",
      "   57  978  484  297 1039  217  681 1027  960  350 1129 1045 1183  906\n",
      "  813  660  266    6  379  526  611  138  163  701 1031 1050   43  733\n",
      "  720  319 1158 1153  877  294  432  531  578  523   19  426  830  927\n",
      "  781 1033 1013  420  137  488  185  920  702 1134 1124  454  327  317\n",
      "  799  655  878  699  981  806  953  750  148  591  915  731  937  365\n",
      "  557  397 1173  779 1185  459  232  389 1147 1011  158  315  256  863\n",
      "   96 1157  445  384  511  817  328  314  357  347  452  298  899  887\n",
      " 1034  839  778  747  249  766   15  159  696  585  360   48  478  330\n",
      "  893  112  338  307  528  422 1142  971  506  504 1049  791  395  669\n",
      "  540  127 1160  902  507  980  237  167  602  983   80  414  640  300\n",
      "  977  324 1117  215 1058   30  242  697  860  951  209  671  742 1072\n",
      "  435   27 1060  518  122 1086  462 1028 1118  603  929  732  722  493\n",
      "  202  403 1089 1179  362  271  466  641 1061  744  272  277 1177   12\n",
      "  597  668  210  651  952  147  442  299  115  375  635  767  748 1080\n",
      "  755  177  472  685   51  810 1079  571  388   22  650  965  423  938\n",
      " 1112  643 1193  142   50  666 1143   81  482  321  948  687 1138   66\n",
      "  909  534  738  417 1005   64  895  886  918  491  700  790   65  968\n",
      "  627  999  296  419  438  261  837  281  631  677  188  881  251  694\n",
      "  514  590  487  273  943 1073  448  411 1131  933 1191  935  149  218\n",
      "  239  718  192  191  945  754  665  705  595  926  708  582 1046  323\n",
      " 1026 1071  658  437  770  833  341    2  229 1098 1152   76 1103  371\n",
      "  180  889  577  736  758 1019 1105 1010  589 1167  812  956  882   37\n",
      "  353 1095  236  928  499  553   55  302  286  800  219  789  387  737\n",
      "  390  352  193  235 1110  480  567  116 1057  957 1106  890  777  120\n",
      "  815  990  773   62  667  453  515  570  223  613  221  673  794  465\n",
      "   52 1093  485 1140 1053  155  912  600 1023  248  441  975 1164  542\n",
      "  117  724  628  712  916  291 1041  381  954  200  596  976 1137 1187\n",
      "  691  505  348 1190  340  615  154 1104  879  995  287  322  819  818\n",
      "  796   82  793  464  797  756 1168  771  333   10  470   32  782  413\n",
      "  580  382  930  760 1133  840 1139  378   89  792  601  213  332  133\n",
      "  346  855    7  941  320 1044  187  972  552  304   31  831  684 1065\n",
      "  240  993  174  245   56  630  583 1025  145   90 1114  392  682  310\n",
      "  359   93  947 1024  376  243 1076   87  214  444  173  425 1040  118\n",
      "  119 1151  905    0  394  401  404  836 1091  162  114  670  150  798\n",
      "  875 1064  368  543  477 1128  532  625  967  512  801   72  258  624\n",
      "  234  135  616  516  866  802  592 1056  501  538  897  728  581  343\n",
      "  925 1002  293  364  883  659   14  686   86  370  165  131  335   59\n",
      "  709  676  354  959  424  292   41  329  614  429   79]\n"
     ]
    }
   ],
   "source": [
    "# shuffled_indices = np.random.permutation(dataset.__len__())\n",
    "\n",
    "# 读取indx\n",
    "shuffled_indices=np.load('shuffled_indices.npy')\n",
    "\n",
    "train_idx = shuffled_indices[:int(train_pct*dataset.__len__())]\n",
    "# train_idx = shuffled_indices\n",
    "tmp = int((train_pct+vali_pct)*dataset.__len__())\n",
    "val_idx = shuffled_indices[int(train_pct*dataset.__len__()):tmp]\n",
    "\n",
    "test_idx = shuffled_indices[tmp:]\n",
    "print(train_idx)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Dataloader and collating\n",
    "1. 主要是对label数据进行collate\n",
    "    - 按照batch中的最大target data长度进行padding，padding with 0\n",
    "2. 返回的结果多一个batch dim,比如下面的`5`\n",
    "    - After collating:\n",
    "        - `torch.Size([5, 3, 300]),torch.Size([5, 87, 2])`\n",
    "        - `87`是最长的targets data长度"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "\n",
    "def my_collate_fn(data):\n",
    "# 这里的data是一个list， list的元素是元组: (self.data, self.label)\n",
    "# collate_fn的作用是把[(data, label),(data, label)...]转化成([data, data...],[label,label...])\n",
    "# 假设self.data的一个data的shape为(channels, length), 每一个channel的length相等,\n",
    "# data[索引到index(batch)][索引到data或者label][索引到channel]\n",
    "\n",
    "    data_list = []\n",
    "    target_list = []\n",
    "    setting_list = []\n",
    "    metric_list = []\n",
    "\n",
    "    data_len = len(data)      # 读进来的data batch的大小\n",
    "\n",
    "    batch = 0\n",
    "\n",
    "    while batch < data_len:\n",
    "        # 只有context feature data\n",
    "        data_tmp = np.array(data[batch][0][2,:])\n",
    "        # metric_tmp = np.array([data[batch][3][GT_CHOSEN]])\n",
    "\n",
    "        # 所有GT model\n",
    "        # data_list.append(torch.tensor(data[batch][0]))\n",
    "        data_list.append(torch.tensor(data_tmp))\n",
    "        target_list.append(torch.tensor(data[batch][1]))\n",
    "        setting_list.append(torch.tensor(data[batch][2]))\n",
    "        metric_list.append(torch.tensor(data[batch][3]))\n",
    "        batch += 1\n",
    "\n",
    "\n",
    "    # Pad target data with zeros\n",
    "    target_padded = pad_sequence(target_list,batch_first=True)\n",
    "    target_tensor = target_padded.float()\n",
    "\n",
    "    data_tensor = torch.stack(data_list).float()\n",
    "    setting_tensor = torch.stack(setting_list).float()\n",
    "    metric_tensor = torch.stack(metric_list).float()\n",
    "\n",
    "    # print(\"data_tensor shape:\",data_tensor.shape)       # [40, 2, 300]\n",
    "    # print(\"metric_tensor shape:\",metric_tensor.shape)   # [40]\n",
    "    return data_tensor, target_tensor, setting_tensor, metric_tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = dataset,batch_size = batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(train_idx), collate_fn = my_collate_fn)\n",
    "\n",
    "val_loader = DataLoader(dataset = dataset,batch_size = batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(val_idx),collate_fn = my_collate_fn)\n",
    "\n",
    "# 注意test_loader的batch size\n",
    "test_loader = DataLoader(dataset = dataset,batch_size = batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(test_idx),collate_fn = my_collate_fn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. The Net and Init\n",
    "1. BatchNorm1d: The mean and std are calculated per-dimension over the mini-batches\n",
    "## 3.1 2层MLP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "# 参数量32253+\n",
    "# 2层\n",
    "class MLP_1(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=300,affine=True)\n",
    "        self.BN2 = nn.BatchNorm1d(num_features=100,affine=True)\n",
    "        self.BN3 = nn.BatchNorm1d(num_features=12,affine=True)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "        self.linear1 = nn.Linear(300, 100)\n",
    "        self.linear2 = nn.Linear(100, 12)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "        self.ac_func2 = nn.SELU()\n",
    "        self.ac_func3 = nn.LeakyReLU()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(12, n_gaussians),  # 30个params要learn\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(12, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(12, n_gaussians)\n",
    "\n",
    "        # SELU Init\n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, (nn.Linear)):\n",
    "        #         nn.init.kaiming_normal_(m.weight, nonlinearity='linear')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        x = self.BN1(x)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.BN2(x)\n",
    "        x = self.ac_func3(self.drop(x))\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = self.BN3(x)\n",
    "        x = self.ac_func3(self.drop(x))\n",
    "\n",
    "        pi = self.z_pi(x)\n",
    "        # pi = self.z_pi(torch.clamp(x,1e-3))\n",
    "        mu = self.z_mu(x)\n",
    "        # sigma = torch.exp(self.z_sigma(x))\n",
    "        sigma = F.elu(self.z_sigma(x)) + 1\n",
    "        sigma = torch.clamp(sigma,1e-4)\n",
    "\n",
    "        # return x\n",
    "        return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. The Loss\n",
    "\n",
    "## 4.0 The metric\n",
    "1. 以NLL作为metric\n",
    "2. 理论模型的metric可以提前算好，读进来，这里读setting是为了？IDK"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def cal_metric(Pi, Mu, Sigma, Duration, N_gaussians, vali_setting):\n",
    "\n",
    "    NLL = torch.tensor(0.,device=device,requires_grad=True)\n",
    "    # print(\"vali_setting: \",vali_setting.shape)   torch.Size([40, 4])\n",
    "    # [id,bidincrement,bidfee,retail]\n",
    "    for i in range(len(Pi)):\n",
    "\n",
    "        # d = vali_setting[i,1]\n",
    "        # b = vali_setting[i,2]\n",
    "        # v = vali_setting[i,3]\n",
    "        # T = int((v-b)/d)\n",
    "\n",
    "        target = Duration[i,:,0]\n",
    "        pi = Pi[i,:]\n",
    "        m = torch.distributions.Normal(loc=Mu[i,:], scale=Sigma[i,:])\n",
    "\n",
    "        # Drop padded data and Expanded to the same dim\n",
    "        idx = torch.nonzero(target)\n",
    "        target_nonzero = target[idx]\n",
    "        # target_nonzero_2 = torch.repeat_interleave(target_nonzero.unsqueeze(dim=1), repeats=N_gaussians, dim=1).to(device)\n",
    "        target_nonzero_2 = target_nonzero.repeat(1,N_gaussians).to(device)\n",
    "\n",
    "        # loss_1 是高斯分布的概率密度value\n",
    "        loss_1 = torch.exp(m.log_prob(target_nonzero_2))\n",
    "\n",
    "        # loss_2 是MDN的概率密度value\n",
    "        loss_2 = torch.sum(loss_1 * pi, dim=1)\n",
    "\n",
    "        ## 两种截断的方式都ok：\n",
    "        ## 方法一：在MDN的prob上设定最小值；\n",
    "        # loss_3 = torch.clamp(loss_2,min=MIN_LOSS)\n",
    "        # loss_4 = torch.log(loss_3)\n",
    "        # # loss_sum = -torch.mean(loss_4) + loss_sum\n",
    "        # loss_sum = -torch.sum(loss_4) + loss_sum\n",
    "\n",
    "        ## 方法二：在prob of each sample后加一个safety数 MIN_LOSS\n",
    "        # loss_2 shape: torch.Size([40, 1])\n",
    "        loss_3 = -torch.log(loss_2+MIN_LOSS)\n",
    "        NLL = torch.sum(loss_3) + NLL\n",
    "\n",
    "    # 求metric不需要除以batch_size，需要累积起来所有vali data的nll\n",
    "    return NLL"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1 version 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def loss_fn_v2(Pi, Mu, Sigma, Duration, N_gaussians):\n",
    "\n",
    "    loss_sum = torch.tensor(0.,device=device,requires_grad=True)\n",
    "\n",
    "    for i in range(len(Pi)):\n",
    "\n",
    "        target = Duration[i,:,0]\n",
    "        pi = Pi[i,:]\n",
    "        m = torch.distributions.Normal(loc=Mu[i,:], scale=Sigma[i,:])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Drop padded data and Expanded to the same dim\n",
    "            idx = torch.nonzero(target)\n",
    "            target_nonzero = target[idx]\n",
    "            # target_nonzero_2 = torch.repeat_interleave(target_nonzero.unsqueeze(dim=1), repeats=N_gaussians, dim=1).to(device)\n",
    "            target_nonzero_2 = target_nonzero.repeat(1,N_gaussians).to(device)\n",
    "\n",
    "        # loss_1 是高斯分布的概率密度value\n",
    "        loss_1 = torch.exp(m.log_prob(target_nonzero_2))\n",
    "\n",
    "        # loss_2 是MDN的概率密度value\n",
    "        loss_2 = torch.sum(loss_1 * pi, dim=1)\n",
    "\n",
    "        ## 两种截断的方式都ok：\n",
    "        ## 方法一：在MDN的prob上设定最小值；\n",
    "        # loss_3 = torch.clamp(loss_2,min=MIN_LOSS)\n",
    "        # loss_4 = torch.log(loss_3)\n",
    "        # # loss_sum = -torch.mean(loss_4) + loss_sum\n",
    "        # loss_sum = -torch.sum(loss_4) + loss_sum\n",
    "\n",
    "        ## 方法二：在prob of each sample后加一个safety数 MIN_LOSS\n",
    "        loss_3 = -torch.log(loss_2+MIN_LOSS)\n",
    "        loss_sum = torch.sum(loss_3) + loss_sum\n",
    "\n",
    "    # 最后loss求一下平均\n",
    "    return loss_sum/len(Pi)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.5 W-distance\n",
    "1. 拿到target data的两列，按照第一列去predict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "p = 2\n",
    "entreg = .1 # entropy regularization factor for Sinkhorn\n",
    "factor = 1  # prob的放大系数\n",
    "# 若以欧式距离为metric，则cost function可以直接用geomloss提供的 Sinkhorn快速解\n",
    "OTLoss = geomloss.SamplesLoss(\n",
    "    loss='sinkhorn', p=p, blur=entreg**(1/p), backend='tensorized', verbose=True)\n",
    "\n",
    "def loss_fn_WD(Pi, Mu, Sigma, Duration, N_gaussians):\n",
    "\n",
    "    loss_sum = torch.tensor(0.,device=device,requires_grad=True)\n",
    "\n",
    "    for i in range(len(Pi)):\n",
    "\n",
    "        target = torch.unique(Duration[i],dim=0)\n",
    "        n_target = target[:,0]\n",
    "        non_zero_idx = torch.nonzero(n_target)\n",
    "        n = n_target[non_zero_idx]\n",
    "        p_target = target[:,1]\n",
    "        p = p_target[non_zero_idx]*factor\n",
    "\n",
    "        # 放大“p”\n",
    "        y_target = torch.cat([n,p],dim=1)\n",
    "\n",
    "        pi = Pi[i,:]\n",
    "        m = torch.distributions.Normal(loc=Mu[i,:], scale=Sigma[i,:])\n",
    "        x = torch.repeat_interleave(n, repeats=N_gaussians, dim=1)   # expand dim\n",
    "        y = (m.cdf(x+TARGET) - m.cdf(x)).to(device=device)\n",
    "        y_cdf = torch.sum(pi*y,dim=1).unsqueeze(dim=1)*factor\n",
    "        y_pred = torch.cat([n,y_cdf],dim=1)\n",
    "\n",
    "        loss_sum = OTLoss(y_pred,y_target) + loss_sum\n",
    "\n",
    "    return loss_sum/len(Pi)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5 Plot\n",
    "1. draw:\n",
    "    - loss: training data的loss和test data的loss趋势\n",
    "    - MLP的网络结构（.png）\n",
    "    - target distrb.和pred. distrb.\n",
    "    - 所有target data的分布图\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(\"logs-MLP\")\n",
    "viz = Visdom(env=\"001\")\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.2\n",
    "### 5.2.1 mdn pdf的图像[with input data]\n",
    "1. 求解[1,6,11...]上的cdf：\n",
    "    - 求解[1,6,11..]的cdf，再求解[6,11,..]，两者相减"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def draw_mdn_2(pi, mu, sigma, target, input, total_train_step, N_gaussians):\n",
    "    \"\"\"\n",
    "    画两条曲线，pred-1表示归一化前，pred-2表示归一化后\n",
    "\n",
    "    :param pi:\n",
    "    :param mu:\n",
    "    :param sigma:\n",
    "    :param target:\n",
    "    :param input:\n",
    "    :param total_train_step:\n",
    "    :param N_gaussians:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 只画一个batch中的第一个\n",
    "\n",
    "    # The target distrb.\n",
    "    target = torch.unique(target,dim=0)    # drop the duplicate\n",
    "    n_target = target[:,0]\n",
    "    non_zero_idx = torch.nonzero(n_target)\n",
    "    n = n_target[non_zero_idx]\n",
    "\n",
    "    p_target = target[:,1]\n",
    "    p = p_target[non_zero_idx]\n",
    "    max_n = int(max(n).item())\n",
    "\n",
    "    # The predicted distrb.\n",
    "    m = torch.distributions.Normal(mu,sigma)          # Gaussian\n",
    "    # m = torch.distributions.Laplace(mu,sigma)           # Laplace\n",
    "\n",
    "    x_0 = torch.arange(1,(max_n+TARGET),TARGET).to(device=device)\n",
    "\n",
    "    x = torch.repeat_interleave(x_0.unsqueeze(dim=1), repeats=N_gaussians, dim=1)   # expand dim\n",
    "    # 求解每个长度为TARGET的区间上的cdf\n",
    "    y = (m.cdf(x+TARGET) - m.cdf(x)).to(device=device)\n",
    "\n",
    "    # 方法一：\n",
    "    y_pred_1 = torch.sum(pi*y,dim=1)\n",
    "    # 方法二：做一下(0,1)上的归一化\n",
    "    y_pred_2 = y_pred_1/y_pred_1.sum()\n",
    "\n",
    "    # The input distrb.\n",
    "    # input_data = input[0:2,:]\n",
    "    input_data = input[0,:]\n",
    "    x_input = torch.arange(1,(300+1),SCALE).to(device=device)\n",
    "    y_input_0 = input_data.to(device=device)\n",
    "    # y_input_0 = (input_data[0,:]).to(device=device)\n",
    "    # y_input_1 = (input_data[1,:]).to(device=device)\n",
    "\n",
    "    # Init\n",
    "    win_str = \"total_train_step = \"+str(total_train_step)\n",
    "    title_str = \"Distrb. in \"+win_str\n",
    "    viz.line(X = [0.],Y = [0.], env=\"001\", win=win_str, opts= dict(title=title_str))\n",
    "\n",
    "    # Visdom本身不能把hist和line画在一个window中\n",
    "    # 如果想画一起只能是两条line\n",
    "    # Plot y_target\n",
    "    # viz.histogram(X = n, env=\"001\", win=win_str,\n",
    "    #         opts= dict(title=title_str))\n",
    "    viz.line(X = n,Y= p, env=\"001\", win=win_str, update=\"append\", name='target',\n",
    "            opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "    # Plot y_pred\n",
    "    viz.line(X = x_0,Y= y_pred_1, env=\"001\", win=win_str, update=\"append\", name='pred-1',\n",
    "            opts= dict(title=title_str))\n",
    "\n",
    "    # Plot y_pred_2\n",
    "    viz.line(X = x_0,Y= y_pred_2, env=\"001\", win=win_str, update=\"append\", name='pred-2',\n",
    "            opts= dict(title=title_str))\n",
    "\n",
    "    # Plot y_input\n",
    "    # 如果input data长度短，全部画完\n",
    "    if(len(x_input) <= max_n):\n",
    "        viz.line(X = x_input,Y= y_input_0, env=\"001\", win=win_str, update=\"append\", name='GT-1', opts= dict(title=title_str))\n",
    "        # viz.line(X = x_input,Y= y_input_1, env=\"001\", win=win_str, update=\"append\", name='GT-2', opts= dict(title=title_str))\n",
    "    # 如果input data长度长，则截断\n",
    "    else:\n",
    "        x_input_0 = x_input[0:max_n]\n",
    "        y_input_2 = y_input_0[0:max_n]/y_input_0[0:max_n].sum()\n",
    "        # y_input_3 = y_input_1[0:max_n]/y_input_1[0:max_n].sum()\n",
    "        viz.line(X = x_input_0,Y= y_input_2, env=\"001\", win=win_str, update=\"append\", name='GT-1', opts= dict(title=title_str))\n",
    "        # viz.line(X = x_input_0,Y= y_input_3, env=\"001\", win=win_str, update=\"append\", name='GT-2', opts= dict(title=title_str))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.4 plot the loss\n",
    "1. 直接根据参数决定draw什么loss，反正loss都画在一个window里面"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "win_train_loss_str = \"The Loss of BATCH in the Training Data\"\n",
    "win_vali_loss_str = \"The Loss in the Vali Data\"\n",
    "win_train_epoch_loss_str = \"The Loss of EPOCH in the Training Data\"\n",
    "\n",
    "def draw_loss(X_step, loss, win_str):\n",
    "    viz.line(X = [X_step], Y = [loss],win=win_str, update=\"append\",\n",
    "        opts= dict(title=win_str))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.5 plot the metric"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "win_vali_metric_str = \"The NLL of ALL vali data\"\n",
    "\n",
    "def draw_metric(X_step, total_vali_metric, GT_metric):\n",
    "\n",
    "    viz.line(X = [X_step], Y = [[total_vali_metric,GT_metric[0,0],GT_metric[0,1]]],win=win_vali_metric_str, update=\"append\", opts= dict(title=win_vali_metric_str, legend=['pred','GT-1','GT-2'], showlegend=True,xlabel=\"epoch\", ylabel=\"NLL\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6 Training\n",
    "## 6.1 Init"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "       BatchNorm1d-1                  [-1, 300]             600\n",
      "           Flatten-2                  [-1, 300]               0\n",
      "            Linear-3                  [-1, 100]          30,100\n",
      "       BatchNorm1d-4                  [-1, 100]             200\n",
      "           Dropout-5                  [-1, 100]               0\n",
      "         LeakyReLU-6                  [-1, 100]               0\n",
      "            Linear-7                   [-1, 12]           1,212\n",
      "       BatchNorm1d-8                   [-1, 12]              24\n",
      "           Dropout-9                   [-1, 12]               0\n",
      "        LeakyReLU-10                   [-1, 12]               0\n",
      "           Linear-11                    [-1, 3]              39\n",
      "          Softmax-12                    [-1, 3]               0\n",
      "           Linear-13                    [-1, 3]              39\n",
      "           Linear-14                    [-1, 3]              39\n",
      "================================================================\n",
      "Total params: 32,253\n",
      "Trainable params: 32,253\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.12\n",
      "Estimated Total Size (MB): 0.13\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "mlp = MLP_1(N_gaussians)\n",
    "\n",
    "# Init the params\n",
    "# # mlp = model_param_init(mlp)\n",
    "# Init the vis\n",
    "viz.line(X = [0.],Y = [0.], env=\"001\", win=win_train_loss_str, opts= dict(title=win_train_loss_str))\n",
    "# viz.line(X = [0.],Y = [0.], env=\"001\", win=win_vali_loss_str, opts= dict(title=win_vali_loss_str))\n",
    "viz.line(X = [0.],Y = [0.], env=\"001\", win=win_train_epoch_loss_str, opts= dict(title=win_train_epoch_loss_str))\n",
    "viz.line(X = [0.],Y = [[0.,0.,0.]], env=\"001\", win=win_vali_metric_str, opts= dict(title=win_vali_metric_str,legend=['pred','GT-1','GT-2'], showlegend=True,xlabel=\"epoch\", ylabel=\"NLL\"))\n",
    "# Save the init params\n",
    "# torch.save(mlp.state_dict(), 'mlp_init.pth')\n",
    "\n",
    "# Load the saved model\n",
    "# model_path_WD = \"mlp_train=300_WD.pth\"\n",
    "# model_path_LSE = \"mlp_train=300_LSE.pth\"\n",
    "# model_data = torch.load(model_path_LSE)\n",
    "# mlp.load_state_dict(model_data)\n",
    "\n",
    "mlp = mlp.to(device=device)\n",
    "summary(mlp, (300,))\n",
    "\n",
    "############ learning rate strategy ############\n",
    "# 1.\n",
    "# Set different lr for params\n",
    "# id: id() 函数返回对象的唯一标识符，标识符是一个整数。返回对象的内存地址。\n",
    "mu_params = list(map(id, mlp.z_mu.parameters()))\n",
    "base_params = filter(lambda p: id(p) not in mu_params, mlp.parameters())\n",
    "params = [{'params': base_params},         # 如果对某个参数不指定学习率，就使用最外层的默认学习率\n",
    "        {'params': mlp.z_mu.parameters(), 'lr': lr_for_mu}]\n",
    "optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "\n",
    "# optimizer = torch.optim.Adagrad(params,lr=learning_rate,weight_decay = 1e-4)\n",
    "# optimizer = torch.optim.Adagrad(params,lr=learning_rate)\n",
    "\n",
    "# 2.\n",
    "# Or set lr decay\n",
    "# StepLR为步进，每step_size个epoch，lr*gamma\n",
    "# scheduler  = torch.optim.lr_scheduler.StepLR(optimizer,step_size=10,gamma=0.8)\n",
    "# 下面这个比较好用\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10,20], gamma=0.5, last_epoch=-1)\n",
    "\n",
    "# # Set the hooks\n",
    "# #mlp.conv1.register_forward_hook(hook_forward_fn)\n",
    "# mlp.conv1.register_full_backward_hook(hook_backward_fn)\n",
    "#\n",
    "# #mlp.linear1.register_forward_hook(hook_forward_fn)\n",
    "# #mlp.linear1.register_full_backward_hook(hook_backward_fn)\n",
    "#\n",
    "# #mlp.z_pi.register_forward_hook(hook_forward_fn)\n",
    "# mlp.z_pi.register_full_backward_hook(hook_backward_fn_pi)\n",
    "#\n",
    "# #mlp.z_mu.register_forward_hook(hook_forward_fn)\n",
    "# mlp.z_mu.register_full_backward_hook(hook_backward_fn_mu)\n",
    "#\n",
    "# mlp.z_sigma.register_forward_hook(hook_forward_fn)\n",
    "# mlp.z_sigma.register_full_[2, 3, 300]backward_hook(hook_backward_fn_sigma)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.3 START HERE\n",
    "1. 使用Laplace分布改：\n",
    "    - loss function和draw"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Now this is EPOCH 0 ==========\n",
      "========== IN EPOCH 0 the total loss is 18360.60073852539 ==========\n",
      "========== Now this is EPOCH 1 ==========\n",
      "========== IN EPOCH 1 the total loss is 14486.708526611328 ==========\n",
      "========== Now this is EPOCH 2 ==========\n",
      "========== IN EPOCH 2 the total loss is 14100.348571777344 ==========\n",
      "========== Now this is EPOCH 3 ==========\n",
      "========== IN EPOCH 3 the total loss is 14423.312561035156 ==========\n",
      "========== Now this is EPOCH 4 ==========\n",
      "========== IN EPOCH 4 the total loss is 14202.377502441406 ==========\n",
      "========== Now this is EPOCH 5 ==========\n",
      "========== IN EPOCH 5 the total loss is 14181.63314819336 ==========\n",
      "========== Now this is EPOCH 6 ==========\n",
      "========== IN EPOCH 6 the total loss is 13973.489440917969 ==========\n",
      "========== Now this is EPOCH 7 ==========\n",
      "========== IN EPOCH 7 the total loss is 13694.00894165039 ==========\n",
      "========== Now this is EPOCH 8 ==========\n",
      "========== IN EPOCH 8 the total loss is 13657.053985595703 ==========\n",
      "========== Now this is EPOCH 9 ==========\n",
      "========== IN EPOCH 9 the total loss is 13480.759368896484 ==========\n",
      "========== Now this is EPOCH 10 ==========\n",
      "========== IN EPOCH 10 the total loss is 13467.355865478516 ==========\n",
      "========== Now this is EPOCH 11 ==========\n",
      "========== IN EPOCH 11 the total loss is 13426.985168457031 ==========\n",
      "========== Now this is EPOCH 12 ==========\n",
      "========== IN EPOCH 12 the total loss is 13507.462890625 ==========\n",
      "========== Now this is EPOCH 13 ==========\n",
      "========== IN EPOCH 13 the total loss is 13093.633575439453 ==========\n",
      "========== Now this is EPOCH 14 ==========\n",
      "========== IN EPOCH 14 the total loss is 13128.208099365234 ==========\n",
      "========== Now this is EPOCH 15 ==========\n",
      "========== IN EPOCH 15 the total loss is 12976.018188476562 ==========\n",
      "========== Now this is EPOCH 16 ==========\n",
      "========== IN EPOCH 16 the total loss is 13130.051300048828 ==========\n",
      "========== Now this is EPOCH 17 ==========\n",
      "========== IN EPOCH 17 the total loss is 13083.105926513672 ==========\n",
      "========== Now this is EPOCH 18 ==========\n",
      "========== IN EPOCH 18 the total loss is 13107.535430908203 ==========\n",
      "========== Now this is EPOCH 19 ==========\n",
      "========== IN EPOCH 19 the total loss is 12775.39926147461 ==========\n",
      "========== Now this is EPOCH 20 ==========\n",
      "========== IN EPOCH 20 the total loss is 12962.544403076172 ==========\n",
      "========== Now this is EPOCH 21 ==========\n",
      "========== IN EPOCH 21 the total loss is 13173.168151855469 ==========\n",
      "========== Now this is EPOCH 22 ==========\n",
      "========== IN EPOCH 22 the total loss is 12975.930206298828 ==========\n",
      "========== Now this is EPOCH 23 ==========\n",
      "========== IN EPOCH 23 the total loss is 12756.484313964844 ==========\n",
      "========== Now this is EPOCH 24 ==========\n",
      "========== IN EPOCH 24 the total loss is 12682.846649169922 ==========\n",
      "========== Now this is EPOCH 25 ==========\n",
      "========== IN EPOCH 25 the total loss is 13080.514343261719 ==========\n",
      "========== Now this is EPOCH 26 ==========\n",
      "========== IN EPOCH 26 the total loss is 12878.749206542969 ==========\n",
      "========== Now this is EPOCH 27 ==========\n",
      "========== IN EPOCH 27 the total loss is 12795.50473022461 ==========\n",
      "========== Now this is EPOCH 28 ==========\n",
      "========== IN EPOCH 28 the total loss is 12790.456817626953 ==========\n",
      "========== Now this is EPOCH 29 ==========\n",
      "========== IN EPOCH 29 the total loss is 13008.126403808594 ==========\n",
      "========== Now this is EPOCH 30 ==========\n",
      "========== IN EPOCH 30 the total loss is 13096.854370117188 ==========\n",
      "========== Now this is EPOCH 31 ==========\n",
      "========== IN EPOCH 31 the total loss is 13049.405151367188 ==========\n",
      "========== Now this is EPOCH 32 ==========\n",
      "========== IN EPOCH 32 the total loss is 13040.198333740234 ==========\n",
      "========== Now this is EPOCH 33 ==========\n",
      "========== IN EPOCH 33 the total loss is 12743.010864257812 ==========\n",
      "========== Now this is EPOCH 34 ==========\n",
      "========== IN EPOCH 34 the total loss is 12779.254211425781 ==========\n",
      "========== Now this is EPOCH 35 ==========\n",
      "========== IN EPOCH 35 the total loss is 12844.887573242188 ==========\n",
      "========== Now this is EPOCH 36 ==========\n",
      "========== IN EPOCH 36 the total loss is 12562.29525756836 ==========\n",
      "========== Now this is EPOCH 37 ==========\n",
      "========== IN EPOCH 37 the total loss is 12559.949951171875 ==========\n",
      "========== Now this is EPOCH 38 ==========\n",
      "========== IN EPOCH 38 the total loss is 12933.021041870117 ==========\n",
      "========== Now this is EPOCH 39 ==========\n",
      "========== IN EPOCH 39 the total loss is 12940.022247314453 ==========\n",
      "========== Now this is EPOCH 40 ==========\n",
      "========== IN EPOCH 40 the total loss is 12584.987762451172 ==========\n",
      "========== Now this is EPOCH 41 ==========\n",
      "========== IN EPOCH 41 the total loss is 12563.41064453125 ==========\n",
      "========== Now this is EPOCH 42 ==========\n",
      "========== IN EPOCH 42 the total loss is 12577.642181396484 ==========\n",
      "========== Now this is EPOCH 43 ==========\n",
      "========== IN EPOCH 43 the total loss is 12661.006439208984 ==========\n",
      "========== Now this is EPOCH 44 ==========\n",
      "========== IN EPOCH 44 the total loss is 12442.735961914062 ==========\n",
      "========== Now this is EPOCH 45 ==========\n",
      "========== IN EPOCH 45 the total loss is 12441.851745605469 ==========\n",
      "========== Now this is EPOCH 46 ==========\n",
      "========== IN EPOCH 46 the total loss is 12778.61898803711 ==========\n",
      "========== Now this is EPOCH 47 ==========\n",
      "========== IN EPOCH 47 the total loss is 12795.43911743164 ==========\n",
      "========== Now this is EPOCH 48 ==========\n",
      "========== IN EPOCH 48 the total loss is 12368.243286132812 ==========\n",
      "========== Now this is EPOCH 49 ==========\n",
      "========== IN EPOCH 49 the total loss is 12524.776336669922 ==========\n",
      "Total training time when epoch= *50* is *530.33713722229 *s\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# filename = \"../log_file.txt\"\n",
    "# f = open(filename,'w')\n",
    "total_train_step = 0\n",
    "EPOCH_NUM = 50\n",
    "\n",
    "train_start_time = time.time()\n",
    "mlp.train()\n",
    "\n",
    "for epoch in range(EPOCH_NUM):\n",
    "    # # Save the net structure\n",
    "    # net_file_name = \"net_before_epoch\"+str(epoch)+\".pth\"\n",
    "    # net_path = net_root_path + net_file_name\n",
    "    # torch.save(mlp.state_dict(), net_path)\n",
    "\n",
    "    epoch_train_loss = 0\n",
    "    print(f\"========== Now this is EPOCH {epoch} ==========\")\n",
    "    for batch_id,data in enumerate(train_loader):\n",
    "\n",
    "        input_data, target, setting, target_metric = data\n",
    "        # print(f\"---- {batch_id} batch----\")\n",
    "        # Do the inference\n",
    "        input_data = input_data.to(device)\n",
    "        target = target.to(device)\n",
    "        pi, mu, sigma = mlp(input_data)\n",
    "\n",
    "        # Save the params\n",
    "        # params = list(mlp.named_parameters())\n",
    "\n",
    "        # Cal the MLE loss and draw the distrb.\n",
    "        loss = loss_fn_v2(pi, mu, sigma, target, N_gaussians)\n",
    "        # loss = loss_fn_v3(pi, mu, sigma, target, N_gaussians)\n",
    "        # loss = loss_fn_v5(pi, mu, sigma, target, N_gaussians)\n",
    "\n",
    "        # Cal the CDF loss and draw the distrb.\n",
    "        # loss = loss_fn_cdf(pi, mu, sigma, target, N_gaussians)\n",
    "\n",
    "        # Cal the CE or KL loss and draw the distrb.\n",
    "        # loss  = loss_fn_CE(pi, mu, sigma, target, N_gaussians)\n",
    "\n",
    "        # Cal the WD loss and draw the distrb.\n",
    "        # loss  = loss_fn_WD(pi, mu, sigma, target, N_gaussians)\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "        draw_loss(total_train_step, loss.item(),win_train_loss_str)\n",
    "\n",
    "        # print(\"训练次数：{}，Loss：{}\".format(total_train_step, loss.item()))\n",
    "\n",
    "        # Optim\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        ########### before step() ###############\n",
    "\n",
    "        # for name, parms in mlp.named_parameters():\n",
    "        #     print(\"=====before step()=====\")\n",
    "        #     print('-->name:', name)\n",
    "        #     print('-->params:', parms.data)\n",
    "        #     # print('-->grad_requirs:',parms.requires_grad)\n",
    "        #     print('-->grad_value:',parms.grad)\n",
    "        #     print(\"===\")\n",
    "\n",
    "        ######\n",
    "\n",
    "        # 反向传播时检测是否有异常值，定位code\n",
    "        # with torch.autograd.detect_anomaly():\n",
    "        #     loss.backward()\n",
    "\n",
    "        ######\n",
    "        # get_dot = register_hooks(loss)\n",
    "        # dot = get_dot()\n",
    "        # dot.save('tmp.dot')   # to get .dot\n",
    "        # dot.render('tmp')     # to get pdf\n",
    "\n",
    "        ######\n",
    "        # Print grad check\n",
    "        # v_n = []    # name\n",
    "        # v_v = []    # value\n",
    "        # v_g = []    # grad\n",
    "        # for name, parameter in mlp.named_parameters():\n",
    "        #     v_n.append(name)\n",
    "        #     v_v.append(parameter.detach().cpu().numpy() if parameter is not None else [0])\n",
    "        #     v_g.append(parameter.grad.detach().cpu().numpy() if parameter.grad is not None else [0])\n",
    "        # for i in range(len(v_n)):\n",
    "        #     # if np.max(v_v[i]).item() - np.min(v_v[i]).item() < 1e-6:\n",
    "        #     #     color = bcolors.FAIL + '*'\n",
    "        #     if np.isnan(v_g[i]).any() or np.isnan(v_v[i]).any():\n",
    "        #         color = bcolors.FAIL + '*'\n",
    "        #     else:\n",
    "        #         color = bcolors.OKGREEN + ' '\n",
    "        #     print('%svalue %s: %.3e ~ %.3e' % (color, v_n[i], np.min(v_v[i]).item(), np.max(v_v[i]).item()))\n",
    "        #     print('%sgrad  %s: %.3e ~ %.3e' % (color, v_n[i], np.min(v_g[i]).item(), np.max(v_g[i]).item()))\n",
    "\n",
    "        ######\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # for name, parms in mlp.named_parameters():\n",
    "        #     print(\"=====After step()=====\")\n",
    "        #     print('-->name:', name)\n",
    "        #     print('-->params:', parms.data)\n",
    "        #     # print('-->grad_requirs:',parms.requires_grad)\n",
    "        #     print('-->grad_value:',parms.grad)\n",
    "        #     print(\"===\")\n",
    "\n",
    "        # Only draw the 1st result in a training batch (5 in total)\n",
    "        # if total_train_step % 20 == 0:\n",
    "        #     with torch.no_grad():\n",
    "        #         draw_mdn_2(pi[0,:].clone().detach(), mu[0,:].clone().detach(), sigma[0,:].clone().detach(), target[0].clone().detach(), input_data[0].clone().detach(),total_train_step, N_gaussians)\n",
    "        #         # draw_mdn_cdf(pi[0,:].detach(), m[0], target[0].detach(), total_train_step, loss_list[0],N_gaussians)\n",
    "        total_train_step += 1\n",
    "\n",
    "    ########### Do validation\n",
    "    with torch.no_grad():\n",
    "        mlp.eval()\n",
    "        total_vali_metric = 0\n",
    "        GT_metric = torch.tensor([0.,0.]).reshape(1,2)\n",
    "\n",
    "        for vali_batch_id, vali_data in enumerate(val_loader):\n",
    "            vali_input_data, vali_target, vali_setting , vali_metric = vali_data\n",
    "            vali_input_data = vali_input_data.to(device)\n",
    "            vali_target = vali_target.to(device)\n",
    "\n",
    "            vali_pi, vali_mu, vali_sigma = mlp(vali_input_data)\n",
    "\n",
    "            # Compute the error/ metric\n",
    "            vali_nll = cal_metric(vali_pi, vali_mu, vali_sigma, vali_target, N_gaussians, vali_setting)\n",
    "            # vali_nll = loss_fn_v2(vali_pi, vali_mu, vali_sigma, vali_target, N_gaussians)\n",
    "            total_vali_metric += vali_nll\n",
    "\n",
    "            # Sum up NLL of all vali data\n",
    "            GT_metric += torch.sum(vali_metric,dim=0)\n",
    "        # total_vali_metric = total_vali_metric/len(val_idx)\n",
    "        # 注意只有1个model时是GT_metric.item()\n",
    "        GT_metric = GT_metric/len(val_idx)\n",
    "        total_vali_metric = total_vali_metric/len(val_idx)\n",
    "\n",
    "        draw_metric(epoch, total_vali_metric.cpu(), GT_metric)\n",
    "        mlp.train()\n",
    "\n",
    "    # Plot the loss in this EPOCH\n",
    "    print(f\"========== IN EPOCH {epoch} the total loss is {epoch_train_loss} ==========\")\n",
    "\n",
    "    # # Save the net structure\n",
    "    # net_file_name = \"net_after_epoch\"+str(epoch)+\".pth\"\n",
    "    # net_path = net_root_path + net_file_name\n",
    "    # torch.save(mlp.state_dict(), net_path)\n",
    "\n",
    "    # Plot loss of this EPOCH\n",
    "    draw_loss(epoch, epoch_train_loss,win_train_epoch_loss_str)\n",
    "    # writer.add_scalars(\"Loss of EPOCH\",{\"Train\":epoch_train_loss},epoch)\n",
    "\n",
    "    # Record the params\n",
    "    # for name,param in params:\n",
    "    #     #参数的梯度\n",
    "    #     #print(\"name:\",name)\n",
    "    #     #print(\"param.grad:\",param.grad)\n",
    "    #     # writer.add_histogram(tag = name +'_grad of EPOCH '+str(epoch),values=param.grad,global_step=epoch)\n",
    "    #     # #参数值\n",
    "    #     writer.add_histogram(tag = name +'data of EPOCH '+str(epoch),values=param.data,global_step=epoch)\n",
    "\n",
    "# f.close()\n",
    "train_end_time = time.time()\n",
    "print(f\"Total training time when epoch= *{EPOCH_NUM}* is *{train_end_time-train_start_time} *s\")\n",
    "writer.close()\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [],
   "source": [
    "model_path_MLE = \"mlp_train=300_MLE.pth\"\n",
    "# model_path_WD = \"mlp_train=300_WD_epoch=80.pth\"\n",
    "model_path_WD = \"mlp_train=300_WD.pth\"\n",
    "model_path_LSE = \"mlp_train=300_LSE.pth\"\n",
    "torch.save(mlp.state_dict(), model_path_LSE)\n",
    "# torch.save(mlp.state_dict(), model_path_MLE)\n",
    "\n",
    "# viz.delete_env(\"001_test\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
