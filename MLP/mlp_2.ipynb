{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2022/6/7 15:17\n",
    "# @Author  : Wang Yujia\n",
    "# @File    : mlp.ipynb\n",
    "# @Description :\n",
    "# @TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0. what for\n",
    "1. loss部分逻辑重写，添加`no grad`\n",
    "2. 添加hooks\n",
    "3. 使用pad_seq函数取代自己写的pad函数\n",
    "4. 效果：可以train虽然loss不下降，不会出现NaN的问题"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Preparations\n",
    "## 1.1 global settings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import torch.utils.data\n",
    "from mydataset import *\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from tensorboardX import SummaryWriter\n",
    "from visdom import Visdom\n",
    "from graphviz import Digraph\n",
    "from torchviz import make_dot\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pygraphviz as pgv\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import geomloss\n",
    "import time\n",
    "\n",
    "import scipy\n",
    "from torch.nn import KLDivLoss\n",
    "from torch.autograd.gradcheck import gradcheck\n",
    "from config import bcolors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from config import DefaultConfig\n",
    "opt = DefaultConfig()\n",
    "\n",
    "# nums of Gaussian kernels\n",
    "N_gaussians = opt.N_gaussians\n",
    "\n",
    "# dataset划分\n",
    "batch_size = opt.batch_size\n",
    "train_pct = opt.train_pct\n",
    "vali_pct = opt.vali_pct\n",
    "test_pct = opt.test_pct\n",
    "\n",
    "# train and optim.\n",
    "learning_rate = opt.lr\n",
    "lr_for_mu = opt.lr_for_mu   # 给mu单独设置learning rate\n",
    "\n",
    "total_train_step = 0\n",
    "total_test_step = 0\n",
    "\n",
    "MIN_LOSS = opt.MIN_LOSS\n",
    "SAFETY = opt.SAFETY\n",
    "\n",
    "# Range【一般用不到】\n",
    "a = 0\n",
    "b = 255\n",
    "\n",
    "# Training data的粒度，画图会使用到\n",
    "SCALE = opt.SCALE\n",
    "# Target data是target_5时\n",
    "TARGET = opt.TARGET\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 the data path\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "outputs": [],
   "source": [
    "# Training data\n",
    "train_path = opt.train_path\n",
    "\n",
    "# Target data\n",
    "target_path = opt.target_path\n",
    "\n",
    "# data keys\n",
    "data_key_path = opt.data_key_path\n",
    "\n",
    "# NLL metric\n",
    "NLL_metric_path = opt.NLL_metric_path\n",
    "\n",
    "# Net path\n",
    "net_root_path = opt.net_root_path"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Dataset and Dataloader\n",
    "1. DataLoader中的`shuffer=True`表示在每一次epoch中都打乱所有数据的顺序，然后以batch为单位从头到尾按顺序取用数据。这样的结果就是不同epoch中的数据都是乱序的,设置随机种子的作用就是让你的每一次训练都乱的一样，\n",
    "\n",
    "## 2.1 Dataset and spliting\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 设置随机数种子"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup_seed(7)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 读取data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "outputs": [],
   "source": [
    "dataset = myDataset(train_path, target_path, data_key_path, NLL_metric_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 产生index的乱序排列"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 873   39  223  511  533  357  977  420 1167   28 1011  334  567 1112\n",
      " 1135   75  921  621  194  745 1183  788 1190 1158  358 1104  220  155\n",
      " 1083  508  665 1119  553  811   45  796  562 1110  340  205 1111  474\n",
      "  514  664  348  624 1047  639  392  181  479  605  739  887  398  766\n",
      " 1155  647  699  993 1082  891   47  797 1100  760  500  686  960  692\n",
      "  756  422  225  264  932  493  893  186  774  480  892  670  844  466\n",
      "  729  814 1172  593  258  329  307  262  663  527  888  973   87  366\n",
      "   73  273  532  784  417  890 1120  897   95  182  695  702    6 1060\n",
      "  124  694  566  862  630 1043   81  109  711  865  156  853   62  408\n",
      " 1191  315  325  445  250 1149  294 1194  432  849 1165  313  515  171\n",
      "  687  939  486  478  183  608  923  375  369   91  477 1044  596  913\n",
      "  539  941  165  196  556  359  604  989  794  975  700  388  518  943\n",
      "  980  909  583  530 1092   82  163  991  352  656  396  571  875  433\n",
      "  232  117  520  113  162  345  954  856  969   98 1089 1166  789  248\n",
      " 1097 1064  607  123  827  936   94  955  574   90 1133  259  254   97\n",
      " 1018  412  834  581  785  799  559  917  937   64  461  708  748  693\n",
      "  385  416  970 1185 1138  449  872  473  815  370  668 1187  810 1029\n",
      "  825  136   34  735 1164  986  609  813  852  502  245  267  868  364\n",
      "  360  306  653 1049  754   85  377  125  931  881  126  310 1058  696\n",
      "  637  623  257  187  427  783  317  121  487  244  545  549   77  282\n",
      "  697 1001  509  569 1037  512 1160   41 1096  942  997  438  281 1182\n",
      "  661  414  333  213  400  676  212  551  290  132  495  206  459  672\n",
      "  418   35  978 1015   80  606  998  195  880  907  305 1006 1075  809\n",
      "  631  120  731  584   67  188  296  894  657  318  726   42  616  440\n",
      " 1014 1069  145  231  411  505  284  312 1000  929  903  510  279  105\n",
      "  276  648  675  650  730 1079  475 1050 1061  614  721 1035  603 1048\n",
      "  361  137  983  962  610  261  517   44 1157  193  546  513  499  912\n",
      "  666 1125  127  732  380  746  662  131  557  256  222  680  178  112\n",
      "  435  860 1074  402  415 1113  462  376  390  660  707 1117 1023 1090\n",
      "  373   79  984 1040   55  106  330  444  737   16 1140   48   69 1088\n",
      "  871  763 1076  561  157  128  531  889  902  899  895  715  638  383\n",
      "  180   84  979  591  777  161  651  425  925  564 1176   70  703  200\n",
      "  667  622  439  636  619 1193  590   13  725  176  144  448  159  705\n",
      " 1118  134 1134  242  521  437  450  632  525  465  504 1173  922   43\n",
      "  541  652  346  208  592  430  812   14   92   51  743  140  878  228\n",
      " 1101  268  948   30 1169  646  287 1137  409  565  671  496 1188  198\n",
      " 1004  506 1136  720  751  403  101   38 1163  724  883 1143  314  911\n",
      "  331  283  953 1007  139   32  718 1159  240  792 1106  874  323   74\n",
      "  339  961  552  114  684  585  405  659  498  280  217  304  736 1175\n",
      "  168  543  249  235  843  356  877  104  229   56 1059  855  800  949\n",
      " 1024  111 1086  362 1116  601   12  485  519  846  503  642  654  627\n",
      " 1145  524  854  573  158  742  712  337  900  221  453  786 1008  501\n",
      "  837  522   99  233  107  885  971  332    4 1105  950  658 1178  691\n",
      " 1031   33  210  297  988  489  319  586  818  189 1124  908  431  701\n",
      "   68  990  387 1078  326  698 1005 1171   52  464 1181  710  395  869\n",
      "  985  538  523  203  613 1146  826   66  768  236 1174  252  839  795\n",
      "  372 1042  488  958 1109 1034 1009 1132  271  830 1102  483  617  833\n",
      "  898  152  972  224 1147    7  587  172  927   27  110  582  336  490\n",
      " 1179  836  255  146  455  129  981  274 1081  542 1025 1032  560  118\n",
      "  679  793   15  184  790  347  600    8  349  870 1095  851   31  928\n",
      "  302  841 1051  275   93  831  595 1091  442   54  103  858  443 1189\n",
      "  933  234  770  160  629  688  300  905  682   24  534   57  399  190\n",
      "  920  807  740  492  266  239  640    2  767  965  141  507 1103  612\n",
      " 1087  924 1062  820  227  598  568 1021  572  882  436  884  655  289\n",
      " 1065 1039   29 1156  848 1180 1128  426  802 1017  602  801  930 1026\n",
      "  108  940  230  423  709  211  578  278  151  761  378  393 1033  829\n",
      "  992  782  397  867  717  681  386  446  201  772 1162  618  588  142\n",
      "  216  778  404  611  451 1168 1027  413 1115  175  237 1114  494  734\n",
      "  321  456   22  482  847 1002 1019  673  987 1152  327  594  421 1129\n",
      "  558  803  625   36  202 1045 1063 1151  689   10   65]\n"
     ]
    }
   ],
   "source": [
    "shuffled_indices = np.random.permutation(dataset.__len__())\n",
    "\n",
    "train_idx = shuffled_indices[:int(train_pct*dataset.__len__())]\n",
    "# train_idx = shuffled_indices\n",
    "tmp = int((train_pct+vali_pct)*dataset.__len__())\n",
    "val_idx = shuffled_indices[int(train_pct*dataset.__len__()):tmp]\n",
    "\n",
    "test_idx = shuffled_indices[tmp:]\n",
    "print(train_idx)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 根据这个乱序排列抽取dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "outputs": [],
   "source": [
    "# 保存idx\n",
    "np.save('shuffled_indices',shuffled_indices)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Dataloader and collating\n",
    "1. 主要是对label数据进行collate\n",
    "    - 按照batch中的最大target data长度进行padding，padding with 0\n",
    "2. 返回的结果多一个batch dim,比如下面的`5`\n",
    "    - After collating:\n",
    "        - `torch.Size([5, 3, 300]),torch.Size([5, 87, 2])`\n",
    "        - `87`是最长的targets data长度"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_pad_sequence():\n",
    "    seq1 = torch.tensor([[ 2., 0.04761905], [3., 0.14285714], [4., 0.04761905]])\n",
    "    seq2 = torch.tensor([[ 1., 0.04761905]])\n",
    "    seq3 = torch.tensor([[ 3., 0.14285714], [4., 0.04761905]])\n",
    "    ls = list((seq1,seq2,seq3))\n",
    "    ls_length = torch.tensor([3,1,2])\n",
    "    ans = pad_sequence(ls,batch_first=True)\n",
    "    print(ans)\n",
    "    # seq_pak = pack_padded_sequence(ans,ls_length,batch_first=True,enforce_sorted=False)\n",
    "    # seq_unpacked, lens_unpacked = pad_packed_sequence(seq_pak, batch_first=True)\n",
    "    # lens_unpacked"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.1 No rescale"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def my_collate_fn(data):\n",
    "# 这里的data是一个list， list的元素是元组: (self.data, self.label)\n",
    "# collate_fn的作用是把[(data, label),(data, label)...]转化成([data, data...],[label,label...])\n",
    "# 假设self.data的一个data的shape为(channels, length), 每一个channel的length相等,\n",
    "# data[索引到index(batch)][索引到data或者label][索引到channel]\n",
    "\n",
    "    data_list = []\n",
    "    target_list = []\n",
    "    setting_list = []\n",
    "    metric_list = []\n",
    "\n",
    "    data_len = len(data)      # 读进来的data batch的大小\n",
    "\n",
    "    batch = 0\n",
    "\n",
    "\n",
    "    while batch < data_len:\n",
    "        # print(\"shape:\",data[batch][0].shape) shape: (3, 300)\n",
    "\n",
    "        # 所有GT model\n",
    "        data_list.append(torch.tensor(data[batch][0]))\n",
    "\n",
    "        target_list.append(torch.tensor(data[batch][1]))\n",
    "        setting_list.append(torch.tensor(data[batch][2]))\n",
    "        metric_list.append(torch.tensor(data[batch][3]))\n",
    "        batch += 1\n",
    "\n",
    "    # Pad target data with zeros\n",
    "    target_padded = pad_sequence(target_list,batch_first=True)\n",
    "    target_tensor = target_padded.float()\n",
    "\n",
    "    data_tensor = torch.stack(data_list).float()\n",
    "    setting_tensor = torch.stack(setting_list).float()\n",
    "    metric_tensor = torch.stack(metric_list).float()\n",
    "\n",
    "    return data_tensor, target_tensor, setting_tensor, metric_tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.2 Rescale"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def my_collate_fn_rescale(data):\n",
    "# 这里的data是一个list， list的元素是元组: (self.data, self.label)\n",
    "# collate_fn的作用是把[(data, label),(data, label)...]转化成([data, data...],[label,label...])\n",
    "# 假设self.data的一个data的shape为(channels, length), 每一个channel的length相等,\n",
    "# data[索引到index(batch)][索引到data或者label][索引到channel]\n",
    "\n",
    "    data_list = []\n",
    "    target_list = []\n",
    "    data_len = len(data)      # 读进来的data batch的大小\n",
    "\n",
    "    batch = 0\n",
    "\n",
    "    while batch < data_len:\n",
    "        data_df = pd.DataFrame(data[batch][0])\n",
    "        data_GT = data_df.iloc[0:2,:]\n",
    "\n",
    "        # Get the max/min in each chanel and broadcast it by hand\n",
    "        data_max = np.tile(pd.DataFrame(np.max(data_GT,axis=1)), (1, data_GT.shape[1]))\n",
    "        data_min = np.tile(pd.DataFrame(np.min(data_GT,axis=1)), (1, data_GT.shape[1]))\n",
    "        # Rescale\n",
    "        data_rescaled = a + (data_GT-data_min)*(b-a) / (data_max - data_min)\n",
    "        data_df.iloc[0:2,:] = data_rescaled\n",
    "\n",
    "        data_list.append(torch.tensor(np.array(data_df,dtype=float)))\n",
    "        # print(\"After scale:\",data_df)\n",
    "        target_list.append(torch.tensor(data[batch][1]))\n",
    "        batch += 1\n",
    "\n",
    "    # Pad target data with zeros\n",
    "    target_padded = pad_sequence(target_list,batch_first=True)\n",
    "\n",
    "    data_tensor = torch.stack(data_list).float()\n",
    "    target_tensor = target_padded.float()\n",
    "\n",
    "    return data_tensor, target_tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.3 log1p"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def my_collate_fn_log1p(data):\n",
    "# 这里的data是一个list， list的元素是元组: (self.data, self.label)\n",
    "# collate_fn的作用是把[(data, label),(data, label)...]转化成([data, data...],[label,label...])\n",
    "# 假设self.data的一个data的shape为(channels, length), 每一个channel的length相等,\n",
    "# data[索引到index(batch)][索引到data或者label][索引到channel]\n",
    "\n",
    "    data_list = []\n",
    "    target_list = []\n",
    "    data_len = len(data)      # 读进来的data batch的大小\n",
    "\n",
    "    batch = 0\n",
    "\n",
    "    while batch < data_len:\n",
    "        data_df = pd.DataFrame(data[batch][0])\n",
    "        data_GT = data_df.iloc[0:2,:]\n",
    "\n",
    "        data_log1p = -np.log(data_GT+MIN_LOSS)\n",
    "        data_df.iloc[0:2,:] = data_log1p\n",
    "\n",
    "        data_list.append(torch.tensor(np.array(data_df,dtype=float)))\n",
    "        target_list.append(torch.tensor(data[batch][1]))\n",
    "        batch += 1\n",
    "\n",
    "    # Pad target data with zeros\n",
    "    target_padded = pad_sequence(target_list,batch_first=True)\n",
    "\n",
    "    data_tensor = torch.stack(data_list).float()\n",
    "    target_tensor = target_padded.float()\n",
    "\n",
    "    return data_tensor, target_tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.4 DataLoader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = dataset,batch_size = batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(train_idx), collate_fn = my_collate_fn)\n",
    "\n",
    "val_loader = DataLoader(dataset = dataset,batch_size = batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(val_idx),collate_fn = my_collate_fn)\n",
    "\n",
    "# 注意test_loader的batch size\n",
    "test_loader = DataLoader(dataset = dataset,batch_size = batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(test_idx),collate_fn = my_collate_fn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. The Net and Init\n",
    "1. BatchNorm1d: The mean and std are calculated per-dimension over the mini-batches\n",
    "2."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### BatchNorm2d测试\n",
    "def test_BN():\n",
    "    m = nn.BatchNorm2d(3, affine=False)  # affine: With Learnable Parameters or not\n",
    "    print('m:', m)\n",
    "    # The mean and std are calculated per-dimension over the mini-batches\n",
    "    input = torch.tensor([\n",
    "        [[1.,2.,3.,4.],[1.,2.,3.,4.],[-1.,-2.,-3.,-4.]],\n",
    "        [[0.,0.,0.,0.],[0.,0.,0.,0.],[0.,0.,0.,0.]]\n",
    "    ], requires_grad=True)\n",
    "\n",
    "    print('input:', input.shape)\n",
    "    input = input.unsqueeze(dim=2)\n",
    "    print('input:', input.shape)\n",
    "    output = m(input) # 归一化\n",
    "    print('output:', output.shape)\n",
    "    print('output:', output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### 复现batchNorm2d在input shape为3维的情况\n",
    "input = torch.tensor([[[1.,2.,3.,4.]],[[0.,0.,0.,0.]]])\n",
    "# print(input.shape)\n",
    "# torch.mean(input),torch.var(input,unbiased = False)\n",
    "(input-torch.mean(input))/ torch.sqrt(torch.var(input,unbiased = False))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 Init, Printer, Hook\n",
    "- 设置网络初始权重: 不太work"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class model_param_init(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        assert isinstance(model, nn.Module), 'model not a class nn.Module'\n",
    "        self.net = model\n",
    "        self.initParam()\n",
    "\n",
    "    def initParam(self):\n",
    "        for param in self.net.parameters():\n",
    "            # nn.init.zeros_(param)\n",
    "            # nn.init.ones_(param)\n",
    "            # nn.init.normal_(param, mean=0, std=1)\n",
    "            # nn.init.uniform_(param, a=0, b=1)\n",
    "            # nn.init.constant_(param, val=1)   # 将所有权重初始化为1\n",
    "            # nn.init.eye_(param)  # 只能将二维的tensor初始化为单位矩阵\n",
    "            # nn.init.xavier_uniform_(param, gain=1)  # Glorot初始化  得到的张量是从-a——a中采用的\n",
    "            # nn.init.xavier_normal_(param, gain=1)   # 得到的张量是从0-std采样的\n",
    "            nn.init.kaiming_normal_(param, a=0, mode='fan_in', nonlinearity='relu') # he初始化方法\n",
    "            # nn.init.kaiming_uniform_(param)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- print网络每层结构"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "class PrintLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if(DEBUG):\n",
    "            print(\"This layer: \")\n",
    "            print(x)      #print(x.shape)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- hook_backward_fn: 输入端的grad，输出端的grad，这里称呼的**输入与输出是站在前向传播的角度的**。如果模块有多个输入与输出的话， 其`grad_input`和`grad_output`可以是tuple类型。\n",
    "- 与forward不同的是，backward传播的时候，**不仅反向传递input和output的grad，还会传递模块Parameter的grad**：\n",
    "    - 比如fc模块，其`grad_input`是一个三元组的tuple，（对bias的梯度，对输入的梯度，对w的梯度）；\n",
    "    - conv模块`grad_input`也是一个三元组tuple，为（对输入的梯度，对w的梯度，对bias的梯度"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# hook functions have to take these 3 input\n",
    "def hook_forward_fn(module, input, output):\n",
    "    print(\"It's forward: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"input: {input}\")\n",
    "    print(f\"output: {output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn(module, grad_input, grad_output):\n",
    "    print(\"It's backward: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn_pi(module, grad_input, grad_output):\n",
    "    print(\"It's backward in pi: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn_sigma(module, grad_input, grad_output):\n",
    "    print(\"It's backward in sigma: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn_mu(module, grad_input, grad_output):\n",
    "    print(\"It's backward in mu: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Conv结构-1\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Not Sequential\n",
    "# class MLP_1_1(nn.Module):\n",
    "#     # code->generate->override methods\n",
    "#     def __init__(self, n_gaussians) -> None:\n",
    "#         super().__init__()\n",
    "#         self.BN = nn.BatchNorm2d(num_features=3,affine=False)\n",
    "#         self.BN_aff1 = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "#         self.BN_aff2 = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "#         self.BN_aff3 = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "#         self.drop = nn.Dropout(0.3)\n",
    "#\n",
    "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#         self.ac_func = nn.Softplus()\n",
    "#\n",
    "#         self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,2), stride=(1,2), padding=0,bias=False)\n",
    "#\n",
    "#         self.conv3 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,5), stride=(1,5), padding=0,bias=False)\n",
    "#\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.linear1 = nn.Linear(30, 9)\n",
    "#\n",
    "#         self.z_pi = nn.Sequential(\n",
    "#             nn.Linear(9, n_gaussians),\n",
    "#             nn.Softmax(dim=1)\n",
    "#         )\n",
    "#\n",
    "#         self.z_mu = nn.Linear(9, n_gaussians)\n",
    "#         self.z_sigma = nn.Linear(9, n_gaussians)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         # 加一个height维度\n",
    "#         x = torch.unsqueeze(x,dim=2)\n",
    "#         x = self.BN(x)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         x = self.BN_aff1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         x = self.BN_aff2(x)\n",
    "#         x = self.conv3(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         x = self.flatten(x)\n",
    "#         x = self.linear1(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         pi = self.z_pi(x)\n",
    "#         mu = self.z_mu(x)\n",
    "#         mu = torch.clamp(mu,1e-4)\n",
    "#\n",
    "#         sigma = torch.exp(self.z_sigma(x))\n",
    "#         sigma = torch.clamp(sigma,1e-4)\n",
    "#\n",
    "#         return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3 Conv结构-2\n",
    "1. 换了一下conv的结构"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 参数量747\n",
    "class MLP_1_2(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=3,affine=False)\n",
    "        self.BN2 = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "        self.BN3 = nn.BatchNorm2d(num_features=6,affine=True)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=(1,6), stride=(1,3), padding=0,bias=False)\n",
    "        self.ac_func = nn.Softplus()\n",
    "        # self.ac_func = nn.ReLU()\n",
    "\n",
    "        # 3, 3, 99\n",
    "        self.conv2 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=(3,6), stride=6, padding=0,bias=False)\n",
    "        # 6,1,33\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=6, out_channels=6, kernel_size=(1,3), stride=3, padding=0,bias=False)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        #self.linear1 = nn.Linear(48, 18)\n",
    "\n",
    "        #无clip\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(30, n_gaussians),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        # 有clip\n",
    "        # self.z_pi = nn.Linear(30, n_gaussians)\n",
    "        self.z_mu = nn.Linear(30, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(30, n_gaussians)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.BN1(x)\n",
    "        # 加一个height维度\n",
    "\n",
    "        x = torch.unsqueeze(x,dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.BN2(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.BN3(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        # x = torch.unsqueeze(x,dim=2)\n",
    "        x = self.conv3(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        # x = self.linear1(x)\n",
    "        # x = self.ac_func(self.drop(x))\n",
    "        mu = self.z_mu(x)\n",
    "        pi = self.z_pi(x)\n",
    "\n",
    "        sigma = torch.exp(self.z_sigma(x))\n",
    "        sigma = torch.clamp(sigma,1e-4)\n",
    "\n",
    "        # return x\n",
    "        return pi,mu,sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4 MLP结构\n",
    "### 3.4.1 4层"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 参数量304,000+\n",
    "# 4层\n",
    "class MLP_1_3(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=3,affine=True)\n",
    "        self.BN2 = nn.BatchNorm1d(num_features=300,affine=True)\n",
    "        self.BN3 = nn.BatchNorm1d(num_features=100,affine=True)\n",
    "        self.BN4 = nn.BatchNorm1d(num_features=30,affine=True)\n",
    "        self.BN5 = nn.BatchNorm1d(num_features=12,affine=True)\n",
    "        self.BN6 = nn.BatchNorm1d(num_features=12,affine=True)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "        self.linear1 = nn.Linear(900, 300)\n",
    "        self.linear2 = nn.Linear(300, 100)\n",
    "        self.linear3 = nn.Linear(100, 30)\n",
    "        self.linear4 = nn.Linear(30, 12)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "        self.ac_func2 = nn.SELU()\n",
    "        self.ac_func3 = nn.LeakyReLU()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(12, n_gaussians),  # 30个params要learn\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(12, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(12, n_gaussians)\n",
    "\n",
    "        # SELU Init\n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, (nn.Linear)):\n",
    "        #         nn.init.kaiming_normal_(m.weight, nonlinearity='linear')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.BN1(x)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.BN2(x)\n",
    "        x = self.ac_func3(self.drop(x))\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = self.BN3(x)\n",
    "        x = self.ac_func3(self.drop(x))\n",
    "\n",
    "        x = self.linear3(x)\n",
    "        x = self.BN4(x)\n",
    "        x = self.ac_func3(self.drop(x))\n",
    "\n",
    "        x = self.linear4(x)\n",
    "        x = self.BN5(x)\n",
    "        x = self.ac_func3(self.drop(x))\n",
    "\n",
    "        x = self.BN6(x)\n",
    "        pi = self.z_pi(x)\n",
    "        # pi = self.z_pi(torch.clamp(x,1e-3))\n",
    "        mu = self.z_mu(x)\n",
    "        # sigma = torch.exp(self.z_sigma(x))\n",
    "        sigma = F.elu(self.z_sigma(x)) + 1\n",
    "        sigma = torch.clamp(sigma,1e-4)\n",
    "\n",
    "        # return x\n",
    "        return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.2 2层"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 参数量91,000+\n",
    "# 2层MLP+MDN\n",
    "class MLP_1_4(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=3,affine=True)\n",
    "        self.BN2 = nn.BatchNorm1d(num_features=100,affine=True)\n",
    "        self.BN3 = nn.BatchNorm1d(num_features=12,affine=True)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "        self.linear1 = nn.Linear(900, 100)\n",
    "        self.linear2 = nn.Linear(100, 12)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "        self.ac_func2 = nn.SELU()\n",
    "        self.ac_func3 = nn.LeakyReLU()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(12, n_gaussians),  # 30个params要learn\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(12, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(12, n_gaussians)\n",
    "\n",
    "        # SELU Init\n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, (nn.Linear)):\n",
    "        #         nn.init.kaiming_normal_(m.weight, nonlinearity='linear')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.BN1(x)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.BN2(x)\n",
    "        x = self.ac_func3(self.drop(x))\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = self.BN3(x)\n",
    "        x = self.ac_func3(self.drop(x))\n",
    "\n",
    "        pi = self.z_pi(x)\n",
    "        # pi = self.z_pi(torch.clamp(x,1e-3))\n",
    "        mu = self.z_mu(x)\n",
    "\n",
    "        sigma = torch.exp(self.z_sigma(x))\n",
    "        # sigma = F.elu(self.z_sigma(x)) + 1\n",
    "        sigma = torch.clamp(sigma,1e-4)\n",
    "\n",
    "        return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.3 1层"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "outputs": [],
   "source": [
    "# 参数量8233\n",
    "# 1层MLP+MDN\n",
    "class MLP_1_5(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=3,affine=True)\n",
    "        self.BN2 = nn.BatchNorm1d(num_features=9,affine=True)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "        self.linear1 = nn.Linear(900, 9)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "        self.ac_func2 = nn.SELU()\n",
    "        self.ac_func3 = nn.LeakyReLU()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(9, n_gaussians),  # 30个params要learn\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(9, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(9, n_gaussians)\n",
    "\n",
    "        # SELU Init\n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, (nn.Linear)):\n",
    "        #         nn.init.kaiming_normal_(m.weight, nonlinearity='linear')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.BN1(x)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.BN2(x)\n",
    "        x = self.ac_func3(self.drop(x))\n",
    "\n",
    "        pi = self.z_pi(x)\n",
    "        # pi = self.z_pi(torch.clamp(x,1e-3))\n",
    "        mu = self.z_mu(x)\n",
    "\n",
    "        sigma = torch.exp(self.z_sigma(x))\n",
    "        # sigma = F.elu(self.z_sigma(x)) + 1\n",
    "        sigma = torch.clamp(sigma,1e-4)\n",
    "\n",
    "        return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.5 (3,100)的结构\n",
    "1. 适配(3,100)的training data长度而不是(3,300)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # 参数量：498\n",
    "# class MLP_2_1(nn.Module):\n",
    "#     # code->generate->override methods\n",
    "#     def __init__(self, n_gaussians) -> None:\n",
    "#         super().__init__()\n",
    "#         self.BN = nn.BatchNorm2d(num_features=3,affine=False)\n",
    "#         self.BN_aff1 = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "#         self.BN_aff2 = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "#         self.BN_aff3 = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "#         self.drop = nn.Dropout(0.3)\n",
    "#\n",
    "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#         # self.ac_func = nn.Softplus()\n",
    "#         # self.ac_func = nn.Sigmoid()\n",
    "#         self.ac_func = nn.Tanh()\n",
    "#\n",
    "#         self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,2), stride=(1,2), padding=0,bias=False)\n",
    "#\n",
    "#         self.conv3 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         # self.linear1 = nn.Linear(30, 9)\n",
    "#\n",
    "#         self.z_pi = nn.Sequential(\n",
    "#             nn.Linear(48, n_gaussians),  # 30个params要learn\n",
    "#             nn.Softmax(dim=1)\n",
    "#         )\n",
    "#\n",
    "#         self.z_mu = nn.Linear(48, n_gaussians)\n",
    "#         self.z_sigma = nn.Linear(48, n_gaussians)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         # 加一个height维度\n",
    "#         x = torch.unsqueeze(x,dim=2)\n",
    "#         x = self.BN(x)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.BN_aff1(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         # x = self.BN_aff2(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.BN_aff2(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         x = self.flatten(x)\n",
    "#\n",
    "#         pi = self.z_pi(torch.clamp(x,1e-3))\n",
    "#         # pi = self.z_pi(x)\n",
    "#         mu = self.z_mu(x)\n",
    "#         # sigma = torch.exp(self.z_sigma(x))\n",
    "#         sigma = F.elu(self.z_sigma(x))+1\n",
    "#\n",
    "#         # pi.retain_grad()\n",
    "#         # mu.retain_grad()\n",
    "#         # sigma.retain_grad()\n",
    "#         # print(\"grad:\")\n",
    "#         # print(\"pi.grad:\",pi.grad)\n",
    "#         # print(\"mu.grad:\",mu.grad)\n",
    "#         # print(\"sigma.grad:\",sigma.grad)\n",
    "#\n",
    "#         # return x\n",
    "#         return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # 参数量：228\n",
    "# class MLP_2_2(nn.Module):\n",
    "#     # code->generate->override methods\n",
    "#     def __init__(self, n_gaussians) -> None:\n",
    "#         super().__init__()\n",
    "#         self.BN = nn.BatchNorm2d(num_features=3,affine=False)\n",
    "#         self.BN_aff = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "#         self.drop = nn.Dropout(0.2)\n",
    "#\n",
    "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#         # self.ac_func = nn.Softplus()\n",
    "#         self.ac_func = nn.LeakyReLU()\n",
    "#         self.ac_func2 = nn.ReLU()\n",
    "#\n",
    "#         # self.ac_func = nn.Sigmoid()\n",
    "#         # self.ac_func = nn.Tanh()\n",
    "#\n",
    "#         self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,2), stride=(1,2), padding=0,bias=False)\n",
    "#\n",
    "#         self.conv3 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#\n",
    "#         self.flatten = nn.Flatten()\n",
    "#\n",
    "#         self.z_pi = nn.Sequential(\n",
    "#             nn.Linear(15, n_gaussians),\n",
    "#             nn.Softmax(dim=1)\n",
    "#         )\n",
    "#\n",
    "#         self.z_mu = nn.Linear(15, n_gaussians)\n",
    "#         self.z_sigma = nn.Linear(15, n_gaussians)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         # 加一个height维度\n",
    "#         x = torch.unsqueeze(x,dim=2)\n",
    "#         x = self.BN(x)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         x = self.BN_aff(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         x = self.BN_aff(x)\n",
    "#         x = self.conv3(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#         #\n",
    "#         x = self.flatten(x)\n",
    "#\n",
    "#         pi = self.z_pi(torch.clamp(x,1e-3))\n",
    "#         # pi = self.z_pi(x)\n",
    "#         mu = self.z_mu(x)\n",
    "#         sigma = torch.exp(self.z_sigma(x))\n",
    "#\n",
    "#         # return x\n",
    "#         return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- MLP结构+长度=100"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class MLP_2_3(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=3,affine=False)\n",
    "        self.BN2 = nn.BatchNorm1d(num_features=1,affine=False)\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "\n",
    "        self.linear1 = nn.Linear(300, 100)\n",
    "        self.linear2 = nn.Linear(100, 30)\n",
    "        self.linear3 = nn.Linear(30, 12)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(12, n_gaussians),  # 30个params要learn\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(12, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(12, n_gaussians)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.BN1(x)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        # x = self.BN2(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        # x = self.BN2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        # pi = self.z_pi(x)\n",
    "        pi = self.z_pi(torch.clamp(x,1e-3))\n",
    "        mu = self.z_mu(x)\n",
    "        # sigma = F.elu(self.z_sigma(x)) + 1\n",
    "        sigma = torch.exp(self.z_sigma(x))\n",
    "\n",
    "        # return x\n",
    "        return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.6 (3,60)的结构\n",
    "1. 适配(3,60)的training data长度而不是(3,300)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # 参数量：174\n",
    "# class MLP_3_1(nn.Module):\n",
    "#     # code->generate->override methods\n",
    "#     def __init__(self, n_gaussians) -> None:\n",
    "#         super().__init__()\n",
    "#         self.BN = nn.BatchNorm2d(num_features=3,affine=False)\n",
    "#         self.BN_aff = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "#         self.drop = nn.Dropout(0.2)\n",
    "#\n",
    "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#         # self.ac_func = nn.Softplus()\n",
    "#         self.ac_func = nn.LeakyReLU()\n",
    "#         self.ac_func2 = nn.ReLU()\n",
    "#\n",
    "#         # self.ac_func = nn.Sigmoid()\n",
    "#         # self.ac_func = nn.Tanh()\n",
    "#\n",
    "#         self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,2), stride=(1,2), padding=0,bias=False)\n",
    "#\n",
    "#         self.conv3 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.linear = nn.Linear(24, 9)\n",
    "#\n",
    "#         self.z_pi = nn.Sequential(\n",
    "#             nn.Linear(9, n_gaussians),\n",
    "#             nn.Softmax(dim=1)\n",
    "#         )\n",
    "#\n",
    "#         self.z_mu = nn.Linear(9, n_gaussians)\n",
    "#         self.z_sigma = nn.Linear(9, n_gaussians)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         # 加一个height维度\n",
    "#         x = torch.unsqueeze(x,dim=2)\n",
    "#         x = self.BN(x)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.ac_func(self.drop(x))      # 3, 1, 20\n",
    "#\n",
    "#         x = self.BN_aff(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.ac_func(self.drop(x))      # 3, 1, 10\n",
    "#\n",
    "#         x = self.BN_aff(x)\n",
    "#         x = self.conv3(x)\n",
    "#         x = self.ac_func(self.drop(x))      # 3, 1, 8\n",
    "#\n",
    "#         x = self.flatten(x)\n",
    "#         # print(x.shape)\n",
    "#         # x = self.linear(x)\n",
    "#         #\n",
    "#         pi = self.z_pi(x)\n",
    "#         mu = self.z_mu(x)\n",
    "#         sigma = torch.exp(self.z_sigma(x))\n",
    "#\n",
    "#         # return x\n",
    "#         return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- MLP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # 参数量：5799\n",
    "# class MLP_3_2(nn.Module):\n",
    "#     # code->generate->override methods\n",
    "#     def __init__(self, n_gaussians) -> None:\n",
    "#         super().__init__()\n",
    "#         self.BN = nn.BatchNorm2d(num_features=3,affine=False)\n",
    "#         self.BN_aff = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "#         self.drop = nn.Dropout(0.2)\n",
    "#\n",
    "#         self.linear1 = nn.Linear(180,30)\n",
    "#         self.linear2 = nn.Linear(30,9)\n",
    "#         # self.ac_func = nn.Softplus()\n",
    "#         self.ac_func = nn.LeakyReLU()\n",
    "#         self.ac_func2 = nn.ReLU()\n",
    "#\n",
    "#         # self.ac_func = nn.Sigmoid()\n",
    "#         # self.ac_func = nn.Tanh()\n",
    "#\n",
    "#         self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,2), stride=(1,2), padding=0,bias=False)\n",
    "#\n",
    "#         self.conv3 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.linear = nn.Linear(24, 9)\n",
    "#\n",
    "#         self.z_pi = nn.Sequential(\n",
    "#             nn.Linear(9, n_gaussians),\n",
    "#             nn.Softmax(dim=1)\n",
    "#         )\n",
    "#\n",
    "#         self.z_mu = nn.Linear(9, n_gaussians)\n",
    "#         self.z_sigma = nn.Linear(9, n_gaussians)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         # 加一个height维度\n",
    "#         x = torch.unsqueeze(x,dim=2)\n",
    "#         x = self.BN(x)\n",
    "#         x = self.flatten(x)\n",
    "#\n",
    "#         x = self.linear1(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         x = self.linear2(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         pi = self.z_pi(x)\n",
    "#         mu = self.z_mu(x)\n",
    "#         sigma = torch.exp(self.z_sigma(x))\n",
    "#\n",
    "#         # return x\n",
    "#         return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input = torch.tensor([\n",
    "    [[1.,2.,3.,4.],[1.,2.,3.,4.],[-1.,-2.,-3.,-4.]],\n",
    "    [[0.,0.,0.,0.],[0.,0.,0.,0.],[0.,0.,0.,0.]]\n",
    "], requires_grad=True)\n",
    "input = input.unsqueeze(dim=2)\n",
    "print(f\"input's shape is {input.shape}\")\n",
    "flt = nn.Flatten(start_dim=1)\n",
    "flt(input).shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. The Loss\n",
    "\n",
    "## 4.0 The metric\n",
    "1. 以NLL作为metric\n",
    "2. 理论模型的metric可以提前算好，读进来.\n",
    "3. 注意这里的metric比较的是target还是target_5:\n",
    "    - 建议使用target data而不是target_5，因为后者是为了方便NN的learning，前者才是真正的比较NLL"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "outputs": [],
   "source": [
    "def cal_metric(Pi, Mu, Sigma, Duration, N_gaussians, vali_setting):\n",
    "\n",
    "    NLL = torch.tensor(0.,device=device,requires_grad=True)\n",
    "    # print(\"vali_setting: \",vali_setting.shape)   torch.Size([40, 4])\n",
    "    # [id,bidincrement,bidfee,retail]\n",
    "    for i in range(len(Pi)):\n",
    "\n",
    "        # d = vali_setting[i,1]\n",
    "        # b = vali_setting[i,2]\n",
    "        # v = vali_setting[i,3]\n",
    "        # T = int((v-b)/d)\n",
    "\n",
    "        target = Duration[i,:,0]\n",
    "        pi = Pi[i,:]\n",
    "        m = torch.distributions.Normal(loc=Mu[i,:], scale=Sigma[i,:])\n",
    "\n",
    "        # Drop padded data and Expanded to the same dim\n",
    "        idx = torch.nonzero(target)\n",
    "        target_nonzero = target[idx]\n",
    "        # target_nonzero_2 = torch.repeat_interleave(target_nonzero.unsqueeze(dim=1), repeats=N_gaussians, dim=1).to(device)\n",
    "        target_nonzero_2 = target_nonzero.repeat(1,N_gaussians).to(device)\n",
    "\n",
    "        # loss_1 是高斯分布的概率密度value\n",
    "        loss_1 = torch.exp(m.log_prob(target_nonzero_2))\n",
    "\n",
    "        # loss_2 是MDN的概率密度value\n",
    "        loss_2 = torch.sum(loss_1 * pi, dim=1)\n",
    "\n",
    "        ## 两种截断的方式都ok：\n",
    "        ## 方法一：在MDN的prob上设定最小值；\n",
    "        # loss_3 = torch.clamp(loss_2,min=MIN_LOSS)\n",
    "        # loss_4 = torch.log(loss_3)\n",
    "        # # loss_sum = -torch.mean(loss_4) + loss_sum\n",
    "        # loss_sum = -torch.sum(loss_4) + loss_sum\n",
    "\n",
    "        ## 方法二：在prob of each sample后加一个safety数 MIN_LOSS\n",
    "        # loss_2 shape: torch.Size([40, 1])\n",
    "        loss_3 = -torch.log(loss_2+MIN_LOSS)\n",
    "        # 除以idx的长度，平均到每个auction\n",
    "        NLL = torch.sum(loss_3)/len(idx) + NLL\n",
    "\n",
    "    # 求metric不需要除以batch_size，需要累积起来所有vali data的nll\n",
    "    return NLL"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1 初始版本\n",
    "### 4.1.1 version 1 [FAILED]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # 当input的shape是[50,3]时，输出应该是50个GMM\n",
    "# # 对这50个GMM看能生成什么output\n",
    "\n",
    "def loss_fn(Pi,duration,m,N_gaussians):\n",
    "    loss_list = []\n",
    "    with torch.no_grad():\n",
    "        # for each GMM\n",
    "        for i in range(len(Pi)):\n",
    "            target = duration[i,:]\n",
    "            pi = Pi[i,:]\n",
    "\n",
    "            # Drop padded data and Expanded to the same dim\n",
    "            target_nonzero = target[torch.nonzero(target)].squeeze_()\n",
    "            target_nonzero = torch.repeat_interleave(target_nonzero.unsqueeze(dim=1), repeats=N_gaussians, dim=1).to(device)\n",
    "            # len_target = len(target_nonzero)       # The length of target data\n",
    "\n",
    "            # loss_1 是高斯分布的概率密度value\n",
    "            loss_1 = torch.exp(m[i].log_prob(target_nonzero))\n",
    "\n",
    "            # loss_2 是MDN的概率密度value\n",
    "            loss_2 = torch.sum(loss_1 * pi, dim=1)\n",
    "\n",
    "            # loss_3 是值非0的概率密度value\n",
    "            loss_3 = loss_2[torch.nonzero(loss_2)].view(-1,1)                # 再去掉所有的log(0)\n",
    "\n",
    "            # 如果loss_2全是0则赋值为1e-40，否则赋值为loss的最小值\n",
    "            MIN_LOSS = torch.min(loss_3) if torch.min(loss_2)>0 else 1e-40\n",
    "            loss_padded = [MIN_LOSS]* (loss_2.shape[0]-loss_3.shape[0])\n",
    "            loss_padded_ts = torch.tensor(loss_padded,device=device)\n",
    "\n",
    "            # loss_4是用MIN_LOSS填充后的loss_3，和loss_2等长\n",
    "            loss_4 = torch.cat((loss_3[:,0],loss_padded_ts))\n",
    "            torch._assert(len(loss_4) == len(loss_2),\"cat不正确\")\n",
    "\n",
    "            # loss_5是log likelihood loss\n",
    "            loss_5 = torch.log(loss_4)\n",
    "            loss_list.append(-torch.mean((loss_5)).item())\n",
    "            # loss_list.append(-torch.sum((loss_5)).item()) # loss值会比较大，\n",
    "\n",
    "    # 最后loss求一下平均\n",
    "    loss = np.mean(loss_list)\n",
    "    loss_ts = torch.tensor(loss,device=device,requires_grad=True)\n",
    "    return loss_ts,loss_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1.2 version 2 [WORK]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "outputs": [],
   "source": [
    "def loss_fn_v2(Pi, Mu, Sigma, Duration, N_gaussians):\n",
    "\n",
    "    loss_sum = torch.tensor(0.,device=device,requires_grad=True)\n",
    "\n",
    "    for i in range(len(Pi)):\n",
    "\n",
    "        target = Duration[i,:,0]\n",
    "        pi = Pi[i,:]\n",
    "        m = torch.distributions.Normal(loc=Mu[i,:], scale=Sigma[i,:])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Drop padded data and Expanded to the same dim\n",
    "            idx = torch.nonzero(target)\n",
    "            target_nonzero = target[idx]\n",
    "            target_nonzero_2 = target_nonzero.repeat(1,N_gaussians).to(device)\n",
    "\n",
    "        # loss_1 是高斯分布的概率密度value\n",
    "        loss_1 = torch.exp(m.log_prob(target_nonzero_2))\n",
    "\n",
    "        # loss_2 是MDN的概率密度value\n",
    "        loss_2 = torch.sum(loss_1 * pi, dim=1)\n",
    "\n",
    "        ## 两种截断的方式都ok：\n",
    "        ## 方法一：在MDN的prob上设定最小值；\n",
    "        # loss_3 = torch.clamp(loss_2,min=MIN_LOSS)\n",
    "        # loss_4 = torch.log(loss_3)\n",
    "        # # loss_sum = -torch.mean(loss_4) + loss_sum\n",
    "        # loss_sum = -torch.sum(loss_4) + loss_sum\n",
    "\n",
    "        SAFETY = 1e-20\n",
    "        ## 方法二：在prob of each sample后加一个safety数 MIN_LOSS\n",
    "        loss_3 = -torch.log(loss_2+SAFETY)\n",
    "        loss_sum = torch.sum(loss_3) + loss_sum\n",
    "\n",
    "    # 最后loss求一下平均\n",
    "    return loss_sum/len(Pi)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1.3 version 3 [LogSumExp]\n",
    "1. $ exponent_i = \\log_{}{\\alpha_i } -1/2*\\log_{}{2\\pi } -\\log_{}{\\sigma_i }- \\frac{(x-\\mu_i )^2}{2\\sigma_i^2} $\n",
    "2. $ NLL =-\\sum_{j}^{N_S}LogSumExp= -\\sum_{j}^{N_S} \\log\\sum_{i}^{N_G} \\exp \\{exponent_i\\}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "outputs": [],
   "source": [
    "def log_sum_exp(x):\n",
    "    \"\"\"Log-sum-exp trick implementation\"\"\"\n",
    "\n",
    "    # x.shape: torch.Size([35, 3])\n",
    "    # x_max: torch.Size([35, 1])\n",
    "    # torch.max()[0]， 只返回最大值的每个数,[1]则返回下标\n",
    "\n",
    "    x_max = torch.max(x, dim=1, keepdim=True)[0]\n",
    "    x_min = torch.abs(torch.min(x, dim=1, keepdim=True)[0])\n",
    "    x_mid = torch.median(x,dim=1,keepdim=True)[0]\n",
    "    # x_min和x_max 通常都是负数\n",
    "\n",
    "    # print(\"x_max:\",x_max)\n",
    "    # print(\"x_min:\",x_min)\n",
    "    return torch.log(torch.sum(torch.exp(x - x_max), dim=1, keepdim=True)) + x_max\n",
    "    # return torch.log(torch.sum(torch.exp(x), dim=1, keepdim=True))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "outputs": [],
   "source": [
    "def loss_fn_v3(Pi, Mu, Sigma, Target, N_gaussians):\n",
    "\n",
    "    loss_sum = torch.tensor(0.,device=device,requires_grad=True)\n",
    "\n",
    "    for i in range(len(Pi)):\n",
    "\n",
    "        target = Target[i,:,0]\n",
    "        pi = Pi[i,:]\n",
    "        sigma = Sigma[i,:]\n",
    "        mu = Mu[i,:]\n",
    "\n",
    "        # Drop padded data and Expanded to the same dim\n",
    "        idx = torch.nonzero(target)\n",
    "        target_nonzero = target[idx]\n",
    "        target_nonzero_2 = target_nonzero.repeat(1,N_gaussians)\n",
    "\n",
    "        # loss_1 是高斯分布的概率密度value\n",
    "        # if(torch.isnan(torch.log(sigma).detach()).any()):\n",
    "        #     print(\"ALERT!!!!!!!!!!!!!!!!!: torch.log(sigma)\")\n",
    "        # if(torch.isnan(torch.log(pi).detach()).any()):\n",
    "        #     print(\"ALERT!!!!!!!!!!!!!!!!!: torch.log(pi)\")\n",
    "        v1 = .5 * torch.log(2 * torch.tensor(np.pi)) -torch.log(sigma)\n",
    "        v2 = (target_nonzero_2 - mu)\n",
    "        # v1 torch.Size([3])\n",
    "        # v2 torch.Size([44, 3])\n",
    "        exponent = torch.log(pi) - v1 - .5*(v2**2)/((sigma)**2)\n",
    "        log_gauss = -log_sum_exp(exponent)\n",
    "\n",
    "        # 在samples维度上继续sum一次\n",
    "        loss_sum = torch.sum(log_gauss) + loss_sum\n",
    "\n",
    "    loss_ts = loss_sum/len(Pi)\n",
    "    return loss_ts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1.4 version 5-Laplace\n",
    "1.version 2 但是是Laplace"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "outputs": [],
   "source": [
    "def loss_fn_v5(Pi, Mu, Sigma, Duration, N_gaussians):\n",
    "\n",
    "    loss_sum = torch.tensor(0.,device=device,requires_grad=True)\n",
    "\n",
    "    for i in range(len(Pi)):\n",
    "\n",
    "        target = Duration[i,:,0]\n",
    "        pi = Pi[i,:]\n",
    "        m = torch.distributions.Laplace(loc=Mu[i,:], scale=Sigma[i,:])\n",
    "\n",
    "        # Drop padded data and Expanded to the same dim\n",
    "        idx = torch.nonzero(target)\n",
    "        target_nonzero = target[idx]\n",
    "        # target_nonzero_2 = torch.repeat_interleave(target_nonzero.unsqueeze(dim=1), repeats=N_gaussians, dim=1).to(device)\n",
    "        target_nonzero_2 = target_nonzero.repeat(1,N_gaussians).to(device)\n",
    "\n",
    "        # loss_1 是高斯分布的概率密度value\n",
    "        loss_1 = torch.exp(m.log_prob(target_nonzero_2))\n",
    "\n",
    "        # loss_2 是MDN的概率密度value\n",
    "        loss_2 = torch.sum(loss_1 * pi, dim=1)\n",
    "\n",
    "        ## 两种截断的方式都ok：\n",
    "        ## 方法一：在MDN的prob上设定最小值；\n",
    "        loss_3 = torch.clamp(loss_2,min=MIN_LOSS)\n",
    "        loss_4 = torch.log(loss_3)\n",
    "        # loss_sum = -torch.mean(loss_4) + loss_sum\n",
    "        loss_sum = -torch.sum(loss_4) + loss_sum\n",
    "\n",
    "        ## 方法二：最后加一个safety数 MIN_LOSS\n",
    "        # loss_3 = torch.log(loss_2+MIN_LOSS)\n",
    "        # loss_sum = -torch.mean(loss_3) + loss_sum\n",
    "\n",
    "    # 最后loss求一下平均\n",
    "    return loss_sum/len(Pi)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 cdf loss\n",
    "1. 注意把targets data的路径改一下"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# cdf loss\n",
    "###################\n",
    "def loss_fn_cdf(Pi, Mu, Sigma, Target, N_gaussians):\n",
    "\n",
    "    loss_sum = torch.tensor(0.,device=device,requires_grad=True)\n",
    "\n",
    "    # for each GMM\n",
    "    for i in range(len(Pi)):\n",
    "        target = Target[i,:,0]\n",
    "        prob_target = Target[i,:,1]\n",
    "        pi = Pi[i,:]\n",
    "        m = torch.distributions.Normal(loc=Mu[i,:], scale=Sigma[i,:])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Drop padded data and Expanded to the same dim\n",
    "            #### 写法一：work。无原地变换\n",
    "            idx = torch.nonzero(target)\n",
    "            target_nonzero = target[idx]\n",
    "            prob_target_nonzero = torch.squeeze(prob_target[idx])\n",
    "\n",
    "            target_nonzero_2 = target_nonzero.repeat(1,N_gaussians).to(device)\n",
    "\n",
    "        # loss_1 是高斯分布的概率密度value\n",
    "        loss_1 = torch.exp(m.cdf(target_nonzero_2))\n",
    "\n",
    "        # loss_2 是MDN的概率密度value\n",
    "        loss_2 = torch.sum(loss_1 * pi, dim=1)\n",
    "\n",
    "        # loss_3 是cdf的差值的一范数or二范数，表示的是MDN和target之间的总体差异\n",
    "        loss_3 = torch.norm((loss_2 - prob_target_nonzero), 2)\n",
    "\n",
    "        # loss_sum = torch.log(loss_3) + loss_sum\n",
    "        loss_sum = loss_3 + loss_sum\n",
    "\n",
    "    # Batch平均loss\n",
    "    return loss_sum/len(Pi)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.3 交叉熵/ KL\n",
    "1. 自己写的，没有用torch的函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 当input的shape是[50,3]时，输出应该是50个GMM\n",
    "def loss_preparation_CE(pi, mu, sigma):\n",
    "\n",
    "    m=[]\n",
    "    for i in range(pi.shape[0]):\n",
    "        m.append(torch.distributions.Normal(loc=mu[i,:], scale=sigma[i,:]))\n",
    "\n",
    "    # target_packed = pack_padded_sequence(target,target_len,batch_first=True,enforce_sorted=False)  # 去掉padded 0并拉成一个vector\n",
    "    # seq_unpacked, lens_unpacked = pad_packed_sequence(target_packed, batch_first=True)             # 和上面互为逆运算\n",
    "\n",
    "    return m"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # 当input的shape是[50,3]时，输出应该是50个GMM\n",
    "# # 对这50个GMM看能生成什么output\n",
    "\n",
    "def loss_fn_CE(Pi, Mu, Sigma, Target, N_gaussians):\n",
    "\n",
    "    loss_sum = torch.tensor(0.,device=device,requires_grad=True)\n",
    "\n",
    "    # for each GMM\n",
    "    for i in range(len(Pi)):\n",
    "        target = Target[i,:,0]\n",
    "        prob_target = Target[i,:,1]\n",
    "        pi = Pi[i,:]\n",
    "        m = torch.distributions.Normal(loc=Mu[i,:], scale=Sigma[i,:])\n",
    "\n",
    "        # Drop padded data and Expanded to the same dim\n",
    "        #### 写法一：work。无原地变换\n",
    "        idx = torch.nonzero(target)\n",
    "        target_nonzero = torch.squeeze(target[idx])\n",
    "        prob_target_nonzero = torch.squeeze(prob_target[idx])\n",
    "\n",
    "        target_nonzero_2 = torch.repeat_interleave(target_nonzero.unsqueeze(dim=1), repeats=N_gaussians, dim=1).to(device)\n",
    "\n",
    "        # loss_1 是高斯分布的概率密度value\n",
    "        loss_1 = torch.exp(m.log_prob(target_nonzero_2))\n",
    "\n",
    "        # loss_2 是MDN的概率密度value\n",
    "        loss_2 = torch.sum(loss_1 * pi, dim=1)\n",
    "\n",
    "        # loss_4是KL loss\n",
    "        # loss_4 = -prob_target_nonzero*torch.log(loss_2) + prob_target_nonzero*torch.log(prob_target_nonzero)\n",
    "        # print(\"prob_target_nonzero shape:\",prob_target_nonzero.shape)\n",
    "        # print(\"loss_2 shape:\",loss_2.shape)\n",
    "\n",
    "        loss_4 = -prob_target_nonzero*torch.log(loss_2+MIN_LOSS)+ prob_target_nonzero*torch.log(prob_target_nonzero)\n",
    "        loss_sum = torch.sum(loss_4)+loss_sum\n",
    "\n",
    "    loss_ts = loss_sum/len(Pi)\n",
    "    return loss_ts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def loss_test():\n",
    "    loss_2 = torch.tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "    loss_3 = loss_2[torch.nonzero(loss_2)].view(-1,1)         # 去掉所有的log(0)\n",
    "    print(\"loss_3:\",loss_3.shape)\n",
    "\n",
    "    MIN_LOSS = torch.min(loss_2) if torch.min(loss_2)>0 else 1e-20\n",
    "\n",
    "    print(\"MIN_LOSS：\",MIN_LOSS)\n",
    "    # 用MIN_loss替代loss_2为0的部分\n",
    "    loss_2[loss_2==0] = MIN_LOSS\n",
    "\n",
    "\n",
    "    loss_padded = [MIN_LOSS]* (loss_2.shape[0]-loss_3.shape[0])\n",
    "    loss_padded_ts = torch.tensor(loss_padded) # ,device=device\n",
    "    loss_3 = torch.cat((loss_3[:,0],loss_padded_ts))\n",
    "    loss_3.shape, torch.log(loss_3)\n",
    "    print(\"data: \",loss_3.data)\n",
    "    print(\"data: \",type(loss_3))\n",
    "# loss_test()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.5 W-distance\n",
    "1. 拿到target data的两列，按照第一列去predict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "p = 2\n",
    "entreg = .1 # entropy regularization factor for Sinkhorn\n",
    "factor = 1  # prob的放大系数\n",
    "# 若以欧式距离为metric，则cost function可以直接用geomloss提供的 Sinkhorn快速解\n",
    "OTLoss = geomloss.SamplesLoss(\n",
    "    loss='sinkhorn', p=p, blur=entreg**(1/p), backend='tensorized', verbose=True)\n",
    "\n",
    "def loss_fn_WD(Pi, Mu, Sigma, Duration, N_gaussians):\n",
    "\n",
    "    loss_sum = torch.tensor(0.,device=device,requires_grad=True)\n",
    "\n",
    "    for i in range(len(Pi)):\n",
    "\n",
    "        target = torch.unique(Duration[i],dim=0)\n",
    "        n_target = target[:,0]\n",
    "        non_zero_idx = torch.nonzero(n_target)\n",
    "        n = n_target[non_zero_idx]\n",
    "        p_target = target[:,1]\n",
    "        p = p_target[non_zero_idx]*factor\n",
    "\n",
    "        # 放大“p”\n",
    "        y_target = torch.cat([n,p],dim=1)\n",
    "\n",
    "        pi = Pi[i,:]\n",
    "        m = torch.distributions.Normal(loc=Mu[i,:], scale=Sigma[i,:])\n",
    "        x = torch.repeat_interleave(n, repeats=N_gaussians, dim=1)   # expand dim\n",
    "        y = (m.cdf(x+TARGET) - m.cdf(x)).to(device=device)\n",
    "        y_cdf = torch.sum(pi*y,dim=1).unsqueeze(dim=1)*factor\n",
    "        y_pred = torch.cat([n,y_cdf],dim=1)\n",
    "\n",
    "        loss_sum = OTLoss(y_pred,y_target) + loss_sum\n",
    "\n",
    "    return loss_sum/len(Pi)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5 Plot\n",
    "1. draw:\n",
    "    - loss: training data的loss和test data的loss趋势\n",
    "    - MLP的网络结构（.png）\n",
    "    - target distrb.和pred. distrb.\n",
    "    - 所有target data的分布图\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"logs-MLP\")\n",
    "viz = Visdom(env=\"001\")\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.1 draw 所有的target data\n",
    "1. 实际上在`i < 50`里决定画前50 or what target data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def draw_all_target_data():\n",
    "    # target data\n",
    "    # target_path = r\"../data/targets\"\n",
    "    target_path = r\"../data/targets_5\"\n",
    "    # target_path里有全部的target data地址\n",
    "    target_all_path = os.listdir(target_path)\n",
    "    len_target = len(target_all_path)\n",
    "    print(f\"一共有 *{len_target}* 组 target data\")\n",
    "\n",
    "    # Init\n",
    "    # viz_env_str = \"target_bar\"\n",
    "    # viz_env_str = \"target_line\"\n",
    "    # viz_env_str = \"target_hist\"\n",
    "    viz_env_str = \"target_scatter\"\n",
    "    viz = Visdom(env = viz_env_str)\n",
    "\n",
    "    for i in range(len_target):\n",
    "        target_path_i_path = os.path.join(target_path,target_all_path[i])\n",
    "        target_df = pd.read_csv(target_path_i_path,encoding=\"utf-8\")\n",
    "\n",
    "        # Init\n",
    "        win_str = str(target_all_path[i])\n",
    "        title_str = \"Target Distrb. of \"+win_str\n",
    "\n",
    "        if(i < 50):\n",
    "            # Hist plot which is not that great\n",
    "            if viz_env_str == \"target_hist\" :\n",
    "                viz.histogram(X = np.array(target_df.N), env=viz_env_str, win=win_str,\n",
    "                            opts= dict(title=title_str,numbins = 50))\n",
    "\n",
    "            # Line plot\n",
    "            if viz_env_str == \"target_line\" :\n",
    "                target_df.drop_duplicates(inplace=True)\n",
    "                viz.line(X = np.array(target_df.N),Y= np.array(target_df.P), env=viz_env_str, win=win_str,opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "            # Bar plot. 这个看起来是最准确的一个\n",
    "            if viz_env_str == \"target_bar\" :\n",
    "                target_df.drop_duplicates(inplace=True)\n",
    "                arr_str = [str(i) for i in np.array(target_df.N)]\n",
    "                viz.bar(X=np.array(target_df.P), env=viz_env_str, win=win_str,\n",
    "                        opts=dict(title=title_str,rownames=arr_str,stacked = False))\n",
    "\n",
    "            # Scatter plot\n",
    "            if viz_env_str == \"target_scatter\" :\n",
    "                target_df.drop_duplicates(inplace=True)\n",
    "                viz.scatter(X=np.array(target_df),env=viz_env_str, win=win_str,opts=dict(title=title_str,markersize = 3))\n",
    "    print(\"Done\")\n",
    "\n",
    "# draw_all_target_data()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 画target data的cdf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def draw_all_target_data_cdf():\n",
    "    # target data\n",
    "    # target_path = r\"../data/targets\"\n",
    "    target_path = r\"../data/targets_5_cdf\"\n",
    "    # target_path里有全部的target data地址\n",
    "    target_all_path = os.listdir(target_path)\n",
    "    len_target = len(target_all_path)\n",
    "    print(f\"一共有 *{len_target}* 组 target data\")\n",
    "\n",
    "    # Init，只需要改这个str就ok\n",
    "    # viz_env_str = \"target_barCDF\"\n",
    "    # viz_env_str = \"target_lineCDF\"\n",
    "    # viz_env_str = \"target_histCDF\"\n",
    "    viz_env_str = \"target_scatterCDF\"\n",
    "    viz = Visdom(env = viz_env_str)\n",
    "\n",
    "    for i in range(len_target):\n",
    "        target_path_i_path = os.path.join(target_path,target_all_path[i])\n",
    "        target_df = pd.read_csv(target_path_i_path,encoding=\"utf-8\")\n",
    "\n",
    "        # Init\n",
    "        win_str = str(target_all_path[i])\n",
    "        title_str = \"Target Distrb. of \"+win_str\n",
    "\n",
    "        if(i < 50):\n",
    "            # Hist plot which is not that great\n",
    "            if viz_env_str == \"target_histCDF\" :\n",
    "                viz.histogram(X = np.array(target_df.N), env=viz_env_str, win=win_str,\n",
    "                            opts= dict(title=title_str,numbins = 50))\n",
    "\n",
    "            # Line plot\n",
    "            if viz_env_str == \"target_lineCDF\" :\n",
    "                target_df.drop_duplicates(inplace=True)\n",
    "                viz.line(X = np.array(target_df.N),Y= np.array(target_df.P), env=viz_env_str, win=win_str,opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "            # Bar plot. 这个看起来是最准确的一个\n",
    "            if viz_env_str == \"target_barCDF\" :\n",
    "                target_df.drop_duplicates(inplace=True)\n",
    "                arr_str = [str(i) for i in np.array(target_df.N)]\n",
    "                viz.bar(X=np.array(target_df.P), env=viz_env_str, win=win_str,\n",
    "                        opts=dict(title=title_str,rownames=arr_str,stacked = False))\n",
    "\n",
    "            # Scatter plot\n",
    "            if viz_env_str == \"target_scatterCDF\" :\n",
    "                target_df.drop_duplicates(inplace=True)\n",
    "                viz.scatter(X=np.array(target_df),env=viz_env_str, win=win_str,opts=dict(title=title_str,markersize = 3))\n",
    "    print(\"Done\")\n",
    "\n",
    "# draw_all_target_data_cdf()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 也是画target data，但是是平滑处理\n",
    "- `savgol_filter`参数解析:\n",
    "    - y：代表曲线点坐标（x,y）中的y\n",
    "    - window_length：窗口长度，该值需为正奇整数。值越小，曲线越贴近真实曲线；值越大，平滑效果越厉害\n",
    "    - polyorder: 对窗口内的数据点进行k阶多项式拟合，k的值需要小于window_length。值越大，曲线越贴近真实曲线；值越小，平滑效果越厉害\n",
    "    - mode：确定了要应用滤波器的填充信号的扩展类型。（This determines the type of extension to use for the padded signal to which the filter is applied. ）\n",
    "- **实际上没用到这个函数**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def draw_smooth_target_data():\n",
    "    # target data\n",
    "    target_path = r\"../data/targets\"\n",
    "    # target_path里有全部的target data地址\n",
    "    target_all_path = os.listdir(target_path)\n",
    "    len_target = len(target_all_path)\n",
    "    print(f\"一共有 *{len_target}* 组 target data\")\n",
    "\n",
    "    # Init\n",
    "    viz_env_str = \"target_bar\"\n",
    "    # viz_env_str = \"target_distrb\"\n",
    "    # viz_env_str = \"target_hist\"\n",
    "    # viz_env_str = \"target_scatter\"\n",
    "    viz = Visdom(env = viz_env_str)\n",
    "\n",
    "    for i in range(len_target):\n",
    "        target_path_i_path = os.path.join(target_path,target_all_path[i])\n",
    "        target_df = pd.read_csv(target_path_i_path,encoding=\"utf-8\")\n",
    "\n",
    "        # Init\n",
    "        win_str = str(target_all_path[i])\n",
    "        title_str = \"Target Distrb. of \"+win_str\n",
    "\n",
    "        if(i < 100):\n",
    "            # Smoothen the data\n",
    "            y_smooth = scipy.signal.savgol_filter(target_df.P,window_length=53,polyorder=3)\n",
    "            # y_smooth = scipy.signal.savgol_filter(target_df.P, 99, 1, mode= 'nearest')\n",
    "            target_df.P = y_smooth\n",
    "\n",
    "            # Hist plot which is not that great\n",
    "            # viz.histogram(X = np.array(target_df.N), env=viz_env_str, win=win_str,\n",
    "            #             opts= dict(title=title_str,numbins = 50))\n",
    "\n",
    "            # Line plot\n",
    "            # target_df.drop_duplicates(inplace=True)\n",
    "            # viz.line(X = np.array(target_df.N),Y= np.array(target_df.P), env=viz_env_str, win=win_str,opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "            # Bar plot. 这个看起来是最准确的一个\n",
    "            target_df.drop_duplicates(inplace=True)\n",
    "            arr_str = [str(i) for i in np.array(target_df.N)]\n",
    "            viz.bar(X=np.array(target_df.P), env=viz_env_str, win=win_str,\n",
    "                    opts=dict(title=title_str,rownames=arr_str))\n",
    "\n",
    "            # Scatter plot\n",
    "            # target_df.drop_duplicates(inplace=True)\n",
    "            # viz.scatter(X=np.array(target_df),env=viz_env_str, win=win_str,opts=dict(title=title_str,markersize = 3))\n",
    "    print(\"Done\")\n",
    "\n",
    "# draw_smooth_target_data()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#### Test for drawing\n",
    "def test_draw():\n",
    "    viz = Visdom(env=\"001\")\n",
    "\n",
    "    mu = torch.tensor([0,10,20])\n",
    "    sigma = torch.tensor([1,1,1])\n",
    "    duration = torch.tensor([0,1,2,0])\n",
    "    duration = torch.repeat_interleave(duration.unsqueeze(dim=1), repeats=3, dim=1)\n",
    "    m = torch.distributions.Normal(loc=mu, scale=sigma)\n",
    "    pi = torch.tensor([0.2,0.3,0.5])\n",
    "\n",
    "    # draw\n",
    "    x_0 = torch.tensor(np.arange(0,1000))\n",
    "    x = torch.repeat_interleave(x_0.unsqueeze(dim=1), repeats=3, dim=1)\n",
    "    y = torch.exp(m.log_prob(x))\n",
    "    y_sum = torch.unsqueeze(torch.sum(pi*y,dim=1),dim=1)\n",
    "    viz.line(X = x_0,Y= torch.cat([y,y_sum],dim = 1), env=\"001\", win=\"test_draw_2\",\n",
    "            opts= dict(title='test_draw', legend=['N1', 'N2', 'N3','NNN']))\n",
    "# test_draw()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.2 plot mdn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2.1 mdn pdf的图像[with input data]\n",
    "1. 求解[1,6,11...]上的cdf：\n",
    "    - 求解[1,6,11..]的cdf，再求解[6,11,..]，两者相减"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def draw_mdn_2(pi, mu, sigma, target, input, total_train_step, N_gaussians):\n",
    "    \"\"\"\n",
    "    画两条曲线，pred-1表示归一化前，pred-2表示归一化后\n",
    "\n",
    "    :param pi:\n",
    "    :param mu:\n",
    "    :param sigma:\n",
    "    :param target:\n",
    "    :param input:\n",
    "    :param total_train_step:\n",
    "    :param N_gaussians:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 只画一个batch中的第一个\n",
    "\n",
    "    # The target distrb.\n",
    "    target = torch.unique(target,dim=0)    # drop the duplicate\n",
    "    n_target = target[:,0]\n",
    "    non_zero_idx = torch.nonzero(n_target)\n",
    "    n = n_target[non_zero_idx]\n",
    "\n",
    "    p_target = target[:,1]\n",
    "    p = p_target[non_zero_idx]\n",
    "    max_n = int(max(n).item())\n",
    "\n",
    "    # The predicted distrb.\n",
    "    m = torch.distributions.Normal(mu,sigma)          # Gaussian\n",
    "    # m = torch.distributions.Laplace(mu,sigma)           # Laplace\n",
    "\n",
    "    x_0 = torch.arange(1,(max_n+TARGET),TARGET).to(device=device)\n",
    "\n",
    "    x = torch.repeat_interleave(x_0.unsqueeze(dim=1), repeats=N_gaussians, dim=1)   # expand dim\n",
    "    # 求解每个长度为TARGET的区间上的cdf\n",
    "    y = (m.cdf(x+TARGET) - m.cdf(x)).to(device=device)\n",
    "\n",
    "    # 方法一：\n",
    "    y_pred_1 = torch.sum(pi*y,dim=1)\n",
    "    # 方法二：做一下(0,1)上的归一化\n",
    "    y_pred_2 = y_pred_1/y_pred_1.sum()\n",
    "\n",
    "    # The input distrb.\n",
    "    input_data = input[0:2,:]\n",
    "    x_input = torch.arange(1,(300+1),SCALE).to(device=device)\n",
    "    y_input_0 = (input_data[0,:]).to(device=device)\n",
    "    y_input_1 = (input_data[1,:]).to(device=device)\n",
    "\n",
    "    # Init\n",
    "    win_str = \"total_train_step = \"+str(total_train_step)\n",
    "    title_str = \"Distrb. in \"+win_str\n",
    "    viz.line(X = [0.],Y = [0.], env=\"001\", win=win_str, opts= dict(title=title_str))\n",
    "\n",
    "    # Visdom本身不能把hist和line画在一个window中\n",
    "    # 如果想画一起只能是两条line\n",
    "    # Plot y_target\n",
    "    # viz.histogram(X = n, env=\"001\", win=win_str,\n",
    "    #         opts= dict(title=title_str))\n",
    "    viz.line(X = n,Y= p, env=\"001\", win=win_str, update=\"append\", name='target',\n",
    "            opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "    # Plot y_pred\n",
    "    viz.line(X = x_0,Y= y_pred_1, env=\"001\", win=win_str, update=\"append\", name='pred-1',\n",
    "            opts= dict(title=title_str))\n",
    "\n",
    "    # Plot y_pred_2\n",
    "    viz.line(X = x_0,Y= y_pred_2, env=\"001\", win=win_str, update=\"append\", name='pred-2',\n",
    "            opts= dict(title=title_str))\n",
    "\n",
    "    # Plot y_input\n",
    "    # 如果input data长度短，全部画完\n",
    "    if(len(x_input) <= max_n):\n",
    "        viz.line(X = x_input,Y= y_input_0, env=\"001\", win=win_str, update=\"append\", name='GT-1', opts= dict(title=title_str))\n",
    "        viz.line(X = x_input,Y= y_input_1, env=\"001\", win=win_str, update=\"append\", name='GT-2', opts= dict(title=title_str))\n",
    "    # 如果input data长度长，则截断\n",
    "    else:\n",
    "        x_input_0 = x_input[0:max_n]\n",
    "        y_input_2 = y_input_0[0:max_n]/y_input_0[0:max_n].sum()\n",
    "        y_input_3 = y_input_1[0:max_n]/y_input_1[0:max_n].sum()\n",
    "        viz.line(X = x_input_0,Y= y_input_2, env=\"001\", win=win_str, update=\"append\", name='GT-1', opts= dict(title=title_str))\n",
    "        viz.line(X = x_input_0,Y= y_input_3, env=\"001\", win=win_str, update=\"append\", name='GT-2', opts= dict(title=title_str))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2.2 plot mdn cdf[无input data]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def draw_mdn_cdf(pi, mu, sigma, target, input, total_train_step, N_gaussians):\n",
    "    # 只画一个batch中的第一个\n",
    "\n",
    "    # The target distrb.\n",
    "    target = torch.unique(target,dim=0)    # drop the duplicate\n",
    "    n_target = target[:,0]\n",
    "    non_zero_idx = torch.nonzero(n_target)\n",
    "    n = n_target[non_zero_idx]\n",
    "\n",
    "    p_target = target[:,1]\n",
    "    p = p_target[non_zero_idx]\n",
    "    max_n = max(n).item()               # 横轴长度\n",
    "\n",
    "    # The predicted distrb.\n",
    "    m = torch.distributions.Normal(mu,sigma)\n",
    "    x_0 = torch.arange(1,max_n).to(device=device)\n",
    "    x = torch.repeat_interleave(x_0.unsqueeze(dim=1), repeats=N_gaussians, dim=1)\n",
    "    y = m.cdf(x).to(device=device)                        # y:多条高斯曲线; y_pred: 一条GMM曲线\n",
    "    # y_pred = torch.unsqueeze(torch.sum(pi*y,dim=1),dim=1)                 # 维度相等才能cat\n",
    "    y_pred = torch.sum(pi*y,dim=1)\n",
    "\n",
    "    # Init\n",
    "    win_str = \"total_train_step = \"+str(total_train_step)\n",
    "    title_str = \"Distrb. in \"+win_str\n",
    "    viz.line(X = [0.],Y = [0.], env=\"001\", win=win_str, opts= dict(title=title_str))\n",
    "\n",
    "    # Visdom本身不能把hist和line画在一个window中\n",
    "    # 如果想画一起只能是两条line\n",
    "    # Plot y_target\n",
    "    # viz.histogram(X = n, env=\"001\", win=win_str,\n",
    "    #         opts= dict(title=title_str))\n",
    "    viz.line(X = n,Y= p, env=\"001\", win=win_str, update=\"append\", name='target',\n",
    "            opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "    # Plot y_pred\n",
    "    viz.line(X = x_0,Y= y_pred, env=\"001\", win=win_str, update=\"append\", name='pred',\n",
    "            opts= dict(title=title_str))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2.3 plot mdn pdf in TEST\n",
    "1. 读入2个model时，在test set上比较效果"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_env = \"003\"\n",
    "def draw_mdn_3(Pi1, Mu1, Sigma1, Pi2, Mu2, Sigma2, Target, Input, test_batch, N_gaussians):\n",
    "    for i in range(len(Pi1)):\n",
    "        # The target distrb.\n",
    "        target = Target[i]\n",
    "        target = torch.unique(target,dim=0)    # drop the duplicate\n",
    "        n_target = target[:,0]\n",
    "        non_zero_idx = torch.nonzero(n_target)\n",
    "        n = n_target[non_zero_idx]\n",
    "\n",
    "        p_target = target[:,1]\n",
    "        p = p_target[non_zero_idx]\n",
    "        max_n = int(max(n).item())\n",
    "\n",
    "        # The predicted distrb.\n",
    "        mu1 = Mu1[i]\n",
    "        mu2 = Mu2[i]\n",
    "        sigma1 = Sigma1[i]\n",
    "        sigma2 = Sigma2[i]\n",
    "        m1 = torch.distributions.Normal(mu1,sigma1)          # Gaussian\n",
    "        m2 = torch.distributions.Normal(mu2,sigma2)          # Gaussian\n",
    "        # m = torch.distributions.Laplace(mu,sigma)           # Laplace\n",
    "\n",
    "        x_0 = torch.arange(1,(max_n+TARGET),TARGET).to(device=device)\n",
    "\n",
    "        x = torch.repeat_interleave(x_0.unsqueeze(dim=1), repeats=N_gaussians, dim=1)   # expand dim\n",
    "        # 求解每个长度为TARGET的区间上的cdf\n",
    "        y1 = (m1.cdf(x+TARGET) - m1.cdf(x)).to(device=device)\n",
    "        y2 = (m2.cdf(x+TARGET) - m2.cdf(x)).to(device=device)\n",
    "\n",
    "        # 方法一：\n",
    "        pi1 = Pi1[i]\n",
    "        pi2 = Pi2[i]\n",
    "        y_mdn_1 = torch.sum(pi1*y1,dim=1)\n",
    "        y_mdn_2 = torch.sum(pi2*y2,dim=1)\n",
    "        # 方法二：做一下(0,1)上的归一化\n",
    "        y_pred_1 = y_mdn_1/y_mdn_1.sum()\n",
    "        y_pred_2 = y_mdn_2/y_mdn_2.sum()\n",
    "\n",
    "        # The input distrb.\n",
    "        input = Input[i]\n",
    "        input_data = input[0:2,:]\n",
    "        x_input = torch.arange(1,(300+1),SCALE).to(device=device)\n",
    "        y_input_0 = (input_data[0,:]).to(device=device)\n",
    "        y_input_1 = (input_data[1,:]).to(device=device)\n",
    "\n",
    "        # Init\n",
    "        win_str = \"Test batch = \"+str(test_batch)+\" idx = \"+str(i)\n",
    "        title_str = \"Distrb. of \"+win_str\n",
    "        viz.line(X = [0.],Y = [0.], env=test_env, win=win_str, opts= dict(title=title_str))\n",
    "\n",
    "        # Visdom本身不能把hist和line画在一个window中\n",
    "        # 如果想画一起只能是两条line\n",
    "        # Plot y_target\n",
    "        # viz.histogram(X = n, env=\"001\", win=win_str,\n",
    "        #         opts= dict(title=title_str))\n",
    "        viz.line(X = n,Y= p, env=test_env, win=win_str, update=\"append\", name='target',\n",
    "                opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "        # Plot y_pred\n",
    "        viz.line(X = x_0,Y= y_pred_1, env=test_env, win=win_str, update=\"append\", name='pred-MLE',opts= dict(title=title_str))\n",
    "        viz.line(X = x_0,Y= y_pred_2, env=test_env, win=win_str, update=\"append\", name='pred-WD',\n",
    "                opts= dict(title=title_str))\n",
    "\n",
    "        # Plot y_input\n",
    "        # 如果input data长度短，全部画完\n",
    "        if(len(x_input) <= max_n):\n",
    "            viz.line(X = x_input,Y= y_input_0, env=test_env, win=win_str, update=\"append\", name='GT-1', opts= dict(title=title_str))\n",
    "            viz.line(X = x_input,Y= y_input_1, env=test_env, win=win_str, update=\"append\", name='GT-2', opts= dict(title=title_str))\n",
    "        # 如果input data长度长，则截断\n",
    "        else:\n",
    "            x_input_0 = x_input[0:max_n]\n",
    "            y_input_2 = y_input_0[0:max_n]/y_input_0[0:max_n].sum()\n",
    "            y_input_3 = y_input_1[0:max_n]/y_input_1[0:max_n].sum()\n",
    "            viz.line(X = x_input_0,Y= y_input_2, env=test_env, win=win_str, update=\"append\", name='GT-1', opts= dict(title=title_str))\n",
    "            viz.line(X = x_input_0,Y= y_input_3, env=test_env, win=win_str, update=\"append\", name='GT-2', opts= dict(title=title_str))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.3 png格式的net structure"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def draw_the_net_png():\n",
    "\n",
    "    x = torch.randn([5, 3, 300])  # 定义网络的输入值\n",
    "    mlp = MLP(N_gaussians)\n",
    "    y = mlp(x)                    # 获取网络的预测值\n",
    "\n",
    "    MyConvNetVis = make_dot(y, params=dict(list(mlp.named_parameters()) + [('x', x)]))\n",
    "    MyConvNetVis.format = \"png\"\n",
    "    # 指定文件生成的文件夹\n",
    "    MyConvNetVis.directory = \"data_pic\"\n",
    "    # 生成文件\n",
    "    MyConvNetVis.view()\n",
    "# draw_the_net_png()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.4 plot the loss\n",
    "1. 直接根据参数决定draw什么loss，反正loss都画在一个window里面"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "win_train_loss_str = \"The Loss of BATCH in the Training Data\"\n",
    "win_vali_loss_str = \"The Loss in the Vali Data\"\n",
    "win_train_epoch_loss_str = \"The Loss of EPOCH in the Training Data\"\n",
    "\n",
    "def draw_loss(X_step, loss, win_str):\n",
    "    viz.line(X = [X_step], Y = [loss],win=win_str, update=\"append\",\n",
    "        opts= dict(title=win_str))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.5 plot the metric"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "win_vali_metric_str = \"The NLL of ALL vali data\"\n",
    "def draw_metric(X_step, total_vali_metric, GT_metric):\n",
    "\n",
    "    viz.line(X = [X_step], Y = [[total_vali_metric,GT_metric[0,0],GT_metric[0,1]]],win=win_vali_metric_str, update=\"append\", opts= dict(title=win_vali_metric_str, legend=['pred','GT-1','GT-2'], showlegend=True,xlabel=\"epoch\", ylabel=\"NLL\"))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.6 grad graph\n",
    "https://github.com/t-vi/pytorch-tvmisc/blob/master/visualize/bad_grad_viz.ipynb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def iter_graph(root, callback):\n",
    "    queue = [root]\n",
    "    seen = set()\n",
    "    while queue:\n",
    "        # 队头出fn然后判断fn是否seen\n",
    "        fn = queue.pop()\n",
    "        if fn in seen:\n",
    "            continue\n",
    "        # unseen加入queue，并且继续沿着fn向下递归（DFS？）\n",
    "        seen.add(fn)\n",
    "        for next_fn, _ in fn.next_functions:\n",
    "            # Alert：只有非none的才会加入queue，然后在register_grad里被fn_dict记录下grad_input\n",
    "            if next_fn is not None:\n",
    "                queue.append(next_fn)\n",
    "        callback(fn)\n",
    "\n",
    "def register_hooks(var):\n",
    "    fn_dict = {}  # 记录了grad名字和值（grad_input）\n",
    "    def hook_cb(fn):\n",
    "        def register_grad(grad_input, grad_output):\n",
    "            fn_dict[fn] = grad_input\n",
    "        fn.register_hook(register_grad)\n",
    "\n",
    "    # 1. 递归注册grad_input\n",
    "    iter_graph(var.grad_fn, hook_cb)\n",
    "\n",
    "    # def is_bad_grad(grad_output):\n",
    "    #     if grad_output is None:\n",
    "    #         return False\n",
    "    #     return grad_output.isnan().any() or (grad_output.abs() >= 1e6).any()\n",
    "    def is_bad_grad(grad_output):\n",
    "        if grad_output is None:\n",
    "                return True\n",
    "        grad_output = grad_output.data\n",
    "        return grad_output.ne(grad_output).any() or grad_output.gt(1e6).any()\n",
    "\n",
    "    # 2. graph已经从grad input构建完了，还有grad output的判断以及颜色的选择\n",
    "    def make_dot():\n",
    "        node_attr = dict(style='filled',\n",
    "                        shape='box',\n",
    "                        align='left',\n",
    "                        fontsize='12',\n",
    "                        ranksep='0.1',\n",
    "                        height='0.2')\n",
    "        dot = Digraph(node_attr=node_attr, graph_attr=dict(size=\"12,12\"))\n",
    "\n",
    "        def size_to_str(size):\n",
    "            return '('+(', ').join(map(str, size))+')'\n",
    "\n",
    "        def build_graph(fn):\n",
    "            if hasattr(fn, 'variable'):  # if GradAccumulator\n",
    "                u = fn.variable\n",
    "                node_name = 'Variable\\n ' + size_to_str(u.size())\n",
    "                dot.node(str(id(u)), node_name, fillcolor='lightblue')\n",
    "            else:  # 不是variable属性，白色\n",
    "                assert fn in fn_dict, fn   # 判断fn在不在fn_dict里面！\n",
    "                fillcolor = 'white'\n",
    "                if any(is_bad_grad(gi) for gi in fn_dict[fn]):\n",
    "                    fillcolor = 'red'\n",
    "                dot.node(str(id(fn)), str(type(fn).__name__), fillcolor=fillcolor)\n",
    "            for next_fn, _ in fn.next_functions:\n",
    "                if next_fn is not None:\n",
    "                    next_id = id(getattr(next_fn, 'variable', next_fn))\n",
    "                    dot.edge(str(next_id), str(id(fn)))\n",
    "        iter_graph(var.grad_fn, build_graph)\n",
    "\n",
    "        return dot\n",
    "\n",
    "    return make_dot\n",
    "\n",
    "# x = torch.randn(10, 10, requires_grad=True)\n",
    "# y = torch.randn(10, 10, requires_grad=True)\n",
    "#\n",
    "# z = x / (y * 0)\n",
    "# z = z.sum() * 2\n",
    "# get_dot = register_hooks(z)\n",
    "\n",
    "\n",
    "# with torch.autograd.detect_anomaly():\n",
    "#     z.backward()\n",
    "#     dot = get_dot()\n",
    "#     dot.save('tmp.dot') # to get .dot\n",
    "#     dot.render('tmp') # to get SVG\n",
    "#     # dot # in Jupyter, you can just render the variable"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6 Training\n",
    "## 6.1 Init"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "       BatchNorm1d-1               [-1, 3, 300]               6\n",
      "           Flatten-2                  [-1, 900]               0\n",
      "            Linear-3                    [-1, 9]           8,109\n",
      "       BatchNorm1d-4                    [-1, 9]              18\n",
      "           Dropout-5                    [-1, 9]               0\n",
      "         LeakyReLU-6                    [-1, 9]               0\n",
      "            Linear-7                    [-1, 3]              30\n",
      "           Softmax-8                    [-1, 3]               0\n",
      "            Linear-9                    [-1, 3]              30\n",
      "           Linear-10                    [-1, 3]              30\n",
      "================================================================\n",
      "Total params: 8,223\n",
      "Trainable params: 8,223\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.03\n",
      "Estimated Total Size (MB): 0.05\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "mlp = MLP_1_5(N_gaussians)\n",
    "\n",
    "# Init the params\n",
    "# # mlp = model_param_init(mlp)\n",
    "# Init the vis\n",
    "viz.line(X = [0.],Y = [0.], env=\"001\", win=win_train_loss_str, opts= dict(title=win_train_loss_str))\n",
    "viz.line(X = [0.],Y = [0.], env=\"001\", win=win_train_epoch_loss_str, opts= dict(title=win_train_epoch_loss_str))\n",
    "viz.line(X = [0.],Y = [[0.,0.,0.]], env=\"001\", win=win_vali_metric_str, opts= dict(title=win_vali_metric_str,legend=['pred','GT-1','GT-2'], showlegend=True,xlabel=\"epoch\", ylabel=\"NLL\"))\n",
    "\n",
    "# Save the init params\n",
    "# torch.save(mlp.state_dict(), 'mlp_init.pth')\n",
    "\n",
    "# Load the saved model\n",
    "# model_path_WD = \"mlp_train=300_WD.pth\"\n",
    "# model_path_LSE = \"mlp_train=300_LSE.pth\"\n",
    "# model_data = torch.load(model_path_LSE)\n",
    "# mlp.load_state_dict(model_data)\n",
    "\n",
    "mlp = mlp.to(device=device)\n",
    "summary(mlp, (3,300))\n",
    "\n",
    "############ learning rate strategy ############\n",
    "# 1.\n",
    "# Set different lr for params\n",
    "# id: id() 函数返回对象的唯一标识符，标识符是一个整数。返回对象的内存地址。\n",
    "mu_params = list(map(id, mlp.z_mu.parameters()))\n",
    "base_params = filter(lambda p: id(p) not in mu_params, mlp.parameters())\n",
    "params = [{'params': base_params},         # 如果对某个参数不指定学习率，就使用最外层的默认学习率\n",
    "        {'params': mlp.z_mu.parameters(), 'lr': lr_for_mu}]\n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "\n",
    "# optimizer = torch.optim.Adagrad(params,lr=learning_rate,weight_decay = 1e-4)\n",
    "# optimizer = torch.optim.Adagrad(params,lr=learning_rate)\n",
    "\n",
    "# 2.\n",
    "# Or set lr decay\n",
    "# StepLR为步进，每step_size个epoch，lr*gamma\n",
    "# scheduler  = torch.optim.lr_scheduler.StepLR(optimizer,step_size=10,gamma=0.8)\n",
    "# 下面这个比较好用\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10,20], gamma=0.5, last_epoch=-1)\n",
    "\n",
    "# # Set the hooks\n",
    "# #mlp.conv1.register_forward_hook(hook_forward_fn)\n",
    "# mlp.conv1.register_full_backward_hook(hook_backward_fn)\n",
    "#\n",
    "# #mlp.linear1.register_forward_hook(hook_forward_fn)\n",
    "# #mlp.linear1.register_full_backward_hook(hook_backward_fn)\n",
    "#\n",
    "# #mlp.z_pi.register_forward_hook(hook_forward_fn)\n",
    "# mlp.z_pi.register_full_backward_hook(hook_backward_fn_pi)\n",
    "#\n",
    "# #mlp.z_mu.register_forward_hook(hook_forward_fn)\n",
    "# mlp.z_mu.register_full_backward_hook(hook_backward_fn_mu)\n",
    "#\n",
    "# mlp.z_sigma.register_forward_hook(hook_forward_fn)\n",
    "# mlp.z_sigma.register_full_[2, 3, 300]backward_hook(hook_backward_fn_sigma)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.2 grad check\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "outputs": [],
   "source": [
    "# # grad check\n",
    "# input_test = torch.randn((1,3,100), requires_grad=True,device=device)\n",
    "# test_ans = gradcheck(mlp.to(device), input_test, eps=1e-3)  #, eps=1e-6, atol=1e-4\n",
    "# print(\"Are the gradients correct: \", test_ans)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.3 START HERE\n",
    "1. 使用Laplace分布改：\n",
    "    - loss function和draw"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- 11 batch----\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Now this is EPOCH 0 ==========\n",
      "========== IN EPOCH 0 the total loss is 17645.664154052734 ==========\n",
      "========== Now this is EPOCH 1 ==========\n",
      "========== IN EPOCH 1 the total loss is 14537.146301269531 ==========\n",
      "========== Now this is EPOCH 2 ==========\n",
      "========== IN EPOCH 2 the total loss is 14625.306365966797 ==========\n",
      "========== Now this is EPOCH 3 ==========\n",
      "========== IN EPOCH 3 the total loss is 14423.18472290039 ==========\n",
      "========== Now this is EPOCH 4 ==========\n",
      "========== IN EPOCH 4 the total loss is 14224.938842773438 ==========\n",
      "========== Now this is EPOCH 5 ==========\n",
      "========== IN EPOCH 5 the total loss is 13933.966491699219 ==========\n",
      "========== Now this is EPOCH 6 ==========\n",
      "========== IN EPOCH 6 the total loss is 13972.314239501953 ==========\n",
      "========== Now this is EPOCH 7 ==========\n",
      "========== IN EPOCH 7 the total loss is 14410.543884277344 ==========\n",
      "========== Now this is EPOCH 8 ==========\n",
      "========== IN EPOCH 8 the total loss is 14280.940460205078 ==========\n",
      "========== Now this is EPOCH 9 ==========\n",
      "========== IN EPOCH 9 the total loss is 14346.427734375 ==========\n",
      "========== Now this is EPOCH 10 ==========\n",
      "========== IN EPOCH 10 the total loss is 14402.429809570312 ==========\n",
      "========== Now this is EPOCH 11 ==========\n",
      "========== IN EPOCH 11 the total loss is 14062.707885742188 ==========\n",
      "========== Now this is EPOCH 12 ==========\n",
      "========== IN EPOCH 12 the total loss is 14414.205352783203 ==========\n",
      "========== Now this is EPOCH 13 ==========\n",
      "========== IN EPOCH 13 the total loss is 14767.513549804688 ==========\n",
      "========== Now this is EPOCH 14 ==========\n",
      "========== IN EPOCH 14 the total loss is 14651.690612792969 ==========\n",
      "========== Now this is EPOCH 15 ==========\n",
      "========== IN EPOCH 15 the total loss is 13809.718292236328 ==========\n",
      "========== Now this is EPOCH 16 ==========\n",
      "========== IN EPOCH 16 the total loss is 13742.901428222656 ==========\n",
      "========== Now this is EPOCH 17 ==========\n",
      "========== IN EPOCH 17 the total loss is 13915.680755615234 ==========\n",
      "========== Now this is EPOCH 18 ==========\n",
      "========== IN EPOCH 18 the total loss is 14280.2861328125 ==========\n",
      "========== Now this is EPOCH 19 ==========\n",
      "========== IN EPOCH 19 the total loss is 13801.107513427734 ==========\n",
      "========== Now this is EPOCH 20 ==========\n",
      "========== IN EPOCH 20 the total loss is 13813.64907836914 ==========\n",
      "========== Now this is EPOCH 21 ==========\n",
      "========== IN EPOCH 21 the total loss is 13818.947326660156 ==========\n",
      "========== Now this is EPOCH 22 ==========\n",
      "========== IN EPOCH 22 the total loss is 13714.503173828125 ==========\n",
      "========== Now this is EPOCH 23 ==========\n",
      "========== IN EPOCH 23 the total loss is 13707.276275634766 ==========\n",
      "========== Now this is EPOCH 24 ==========\n",
      "========== IN EPOCH 24 the total loss is 13551.084899902344 ==========\n",
      "========== Now this is EPOCH 25 ==========\n",
      "========== IN EPOCH 25 the total loss is 13628.681182861328 ==========\n",
      "========== Now this is EPOCH 26 ==========\n",
      "========== IN EPOCH 26 the total loss is 13523.810272216797 ==========\n",
      "========== Now this is EPOCH 27 ==========\n",
      "========== IN EPOCH 27 the total loss is 13802.402923583984 ==========\n",
      "========== Now this is EPOCH 28 ==========\n",
      "========== IN EPOCH 28 the total loss is 13494.236602783203 ==========\n",
      "========== Now this is EPOCH 29 ==========\n",
      "========== IN EPOCH 29 the total loss is 13461.488159179688 ==========\n",
      "========== Now this is EPOCH 30 ==========\n",
      "========== IN EPOCH 30 the total loss is 13432.338653564453 ==========\n",
      "========== Now this is EPOCH 31 ==========\n",
      "========== IN EPOCH 31 the total loss is 13709.239074707031 ==========\n",
      "========== Now this is EPOCH 32 ==========\n",
      "========== IN EPOCH 32 the total loss is 13760.7783203125 ==========\n",
      "========== Now this is EPOCH 33 ==========\n",
      "========== IN EPOCH 33 the total loss is 13629.730865478516 ==========\n",
      "========== Now this is EPOCH 34 ==========\n",
      "========== IN EPOCH 34 the total loss is 13682.360778808594 ==========\n",
      "========== Now this is EPOCH 35 ==========\n",
      "========== IN EPOCH 35 the total loss is 13639.42446899414 ==========\n",
      "========== Now this is EPOCH 36 ==========\n",
      "========== IN EPOCH 36 the total loss is 13574.568817138672 ==========\n",
      "========== Now this is EPOCH 37 ==========\n",
      "========== IN EPOCH 37 the total loss is 13436.925567626953 ==========\n",
      "========== Now this is EPOCH 38 ==========\n",
      "========== IN EPOCH 38 the total loss is 13558.411651611328 ==========\n",
      "========== Now this is EPOCH 39 ==========\n",
      "========== IN EPOCH 39 the total loss is 13487.615356445312 ==========\n",
      "========== Now this is EPOCH 40 ==========\n",
      "========== IN EPOCH 40 the total loss is 13625.459106445312 ==========\n",
      "========== Now this is EPOCH 41 ==========\n",
      "========== IN EPOCH 41 the total loss is 13433.677459716797 ==========\n",
      "========== Now this is EPOCH 42 ==========\n",
      "========== IN EPOCH 42 the total loss is 13493.318725585938 ==========\n",
      "========== Now this is EPOCH 43 ==========\n",
      "========== IN EPOCH 43 the total loss is 13342.776062011719 ==========\n",
      "========== Now this is EPOCH 44 ==========\n",
      "========== IN EPOCH 44 the total loss is 13173.118286132812 ==========\n",
      "========== Now this is EPOCH 45 ==========\n",
      "========== IN EPOCH 45 the total loss is 13246.631896972656 ==========\n",
      "========== Now this is EPOCH 46 ==========\n",
      "========== IN EPOCH 46 the total loss is 13188.307800292969 ==========\n",
      "========== Now this is EPOCH 47 ==========\n",
      "========== IN EPOCH 47 the total loss is 13605.532958984375 ==========\n",
      "========== Now this is EPOCH 48 ==========\n",
      "========== IN EPOCH 48 the total loss is 13656.184600830078 ==========\n",
      "========== Now this is EPOCH 49 ==========\n",
      "========== IN EPOCH 49 the total loss is 13259.147537231445 ==========\n",
      "Total training time when epoch= *50* is *427.59306478500366 *s\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# filename = \"../log_file.txt\"\n",
    "# f = open(filename,'w')\n",
    "total_train_step = 0\n",
    "EPOCH_NUM = 50\n",
    "\n",
    "train_start_time = time.time()\n",
    "mlp.train()\n",
    "\n",
    "for epoch in range(EPOCH_NUM):\n",
    "    # # Save the net structure\n",
    "    # net_file_name = \"net_before_epoch\"+str(epoch)+\".pth\"\n",
    "    # net_path = net_root_path + net_file_name\n",
    "    # torch.save(mlp.state_dict(), net_path)\n",
    "\n",
    "    epoch_train_loss = 0\n",
    "    print(f\"========== Now this is EPOCH {epoch} ==========\")\n",
    "    for batch_id,data in enumerate(train_loader):\n",
    "\n",
    "        input_data, target, setting, target_metric = data\n",
    "        # print(f\"---- {batch_id} batch----\")\n",
    "        # Do the inference\n",
    "        input_data = input_data.to(device)\n",
    "        target = target.to(device)\n",
    "        pi, mu, sigma = mlp(input_data)\n",
    "\n",
    "        # Save the params\n",
    "        # params = list(mlp.named_parameters())\n",
    "\n",
    "        # Cal the MLE loss and draw the distrb.\n",
    "        loss = loss_fn_v2(pi, mu, sigma, target, N_gaussians)\n",
    "        # loss = loss_fn_v3(pi, mu, sigma, target, N_gaussians)\n",
    "        # loss = loss_fn_v5(pi, mu, sigma, target, N_gaussians)\n",
    "\n",
    "        # Cal the CDF loss and draw the distrb.\n",
    "        # loss = loss_fn_cdf(pi, mu, sigma, target, N_gaussians)\n",
    "\n",
    "        # Cal the CE or KL loss and draw the distrb.\n",
    "        # loss  = loss_fn_CE(pi, mu, sigma, target, N_gaussians)\n",
    "\n",
    "        # Cal the WD loss and draw the distrb.\n",
    "        # loss  = loss_fn_WD(pi, mu, sigma, target, N_gaussians)\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "        draw_loss(total_train_step, loss.item(),win_train_loss_str)\n",
    "\n",
    "        # print(\"训练次数：{}，Loss：{}\".format(total_train_step, loss.item()))\n",
    "\n",
    "        # Optim\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        ########### before step() ###############\n",
    "\n",
    "        # for name, parms in mlp.named_parameters():\n",
    "        #     print(\"=====before step()=====\")\n",
    "        #     print('-->name:', name)\n",
    "        #     print('-->params:', parms.data)\n",
    "        #     # print('-->grad_requirs:',parms.requires_grad)\n",
    "        #     print('-->grad_value:',parms.grad)\n",
    "        #     print(\"===\")\n",
    "\n",
    "        ######\n",
    "\n",
    "        # 反向传播时检测是否有异常值，定位code\n",
    "        # with torch.autograd.detect_anomaly():\n",
    "        #     loss.backward()\n",
    "\n",
    "        ######\n",
    "        # get_dot = register_hooks(loss)\n",
    "        # dot = get_dot()\n",
    "        # dot.save('tmp.dot')   # to get .dot\n",
    "        # dot.render('tmp')     # to get pdf\n",
    "\n",
    "        ######\n",
    "        # Print grad check\n",
    "        # v_n = []    # name\n",
    "        # v_v = []    # value\n",
    "        # v_g = []    # grad\n",
    "        # for name, parameter in mlp.named_parameters():\n",
    "        #     v_n.append(name)\n",
    "        #     v_v.append(parameter.detach().cpu().numpy() if parameter is not None else [0])\n",
    "        #     v_g.append(parameter.grad.detach().cpu().numpy() if parameter.grad is not None else [0])\n",
    "        # for i in range(len(v_n)):\n",
    "        #     # if np.max(v_v[i]).item() - np.min(v_v[i]).item() < 1e-6:\n",
    "        #     #     color = bcolors.FAIL + '*'\n",
    "        #     if np.isnan(v_g[i]).any() or np.isnan(v_v[i]).any():\n",
    "        #         color = bcolors.FAIL + '*'\n",
    "        #     else:\n",
    "        #         color = bcolors.OKGREEN + ' '\n",
    "        #     print('%svalue %s: %.3e ~ %.3e' % (color, v_n[i], np.min(v_v[i]).item(), np.max(v_v[i]).item()))\n",
    "        #     print('%sgrad  %s: %.3e ~ %.3e' % (color, v_n[i], np.min(v_g[i]).item(), np.max(v_g[i]).item()))\n",
    "\n",
    "        ######\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # for name, parms in mlp.named_parameters():\n",
    "        #     print(\"=====After step()=====\")\n",
    "        #     print('-->name:', name)\n",
    "        #     print('-->params:', parms.data)\n",
    "        #     # print('-->grad_requirs:',parms.requires_grad)\n",
    "        #     print('-->grad_value:',parms.grad)\n",
    "        #     print(\"===\")\n",
    "\n",
    "        # Only draw the 1st result in a training batch (5 in total)\n",
    "        # if total_train_step % 20 == 0:\n",
    "        #     with torch.no_grad():\n",
    "        #         draw_mdn_2(pi[0,:].clone().detach(), mu[0,:].clone().detach(), sigma[0,:].clone().detach(), target[0].clone().detach(), input_data[0].clone().detach(),total_train_step, N_gaussians)\n",
    "        #         # draw_mdn_cdf(pi[0,:].detach(), m[0], target[0].detach(), total_train_step, loss_list[0],N_gaussians)\n",
    "        total_train_step += 1\n",
    "\n",
    "\n",
    "    ########### Do validation\n",
    "    with torch.no_grad():\n",
    "        mlp.eval()\n",
    "        total_vali_metric = 0\n",
    "        GT_metric = torch.tensor([0.,0.]).reshape(1,2)\n",
    "\n",
    "        for vali_batch_id, vali_data in enumerate(val_loader):\n",
    "            vali_input_data, vali_target, vali_setting , vali_metric = vali_data\n",
    "            vali_input_data = vali_input_data.to(device)\n",
    "            vali_target = vali_target.to(device)\n",
    "            # vali_metric = vali_metric\n",
    "\n",
    "            vali_pi, vali_mu, vali_sigma = mlp(vali_input_data)\n",
    "\n",
    "            # Compute the error/ metric\n",
    "            vali_nll = cal_metric(vali_pi, vali_mu, vali_sigma, vali_target, N_gaussians, vali_setting)\n",
    "            total_vali_metric += vali_nll\n",
    "\n",
    "            # Sum up NLL of all vali data\n",
    "            GT_metric += torch.sum(vali_metric,dim=0)\n",
    "        # total_vali_metric = total_vali_metric/len(val_idx)\n",
    "\n",
    "        # Get metric of GT model\n",
    "        GT_metric = GT_metric/len(val_idx)\n",
    "        total_vali_metric = total_vali_metric/len(val_idx)\n",
    "\n",
    "        draw_metric(epoch, total_vali_metric.cpu(), GT_metric)\n",
    "        mlp.train()\n",
    "\n",
    "    # Plot the loss in this EPOCH\n",
    "    print(f\"========== IN EPOCH {epoch} the total loss is {epoch_train_loss} ==========\")\n",
    "\n",
    "    # # Save the net structure\n",
    "    # net_file_name = \"net_after_epoch\"+str(epoch)+\".pth\"\n",
    "    # net_path = net_root_path + net_file_name\n",
    "    # torch.save(mlp.state_dict(), net_path)\n",
    "\n",
    "    # Plot loss of this EPOCH\n",
    "    draw_loss(epoch, epoch_train_loss,win_train_epoch_loss_str)\n",
    "    # writer.add_scalars(\"Loss of EPOCH\",{\"Train\":epoch_train_loss},epoch)\n",
    "\n",
    "    # Record the params\n",
    "    # for name,param in params:\n",
    "    #     #参数的梯度\n",
    "    #     #print(\"name:\",name)\n",
    "    #     #print(\"param.grad:\",param.grad)\n",
    "    #     # writer.add_histogram(tag = name +'_grad of EPOCH '+str(epoch),values=param.grad,global_step=epoch)\n",
    "    #     # #参数值\n",
    "    #     writer.add_histogram(tag = name +'data of EPOCH '+str(epoch),values=param.data,global_step=epoch)\n",
    "\n",
    "# f.close()\n",
    "train_end_time = time.time()\n",
    "print(f\"Total training time when epoch= *{EPOCH_NUM}* is *{train_end_time-train_start_time} *s\")\n",
    "writer.close()\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- save"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_path_MLE = \"AAAmlp_train=300_MLE.pth\"\n",
    "# model_path_WD = \"mlp_train=300_WD_epoch=80.pth\"\n",
    "model_path_WD = \"mlp_train=300_WD.pth\"\n",
    "model_path_LSE = \"mlp_train=300_LSE.pth\"\n",
    "torch.save(mlp.state_dict(), model_path_MLE)\n",
    "# torch.save(mlp.state_dict(), model_path_MLE)\n",
    "\n",
    "# viz.delete_env(\"001_test\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Gtmp = pgv.AGraph('tmp.dot')\n",
    "# G = nx.Graph(Gtmp)\n",
    "# nx.draw(G)\n",
    "# # plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "$\\mathcal{L}(y \\vert x) = - \\log\\bigg\\{\\sum_{k=1}^K \\pi_k(x)  \\mathcal{N}\\big(y \\vert \\mu_k(x), \\Sigma_k(x)\\big)\\bigg\\}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Load and test\n",
    "## 7.1 small batch\n",
    "- 对某个小batch画图，batch size可以=1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "small_batch_size = 1\n",
    "\n",
    "small_test_idx = train_idx[3:4]\n",
    "small_test_loader = DataLoader(dataset = dataset,batch_size = small_batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(small_test_idx),collate_fn = my_collate_fn)\n",
    "small_test_idx"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot\n",
    "test_env = \"002\"\n",
    "def draw_mdn_test(pi, m, target, input, epoch, idx,N_gaussians,flag):\n",
    "    # 只画一个batch中的第一个\n",
    "\n",
    "    # The target distrb.\n",
    "    target = torch.unique(target,dim=0)    # drop the duplicate\n",
    "    n_target = target[:,0]\n",
    "    non_zero_idx = torch.nonzero(n_target)\n",
    "    n = n_target[non_zero_idx]\n",
    "\n",
    "    p_target = target[:,1]\n",
    "    p = p_target[non_zero_idx]\n",
    "    max_n = max(n).item()               # 横轴长度\n",
    "\n",
    "    # The predicted distrb.\n",
    "    x_0 = torch.arange(1,max_n).to(device=device)\n",
    "    x = torch.repeat_interleave(x_0.unsqueeze(dim=1), repeats=N_gaussians, dim=1)\n",
    "    y = torch.exp(m.log_prob(x)).to(device=device)                          # y:多条高斯曲线; y_pred: 一条GMM曲线\n",
    "    # y_pred = torch.unsqueeze(torch.sum(pi*y,dim=1),dim=1)                 # 维度相等才能cat\n",
    "    y_pred = torch.sum(pi*y,dim=1)\n",
    "\n",
    "    # The input distrb.\n",
    "    input_data = input[0:2,:]\n",
    "    x_input = torch.arange(1,input_data.shape[1]+1).to(device=device)\n",
    "    y_input_0 = input_data[0,:].to(device=device)\n",
    "    y_input_1 = input_data[1,:].to(device=device)\n",
    "\n",
    "    # Init\n",
    "    win_str = flag+ \" epoch=\"+str(epoch)+\" | idx=\"+str(idx)\n",
    "    title_str = \"Distrb. \"+win_str\n",
    "    viz.line(X = [0.],Y = [0.], env=test_env, win=win_str, opts= dict(title=title_str))\n",
    "\n",
    "    # Visdom本身不能把hist和line画在一个window中\n",
    "    # 如果想画一起只能是两条line\n",
    "    # Plot y_target\n",
    "    # viz.histogram(X = n, env=\"001\", win=win_str,\n",
    "    #         opts= dict(title=title_str))\n",
    "    viz.line(X = n,Y= p, env=test_env, win=win_str, update=\"append\", name='target',\n",
    "            opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "    # Plot y_pred\n",
    "    viz.line(X = x_0,Y= y_pred, env=test_env, win=win_str, update=\"append\", name='pred',\n",
    "            opts= dict(title=title_str))\n",
    "\n",
    "    # Plot y_input\n",
    "    viz.line(X = x_input,Y= y_input_0, env=test_env, win=win_str, update=\"append\", name='GT-1', opts= dict(title=title_str))\n",
    "    viz.line(X = x_input,Y= y_input_1, env=test_env, win=win_str, update=\"append\", name='GT-2', opts= dict(title=title_str))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(EPOCH_NUM):\n",
    "\n",
    "    net_file_name_0 = \"net_before_epoch\"+str(epoch)+\".pth\"\n",
    "    net_file_name_1 = \"net_after_epoch\"+str(epoch)+\".pth\"\n",
    "    net_path0 = net_root_path + net_file_name_0\n",
    "    net_path1 = net_root_path + net_file_name_1\n",
    "\n",
    "    # Load the model\n",
    "    mlp0 = MLP(N_gaussians)\n",
    "    mlp1 = MLP(N_gaussians)\n",
    "    mlp0.load_state_dict(torch.load(net_path0))\n",
    "    mlp1.load_state_dict(torch.load(net_path1))\n",
    "    mlp0.to(device)\n",
    "    mlp1.to(device)\n",
    "\n",
    "    # 其实每个epoch只会执行一个batch\n",
    "    for batch_id,data in enumerate(small_test_loader):\n",
    "\n",
    "        # Predict\n",
    "        input_data, target = data\n",
    "        input_data = input_data.to(device)\n",
    "        target = target.to(device)\n",
    "        pi0, mu0, sigma0 = mlp0(input_data)\n",
    "        pi1, mu1, sigma1 = mlp1(input_data)\n",
    "        # Get the mdn\n",
    "        _,m0  = loss_preparation(pi0.detach(), mu0.detach(), sigma0.detach(), target.detach())\n",
    "        _,m1  = loss_preparation(pi1.detach(), mu1.detach(), sigma1.detach(), target.detach())\n",
    "\n",
    "        # Plot for 'small_batch_size' times\n",
    "        for j in range(small_batch_size):\n",
    "            draw_mdn_test(pi0[j,:].detach(), m0[j], target[j].detach(), input_data[j].detach(), epoch, j,N_gaussians,flag=\"before\")\n",
    "            draw_mdn_test(pi1[j,:].detach(), m1[j], target[j].detach(), input_data[j].detach(), epoch, j,N_gaussians,flag=\"after\")\n",
    "\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7.2 Test dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "# model_path_WD_2 = \"mlp_train=300_WD_epoch=80.pth\"\n",
    "model_path_WD = \"mlp_train=300_WD.pth\"\n",
    "model_path_MLE = \"mlp_train=300_MLE.pth\"\n",
    "\n",
    "\n",
    "mlp_1 = MLP_1_3(N_gaussians)\n",
    "mlp_2 = MLP_1_3(N_gaussians)\n",
    "\n",
    "model_data_1 = torch.load(model_path_MLE)\n",
    "# model_data_1 = torch.load(model_path_WD_2)\n",
    "model_data_2 = torch.load(model_path_WD)\n",
    "\n",
    "mlp_1.load_state_dict(model_data_1)\n",
    "mlp_2.load_state_dict(model_data_2)\n",
    "\n",
    "mlp_1.to(device)\n",
    "mlp_2.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total_test_loss_1 = 0\n",
    "total_test_loss_2 = 0\n",
    "i = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_id,data in enumerate(test_loader):\n",
    "            input_data, target = data\n",
    "            input_data = input_data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            pi_1, mu_1, sigma_1 = mlp_1(input_data)\n",
    "            pi_2, mu_2, sigma_2 = mlp_2(input_data)\n",
    "\n",
    "            draw_mdn_3(pi_1, mu_1, sigma_1, pi_2, mu_2, sigma_2, target, input_data,i, N_gaussians)\n",
    "\n",
    "            loss_1 = loss_fn_v2(pi_1, mu_1, sigma_1, target, N_gaussians)\n",
    "            loss_2 = loss_fn_v2(pi_2, mu_2, sigma_2, target, N_gaussians)\n",
    "\n",
    "            total_test_loss_1 += loss_1.item()\n",
    "            total_test_loss_2 += loss_2.item()\n",
    "\n",
    "            i += 1\n",
    "    # 这里print的loss可以视作平均到每个sample上的loss\n",
    "    print(\"total_test_loss_1 MLE:\",total_test_loss_1/len(test_loader))\n",
    "    print(\"total_test_loss_2 WD :\",total_test_loss_2/len(test_loader))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
