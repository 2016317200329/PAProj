{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2022/6/7 15:17\n",
    "# @Author  : Wang Yujia\n",
    "# @File    : mlp.ipynb\n",
    "# @Description :\n",
    "# @TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0. what for\n",
    "1. loss部分逻辑重写，添加`no grad`\n",
    "2. 添加hooks\n",
    "3. 使用pad_seq函数取代自己写的pad函数\n",
    "4. 效果：可以train虽然loss不下降，不会出现NaN的问题"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Preparations\n",
    "## 1.1 Import"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import torch.utils.data\n",
    "from mydataset import *\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from tensorboardX import SummaryWriter\n",
    "from visdom import Visdom\n",
    "from graphviz import Digraph\n",
    "import torchvision\n",
    "from torchviz import make_dot\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pygraphviz as pgv\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import geomloss\n",
    "import time\n",
    "from importlib import reload\n",
    "from functools import partial\n",
    "import logging\n",
    "import pytorch_warmup as warmup\n",
    "logging.basicConfig(filename=\"loss_info\",filemode=\"w\",level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import scipy\n",
    "from torch.nn import KLDivLoss\n",
    "from torch.autograd.gradcheck import gradcheck\n",
    "from config import bcolors\n",
    "\n",
    "import config\n",
    "import loss\n",
    "import plot"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Global settings and reloading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "reload(config)  # 必须reload！！\n",
    "reload(loss)    # 必须reload！！\n",
    "reload(plot)\n",
    "from config import DefaultConfig\n",
    "from loss import cal_metric\n",
    "from loss import loss_fn_v2\n",
    "from loss import loss_fn_wei\n",
    "from loss import loss_fn_WD\n",
    "from loss import loss_fn_CE\n",
    "from loss import validate\n",
    "from plot import plot_conv_weight\n",
    "from plot import plot_mu_weight\n",
    "from plot import plot_pi_weight\n",
    "from plot import plot_sigma_weight\n",
    "from plot import plot_net\n",
    "\n",
    "opt = DefaultConfig()\n",
    "\n",
    "total_train_step = 0\n",
    "total_test_step = 0\n",
    "\n",
    "# Range【一般用不到】\n",
    "a = 0\n",
    "b = 255\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Dataset and Dataloader\n",
    "1. DataLoader中的`shuffer=True`表示在每一次epoch中都打乱所有数据的顺序，然后以batch为单位从头到尾按顺序取用数据。这样的结果就是不同epoch中的数据都是乱序的,设置随机种子的作用就是让你的每一次训练都乱的一样，\n",
    "\n",
    "## 2.1 Dataset and spliting\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 设置随机数种子"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup_seed(7)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 读取data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "dataset = myDataset(opt.train_path, opt.target_path_metric, opt.target_path_loss, opt.data_key_path, opt.NLL_metric_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 产生index的乱序排列"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 807  305  455  939  508  594  835 1082  598 1102   46  761  841  141\n",
      "  407  334  253  500  734  936  698  446  907 1087 1009  140  463  547\n",
      " 1155  856   34 1156  703 1121  751  587 1100  509  473  128  788   97\n",
      "  471  385 1085  525  679 1135  284 1146  186  318 1088 1113    9  457\n",
      " 1047  451  569 1120 1074  326  377  809  109   98  620 1194  825  828\n",
      "   83  113  556  674  568  853  351  558   54  656  804 1149  101  344\n",
      "  851  544  955   40 1021  489  626  664  657  868 1169  151  179 1130\n",
      "  564  713  743 1014  966  861 1132  146  610  408  662  551  172 1020\n",
      "  982 1148  431  517  270  858  170  374  816  618  205   17   53 1003\n",
      "  263  857  716  843  498  228  339  725  752  278  649 1017  108  642\n",
      " 1195 1174   99  530  632  888  189  961  358 1078  663  757 1051  204\n",
      "  409  283  562   23  619  216  474  921  950 1162  123  785  769  621\n",
      "  262  586 1178  541  795   70 1189  396  171  845  168 1125  361  224\n",
      "  125  706  164  231  264   42  746  872  998  132 1035  430   38  522\n",
      "  880  325  285  367  870  418   13  166  513  901  617  740  680 1126\n",
      "   20  372   71 1038  355   60  220  110    4 1115  572  415  336  316\n",
      "  449  412 1066  768 1055  704  869   94   95 1163 1145  648  884  885\n",
      " 1170  848   58  207  306 1067  729   24  644  633  252  832  244   25\n",
      " 1094   63  222 1070  996 1063  130  153 1180  922  399  997   75  894\n",
      " 1048  308  280  934  721  199  820  134  289  259 1159    3  510  492\n",
      "  196  565  563   26  913  483 1141  653   61  623  984  652  609  739\n",
      "  824 1018  891  467  842    8  503  714 1015  447  783  992   74 1184\n",
      "   57  978  484  297 1039  217  681 1027  960  350 1129 1045 1183  906\n",
      "  813  660  266    6  379  526  611  138  163  701 1031 1050   43  733\n",
      "  720  319 1158 1153  877  294  432  531  578  523   19  426  830  927\n",
      "  781 1033 1013  420  137  488  185  920  702 1134 1124  454  327  317\n",
      "  799  655  878  699  981  806  953  750  148  591  915  731  937  365\n",
      "  557  397 1173  779 1185  459  232  389 1147 1011  158  315  256  863\n",
      "   96 1157  445  384  511  817  328  314  357  347  452  298  899  887\n",
      " 1034  839  778  747  249  766   15  159  696  585  360   48  478  330\n",
      "  893  112  338  307  528  422 1142  971  506  504 1049  791  395  669\n",
      "  540  127 1160  902  507  980  237  167  602  983   80  414  640  300\n",
      "  977  324 1117  215 1058   30  242  697  860  951  209  671  742 1072\n",
      "  435   27 1060  518  122 1086  462 1028 1118  603  929  732  722  493\n",
      "  202  403 1089 1179  362  271  466  641 1061  744  272  277 1177   12\n",
      "  597  668  210  651  952  147  442  299  115  375  635  767  748 1080\n",
      "  755  177  472  685   51  810 1079  571  388   22  650  965  423  938\n",
      " 1112  643 1193  142   50  666 1143   81  482  321  948  687 1138   66\n",
      "  909  534  738  417 1005   64  895  886  918  491  700  790   65  968\n",
      "  627  999  296  419  438  261  837  281  631  677  188  881  251  694\n",
      "  514  590  487  273  943 1073  448  411 1131  933 1191  935  149  218\n",
      "  239  718  192  191  945  754  665  705  595  926  708  582 1046  323\n",
      " 1026 1071  658  437  770  833  341    2  229 1098 1152   76 1103  371\n",
      "  180  889  577  736  758 1019 1105 1010  589 1167  812  956  882   37\n",
      "  353 1095  236  928  499  553   55  302  286  800  219  789  387  737\n",
      "  390  352  193  235 1110  480  567  116 1057  957 1106  890  777  120\n",
      "  815  990  773   62  667  453  515  570  223  613  221  673  794  465\n",
      "   52 1093  485 1140 1053  155  912  600 1023  248  441  975 1164  542\n",
      "  117  724  628  712  916  291 1041  381  954  200  596  976 1137 1187\n",
      "  691  505  348 1190  340  615  154 1104  879  995  287  322  819  818\n",
      "  796   82  793  464  797  756 1168  771  333   10  470   32  782  413\n",
      "  580  382  930  760 1133  840 1139  378   89  792  601  213  332  133\n",
      "  346  855    7  941  320 1044  187  972  552  304   31  831  684 1065\n",
      "  240  993  174  245   56  630  583 1025  145   90 1114  392  682  310\n",
      "  359   93  947 1024  376  243 1076   87  214  444  173  425 1040  118\n",
      "  119 1151  905    0  394  401  404  836 1091  162  114  670  150  798\n",
      "  875 1064  368  543  477 1128  532  625  967  512  801   72  258  624\n",
      "  234  135  616  516  866  802  592 1056  501  538  897  728  581  343\n",
      "  925 1002  293  364  883  659   14  686   86  370  165  131  335   59\n",
      "  709  676  354  959  424  292   41  329  614  429   79]\n"
     ]
    }
   ],
   "source": [
    "shuffled_indices = []\n",
    "DATA_len = 0\n",
    "# 使用全部的data\n",
    "if not opt.arr_flag:\n",
    "    DATA_len = dataset.__len__()\n",
    "    shuffled_indices = np.random.permutation(DATA_len)\n",
    "\n",
    "# 使用指定的data\n",
    "if opt.arr_flag:\n",
    "    shuffled_indices = np.load(opt.arr_path)\n",
    "    DATA_len = len(shuffled_indices)\n",
    "    # np.random.shuffle(shuffled_indices)\n",
    "\n",
    "train_idx = shuffled_indices[:int(opt.train_pct*DATA_len)]\n",
    "# train_idx = shuffled_indices\n",
    "tmp = int((opt.train_pct+opt.vali_pct)*DATA_len)\n",
    "val_idx = shuffled_indices[int(opt.train_pct*DATA_len):tmp]\n",
    "\n",
    "test_idx = shuffled_indices[tmp:]\n",
    "print(train_idx)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 根据这个乱序排列抽取dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# 保存idx\n",
    "np.save('shuffled_indices',shuffled_indices)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Dataloader and collating\n",
    "1. 主要是对label数据进行collate\n",
    "    - 按照batch中的最大target data长度进行padding，padding with 0\n",
    "2. 返回的结果多一个batch dim,比如下面的`5`\n",
    "    - After collating:\n",
    "        - `torch.Size([5, 3, 300]),torch.Size([5, 87, 2])`\n",
    "        - `87`是最长的targets data长度"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_pad_sequence():\n",
    "    seq1 = torch.tensor([[ 2., 0.04761905], [3., 0.14285714], [4., 0.04761905]])\n",
    "    seq2 = torch.tensor([[ 1., 0.04761905]])\n",
    "    seq3 = torch.tensor([[ 3., 0.14285714], [4., 0.04761905]])\n",
    "    ls = list((seq1,seq2,seq3))\n",
    "    ls_length = torch.tensor([3,1,2])\n",
    "    ans = pad_sequence(ls,batch_first=True)\n",
    "    print(ans)\n",
    "    # seq_pak = pack_padded_sequence(ans,ls_length,batch_first=True,enforce_sorted=False)\n",
    "    # seq_unpacked, lens_unpacked = pad_packed_sequence(seq_pak, batch_first=True)\n",
    "    # lens_unpacked"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.1 No rescale"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def my_collate_fn(data):\n",
    "# 这里的data是一个list， list的元素是元组: (self.data, self.label)\n",
    "# collate_fn的作用是把[(data, label),(data, label)...]转化成([data, data...],[label,label...])\n",
    "# 假设self.data的一个data的shape为(channels, length), 每一个channel的length相等,\n",
    "# data[索引到index(batch)][索引到data或者label][索引到channel]\n",
    "\n",
    "    data_list = []              # training data\n",
    "    target_metric_list = []     # target data for computing metric of NN, (TARGET=1)\n",
    "    target_loss_list = []       # target data for computing loss of NN, (TARGET=5 or 1)\n",
    "    setting_list = []\n",
    "    metric_list = []\n",
    "\n",
    "    data_len = len(data)        # 读进来的data batch的大小\n",
    "\n",
    "    batch = 0\n",
    "\n",
    "    while batch < data_len:\n",
    "        # print(\"shape:\",data[batch][0].shape) #shape: (3, 300)\n",
    "\n",
    "        # 所有GT model\n",
    "        data_list.append(torch.tensor(data[batch][0]))\n",
    "\n",
    "        target_metric_list.append(torch.tensor(data[batch][1]))\n",
    "        target_loss_list.append(torch.tensor(data[batch][2]))\n",
    "        setting_list.append(torch.tensor(data[batch][3]))\n",
    "        metric_list.append(torch.tensor(data[batch][4]))\n",
    "        batch += 1\n",
    "\n",
    "    # Pad target data with zeros\n",
    "    target_metric_padded = pad_sequence(target_metric_list,batch_first=True)\n",
    "    target_loss_padded = pad_sequence(target_loss_list,batch_first=True)\n",
    "    target_metric_tensor = target_metric_padded.float()\n",
    "    target_loss_padded = target_loss_padded.float()\n",
    "\n",
    "    data_tensor = torch.stack(data_list).float()\n",
    "    setting_tensor = torch.stack(setting_list).float()\n",
    "    metric_tensor = torch.stack(metric_list).float()\n",
    "\n",
    "    return data_tensor, target_metric_tensor, target_loss_padded, setting_tensor, metric_tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.2 Rescale\n",
    "1. padding 太多0不利于学习，因此把padding的0替换一下：替换成`min(min(data[batch][0][0]),opt.SAFETY)`\n",
    "    - 替换`min(data[batch][0][0])`/ 一列中最小的prob，但是如果这个值太大，就替换成`opt.SAFETY`\n",
    "2. 效果: 不全是正向的，效果待定"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def my_collate_fn_2(data):\n",
    "# 这里的data是一个list， list的元素是元组: (self.data, self.label)\n",
    "# collate_fn的作用是把[(data, label),(data, label)...]转化成([data, data...],[label,label...])\n",
    "# 假设self.data的一个data的shape为(channels, length), 每一个channel的length相等,\n",
    "# data[索引到index(batch)][索引到data或者label][索引到channel]\n",
    "\n",
    "    data_list = []              # training data\n",
    "    target_metric_list = []     # target data for computing metric of NN, (TARGET=1)\n",
    "    target_loss_list = []       # target data for computing loss of NN, (TARGET=5)\n",
    "    setting_list = []\n",
    "    metric_list = []\n",
    "\n",
    "    data_len = len(data)        # 读进来的data batch的大小\n",
    "\n",
    "    batch = 0\n",
    "\n",
    "    while batch < data_len:\n",
    "        # print(\"shape:\",data[batch][0].shape) #shape: (3, 300)\n",
    "\n",
    "        # 对于所有用0填充的data来说，最好用一个数字补齐这些0\n",
    "        # 补齐方法1:\n",
    "        data[batch][0][0,np.where(data[batch][0][0]==0)]= min(min(data[batch][0][0]),opt.SAFETY)\n",
    "        data[batch][0][1,np.where(data[batch][0][1]==0)]= min(min(data[batch][0][1]),opt.SAFETY)\n",
    "\n",
    "        data_list.append(torch.tensor(data[batch][0]))\n",
    "        target_metric_list.append(torch.tensor(data[batch][1]))\n",
    "        target_loss_list.append(torch.tensor(data[batch][2]))\n",
    "        setting_list.append(torch.tensor(data[batch][3]))\n",
    "        metric_list.append(torch.tensor(data[batch][4]))\n",
    "        batch += 1\n",
    "\n",
    "    # Pad target data with zeros\n",
    "    target_metric_padded = pad_sequence(target_metric_list,batch_first=True)\n",
    "    target_loss_padded = pad_sequence(target_loss_list,batch_first=True)\n",
    "    target_metric_tensor = target_metric_padded.float()\n",
    "    target_loss_padded = target_loss_padded.float()\n",
    "\n",
    "    data_tensor = torch.stack(data_list).float()\n",
    "    setting_tensor = torch.stack(setting_list).float()\n",
    "    metric_tensor = torch.stack(metric_list).float()\n",
    "\n",
    "    return data_tensor, target_metric_tensor, target_loss_padded, setting_tensor, metric_tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.3 Rescale-2\n",
    "1. padding 太多0不利于学习，"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "outputs": [],
   "source": [
    "def my_collate_fn_3(data):\n",
    "# 这里的data是一个list， list的元素是元组: (self.data, self.label)\n",
    "# collate_fn的作用是把[(data, label),(data, label)...]转化成([data, data...],[label,label...])\n",
    "# 假设self.data的一个data的shape为(channels, length), 每一个channel的length相等,\n",
    "# data[索引到index(batch)][索引到data或者label][索引到channel]\n",
    "\n",
    "    data_list = []              # training data\n",
    "    target_metric_list = []     # target data for computing metric of NN, (TARGET=1)\n",
    "    target_loss_list = []       # target data for computing loss of NN, (TARGET=5)\n",
    "    setting_list = []\n",
    "    metric_list = []\n",
    "\n",
    "    data_len = len(data)        # 读进来的data batch的大小\n",
    "\n",
    "    batch = 0\n",
    "    # min_pad_0 = np.min(data[:][0][0])\n",
    "    # min_pad_1 = np.min(data[:][0][1])\n",
    "    min_pad_0 = np.inf\n",
    "    min_pad_1 = np.inf\n",
    "\n",
    "    while batch < data_len:\n",
    "        # print(\"shape:\",data[batch][0].shape) #shape: (3, 300)\n",
    "        # # 补齐方法2: batch中的最小值\n",
    "        # min_pad_0 = min(min_pad_0,min(data[batch][0][0]))\n",
    "        # min_pad_1 = min(min_pad_1,min(data[batch][0][0]))\n",
    "        # assert np.min(data[batch][0][0])>=0,\"<0!\"\n",
    "        # assert np.min(data[batch][0][1])>=0,\"<0!!\"\n",
    "\n",
    "        data_list.append(torch.tensor(data[batch][0]))\n",
    "        target_metric_list.append(torch.tensor(data[batch][1]))\n",
    "        target_loss_list.append(torch.tensor(data[batch][2]))\n",
    "        setting_list.append(torch.tensor(data[batch][3]))\n",
    "        metric_list.append(torch.tensor(data[batch][4]))\n",
    "        batch += 1\n",
    "\n",
    "    # Pad target data with zeros\n",
    "    target_metric_padded = pad_sequence(target_metric_list,batch_first=True)\n",
    "    target_loss_padded = pad_sequence(target_loss_list,batch_first=True)\n",
    "    target_metric_tensor = target_metric_padded.float()\n",
    "    target_loss_padded = target_loss_padded.float()\n",
    "\n",
    "    data_tensor = torch.stack(data_list).float()\n",
    "\n",
    "    data_tensor[data_tensor == 0] = torch.min(data_tensor)\n",
    "    setting_tensor = torch.stack(setting_list).float()\n",
    "    metric_tensor = torch.stack(metric_list).float()\n",
    "\n",
    "    return data_tensor, target_metric_tensor, target_loss_padded, setting_tensor, metric_tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.4 DataLoader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = dataset,batch_size = opt.batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(train_idx), collate_fn = my_collate_fn_2)\n",
    "\n",
    "val_loader = DataLoader(dataset = dataset,batch_size = opt.batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(val_idx),collate_fn = my_collate_fn_2)\n",
    "\n",
    "# 注意test_loader的batch size\n",
    "test_loader = DataLoader(dataset = dataset,batch_size = opt.batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(test_idx),collate_fn = my_collate_fn_2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. The Net and Init\n",
    "1. BatchNorm1d: The mean and std are calculated per-dimension over the mini-batches\n",
    "2."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### BatchNorm2d测试\n",
    "def test_BN():\n",
    "    m = nn.BatchNorm2d(3, affine=False)  # affine: With Learnable Parameters or not\n",
    "    print('m:', m)\n",
    "    # The mean and std are calculated per-dimension over the mini-batches\n",
    "    input = torch.tensor([\n",
    "        [[1.,2.,3.,4.],[1.,2.,3.,4.],[-1.,-2.,-3.,-4.]],\n",
    "        [[0.,0.,0.,0.],[0.,0.,0.,0.],[0.,0.,0.,0.]]\n",
    "    ], requires_grad=True)\n",
    "\n",
    "    print('input:', input.shape)\n",
    "    input = input.unsqueeze(dim=2)\n",
    "    print('input:', input.shape)\n",
    "    output = m(input) # 归一化\n",
    "    print('output:', output.shape)\n",
    "    print('output:', output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### 复现batchNorm2d在input shape为3维的情况\n",
    "input = torch.tensor([[[1.,2.,3.,4.]],[[0.,0.,0.,0.]]])\n",
    "# print(input.shape)\n",
    "# torch.mean(input),torch.var(input,unbiased = False)\n",
    "(input-torch.mean(input))/ torch.sqrt(torch.var(input,unbiased = False))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 Init, Printer, Hook\n",
    "- 设置网络初始权重: 不太work"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [],
   "source": [
    "\n",
    "class model_param_init(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        assert isinstance(model, nn.Module), 'model not a class nn.Module'\n",
    "        self.net = model\n",
    "        self.initParam()\n",
    "\n",
    "    def forward(self):\n",
    "        for param in self.net.parameters():\n",
    "            # nn.init.zeros_(param)\n",
    "            # nn.init.ones_(param)\n",
    "            # nn.init.normal_(param, mean=0, std=1)\n",
    "            # nn.init.uniform_(param, a=0, b=1)\n",
    "            # nn.init.constant_(param, val=1)   # 将所有权重初始化为1\n",
    "            # nn.init.eye_(param)  # 只能将二维的tensor初始化为单位矩阵\n",
    "            # nn.init.xavier_uniform_(param, gain=1)  # Glorot初始化  得到的张量是从-a——a中采用的\n",
    "            # print(param)  x.unsqueeze(0)\n",
    "            nn.init.xavier_normal_(param.unsqueeze(0), gain=1)   # 得到的张量是从0-std采样的\n",
    "            # nn.init.kaiming_normal_(param, a=0, mode='fan_in', nonlinearity='relu') # he初始化方法\n",
    "            # nn.init.kaiming_uniform_(param)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- print网络每层结构"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "class PrintLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if(DEBUG):\n",
    "            print(\"This layer: \")\n",
    "            print(x)      #print(x.shape)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- hook_backward_fn: 输入端的grad，输出端的grad，这里称呼的**输入与输出是站在前向传播的角度的**。如果模块有多个输入与输出的话， 其`grad_input`和`grad_output`可以是tuple类型。\n",
    "- 与forward不同的是，backward传播的时候，**不仅反向传递input和output的grad，还会传递模块Parameter的grad**：\n",
    "    - 比如fc模块，其`grad_input`是一个三元组的tuple，（对bias的梯度，对输入的梯度，对w的梯度）；\n",
    "    - conv模块`grad_input`也是一个三元组tuple，为（对输入的梯度，对w的梯度，对bias的梯度"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# hook functions have to take these 3 input\n",
    "def hook_forward_fn(module, input, output):\n",
    "    print(\"It's forward: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"input: {input}\")\n",
    "    print(f\"output: {output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn(module, grad_input, grad_output):\n",
    "    print(\"It's backward: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn_pi(module, grad_input, grad_output):\n",
    "    print(\"It's backward in pi: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn_sigma(module, grad_input, grad_output):\n",
    "    print(\"It's backward in sigma: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn_mu(module, grad_input, grad_output):\n",
    "    print(\"It's backward in mu: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Conv结构-1\n",
    "### 3.2.1 1层conv: 1=>1\n",
    "1.  Conv=>BN=>AC"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# 1569\n",
    "class Conv_block_1(nn.Module):\n",
    "    def __init__(self, ch_out=1,kernel_size=9, stride=3) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.kernel_size = (3,kernel_size)\n",
    "        self.stride = (3,stride)\n",
    "        self.ln_in = int((300-self.kernel_size[1])/self.stride[1]+1)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=ch_out, kernel_size=self.kernel_size, stride=self.stride, padding=0,bias=True)\n",
    "        self.BN_aff1 = nn.BatchNorm1d(num_features=1,affine=True)\n",
    "        self.BN_aff2 = nn.BatchNorm1d(num_features=self.ln_in,affine=True)      # works better\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Conv=>BN=>AC\n",
    "        x = self.conv(x)\n",
    "        # 方法一：\n",
    "        # x = torch.squeeze(x,dim=2)\n",
    "        # x = self.ac_func(self.BN_aff1(x))\n",
    "\n",
    "        # 方法二：works better\n",
    "        x = torch.squeeze(x)\n",
    "        x = self.ac_func(self.BN_aff2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "class Conv_1_1(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians, ch_out=1, kernel_size=9, stride=3) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.kernel_size = (3,kernel_size)\n",
    "        self.stride = (3,stride)\n",
    "        self.ln_in = int((300-self.kernel_size[1])/self.stride[1]+1)\n",
    "\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=3,affine=True)\n",
    "\n",
    "        self.layer_pi = Conv_block_1(ch_out=1,kernel_size=kernel_size,stride=stride)\n",
    "        self.layer_mu = Conv_block_1(ch_out=1,kernel_size=kernel_size,stride=stride)\n",
    "        self.layer_sigma = Conv_block_1(ch_out=1,kernel_size=kernel_size,stride=stride)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(self.ln_in, n_gaussians),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(self.ln_in, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(self.ln_in, n_gaussians)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.BN1(x)\n",
    "        x = torch.unsqueeze(x,dim=1)                     # torch.Size([B, 1, 3, 300])\n",
    "\n",
    "        x_pi = torch.squeeze(self.layer_pi(x))          # 不加squeeze不行\n",
    "        x_mu = torch.squeeze(self.layer_mu(x))\n",
    "        x_sigma = torch.squeeze(self.layer_sigma(x))\n",
    "\n",
    "        pi = self.z_pi(x_pi)\n",
    "        mu = self.z_mu(x_mu)\n",
    "        sigma = self.z_sigma(x_sigma)\n",
    "\n",
    "        sigma = torch.exp(sigma)\n",
    "        sigma = torch.clamp(sigma,1e-4)\n",
    "\n",
    "        # return x1_3\n",
    "        return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "       BatchNorm1d-1               [-1, 3, 300]               6\n",
      "            Conv2d-2             [-1, 1, 1, 98]              28\n",
      "       BatchNorm1d-3                   [-1, 98]             196\n",
      "          Softplus-4                   [-1, 98]               0\n",
      "      Conv_block_1-5                   [-1, 98]               0\n",
      "            Conv2d-6             [-1, 1, 1, 98]              28\n",
      "       BatchNorm1d-7                   [-1, 98]             196\n",
      "          Softplus-8                   [-1, 98]               0\n",
      "      Conv_block_1-9                   [-1, 98]               0\n",
      "           Conv2d-10             [-1, 1, 1, 98]              28\n",
      "      BatchNorm1d-11                   [-1, 98]             196\n",
      "         Softplus-12                   [-1, 98]               0\n",
      "     Conv_block_1-13                   [-1, 98]               0\n",
      "           Linear-14                    [-1, 2]             198\n",
      "          Softmax-15                    [-1, 2]               0\n",
      "           Linear-16                    [-1, 2]             198\n",
      "           Linear-17                    [-1, 2]             198\n",
      "================================================================\n",
      "Total params: 1,272\n",
      "Trainable params: 1,272\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.02\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mlp = Conv_1_1(opt.N_gaussians)\n",
    "mlp = mlp.to(device=device)\n",
    "summary(mlp, (3,300))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2.2 2层conv: 1=>1=>1\n",
    "1. 比上面多了一个conv1d的提取\n",
    "2. 做BN有2个思路：\n",
    "    - 第一个conv结束，有91个features，BN之后接另一个Conv提取这些不同features里的信息，目的是最后得到mu等的特征\n",
    "    - 第一个conv结束，有1个features，BN之后接另一个Conv提取/压缩这1个features里的信息，目的是最后得到mu等的特征"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1293,
   "outputs": [],
   "source": [
    "# 1569\n",
    "class Conv_block_2(nn.Module):\n",
    "    def __init__(self, ch_out=1,kernel_size=12, stride=3) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.kernel_size1 = (3,kernel_size)\n",
    "        self.stride1 = (3,stride)\n",
    "        self.ln_in = int((300-self.kernel_size1[1])/self.stride1[1]+1)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=self.kernel_size1, stride=self.stride1, padding=0,bias=True)\n",
    "        # self.BN_aff1 = nn.BatchNorm1d(num_features=1,affine=True)\n",
    "        self.BN_aff1 = nn.BatchNorm1d(num_features=self.ln_in,affine=True)      # works better\n",
    "\n",
    "        self.kernel_size2 = int(kernel_size/2)     # 可以调这个\n",
    "        self.stride2 = stride\n",
    "        self.ln_in2 = int((self.ln_in-self.kernel_size2)/self.stride2+1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=1, out_channels=ch_out, kernel_size=self.kernel_size2, stride=self.stride2, padding=0,bias=True)\n",
    "        self.BN_aff2 = nn.BatchNorm1d(num_features=self.ln_in2,affine=True)      # works better\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Conv=>BN=>AC\n",
    "        x = self.conv1(x)\n",
    "        # 方法二：works better\n",
    "        x = torch.squeeze(x)\n",
    "        x = self.ac_func(self.BN_aff1(x))\n",
    "\n",
    "        x = torch.unsqueeze(x,dim=1)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = torch.squeeze(x)\n",
    "        x = self.ac_func(self.BN_aff2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "class Conv_1_2(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians, ch_out=1, kernel_size=12, stride=3) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.kernel_size = (3,kernel_size)\n",
    "        self.stride = (3,stride)\n",
    "        # self.ln_in = int((300-self.kernel_size[1])/self.stride[1]+1)\n",
    "        self.ln_in = 31\n",
    "\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=3,affine=True)\n",
    "\n",
    "        self.layer_pi = Conv_block_2(ch_out=1,kernel_size=kernel_size,stride=stride)\n",
    "        self.layer_mu = Conv_block_2(ch_out=1,kernel_size=kernel_size,stride=stride)\n",
    "        self.layer_sigma = Conv_block_2(ch_out=1,kernel_size=kernel_size,stride=stride)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(self.ln_in, n_gaussians),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(self.ln_in, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(self.ln_in, n_gaussians)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.BN1(x)\n",
    "        x = torch.unsqueeze(x,dim=1)                     # torch.Size([B, 1, 3, 300])\n",
    "\n",
    "        x_pi = torch.squeeze(self.layer_pi(x))          # 不加squeeze不行\n",
    "        x_mu = torch.squeeze(self.layer_pi(x))\n",
    "        x_sigma = torch.squeeze(self.layer_pi(x))\n",
    "\n",
    "        pi = self.z_pi(x_pi)\n",
    "\n",
    "        # mu = self.z_mu(x_mu)\n",
    "        # mu = self.ac_func(self.z_mu(x_mu))\n",
    "        mu = self.z_mu(x_mu)\n",
    "        sigma = self.z_sigma(x_sigma)\n",
    "\n",
    "        sigma = torch.exp(sigma)\n",
    "        sigma = torch.clamp(sigma,1e-4)\n",
    "\n",
    "        # return x_pi\n",
    "        return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1294,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "       BatchNorm1d-1               [-1, 3, 300]               6\n",
      "            Conv2d-2             [-1, 1, 1, 97]              37\n",
      "       BatchNorm1d-3                   [-1, 97]             194\n",
      "          Softplus-4                   [-1, 97]               0\n",
      "            Conv1d-5                [-1, 1, 31]               7\n",
      "       BatchNorm1d-6                   [-1, 31]              62\n",
      "          Softplus-7                   [-1, 31]               0\n",
      "      Conv_block_2-8                   [-1, 31]               0\n",
      "            Conv2d-9             [-1, 1, 1, 97]              37\n",
      "      BatchNorm1d-10                   [-1, 97]             194\n",
      "         Softplus-11                   [-1, 97]               0\n",
      "           Conv1d-12                [-1, 1, 31]               7\n",
      "      BatchNorm1d-13                   [-1, 31]              62\n",
      "         Softplus-14                   [-1, 31]               0\n",
      "     Conv_block_2-15                   [-1, 31]               0\n",
      "           Conv2d-16             [-1, 1, 1, 97]              37\n",
      "      BatchNorm1d-17                   [-1, 97]             194\n",
      "         Softplus-18                   [-1, 97]               0\n",
      "           Conv1d-19                [-1, 1, 31]               7\n",
      "      BatchNorm1d-20                   [-1, 31]              62\n",
      "         Softplus-21                   [-1, 31]               0\n",
      "     Conv_block_2-22                   [-1, 31]               0\n",
      "           Linear-23                    [-1, 3]              96\n",
      "          Softmax-24                    [-1, 3]               0\n",
      "           Linear-25                    [-1, 3]              96\n",
      "           Linear-26                    [-1, 3]              96\n",
      "================================================================\n",
      "Total params: 1,194\n",
      "Trainable params: 1,194\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.02\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mlp = Conv_1_2(opt.N_gaussians)\n",
    "mlp = mlp.to(device=device)\n",
    "summary(mlp, (3,300))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2.3 2层conv: 1=>2=>1\n",
    "1. Conv2d+squeeze+conv1d"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1273,
   "outputs": [],
   "source": [
    "# 573\n",
    "class Conv_block_3(nn.Module):\n",
    "    def __init__(self, ch_out=2,kernel_size=12, stride=3) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.kernel_size1 = (3,kernel_size)\n",
    "        self.stride1 = (3,stride)\n",
    "        self.ln_in = int((300-self.kernel_size1[1])/self.stride1[1]+1)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "        # [B, 1, 3, 300] => [B, 2, 1, ln_in]\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=ch_out, kernel_size=self.kernel_size1, stride=self.stride1, padding=0,bias=True)\n",
    "        self.BN_aff1 = nn.BatchNorm1d(num_features=ch_out,affine=True)\n",
    "\n",
    "\n",
    "        self.kernel_size2 = int(kernel_size/2)\n",
    "        self.stride2 = stride\n",
    "        self.conv2 = nn.Conv1d(in_channels=ch_out, out_channels=1, kernel_size=self.kernel_size2, stride=self.stride2, padding=0,bias=True)\n",
    "        self.BN_aff2 = nn.BatchNorm1d(num_features=31,affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Conv=>BN=>AC\n",
    "        x = self.conv1(x)\n",
    "        x = torch.squeeze(x,dim=2)  # [2, 2, 1, 97]\n",
    "        x = self.ac_func(self.BN_aff1(x))\n",
    "\n",
    "        x = torch.squeeze(x)\n",
    "        # [B, 2, 97] => [B, 1, 31]\n",
    "        x = self.conv2(x)\n",
    "        x = torch.squeeze(x)       # [B, 1, 31] => [B, 31]\n",
    "        x = self.ac_func(self.BN_aff2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "class Conv_1_3(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians, ch_out=2, kernel_size=12, stride=3) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.kernel_size = (3,kernel_size)\n",
    "        self.stride = (3,stride)\n",
    "        # self.ln_in_1 = int((300-self.kernel_size[1])/self.stride[1]+1)\n",
    "        self.ln_in = 31\n",
    "\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=3,affine=True)\n",
    "\n",
    "        self.layer_pi = Conv_block_3(ch_out=ch_out,kernel_size=kernel_size,stride=stride)\n",
    "        self.layer_mu = Conv_block_3(ch_out=ch_out,kernel_size=kernel_size,stride=stride)\n",
    "        self.layer_sigma = Conv_block_3(ch_out=ch_out,kernel_size=kernel_size,stride=stride)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(self.ln_in, n_gaussians),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(self.ln_in, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(self.ln_in, n_gaussians)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.BN1(x)\n",
    "        x = torch.unsqueeze(x,dim=1)                     # torch.Size([B, 1, 3, 300])\n",
    "\n",
    "        x_pi = torch.squeeze(self.layer_pi(x))          # 不加squeeze不行\n",
    "        x_mu = torch.squeeze(self.layer_pi(x))\n",
    "        x_sigma = torch.squeeze(self.layer_pi(x))\n",
    "\n",
    "        pi = self.z_pi(x_pi)\n",
    "\n",
    "        # mu = self.z_mu(x_mu)\n",
    "        mu = self.ac_func(self.z_mu(x_mu))\n",
    "        sigma = self.z_sigma(x_sigma)\n",
    "\n",
    "        sigma = torch.exp(sigma)\n",
    "        sigma = torch.clamp(sigma,1e-4)\n",
    "\n",
    "        # return x_pi\n",
    "        return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1256,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "       BatchNorm1d-1               [-1, 3, 300]               6\n",
      "            Conv2d-2             [-1, 2, 1, 97]              74\n",
      "       BatchNorm1d-3                [-1, 2, 97]               4\n",
      "          Softplus-4                [-1, 2, 97]               0\n",
      "            Conv1d-5                [-1, 1, 31]              13\n",
      "       BatchNorm1d-6                   [-1, 31]              62\n",
      "          Softplus-7                   [-1, 31]               0\n",
      "      Conv_block_2-8                   [-1, 31]               0\n",
      "            Conv2d-9             [-1, 2, 1, 97]              74\n",
      "      BatchNorm1d-10                [-1, 2, 97]               4\n",
      "         Softplus-11                [-1, 2, 97]               0\n",
      "           Conv1d-12                [-1, 1, 31]              13\n",
      "      BatchNorm1d-13                   [-1, 31]              62\n",
      "         Softplus-14                   [-1, 31]               0\n",
      "     Conv_block_2-15                   [-1, 31]               0\n",
      "           Conv2d-16             [-1, 2, 1, 97]              74\n",
      "      BatchNorm1d-17                [-1, 2, 97]               4\n",
      "         Softplus-18                [-1, 2, 97]               0\n",
      "           Conv1d-19                [-1, 1, 31]              13\n",
      "      BatchNorm1d-20                   [-1, 31]              62\n",
      "         Softplus-21                   [-1, 31]               0\n",
      "     Conv_block_2-22                   [-1, 31]               0\n",
      "           Linear-23                    [-1, 3]              96\n",
      "          Softmax-24                    [-1, 3]               0\n",
      "           Linear-25                    [-1, 3]              96\n",
      "         Softplus-26                    [-1, 3]               0\n",
      "           Linear-27                    [-1, 3]              96\n",
      "================================================================\n",
      "Total params: 753\n",
      "Trainable params: 753\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mlp = Conv_1_3(opt.N_gaussians)\n",
    "mlp = mlp.to(device=device)\n",
    "summary(mlp, (3,300))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2.4 weibull\n",
    "1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# 1569\n",
    "class Conv_block_4(nn.Module):\n",
    "    def __init__(self, ch_out=1,kernel_size=12, stride=3) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.kernel_size = (3,kernel_size)\n",
    "        self.stride = (3,stride)\n",
    "        self.ln_in = int((300-self.kernel_size[1])/self.stride[1]+1)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=ch_out, kernel_size=self.kernel_size, stride=self.stride, padding=0,bias=True)\n",
    "        self.BN_aff1 = nn.BatchNorm1d(num_features=1,affine=True)\n",
    "        self.BN_aff2 = nn.BatchNorm1d(num_features=self.ln_in,affine=True)      # works better\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Conv=>BN=>AC\n",
    "        x = self.conv(x)\n",
    "        # 方法一：\n",
    "        # x = torch.squeeze(x,dim=2)\n",
    "        # x = self.ac_func(self.BN_aff1(x))\n",
    "\n",
    "        # 方法二：works better\n",
    "        x = torch.squeeze(x)\n",
    "        x = self.ac_func(self.BN_aff2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "class Conv_1_4(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians, ch_out=1, kernel_size=12, stride=3) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.kernel_size = (3,kernel_size)\n",
    "        self.stride = (3,stride)\n",
    "        self.ln_in = int((300-self.kernel_size[1])/self.stride[1]+1)\n",
    "\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=3,affine=True)\n",
    "\n",
    "        self.layer_pi = Conv_block_4(ch_out=1,kernel_size=kernel_size,stride=stride)\n",
    "        self.layer_scale = Conv_block_4(ch_out=1,kernel_size=kernel_size,stride=stride)\n",
    "        self.layer_shape = Conv_block_4(ch_out=1,kernel_size=kernel_size,stride=stride)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(self.ln_in, n_gaussians),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.z_scale = nn.Linear(self.ln_in, n_gaussians)\n",
    "        self.z_shape = nn.Linear(self.ln_in, n_gaussians)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.BN1(x)\n",
    "        x = torch.unsqueeze(x,dim=1)                     # torch.Size([B, 1, 3, 300])\n",
    "\n",
    "        x_pi = torch.squeeze(self.layer_pi(x))          # 不加squeeze不行\n",
    "        x_scale = torch.squeeze(self.layer_scale(x))\n",
    "        x_shape = torch.squeeze(self.layer_shape(x))\n",
    "\n",
    "        pi = self.z_pi(x_pi)\n",
    "        scale = torch.exp(self.z_scale(x_scale))\n",
    "        scale = torch.clamp(scale,1e-4)\n",
    "        shape = torch.exp(self.z_shape(x_shape))\n",
    "        shape = torch.clamp(shape,1e-4)\n",
    "\n",
    "        return pi,scale,shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1546,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "       BatchNorm1d-1               [-1, 3, 300]               6\n",
      "            Conv2d-2             [-1, 1, 1, 97]              37\n",
      "       BatchNorm1d-3                   [-1, 97]             194\n",
      "          Softplus-4                   [-1, 97]               0\n",
      "      Conv_block_4-5                   [-1, 97]               0\n",
      "            Conv2d-6             [-1, 1, 1, 97]              37\n",
      "       BatchNorm1d-7                   [-1, 97]             194\n",
      "          Softplus-8                   [-1, 97]               0\n",
      "      Conv_block_4-9                   [-1, 97]               0\n",
      "           Conv2d-10             [-1, 1, 1, 97]              37\n",
      "      BatchNorm1d-11                   [-1, 97]             194\n",
      "         Softplus-12                   [-1, 97]               0\n",
      "     Conv_block_4-13                   [-1, 97]               0\n",
      "           Linear-14                    [-1, 3]             294\n",
      "          Softmax-15                    [-1, 3]               0\n",
      "           Linear-16                    [-1, 3]             294\n",
      "           Linear-17                    [-1, 3]             294\n",
      "================================================================\n",
      "Total params: 1,581\n",
      "Trainable params: 1,581\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mlp = Conv_1_4(opt.N_gaussians)\n",
    "mlp = mlp.to(device=device)\n",
    "summary(mlp, (3,300))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3 Conv结构-2\n",
    "1. 换了一下conv的结构"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# 参数量747\n",
    "class MLP_1_2(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=3,affine=False)\n",
    "        self.BN2 = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "        self.BN3 = nn.BatchNorm2d(num_features=6,affine=True)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=(1,6), stride=(1,3), padding=0,bias=False)\n",
    "        self.ac_func = nn.Softplus()\n",
    "        # self.ac_func = nn.ReLU()\n",
    "\n",
    "        # 3, 3, 99\n",
    "        self.conv2 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=(3,6), stride=6, padding=0,bias=False)\n",
    "        # 6,1,33\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=6, out_channels=6, kernel_size=(1,3), stride=3, padding=0,bias=False)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        #self.linear1 = nn.Linear(48, 18)\n",
    "\n",
    "        #无clip\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(30, n_gaussians),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        # 有clip\n",
    "        # self.z_pi = nn.Linear(30, n_gaussians)\n",
    "        self.z_mu = nn.Linear(30, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(30, n_gaussians)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.BN1(x)\n",
    "        # 加一个height维度\n",
    "\n",
    "        x = torch.unsqueeze(x,dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.BN2(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.BN3(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        # x = torch.unsqueeze(x,dim=2)\n",
    "        x = self.conv3(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        # x = self.linear1(x)\n",
    "        # x = self.ac_func(self.drop(x))\n",
    "        mu = self.z_mu(x)\n",
    "        pi = self.z_pi(x)\n",
    "\n",
    "        sigma = torch.exp(self.z_sigma(x))\n",
    "        sigma = torch.clamp(sigma,1e-4)\n",
    "\n",
    "        # return x\n",
    "        return pi,mu,sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4 MLP结构\n",
    "### 3.4.1 4层"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# 参数量304,000+\n",
    "# 4层\n",
    "class MLP_1_3(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=3,affine=True)\n",
    "        self.BN2 = nn.BatchNorm1d(num_features=300,affine=True)\n",
    "        self.BN3 = nn.BatchNorm1d(num_features=100,affine=True)\n",
    "        self.BN4 = nn.BatchNorm1d(num_features=30,affine=True)\n",
    "        self.BN5 = nn.BatchNorm1d(num_features=12,affine=True)\n",
    "        self.BN6 = nn.BatchNorm1d(num_features=12,affine=True)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "        self.linear1 = nn.Linear(900, 300)\n",
    "        self.linear2 = nn.Linear(300, 100)\n",
    "        self.linear3 = nn.Linear(100, 30)\n",
    "        self.linear4 = nn.Linear(30, 12)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "        self.ac_func2 = nn.SELU()\n",
    "        self.ac_func3 = nn.LeakyReLU()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(12, n_gaussians),  # 30个params要learn\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(12, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(12, n_gaussians)\n",
    "\n",
    "        # SELU Init\n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, (nn.Linear)):\n",
    "        #         nn.init.kaiming_normal_(m.weight, nonlinearity='linear')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.BN1(x)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.BN2(x)\n",
    "        x = self.ac_func3(self.drop(x))\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = self.BN3(x)\n",
    "        x = self.ac_func3(self.drop(x))\n",
    "\n",
    "        x = self.linear3(x)\n",
    "        x = self.BN4(x)\n",
    "        x = self.ac_func3(self.drop(x))\n",
    "\n",
    "        x = self.linear4(x)\n",
    "        x = self.BN5(x)\n",
    "        x = self.ac_func3(self.drop(x))\n",
    "\n",
    "        x = self.BN6(x)\n",
    "        pi = self.z_pi(x)\n",
    "        # pi = self.z_pi(torch.clamp(x,1e-3))\n",
    "        mu = self.z_mu(x)\n",
    "        # sigma = torch.exp(self.z_sigma(x))\n",
    "        sigma = F.elu(self.z_sigma(x)) + 1\n",
    "        sigma = torch.clamp(sigma,1e-4)\n",
    "\n",
    "        # return x\n",
    "        return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.2 2层"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1660,
   "outputs": [],
   "source": [
    "# 参数量91,000+\n",
    "# 2层MLP+MDN\n",
    "class MLP_1_4(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=3,affine=True)\n",
    "        self.BN2 = nn.BatchNorm1d(num_features=100,affine=True)\n",
    "        self.BN3 = nn.BatchNorm1d(num_features=12,affine=True)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "        self.linear1 = nn.Linear(900, 100)\n",
    "        self.linear2 = nn.Linear(100, 12)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "        self.ac_func2 = nn.SELU()\n",
    "        self.ac_func3 = nn.LeakyReLU()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(12, n_gaussians),  # 30个params要learn\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(12, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(12, n_gaussians)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"x.shape: \",x.shape)  # torch.Size([40, 3, 300])\n",
    "        x = self.BN1(x)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        # x = self.BN2(x)\n",
    "        x = self.ac_func3(self.drop(x))\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        # x = self.BN3(x)\n",
    "        x = self.ac_func3(self.drop(x))\n",
    "\n",
    "        # 连续的ac？\n",
    "        pi = self.z_pi(x)\n",
    "        # mu = self.z_mu(x)\n",
    "\n",
    "        # # sigma = torch.exp(self.z_sigma(x))\n",
    "        # # sigma = torch.exp(self.ac_func(self.z_sigma(x)))\n",
    "        # sigma = torch.exp(self.z_sigma(x))\n",
    "        # # sigma = F.elu(self.z_sigma(x)) + 1\n",
    "        # sigma = torch.clamp(sigma,1e-4)\n",
    "\n",
    "        scale = torch.exp(self.z_mu(x))\n",
    "        scale = torch.clamp(scale,1e-4)\n",
    "        shape = torch.exp(self.z_sigma(x))\n",
    "        shape = torch.clamp(shape,1e-4)\n",
    "\n",
    "        # return pi, mu, sigma\n",
    "        return pi, scale, shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.4 1层+avgpool"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "outputs": [],
   "source": [
    "# 参数量2823\n",
    "# 1层MLP+MDN\n",
    "class MLP_1_5(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=3,affine=True)\n",
    "        self.BN2 = nn.BatchNorm1d(num_features=9,affine=True)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=(1,3))  # kernel_size=(1,Channel)\n",
    "        self.linear1 = nn.Linear(300, 9)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(9, n_gaussians),  # 30个params要learn\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(9, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(9, n_gaussians)\n",
    "\n",
    "        # SELU Init\n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, (nn.Linear)):\n",
    "        #         nn.init.kaiming_normal_(m.weight, nonlinearity='linear')\n",
    "\n",
    "        # 初始化权重\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                # m.weight.data.normal_(0, 0.02)\n",
    "                # nn.init.xavier_normal_(m.weight,gain=1)\n",
    "                nn.init.orthogonal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "        # nn.init.xavier_normal_(self.z_mu.weight,gain=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.BN1(x)\n",
    "        # print(\"before transpose: \",x.shape)\n",
    "        x = torch.transpose(x, 1, 2)        # [B,N,C]\n",
    "        # print(\"before avgpool: \",x.shape)\n",
    "        x = self.avgpool(x)\n",
    "        # print(\"after avgpool: \",x.shape)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.BN2(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        pi = self.z_pi(x)\n",
    "        # pi = self.z_pi(torch.clamp(x,1e-3))\n",
    "        mu = self.z_mu(x)\n",
    "\n",
    "        sigma = torch.exp(self.z_sigma(x))\n",
    "        # sigma = F.elu(self.z_sigma(x)) + 1\n",
    "        sigma = torch.clamp(sigma,1e-4)\n",
    "\n",
    "        return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "outputs": [],
   "source": [
    "def test_pool():\n",
    "    # torch.Size([40, 3, 300])\n",
    "    a = torch.tensor([[[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5]]])\n",
    "    # print(a.shape)\n",
    "    a = torch.transpose(a, 1, 2)\n",
    "    print(a)\n",
    "    ans = F.avg_pool2d(a,(1,3))\n",
    "    print(ans.shape)    # [B,N,C]\n",
    "# test_pool()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.5 1层+无avgpool"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "outputs": [],
   "source": [
    "# 参数量8233\n",
    "# 1层MLP+MDN\n",
    "class MLP_1_6(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=3,affine=True)\n",
    "        self.BN2 = nn.BatchNorm1d(num_features=9,affine=True)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "        self.linear1 = nn.Linear(900, 9)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(9, n_gaussians),  # 30个params要learn\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(9, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(9, n_gaussians)\n",
    "\n",
    "        # SELU Init\n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, (nn.Linear)):\n",
    "        #         nn.init.kaiming_normal_(m.weight, nonlinearity='linear')\n",
    "\n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, nn.BatchNorm1d):\n",
    "        #         m.weight.data.fill_(1)\n",
    "        #         m.bias.data.zero_()\n",
    "        #     elif isinstance(m, nn.Linear):\n",
    "        #         # m.weight.data.normal_(0, 0.02)\n",
    "        #         # nn.init.xavier_normal_(m.weight,gain=1)\n",
    "        #         nn.init.orthogonal_(m.weight)\n",
    "        #         nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.BN1(x)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.BN2(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        pi = self.z_pi(x)\n",
    "        # pi = self.z_pi(torch.clamp(x,1e-3))\n",
    "        mu = self.z_mu(x)\n",
    "\n",
    "        sigma = torch.exp(self.z_sigma(x))\n",
    "        # sigma = F.elu(self.z_sigma(x)) + 1\n",
    "        sigma = torch.clamp(sigma,1e-4)\n",
    "\n",
    "        return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.5 (3,100)的结构\n",
    "1. 适配(3,100)的training data长度而不是(3,300)\n",
    "### 3.5.1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # 参数量：228\n",
    "# class MLP_2_2(nn.Module):\n",
    "#     # code->generate->override methods\n",
    "#     def __init__(self, n_gaussians) -> None:\n",
    "#         super().__init__()\n",
    "#         self.BN = nn.BatchNorm2d(num_features=3,affine=False)\n",
    "#         self.BN_aff = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "#         self.drop = nn.Dropout(0.2)\n",
    "#\n",
    "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#         # self.ac_func = nn.Softplus()\n",
    "#         self.ac_func = nn.LeakyReLU()\n",
    "#         self.ac_func2 = nn.ReLU()\n",
    "#\n",
    "#         # self.ac_func = nn.Sigmoid()\n",
    "#         # self.ac_func = nn.Tanh()\n",
    "#\n",
    "#         self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,2), stride=(1,2), padding=0,bias=False)\n",
    "#\n",
    "#         self.conv3 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#\n",
    "#         self.flatten = nn.Flatten()\n",
    "#\n",
    "#         self.z_pi = nn.Sequential(\n",
    "#             nn.Linear(15, n_gaussians),\n",
    "#             nn.Softmax(dim=1)\n",
    "#         )\n",
    "#\n",
    "#         self.z_mu = nn.Linear(15, n_gaussians)\n",
    "#         self.z_sigma = nn.Linear(15, n_gaussians)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         # 加一个height维度\n",
    "#         x = torch.unsqueeze(x,dim=2)\n",
    "#         x = self.BN(x)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         x = self.BN_aff(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         x = self.BN_aff(x)\n",
    "#         x = self.conv3(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#         #\n",
    "#         x = self.flatten(x)\n",
    "#\n",
    "#         pi = self.z_pi(torch.clamp(x,1e-3))\n",
    "#         # pi = self.z_pi(x)\n",
    "#         mu = self.z_mu(x)\n",
    "#         sigma = torch.exp(self.z_sigma(x))\n",
    "#\n",
    "#         # return x\n",
    "#         return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- MLP结构+长度=100"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.5.2 MLP-2层"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "# 参数量 18699\n",
    "class MLP_2_3(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=3,affine=True)\n",
    "        self.BN2 = nn.BatchNorm1d(num_features=60,affine=True)\n",
    "        self.BN3 = nn.BatchNorm1d(num_features=9,affine=True)\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "\n",
    "        self.linear1 = nn.Linear(300, 60)\n",
    "        self.linear2 = nn.Linear(60, 9)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(9, n_gaussians),  # 30个params要learn\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(9, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(9, n_gaussians)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.BN1(x)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.BN2(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = self.BN3(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        pi = self.z_pi(x)\n",
    "        # pi = self.z_pi(torch.clamp(x,1e-3))\n",
    "        mu = self.z_mu(x)\n",
    "        sigma = torch.exp(self.z_sigma(x))\n",
    "        sigma = torch.clamp(sigma,1e-4)\n",
    "\n",
    "        # return x\n",
    "        return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.5.3 MLP-1层"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [
    "# 参数量2823\n",
    "class MLP_2_4(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=3,affine=True)\n",
    "        self.BN2 = nn.BatchNorm1d(num_features=9,affine=True)\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "\n",
    "        self.linear1 = nn.Linear(300, 9)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(9, n_gaussians),  # 30个params要learn\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(9, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(9, n_gaussians)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.BN1(x)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.BN2(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        pi = self.z_pi(x)\n",
    "        # pi = self.z_pi(torch.clamp(x,1e-3))\n",
    "        mu = self.z_mu(x)\n",
    "        sigma = torch.exp(self.z_sigma(x))\n",
    "        sigma = torch.clamp(sigma,1e-4)\n",
    "\n",
    "        # return x\n",
    "        return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.6 (3,60)的结构\n",
    "1. 适配(3,60)的training data长度而不是(3,300)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # 参数量：174\n",
    "# class MLP_3_1(nn.Module):\n",
    "#     # code->generate->override methods\n",
    "#     def __init__(self, n_gaussians) -> None:\n",
    "#         super().__init__()\n",
    "#         self.BN = nn.BatchNorm2d(num_features=3,affine=False)\n",
    "#         self.BN_aff = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "#         self.drop = nn.Dropout(0.2)\n",
    "#\n",
    "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#         # self.ac_func = nn.Softplus()\n",
    "#         self.ac_func = nn.LeakyReLU()\n",
    "#         self.ac_func2 = nn.ReLU()\n",
    "#\n",
    "#         # self.ac_func = nn.Sigmoid()\n",
    "#         # self.ac_func = nn.Tanh()\n",
    "#\n",
    "#         self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,2), stride=(1,2), padding=0,bias=False)\n",
    "#\n",
    "#         self.conv3 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.linear = nn.Linear(24, 9)\n",
    "#\n",
    "#         self.z_pi = nn.Sequential(\n",
    "#             nn.Linear(9, n_gaussians),\n",
    "#             nn.Softmax(dim=1)\n",
    "#         )\n",
    "#\n",
    "#         self.z_mu = nn.Linear(9, n_gaussians)\n",
    "#         self.z_sigma = nn.Linear(9, n_gaussians)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         # 加一个height维度\n",
    "#         x = torch.unsqueeze(x,dim=2)\n",
    "#         x = self.BN(x)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.ac_func(self.drop(x))      # 3, 1, 20\n",
    "#\n",
    "#         x = self.BN_aff(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.ac_func(self.drop(x))      # 3, 1, 10\n",
    "#\n",
    "#         x = self.BN_aff(x)\n",
    "#         x = self.conv3(x)\n",
    "#         x = self.ac_func(self.drop(x))      # 3, 1, 8\n",
    "#\n",
    "#         x = self.flatten(x)\n",
    "#         # print(x.shape)\n",
    "#         # x = self.linear(x)\n",
    "#         #\n",
    "#         pi = self.z_pi(x)\n",
    "#         mu = self.z_mu(x)\n",
    "#         sigma = torch.exp(self.z_sigma(x))\n",
    "#\n",
    "#         # return x\n",
    "#         return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- MLP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # 参数量：5799\n",
    "# class MLP_3_2(nn.Module):\n",
    "#     # code->generate->override methods\n",
    "#     def __init__(self, n_gaussians) -> None:\n",
    "#         super().__init__()\n",
    "#         self.BN = nn.BatchNorm2d(num_features=3,affine=False)\n",
    "#         self.BN_aff = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "#         self.drop = nn.Dropout(0.2)\n",
    "#\n",
    "#         self.linear1 = nn.Linear(180,30)\n",
    "#         self.linear2 = nn.Linear(30,9)\n",
    "#         # self.ac_func = nn.Softplus()\n",
    "#         self.ac_func = nn.LeakyReLU()\n",
    "#         self.ac_func2 = nn.ReLU()\n",
    "#\n",
    "#         # self.ac_func = nn.Sigmoid()\n",
    "#         # self.ac_func = nn.Tanh()\n",
    "#\n",
    "#         self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,2), stride=(1,2), padding=0,bias=False)\n",
    "#\n",
    "#         self.conv3 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.linear = nn.Linear(24, 9)\n",
    "#\n",
    "#         self.z_pi = nn.Sequential(\n",
    "#             nn.Linear(9, n_gaussians),\n",
    "#             nn.Softmax(dim=1)\n",
    "#         )\n",
    "#\n",
    "#         self.z_mu = nn.Linear(9, n_gaussians)\n",
    "#         self.z_sigma = nn.Linear(9, n_gaussians)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         # 加一个height维度\n",
    "#         x = torch.unsqueeze(x,dim=2)\n",
    "#         x = self.BN(x)\n",
    "#         x = self.flatten(x)\n",
    "#\n",
    "#         x = self.linear1(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         x = self.linear2(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         pi = self.z_pi(x)\n",
    "#         mu = self.z_mu(x)\n",
    "#         sigma = torch.exp(self.z_sigma(x))\n",
    "#\n",
    "#         # return x\n",
    "#         return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. The Loss\n",
    "\n",
    "## 4.0 The metric\n",
    "1. 以NLL作为metric\n",
    "2. 理论模型的metric已经提前算好，读进来.\n",
    "3. 注意这里的metric比较的是target还是target_5:\n",
    "    - 建议使用target data而不是target_5，因为后者是为了方便NN的learning，前者才是真正的比较NLL\n",
    "    - 使用前者也要用cdf进行比较吧。。比如GT的4实际上是我们这里的[3.5,4.5]？是高斯分布在[a-0.5,a+0.5]上的cdf作为a的“单点分布”"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# from loss import cal_metric"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1 NLL loss\n",
    "### 4.1.1 original version"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# from loss import loss_fn_v2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1.2 version 3 [LogSumExp]\n",
    "1. $ exponent_i = \\log_{}{\\alpha_i } -1/2*\\log_{}{2\\pi } -\\log_{}{\\sigma_i }- \\frac{(x-\\mu_i )^2}{2\\sigma_i^2} $\n",
    "2. $ NLL =-\\sum_{j}^{N_S}LogSumExp= -\\sum_{j}^{N_S} \\log\\sum_{i}^{N_G} \\exp \\{exponent_i\\}$\n",
    "3. 不稳定！lr不要设超过1e-2；而且loss会变成负值。。最开始x_max是负值，后面会变成正值 which means prob值开始>1了"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from loss import log_sum_exp\n",
    "from loss import loss_fn_v3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 cdf loss\n",
    "1. 注意把targets data的路径改一下"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# cdf loss\n",
    "from loss import loss_fn_cdf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.3 交叉熵/ KL\n",
    "1. 自己写的，没有用torch的函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from loss import loss_fn_CE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def loss_test():\n",
    "    loss_2 = torch.tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "    loss_3 = loss_2[torch.nonzero(loss_2)].view(-1,1)         # 去掉所有的log(0)\n",
    "    print(\"loss_3:\",loss_3.shape)\n",
    "\n",
    "    MIN_LOSS = torch.min(loss_2) if torch.min(loss_2)>0 else 1e-20\n",
    "\n",
    "    print(\"MIN_LOSS：\",MIN_LOSS)\n",
    "    # 用MIN_loss替代loss_2为0的部分\n",
    "    loss_2[loss_2==0] = MIN_LOSS\n",
    "\n",
    "\n",
    "    loss_padded = [MIN_LOSS]* (loss_2.shape[0]-loss_3.shape[0])\n",
    "    loss_padded_ts = torch.tensor(loss_padded) # ,device=device\n",
    "    loss_3 = torch.cat((loss_3[:,0],loss_padded_ts))\n",
    "    loss_3.shape, torch.log(loss_3)\n",
    "    print(\"data: \",loss_3.data)\n",
    "    print(\"data: \",type(loss_3))\n",
    "# loss_test()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.4 W-distance\n",
    "1. 拿到target data的两列，按照第一列去predict\n",
    "2. Ref: https://www.kernel-operations.io/geomloss/api/pytorch-api.html"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5 Plot\n",
    "1. draw:\n",
    "    - loss: training data的loss和test data的loss趋势\n",
    "    - MLP的网络结构（.png）\n",
    "    - target distrb.和pred. distrb.\n",
    "    - 所有target data的分布图\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "viz = Visdom(env=\"001\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.1 draw 所有的target data\n",
    "1. 实际上在`i < 50`里决定画前50 or what target data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def draw_all_target_data():\n",
    "    # target data\n",
    "    # target_path = r\"../data/targets\"\n",
    "    target_path = r\"../data/targets_5\"\n",
    "    # target_path里有全部的target data地址\n",
    "    target_all_path = os.listdir(target_path)\n",
    "    len_target = len(target_all_path)\n",
    "    print(f\"一共有 *{len_target}* 组 target data\")\n",
    "\n",
    "    # Init\n",
    "    # viz_env_str = \"target_bar\"\n",
    "    # viz_env_str = \"target_line\"\n",
    "    # viz_env_str = \"target_hist\"\n",
    "    viz_env_str = \"target_scatter\"\n",
    "    viz = Visdom(env = viz_env_str)\n",
    "\n",
    "    for i in range(len_target):\n",
    "        target_path_i_path = os.path.join(target_path,target_all_path[i])\n",
    "        target_df = pd.read_csv(target_path_i_path,encoding=\"utf-8\")\n",
    "\n",
    "        # Init\n",
    "        win_str = str(target_all_path[i])\n",
    "        title_str = \"Target Distrb. of \"+win_str\n",
    "\n",
    "        if(i < 50):\n",
    "            # Hist plot which is not that great\n",
    "            if viz_env_str == \"target_hist\" :\n",
    "                viz.histogram(X = np.array(target_df.N), env=viz_env_str, win=win_str,\n",
    "                            opts= dict(title=title_str,numbins = 50))\n",
    "\n",
    "            # Line plot\n",
    "            if viz_env_str == \"target_line\" :\n",
    "                target_df.drop_duplicates(inplace=True)\n",
    "                viz.line(X = np.array(target_df.N),Y= np.array(target_df.P), env=viz_env_str, win=win_str,opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "            # Bar plot. 这个看起来是最准确的一个\n",
    "            if viz_env_str == \"target_bar\" :\n",
    "                target_df.drop_duplicates(inplace=True)\n",
    "                arr_str = [str(i) for i in np.array(target_df.N)]\n",
    "                viz.bar(X=np.array(target_df.P), env=viz_env_str, win=win_str,\n",
    "                        opts=dict(title=title_str,rownames=arr_str,stacked = False))\n",
    "\n",
    "            # Scatter plot\n",
    "            if viz_env_str == \"target_scatter\" :\n",
    "                target_df.drop_duplicates(inplace=True)\n",
    "                viz.scatter(X=np.array(target_df),env=viz_env_str, win=win_str,opts=dict(title=title_str,markersize = 3))\n",
    "    print(\"Done\")\n",
    "\n",
    "# draw_all_target_data()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 画target data的cdf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def draw_all_target_data_cdf():\n",
    "    # target data\n",
    "    # target_path = r\"../data/targets\"\n",
    "    target_path = r\"../data/targets_5_cdf\"\n",
    "    # target_path里有全部的target data地址\n",
    "    target_all_path = os.listdir(target_path)\n",
    "    len_target = len(target_all_path)\n",
    "    print(f\"一共有 *{len_target}* 组 target data\")\n",
    "\n",
    "    # Init，只需要改这个str就ok\n",
    "    # viz_env_str = \"target_barCDF\"\n",
    "    # viz_env_str = \"target_lineCDF\"\n",
    "    # viz_env_str = \"target_histCDF\"\n",
    "    viz_env_str = \"target_scatterCDF\"\n",
    "    viz = Visdom(env = viz_env_str)\n",
    "\n",
    "    for i in range(len_target):\n",
    "        target_path_i_path = os.path.join(target_path,target_all_path[i])\n",
    "        target_df = pd.read_csv(target_path_i_path,encoding=\"utf-8\")\n",
    "\n",
    "        # Init\n",
    "        win_str = str(target_all_path[i])\n",
    "        title_str = \"Target Distrb. of \"+win_str\n",
    "\n",
    "        if(i < 50):\n",
    "            # Hist plot which is not that great\n",
    "            if viz_env_str == \"target_histCDF\" :\n",
    "                viz.histogram(X = np.array(target_df.N), env=viz_env_str, win=win_str,\n",
    "                            opts= dict(title=title_str,numbins = 50))\n",
    "\n",
    "            # Line plot\n",
    "            if viz_env_str == \"target_lineCDF\" :\n",
    "                target_df.drop_duplicates(inplace=True)\n",
    "                viz.line(X = np.array(target_df.N),Y= np.array(target_df.P), env=viz_env_str, win=win_str,opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "            # Bar plot. 这个看起来是最准确的一个\n",
    "            if viz_env_str == \"target_barCDF\" :\n",
    "                target_df.drop_duplicates(inplace=True)\n",
    "                arr_str = [str(i) for i in np.array(target_df.N)]\n",
    "                viz.bar(X=np.array(target_df.P), env=viz_env_str, win=win_str,\n",
    "                        opts=dict(title=title_str,rownames=arr_str,stacked = False))\n",
    "\n",
    "            # Scatter plot\n",
    "            if viz_env_str == \"target_scatterCDF\" :\n",
    "                target_df.drop_duplicates(inplace=True)\n",
    "                viz.scatter(X=np.array(target_df),env=viz_env_str, win=win_str,opts=dict(title=title_str,markersize = 3))\n",
    "    print(\"Done\")\n",
    "\n",
    "# draw_all_target_data_cdf()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 也是画target data，但是是平滑处理\n",
    "- `savgol_filter`参数解析:\n",
    "    - y：代表曲线点坐标（x,y）中的y\n",
    "    - window_length：窗口长度，该值需为正奇整数。值越小，曲线越贴近真实曲线；值越大，平滑效果越厉害\n",
    "    - polyorder: 对窗口内的数据点进行k阶多项式拟合，k的值需要小于window_length。值越大，曲线越贴近真实曲线；值越小，平滑效果越厉害\n",
    "    - mode：确定了要应用滤波器的填充信号的扩展类型。（This determines the type of extension to use for the padded signal to which the filter is applied. ）\n",
    "- **实际上没用到这个函数**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def draw_smooth_target_data():\n",
    "    # target data\n",
    "    target_path = r\"../data/targets\"\n",
    "    # target_path里有全部的target data地址\n",
    "    target_all_path = os.listdir(target_path)\n",
    "    len_target = len(target_all_path)\n",
    "    print(f\"一共有 *{len_target}* 组 target data\")\n",
    "\n",
    "    # Init\n",
    "    viz_env_str = \"target_bar\"\n",
    "    # viz_env_str = \"target_distrb\"\n",
    "    # viz_env_str = \"target_hist\"\n",
    "    # viz_env_str = \"target_scatter\"\n",
    "    viz = Visdom(env = viz_env_str)\n",
    "\n",
    "    for i in range(len_target):\n",
    "        target_path_i_path = os.path.join(target_path,target_all_path[i])\n",
    "        target_df = pd.read_csv(target_path_i_path,encoding=\"utf-8\")\n",
    "\n",
    "        # Init\n",
    "        win_str = str(target_all_path[i])\n",
    "        title_str = \"Target Distrb. of \"+win_str\n",
    "\n",
    "        if(i < 100):\n",
    "            # Smoothen the data\n",
    "            y_smooth = scipy.signal.savgol_filter(target_df.P,window_length=53,polyorder=3)\n",
    "            # y_smooth = scipy.signal.savgol_filter(target_df.P, 99, 1, mode= 'nearest')\n",
    "            target_df.P = y_smooth\n",
    "\n",
    "            # Hist plot which is not that great\n",
    "            # viz.histogram(X = np.array(target_df.N), env=viz_env_str, win=win_str,\n",
    "            #             opts= dict(title=title_str,numbins = 50))\n",
    "\n",
    "            # Line plot\n",
    "            # target_df.drop_duplicates(inplace=True)\n",
    "            # viz.line(X = np.array(target_df.N),Y= np.array(target_df.P), env=viz_env_str, win=win_str,opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "            # Bar plot. 这个看起来是最准确的一个\n",
    "            target_df.drop_duplicates(inplace=True)\n",
    "            arr_str = [str(i) for i in np.array(target_df.N)]\n",
    "            viz.bar(X=np.array(target_df.P), env=viz_env_str, win=win_str,\n",
    "                    opts=dict(title=title_str,rownames=arr_str))\n",
    "\n",
    "            # Scatter plot\n",
    "            # target_df.drop_duplicates(inplace=True)\n",
    "            # viz.scatter(X=np.array(target_df),env=viz_env_str, win=win_str,opts=dict(title=title_str,markersize = 3))\n",
    "    print(\"Done\")\n",
    "\n",
    "# draw_smooth_target_data()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#### Test for drawing\n",
    "def test_draw():\n",
    "    viz = Visdom(env=\"001\")\n",
    "\n",
    "    mu = torch.tensor([0,10,20])\n",
    "    sigma = torch.tensor([1,1,1])\n",
    "    duration = torch.tensor([0,1,2,0])\n",
    "    duration = torch.repeat_interleave(duration.unsqueeze(dim=1), repeats=3, dim=1)\n",
    "    m = torch.distributions.Normal(loc=mu, scale=sigma)\n",
    "    pi = torch.tensor([0.2,0.3,0.5])\n",
    "\n",
    "    # draw\n",
    "    x_0 = torch.tensor(np.arange(0,1000))\n",
    "    x = torch.repeat_interleave(x_0.unsqueeze(dim=1), repeats=3, dim=1)\n",
    "    y = torch.exp(m.log_prob(x))\n",
    "    y_sum = torch.unsqueeze(torch.sum(pi*y,dim=1),dim=1)\n",
    "    viz.line(X = x_0,Y= torch.cat([y,y_sum],dim = 1), env=\"001\", win=\"test_draw_2\",\n",
    "            opts= dict(title='test_draw', legend=['N1', 'N2', 'N3','NNN']))\n",
    "# test_draw()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.2 plot mdn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2.1 mdn pdf的图像[with input data]\n",
    "1. 求解[1,6,11...]上的cdf：\n",
    "    - 求解[1,6,11..]的cdf，再求解[6,11,..]，两者相减"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# smoothed_y[t] = average(y[t-k], y[t-k+1], ..., y[t+k-1], y[t+k])\n",
    "# sm表示滑动窗口大小,为2*k+1,\n",
    "def smooth(data, sm=1):\n",
    "    if sm > 1:\n",
    "        smooth_data = []\n",
    "        for d in data:\n",
    "            y = np.ones(sm)*1.0/sm\n",
    "            d = np.convolve(y, d, \"same\")\n",
    "\n",
    "            smooth_data.append(d)\n",
    "\n",
    "    return smooth_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def draw_mdn_2(pi, mu, sigma, target, input, total_train_step, N_gaussians):\n",
    "    \"\"\"\n",
    "    画两条曲线，pred-1表示归一化前，pred-2表示归一化后\n",
    "\n",
    "    :param pi:\n",
    "    :param mu:\n",
    "    :param sigma:\n",
    "    :param target:\n",
    "    :param input:\n",
    "    :param total_train_step:\n",
    "    :param opt.N_gaussians:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 只画一个batch中的第一个\n",
    "\n",
    "    # The target distrb.\n",
    "    target = torch.unique(target,dim=0)    # drop the duplicate\n",
    "    n_target = target[:,0]\n",
    "    non_zero_idx = torch.nonzero(n_target)\n",
    "    n = n_target[non_zero_idx]\n",
    "\n",
    "    p_target = target[:,1]\n",
    "    p = p_target[non_zero_idx]\n",
    "    max_n = int(max(n).item())\n",
    "\n",
    "    # The predicted distrb.\n",
    "    m = torch.distributions.Normal(mu,sigma)          # Gaussian\n",
    "    # m = torch.distributions.Laplace(mu,sigma)           # Laplace\n",
    "\n",
    "    x_0 = torch.arange(1,(max_n+opt.TARGET),opt.TARGET).to(device=device)\n",
    "\n",
    "    x = torch.repeat_interleave(x_0.unsqueeze(dim=1), repeats=opt.N_gaussians, dim=1)   # expand dim\n",
    "    # 求解每个长度为TARGET的区间上的cdf\n",
    "    y = (m.cdf(x+opt.TARGET) - m.cdf(x)).to(device=device)\n",
    "\n",
    "    # 方法一：\n",
    "    y_pred_1 = torch.sum(pi*y,dim=1)\n",
    "    # 方法二：做一下(0,1)上的归一化\n",
    "    y_pred_2 = y_pred_1/y_pred_1.sum()\n",
    "\n",
    "    # The input distrb.\n",
    "    input_data = input[0:2,:]\n",
    "    x_input = torch.arange(1,(300+1),opt.SCALE).to(device=device)\n",
    "    y_input_0 = (input_data[0,:]).to(device=device)\n",
    "    y_input_1 = (input_data[1,:]).to(device=device)\n",
    "\n",
    "    # Init\n",
    "    win_str = \"total_train_step = \"+str(total_train_step)\n",
    "    title_str = \"Distrb. in \"+win_str\n",
    "    viz.line(X = [0.],Y = [0.], env=\"001\", win=win_str, opts= dict(title=title_str))\n",
    "\n",
    "    # Visdom本身不能把hist和line画在一个window中\n",
    "    # 如果想画一起只能是两条line\n",
    "    # Plot y_target\n",
    "    # viz.histogram(X = n, env=\"001\", win=win_str,\n",
    "    #         opts= dict(title=title_str))\n",
    "    viz.line(X = n,Y= p, env=\"001\", win=win_str, update=\"append\", name='target',\n",
    "            opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "    # Plot y_pred\n",
    "    viz.line(X = x_0,Y= y_pred_1, env=\"001\", win=win_str, update=\"append\", name='pred-1',\n",
    "            opts= dict(title=title_str))\n",
    "\n",
    "    # Plot y_pred_2\n",
    "    viz.line(X = x_0,Y= y_pred_2, env=\"001\", win=win_str, update=\"append\", name='pred-2',\n",
    "            opts= dict(title=title_str))\n",
    "\n",
    "    # Plot y_input\n",
    "    # 如果input data长度短，全部画完\n",
    "    if(len(x_input) <= max_n):\n",
    "        viz.line(X = x_input,Y= y_input_0, env=\"001\", win=win_str, update=\"append\", name='GT-1', opts= dict(title=title_str))\n",
    "        viz.line(X = x_input,Y= y_input_1, env=\"001\", win=win_str, update=\"append\", name='GT-2', opts= dict(title=title_str))\n",
    "    # 如果input data长度长，则截断\n",
    "    else:\n",
    "        x_input_0 = x_input[0:max_n]\n",
    "        y_input_2 = y_input_0[0:max_n]/y_input_0[0:max_n].sum()\n",
    "        y_input_3 = y_input_1[0:max_n]/y_input_1[0:max_n].sum()\n",
    "        viz.line(X = x_input_0,Y= y_input_2, env=\"001\", win=win_str, update=\"append\", name='GT-1', opts= dict(title=title_str))\n",
    "        viz.line(X = x_input_0,Y= y_input_3, env=\"001\", win=win_str, update=\"append\", name='GT-2', opts= dict(title=title_str))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2.2 MDN+weibull[with input data]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1840,
   "outputs": [],
   "source": [
    "def draw_mdn_wei(pi, scale, shape, target, input, total_train_step, N_gaussians):\n",
    "    \"\"\"\n",
    "    画两条曲线，pred-1表示归一化前，pred-2表示归一化后\n",
    "\n",
    "    :param pi:\n",
    "    :param scale:\n",
    "    :param shape:\n",
    "    :param target: target data records\n",
    "    :param input:  input data of NN / GT pred. 但是不能画这个！！这个是为了input做了归一化的\n",
    "    :param total_train_step:\n",
    "    :param opt.N_gaussians:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 只画一个batch中的第一个\n",
    "\n",
    "    # The target distrb.\n",
    "    target = torch.unique(target,dim=0)    # drop the duplicate\n",
    "    n_target = target[:,0]\n",
    "    non_zero_idx = torch.nonzero(n_target)\n",
    "    n = n_target[non_zero_idx]\n",
    "\n",
    "    p_target = target[:,1]\n",
    "    p = p_target[non_zero_idx]\n",
    "    max_n = int(max(n).item())\n",
    "\n",
    "    # The predicted distrb.\n",
    "    m = torch.distributions.Weibull(scale, shape)\n",
    "\n",
    "    x_0 = torch.arange(1,(max_n+1)).to(device=device)\n",
    "    x = torch.repeat_interleave(x_0.unsqueeze(dim=1), repeats=N_gaussians, dim=1)   # expand dim\n",
    "    # 求解每个小区间上的cdf作为likelihood\n",
    "    y = (m.cdf(x+0.5) - m.cdf(x-0.5)).to(device=device)\n",
    "\n",
    "    # 方法一：\n",
    "    y_pred_1 = torch.sum(pi*y,dim=1)\n",
    "    # 方法二：做一下(0,1)上的归一化\n",
    "    y_pred_2 = y_pred_1/y_pred_1.sum()\n",
    "\n",
    "    # The input distrb.\n",
    "    input_data = input[0:2,:]\n",
    "    x_input = torch.arange(1,(300+1),opt.SCALE).to(device=device)\n",
    "    y_input_0 = (input_data[0,:]).to(device=device)\n",
    "    y_input_1 = (input_data[1,:]).to(device=device)\n",
    "\n",
    "    # Init\n",
    "    win_str = \"total_train_step = \"+str(total_train_step)\n",
    "    title_str = \"Distrb. in \"+win_str\n",
    "    viz.line(X = [0.],Y = [0.], env=\"001\", win=win_str, opts= dict(title=title_str))\n",
    "\n",
    "    # Visdom本身不能把hist和line画在一个window中\n",
    "    # 如果想画一起只能是两条line\n",
    "    # Plot y_target\n",
    "    # viz.histogram(X = n, env=\"001\", win=win_str,\n",
    "    #         opts= dict(title=title_str))\n",
    "    viz.line(X = n,Y= p, env=\"001\", win=win_str, update=\"append\", name='target',\n",
    "            opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "    # Plot y_pred\n",
    "    viz.line(X = x_0,Y= y_pred_1, env=\"001\", win=win_str, update=\"append\", name='pred-1',\n",
    "            opts= dict(title=title_str))\n",
    "\n",
    "    # Plot y_pred_2\n",
    "    viz.line(X = x_0,Y= y_pred_2, env=\"001\", win=win_str, update=\"append\", name='pred-2',\n",
    "            opts= dict(title=title_str))\n",
    "\n",
    "    # # Plot y_input\n",
    "    # # 如果input data长度短，全部画完\n",
    "    # if(len(x_input) <= max_n):\n",
    "    #     viz.line(X = x_input,Y= y_input_0, env=\"001\", win=win_str, update=\"append\", name='GT-1', opts= dict(title=title_str))\n",
    "    #     viz.line(X = x_input,Y= y_input_1, env=\"001\", win=win_str, update=\"append\", name='GT-2', opts= dict(title=title_str))\n",
    "    # # 如果input data长度长，则截断\n",
    "    # else:\n",
    "    #     x_input_0 = x_input[0:max_n]\n",
    "    #     y_input_2 = y_input_0[0:max_n]/y_input_0[0:max_n].sum()\n",
    "    #     y_input_3 = y_input_1[0:max_n]/y_input_1[0:max_n].sum()\n",
    "    #     viz.line(X = x_input_0,Y= y_input_2, env=\"001\", win=win_str, update=\"append\", name='GT-1', opts= dict(title=title_str))\n",
    "    #     viz.line(X = x_input_0,Y= y_input_3, env=\"001\", win=win_str, update=\"append\", name='GT-2', opts= dict(title=title_str))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.3 png格式的net structure"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def draw_the_net_png():\n",
    "\n",
    "    x = torch.randn([1, 3, 300])  # 定义网络的输入值\n",
    "    mlp = MLP_1_1(opt.N_gaussians)\n",
    "    y = mlp(x)                    # 获取网络的预测值\n",
    "\n",
    "    MyConvNetVis = make_dot(y, params=dict(list(mlp.named_parameters()) + [('x', x)]))\n",
    "    MyConvNetVis.format = \"png\"\n",
    "    # 指定文件生成的文件夹\n",
    "    MyConvNetVis.directory = \"data_pic\"\n",
    "    # 生成文件\n",
    "    MyConvNetVis.view()\n",
    "# draw_the_net_png()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.4 plot the loss\n",
    "1. 直接根据参数决定draw什么loss，反正loss都画在一个window里面"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "win_train_loss_str = \"The Loss of BATCH in the Training Data\"\n",
    "win_vali_loss_str = \"The Loss in the Vali Data\"\n",
    "win_train_epoch_loss_str = \"The Loss of EPOCH in the Training Data\"\n",
    "\n",
    "def draw_loss(X_step, loss, win_str):\n",
    "    viz.line(X = [X_step], Y = [loss],win=win_str, update=\"append\",\n",
    "        opts= dict(title=win_str))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.5 plot the metric"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "win_vali_metric_str = \"The NLL of ALL vali data\"\n",
    "def draw_metric(X_step, total_vali_metric, GT_metric):\n",
    "\n",
    "    viz.line(X = [X_step], Y = [[total_vali_metric,GT_metric[0,0],GT_metric[0,1]]],win=win_vali_metric_str, update=\"append\", opts= dict(title=win_vali_metric_str, legend=['pred','GT-1','GT-2'], showlegend=True,xlabel=\"epoch\", ylabel=\"NLL\"))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.6 grad graph\n",
    "https://github.com/t-vi/pytorch-tvmisc/blob/master/visualize/bad_grad_viz.ipynb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "outputs": [],
   "source": [
    "\n",
    "def iter_graph(root, callback):\n",
    "    queue = [root]\n",
    "    seen = set()\n",
    "    while queue:\n",
    "        # 队头出fn然后判断fn是否seen\n",
    "        fn = queue.pop()\n",
    "        if fn in seen:\n",
    "            continue\n",
    "        # unseen加入queue，并且继续沿着fn向下递归（DFS？）\n",
    "        seen.add(fn)\n",
    "        for next_fn, _ in fn.next_functions:\n",
    "            # Alert：只有非none的才会加入queue，然后在register_grad里被fn_dict记录下grad_input\n",
    "            if next_fn is not None:\n",
    "                queue.append(next_fn)\n",
    "        callback(fn)\n",
    "\n",
    "def register_hooks(var):\n",
    "    fn_dict = {}  # 记录了grad名字和值（grad_input）\n",
    "    def hook_cb(fn):\n",
    "        def register_grad(grad_input, grad_output):\n",
    "            fn_dict[fn] = grad_input\n",
    "        fn.register_hook(register_grad)\n",
    "\n",
    "    # 1. 递归注册grad_input\n",
    "    iter_graph(var.grad_fn, hook_cb)\n",
    "\n",
    "    # def is_bad_grad(grad_output):\n",
    "    #     if grad_output is None:\n",
    "    #         return False\n",
    "    #     return grad_output.isnan().any() or (grad_output.abs() >= 1e6).any()\n",
    "    def is_bad_grad(grad_output):\n",
    "        if grad_output is None:\n",
    "                return True\n",
    "        grad_output = grad_output.data\n",
    "        return grad_output.ne(grad_output).any() or grad_output.gt(1e6).any()\n",
    "\n",
    "    # 2. graph已经从grad input构建完了，还有grad output的判断以及颜色的选择\n",
    "    def make_dot():\n",
    "        node_attr = dict(style='filled',\n",
    "                        shape='box',\n",
    "                        align='left',\n",
    "                        fontsize='12',\n",
    "                        ranksep='0.1',\n",
    "                        height='0.2')\n",
    "        dot = Digraph(node_attr=node_attr, graph_attr=dict(size=\"12,12\"))\n",
    "\n",
    "        def size_to_str(size):\n",
    "            return '('+(', ').join(map(str, size))+')'\n",
    "\n",
    "        def build_graph(fn):\n",
    "            if hasattr(fn, 'variable'):  # if GradAccumulator\n",
    "                u = fn.variable\n",
    "                node_name = 'Variable\\n ' + size_to_str(u.size())\n",
    "                dot.node(str(id(u)), node_name, fillcolor='lightblue')\n",
    "            else:  # 不是variable属性，白色\n",
    "                assert fn in fn_dict, fn   # 判断fn在不在fn_dict里面！\n",
    "                fillcolor = 'white'\n",
    "                if any(is_bad_grad(gi) for gi in fn_dict[fn]):\n",
    "                    fillcolor = 'red'\n",
    "                dot.node(str(id(fn)), str(type(fn).__name__), fillcolor=fillcolor)\n",
    "            for next_fn, _ in fn.next_functions:\n",
    "                if next_fn is not None:\n",
    "                    next_id = id(getattr(next_fn, 'variable', next_fn))\n",
    "                    dot.edge(str(next_id), str(id(fn)))\n",
    "        iter_graph(var.grad_fn, build_graph)\n",
    "\n",
    "        return dot\n",
    "\n",
    "    return make_dot\n",
    "\n",
    "# x = torch.randn(10, 10, requires_grad=True)\n",
    "# y = torch.randn(10, 10, requires_grad=True)\n",
    "#\n",
    "# z = x / (y * 0)\n",
    "# z = z.sum() * 2\n",
    "# get_dot = register_hooks(z)\n",
    "\n",
    "\n",
    "# with torch.autograd.detect_anomaly():\n",
    "#     z.backward()\n",
    "#     dot = get_dot()\n",
    "#     dot.save('tmp.dot') # to get .dot\n",
    "#     dot.render('tmp') # to get SVG\n",
    "#     # dot # in Jupyter, you can just render the variable"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6 Training\n",
    "## 6.1 Init"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "       BatchNorm1d-1               [-1, 3, 300]               6\n",
      "            Conv2d-2             [-1, 1, 1, 98]              28\n",
      "       BatchNorm1d-3                   [-1, 98]             196\n",
      "          Softplus-4                   [-1, 98]               0\n",
      "      Conv_block_4-5                   [-1, 98]               0\n",
      "            Conv2d-6             [-1, 1, 1, 98]              28\n",
      "       BatchNorm1d-7                   [-1, 98]             196\n",
      "          Softplus-8                   [-1, 98]               0\n",
      "      Conv_block_4-9                   [-1, 98]               0\n",
      "           Conv2d-10             [-1, 1, 1, 98]              28\n",
      "      BatchNorm1d-11                   [-1, 98]             196\n",
      "         Softplus-12                   [-1, 98]               0\n",
      "     Conv_block_4-13                   [-1, 98]               0\n",
      "           Linear-14                    [-1, 2]             198\n",
      "          Softmax-15                    [-1, 2]               0\n",
      "           Linear-16                    [-1, 2]             198\n",
      "           Linear-17                    [-1, 2]             198\n",
      "================================================================\n",
      "Total params: 1,272\n",
      "Trainable params: 1,272\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.02\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)\n",
    "# mlp = MLP_1_4(opt.N_gaussians)\n",
    "# (9,3)不错\n",
    "mlp = Conv_1_4(opt.N_gaussians,kernel_size=9,stride=3)\n",
    "# mlp = Conv_1_1(opt.N_gaussians,kernel_size=12,stride=3)\n",
    "# mlp = Conv_1_2(opt.N_gaussians)\n",
    "\n",
    "# Init the params\n",
    "# mlp = model_param_init(mlp)\n",
    "\n",
    "# Save the init params\n",
    "# torch.save(mlp.state_dict(), 'mlp_init.pth')\n",
    "\n",
    "# Load the saved model\n",
    "# model_path_WD = \"mlp_train=300_WD.pth\"\n",
    "# model_path_init = 'mlp_init.pth'\n",
    "# model_path_LSE = \"mlp_train=300_LSE.pth\"\n",
    "# model_path_MLE = \"mlp_train=300_MLE.pth\"\n",
    "#\n",
    "# model_data = torch.load(model_path_init)\n",
    "# mlp.load_state_dict(model_data)\n",
    "\n",
    "mlp = mlp.to(device=device)\n",
    "summary(mlp, (3,300))\n",
    "\n",
    "############ learning rate strategy ############\n",
    "# 1.\n",
    "# Set different lr for params\n",
    "# id: id() 函数返回对象的唯一标识符，标识符是一个整数。返回对象的内存地址。\n",
    "shape_params = list(map(id, mlp.z_shape.parameters()))\n",
    "scale_params = list(map(id, mlp.z_scale.parameters()))\n",
    "pi_params = list(map(id, mlp.z_pi.parameters()))\n",
    "\n",
    "\n",
    "params_id = shape_params + scale_params + pi_params\n",
    "\n",
    "base_params = filter(lambda p: id(p) not in params_id, mlp.parameters())\n",
    "params = [{'params': base_params},         # 如果对某个参数不指定学习率，就使用最外层的默认学习率\n",
    "          {'params': mlp.z_pi.parameters(), 'lr': opt.lr_for_pi},\n",
    "        {'params': mlp.z_shape.parameters(), 'lr': opt.learning_rate},\n",
    "          {'params': mlp.z_scale.parameters(), 'lr': opt.lr_for_scale}]\n",
    "\n",
    "optimizer = torch.optim.AdamW(params, lr=opt.learning_rate,weight_decay=opt.weight_decay)\n",
    "\n",
    "# warmup_scheduler = warmup.ExponentialWarmup(optimizer, warmup_period=10)\n",
    "# 2.\n",
    "# Or set lr decay\n",
    "# StepLR为步进，每step_size个epoch，lr*gamma\n",
    "scheduler  = torch.optim.lr_scheduler.StepLR(optimizer,step_size=opt.StepLR_step_size,gamma=0.9)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 70, eta_min = 1e-7)\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.98, last_epoch=-1)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=5, verbose=False, threshold=1e-3, threshold_mode='abs', cooldown=0, min_lr=1e-7, eps=1e-7)\n",
    "# threshold:只关注超过阈值的显著变化；cooldown：触发一次条件后，等待一定epoch再进行检测，避免lr下降过速；\n",
    "\n",
    "# # Set the hooks\n",
    "# #mlp.conv1.register_forward_hook(hook_forward_fn)\n",
    "# mlp.conv1.register_full_backward_hook(hook_backward_fn)\n",
    "#\n",
    "# #mlp.linear1.register_forward_hook(hook_forward_fn)\n",
    "# #mlp.linear1.register_full_backward_hook(hook_backward_fn)\n",
    "#\n",
    "# #mlp.z_pi.register_forward_hook(hook_forward_fn)\n",
    "# mlp.z_pi.register_full_backward_hook(hook_backward_fn_pi)\n",
    "#\n",
    "# #mlp.z_mu.register_forward_hook(hook_forward_fn)\n",
    "# mlp.z_mu.register_full_backward_hook(hook_backward_fn_mu)\n",
    "#\n",
    "# mlp.z_sigma.register_forward_hook(hook_forward_fn)\n",
    "# mlp.z_sigma.register_full_[2, 3, 300]backward_hook(hook_backward_fn_sigma)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir=\"logs-MLP/\"+opt.logs_str,flush_secs=60)\n",
    "\n",
    "# Init the vis win\n",
    "viz.line(X = [0.],Y = [0.], env=\"001\", win=win_train_loss_str, opts= dict(title=win_train_loss_str))\n",
    "viz.line(X = [0.],Y = [0.], env=\"001\", win=win_train_epoch_loss_str, opts= dict(title=win_train_epoch_loss_str))\n",
    "viz.line(X = [0.],Y = [[0.,0.,0.]], env=\"001\", win=win_vali_metric_str, opts= dict(title=win_vali_metric_str,legend=['pred','GT-1','GT-2'], showlegend=True,xlabel=\"epoch\", ylabel=\"NLL\"))\n",
    "\n",
    "mlp.eval()\n",
    "with torch.no_grad():\n",
    "    total_vali_metric, GT_metric = validate(mlp,val_loader,opt.N_gaussians, opt.MIN_LOSS,device)\n",
    "    draw_metric(0, total_vali_metric.cpu(), GT_metric)\n",
    "plot_net(writer,mlp,torch.randn((2,3,300),device=device))\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.2 grad check\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# # grad check\n",
    "# input_test = torch.randn((2,3,300), requires_grad=True,device=device)\n",
    "# test_ans = gradcheck(mlp.to(device), input_test, eps=1e-3)  #, eps=1e-6, atol=1e-4\n",
    "# print(\"Are the gradients correct: \", test_ans)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.3 START HERE\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- 11 batch----\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Now this is EPOCH 0 ,lr is [0.005, 0.001, 0.005, 0.001]==========\n",
      "========== IN EPOCH 0 the total loss is 34222.83172607422 ==========\n",
      "========== IN EPOCH 0 the vali NLL loss is 8.253518104553223 ==========\n",
      "========== IN EPOCH 0 the GT metric is [[ 6.568752 10.179958]] ==========\n",
      "========== Now this is EPOCH 1 ,lr is [0.005, 0.001, 0.005, 0.001]==========\n",
      "========== IN EPOCH 1 the total loss is 17902.85125732422 ==========\n",
      "========== IN EPOCH 1 the vali NLL loss is 7.874137878417969 ==========\n",
      "========== IN EPOCH 1 the GT metric is [[ 6.568752 10.179958]] ==========\n",
      "========== Now this is EPOCH 2 ,lr is [0.005, 0.001, 0.005, 0.001]==========\n",
      "========== IN EPOCH 2 the total loss is 16732.331329345703 ==========\n",
      "========== IN EPOCH 2 the vali NLL loss is 7.429294109344482 ==========\n",
      "========== IN EPOCH 2 the GT metric is [[ 6.5687523 10.179958 ]] ==========\n",
      "========== Now this is EPOCH 3 ,lr is [0.005, 0.001, 0.005, 0.001]==========\n",
      "========== IN EPOCH 3 the total loss is 15105.083435058594 ==========\n",
      "========== IN EPOCH 3 the vali NLL loss is 6.7821245193481445 ==========\n",
      "========== IN EPOCH 3 the GT metric is [[ 6.5687523 10.179957 ]] ==========\n",
      "========== Now this is EPOCH 4 ,lr is [0.005, 0.001, 0.005, 0.001]==========\n",
      "========== IN EPOCH 4 the total loss is 13477.402709960938 ==========\n",
      "========== IN EPOCH 4 the vali NLL loss is 6.62694787979126 ==========\n",
      "========== IN EPOCH 4 the GT metric is [[ 6.5687523 10.179957 ]] ==========\n",
      "========== Now this is EPOCH 5 ,lr is [0.005, 0.001, 0.005, 0.001]==========\n",
      "========== IN EPOCH 5 the total loss is 13251.627838134766 ==========\n",
      "========== IN EPOCH 5 the vali NLL loss is 6.403542518615723 ==========\n",
      "========== IN EPOCH 5 the GT metric is [[ 6.5687523 10.179958 ]] ==========\n",
      "========== Now this is EPOCH 6 ,lr is [0.005, 0.001, 0.005, 0.001]==========\n",
      "========== IN EPOCH 6 the total loss is 13258.827758789062 ==========\n",
      "========== IN EPOCH 6 the vali NLL loss is 6.3881025314331055 ==========\n",
      "========== IN EPOCH 6 the GT metric is [[ 6.568752 10.179958]] ==========\n",
      "========== Now this is EPOCH 7 ,lr is [0.005, 0.001, 0.005, 0.001]==========\n",
      "========== IN EPOCH 7 the total loss is 13232.770599365234 ==========\n",
      "========== IN EPOCH 7 the vali NLL loss is 6.454152584075928 ==========\n",
      "========== IN EPOCH 7 the GT metric is [[ 6.5687523 10.179957 ]] ==========\n",
      "========== Now this is EPOCH 8 ,lr is [0.005, 0.001, 0.005, 0.001]==========\n",
      "========== IN EPOCH 8 the total loss is 13105.103210449219 ==========\n",
      "========== IN EPOCH 8 the vali NLL loss is 6.454122543334961 ==========\n",
      "========== IN EPOCH 8 the GT metric is [[ 6.568753 10.179957]] ==========\n",
      "========== Now this is EPOCH 9 ,lr is [0.005, 0.001, 0.005, 0.001]==========\n",
      "========== IN EPOCH 9 the total loss is 13279.521301269531 ==========\n",
      "========== IN EPOCH 9 the vali NLL loss is 6.413284778594971 ==========\n",
      "========== IN EPOCH 9 the GT metric is [[ 6.5687523 10.179958 ]] ==========\n",
      "========== Now this is EPOCH 10 ,lr is [0.0045000000000000005, 0.0009000000000000001, 0.0045000000000000005, 0.0009000000000000001]==========\n",
      "========== IN EPOCH 10 the total loss is 13055.0126953125 ==========\n",
      "========== IN EPOCH 10 the vali NLL loss is 6.347705841064453 ==========\n",
      "========== IN EPOCH 10 the GT metric is [[ 6.568752 10.179958]] ==========\n",
      "========== Now this is EPOCH 11 ,lr is [0.0045000000000000005, 0.0009000000000000001, 0.0045000000000000005, 0.0009000000000000001]==========\n",
      "========== IN EPOCH 11 the total loss is 13033.041625976562 ==========\n",
      "========== IN EPOCH 11 the vali NLL loss is 6.347450256347656 ==========\n",
      "========== IN EPOCH 11 the GT metric is [[ 6.568752 10.179957]] ==========\n",
      "========== Now this is EPOCH 12 ,lr is [0.0045000000000000005, 0.0009000000000000001, 0.0045000000000000005, 0.0009000000000000001]==========\n",
      "========== IN EPOCH 12 the total loss is 12964.017059326172 ==========\n",
      "========== IN EPOCH 12 the vali NLL loss is 6.399467945098877 ==========\n",
      "========== IN EPOCH 12 the GT metric is [[ 6.568752 10.179957]] ==========\n",
      "========== Now this is EPOCH 13 ,lr is [0.0045000000000000005, 0.0009000000000000001, 0.0045000000000000005, 0.0009000000000000001]==========\n",
      "========== IN EPOCH 13 the total loss is 13082.488372802734 ==========\n",
      "========== IN EPOCH 13 the vali NLL loss is 6.399009704589844 ==========\n",
      "========== IN EPOCH 13 the GT metric is [[ 6.568752 10.179958]] ==========\n",
      "========== Now this is EPOCH 14 ,lr is [0.0045000000000000005, 0.0009000000000000001, 0.0045000000000000005, 0.0009000000000000001]==========\n",
      "========== IN EPOCH 14 the total loss is 12974.607788085938 ==========\n",
      "========== IN EPOCH 14 the vali NLL loss is 6.341836452484131 ==========\n",
      "========== IN EPOCH 14 the GT metric is [[ 6.5687523 10.179957 ]] ==========\n",
      "========== Now this is EPOCH 15 ,lr is [0.0045000000000000005, 0.0009000000000000001, 0.0045000000000000005, 0.0009000000000000001]==========\n",
      "========== IN EPOCH 15 the total loss is 12950.774444580078 ==========\n",
      "========== IN EPOCH 15 the vali NLL loss is 6.355963230133057 ==========\n",
      "========== IN EPOCH 15 the GT metric is [[ 6.568752 10.179958]] ==========\n",
      "========== Now this is EPOCH 16 ,lr is [0.0045000000000000005, 0.0009000000000000001, 0.0045000000000000005, 0.0009000000000000001]==========\n",
      "========== IN EPOCH 16 the total loss is 13042.205474853516 ==========\n",
      "========== IN EPOCH 16 the vali NLL loss is 6.361716270446777 ==========\n",
      "========== IN EPOCH 16 the GT metric is [[ 6.5687523 10.179958 ]] ==========\n",
      "========== Now this is EPOCH 17 ,lr is [0.0045000000000000005, 0.0009000000000000001, 0.0045000000000000005, 0.0009000000000000001]==========\n",
      "========== IN EPOCH 17 the total loss is 12925.267608642578 ==========\n",
      "========== IN EPOCH 17 the vali NLL loss is 6.318943500518799 ==========\n",
      "========== IN EPOCH 17 the GT metric is [[ 6.568753 10.179958]] ==========\n",
      "========== Now this is EPOCH 18 ,lr is [0.0045000000000000005, 0.0009000000000000001, 0.0045000000000000005, 0.0009000000000000001]==========\n",
      "========== IN EPOCH 18 the total loss is 12937.621368408203 ==========\n",
      "========== IN EPOCH 18 the vali NLL loss is 6.340517997741699 ==========\n",
      "========== IN EPOCH 18 the GT metric is [[ 6.568752 10.179958]] ==========\n",
      "========== Now this is EPOCH 19 ,lr is [0.0045000000000000005, 0.0009000000000000001, 0.0045000000000000005, 0.0009000000000000001]==========\n",
      "========== IN EPOCH 19 the total loss is 12951.97152709961 ==========\n",
      "========== IN EPOCH 19 the vali NLL loss is 6.325437068939209 ==========\n",
      "========== IN EPOCH 19 the GT metric is [[ 6.568752 10.179957]] ==========\n",
      "========== Now this is EPOCH 20 ,lr is [0.004050000000000001, 0.0008100000000000001, 0.004050000000000001, 0.0008100000000000001]==========\n",
      "========== IN EPOCH 20 the total loss is 13399.110595703125 ==========\n",
      "========== IN EPOCH 20 the vali NLL loss is 6.370272636413574 ==========\n",
      "========== IN EPOCH 20 the GT metric is [[ 6.568752 10.179958]] ==========\n",
      "========== Now this is EPOCH 21 ,lr is [0.004050000000000001, 0.0008100000000000001, 0.004050000000000001, 0.0008100000000000001]==========\n",
      "========== IN EPOCH 21 the total loss is 14726.932250976562 ==========\n",
      "========== IN EPOCH 21 the vali NLL loss is 7.470520973205566 ==========\n",
      "========== IN EPOCH 21 the GT metric is [[ 6.568752 10.179957]] ==========\n",
      "========== Now this is EPOCH 22 ,lr is [0.004050000000000001, 0.0008100000000000001, 0.004050000000000001, 0.0008100000000000001]==========\n",
      "========== IN EPOCH 22 the total loss is 14559.364379882812 ==========\n",
      "========== IN EPOCH 22 the vali NLL loss is 6.623603343963623 ==========\n",
      "========== IN EPOCH 22 the GT metric is [[ 6.568752 10.179958]] ==========\n",
      "========== Now this is EPOCH 23 ,lr is [0.004050000000000001, 0.0008100000000000001, 0.004050000000000001, 0.0008100000000000001]==========\n",
      "========== IN EPOCH 23 the total loss is 13405.312713623047 ==========\n",
      "========== IN EPOCH 23 the vali NLL loss is 6.508145809173584 ==========\n",
      "========== IN EPOCH 23 the GT metric is [[ 6.568752 10.179958]] ==========\n",
      "========== Now this is EPOCH 24 ,lr is [0.004050000000000001, 0.0008100000000000001, 0.004050000000000001, 0.0008100000000000001]==========\n",
      "========== IN EPOCH 24 the total loss is 13158.708251953125 ==========\n",
      "========== IN EPOCH 24 the vali NLL loss is 6.3817572593688965 ==========\n",
      "========== IN EPOCH 24 the GT metric is [[ 6.568752 10.179957]] ==========\n",
      "========== Now this is EPOCH 25 ,lr is [0.004050000000000001, 0.0008100000000000001, 0.004050000000000001, 0.0008100000000000001]==========\n",
      "========== IN EPOCH 25 the total loss is 12953.848388671875 ==========\n",
      "========== IN EPOCH 25 the vali NLL loss is 6.35203742980957 ==========\n",
      "========== IN EPOCH 25 the GT metric is [[ 6.568752 10.179958]] ==========\n",
      "========== Now this is EPOCH 26 ,lr is [0.004050000000000001, 0.0008100000000000001, 0.004050000000000001, 0.0008100000000000001]==========\n",
      "========== IN EPOCH 26 the total loss is 12913.169250488281 ==========\n",
      "========== IN EPOCH 26 the vali NLL loss is 6.343273162841797 ==========\n",
      "========== IN EPOCH 26 the GT metric is [[ 6.568752 10.179958]] ==========\n",
      "========== Now this is EPOCH 27 ,lr is [0.004050000000000001, 0.0008100000000000001, 0.004050000000000001, 0.0008100000000000001]==========\n",
      "========== IN EPOCH 27 the total loss is 12883.321868896484 ==========\n",
      "========== IN EPOCH 27 the vali NLL loss is 6.356270790100098 ==========\n",
      "========== IN EPOCH 27 the GT metric is [[ 6.5687523 10.179957 ]] ==========\n",
      "========== Now this is EPOCH 28 ,lr is [0.004050000000000001, 0.0008100000000000001, 0.004050000000000001, 0.0008100000000000001]==========\n",
      "========== IN EPOCH 28 the total loss is 12863.914978027344 ==========\n",
      "========== IN EPOCH 28 the vali NLL loss is 6.335471153259277 ==========\n",
      "========== IN EPOCH 28 the GT metric is [[ 6.568752 10.179958]] ==========\n",
      "========== Now this is EPOCH 29 ,lr is [0.004050000000000001, 0.0008100000000000001, 0.004050000000000001, 0.0008100000000000001]==========\n",
      "========== IN EPOCH 29 the total loss is 13032.075653076172 ==========\n",
      "========== IN EPOCH 29 the vali NLL loss is 6.3549652099609375 ==========\n",
      "========== IN EPOCH 29 the GT metric is [[ 6.568752 10.179957]] ==========\n",
      "========== Now this is EPOCH 30 ,lr is [0.0036450000000000007, 0.000729, 0.0036450000000000007, 0.000729]==========\n",
      "========== IN EPOCH 30 the total loss is 12941.569152832031 ==========\n",
      "========== IN EPOCH 30 the vali NLL loss is 6.333261489868164 ==========\n",
      "========== IN EPOCH 30 the GT metric is [[ 6.568752 10.179957]] ==========\n",
      "========== Now this is EPOCH 31 ,lr is [0.0036450000000000007, 0.000729, 0.0036450000000000007, 0.000729]==========\n",
      "========== IN EPOCH 31 the total loss is 12947.74853515625 ==========\n",
      "========== IN EPOCH 31 the vali NLL loss is 6.329347610473633 ==========\n",
      "========== IN EPOCH 31 the GT metric is [[ 6.5687523 10.179957 ]] ==========\n",
      "========== Now this is EPOCH 32 ,lr is [0.0036450000000000007, 0.000729, 0.0036450000000000007, 0.000729]==========\n",
      "========== IN EPOCH 32 the total loss is 12885.116760253906 ==========\n",
      "========== IN EPOCH 32 the vali NLL loss is 6.336160182952881 ==========\n",
      "========== IN EPOCH 32 the GT metric is [[ 6.5687523 10.179957 ]] ==========\n",
      "========== Now this is EPOCH 33 ,lr is [0.0036450000000000007, 0.000729, 0.0036450000000000007, 0.000729]==========\n",
      "========== IN EPOCH 33 the total loss is 12925.948028564453 ==========\n",
      "========== IN EPOCH 33 the vali NLL loss is 6.344798564910889 ==========\n",
      "========== IN EPOCH 33 the GT metric is [[ 6.5687523 10.179958 ]] ==========\n",
      "========== Now this is EPOCH 34 ,lr is [0.0036450000000000007, 0.000729, 0.0036450000000000007, 0.000729]==========\n",
      "========== IN EPOCH 34 the total loss is 12904.491027832031 ==========\n",
      "========== IN EPOCH 34 the vali NLL loss is 6.321930408477783 ==========\n",
      "========== IN EPOCH 34 the GT metric is [[ 6.568752 10.179957]] ==========\n",
      "========== Now this is EPOCH 35 ,lr is [0.0036450000000000007, 0.000729, 0.0036450000000000007, 0.000729]==========\n",
      "========== IN EPOCH 35 the total loss is 12815.673553466797 ==========\n",
      "========== IN EPOCH 35 the vali NLL loss is 6.320814609527588 ==========\n",
      "========== IN EPOCH 35 the GT metric is [[ 6.568752 10.179958]] ==========\n",
      "========== Now this is EPOCH 36 ,lr is [0.0036450000000000007, 0.000729, 0.0036450000000000007, 0.000729]==========\n",
      "========== IN EPOCH 36 the total loss is 12833.822845458984 ==========\n",
      "========== IN EPOCH 36 the vali NLL loss is 6.317509174346924 ==========\n",
      "========== IN EPOCH 36 the GT metric is [[ 6.5687513 10.179958 ]] ==========\n",
      "========== Now this is EPOCH 37 ,lr is [0.0036450000000000007, 0.000729, 0.0036450000000000007, 0.000729]==========\n",
      "========== IN EPOCH 37 the total loss is 13005.451629638672 ==========\n",
      "========== IN EPOCH 37 the vali NLL loss is 6.341925621032715 ==========\n",
      "========== IN EPOCH 37 the GT metric is [[ 6.5687523 10.179958 ]] ==========\n",
      "========== Now this is EPOCH 38 ,lr is [0.0036450000000000007, 0.000729, 0.0036450000000000007, 0.000729]==========\n",
      "========== IN EPOCH 38 the total loss is 12901.592895507812 ==========\n",
      "========== IN EPOCH 38 the vali NLL loss is 6.324520111083984 ==========\n",
      "========== IN EPOCH 38 the GT metric is [[ 6.568752 10.179957]] ==========\n",
      "========== Now this is EPOCH 39 ,lr is [0.0036450000000000007, 0.000729, 0.0036450000000000007, 0.000729]==========\n",
      "========== IN EPOCH 39 the total loss is 12910.850372314453 ==========\n",
      "========== IN EPOCH 39 the vali NLL loss is 6.311925411224365 ==========\n",
      "========== IN EPOCH 39 the GT metric is [[ 6.5687523 10.179957 ]] ==========\n",
      "========== Now this is EPOCH 40 ,lr is [0.003280500000000001, 0.0006561000000000001, 0.003280500000000001, 0.0006561000000000001]==========\n",
      "========== IN EPOCH 40 the total loss is 12826.820037841797 ==========\n",
      "========== IN EPOCH 40 the vali NLL loss is 6.316875457763672 ==========\n",
      "========== IN EPOCH 40 the GT metric is [[ 6.5687523 10.179958 ]] ==========\n",
      "========== Now this is EPOCH 41 ,lr is [0.003280500000000001, 0.0006561000000000001, 0.003280500000000001, 0.0006561000000000001]==========\n",
      "========== IN EPOCH 41 the total loss is 12874.963470458984 ==========\n",
      "========== IN EPOCH 41 the vali NLL loss is 6.3102617263793945 ==========\n",
      "========== IN EPOCH 41 the GT metric is [[ 6.5687523 10.179957 ]] ==========\n",
      "========== Now this is EPOCH 42 ,lr is [0.003280500000000001, 0.0006561000000000001, 0.003280500000000001, 0.0006561000000000001]==========\n",
      "========== IN EPOCH 42 the total loss is 12839.58236694336 ==========\n",
      "========== IN EPOCH 42 the vali NLL loss is 6.311585903167725 ==========\n",
      "========== IN EPOCH 42 the GT metric is [[ 6.568752 10.179958]] ==========\n",
      "========== Now this is EPOCH 43 ,lr is [0.003280500000000001, 0.0006561000000000001, 0.003280500000000001, 0.0006561000000000001]==========\n",
      "========== IN EPOCH 43 the total loss is 12878.184173583984 ==========\n",
      "========== IN EPOCH 43 the vali NLL loss is 6.3186869621276855 ==========\n",
      "========== IN EPOCH 43 the GT metric is [[ 6.568752 10.179957]] ==========\n",
      "========== Now this is EPOCH 44 ,lr is [0.003280500000000001, 0.0006561000000000001, 0.003280500000000001, 0.0006561000000000001]==========\n",
      "========== IN EPOCH 44 the total loss is 12916.443115234375 ==========\n",
      "========== IN EPOCH 44 the vali NLL loss is 6.345091819763184 ==========\n",
      "========== IN EPOCH 44 the GT metric is [[ 6.568752 10.179958]] ==========\n",
      "========== Now this is EPOCH 45 ,lr is [0.003280500000000001, 0.0006561000000000001, 0.003280500000000001, 0.0006561000000000001]==========\n",
      "========== IN EPOCH 45 the total loss is 12944.920196533203 ==========\n",
      "========== IN EPOCH 45 the vali NLL loss is 6.3313469886779785 ==========\n",
      "========== IN EPOCH 45 the GT metric is [[ 6.5687523 10.179958 ]] ==========\n",
      "========== Now this is EPOCH 46 ,lr is [0.003280500000000001, 0.0006561000000000001, 0.003280500000000001, 0.0006561000000000001]==========\n",
      "========== IN EPOCH 46 the total loss is 12914.176513671875 ==========\n",
      "========== IN EPOCH 46 the vali NLL loss is 6.314290523529053 ==========\n",
      "========== IN EPOCH 46 the GT metric is [[ 6.568752 10.179958]] ==========\n",
      "========== Now this is EPOCH 47 ,lr is [0.003280500000000001, 0.0006561000000000001, 0.003280500000000001, 0.0006561000000000001]==========\n",
      "========== IN EPOCH 47 the total loss is 12877.381652832031 ==========\n",
      "========== IN EPOCH 47 the vali NLL loss is 6.316760540008545 ==========\n",
      "========== IN EPOCH 47 the GT metric is [[ 6.568752 10.179958]] ==========\n",
      "========== Now this is EPOCH 48 ,lr is [0.003280500000000001, 0.0006561000000000001, 0.003280500000000001, 0.0006561000000000001]==========\n",
      "========== IN EPOCH 48 the total loss is 12831.60385131836 ==========\n",
      "========== IN EPOCH 48 the vali NLL loss is 6.31328821182251 ==========\n",
      "========== IN EPOCH 48 the GT metric is [[ 6.5687523 10.179957 ]] ==========\n",
      "========== Now this is EPOCH 49 ,lr is [0.003280500000000001, 0.0006561000000000001, 0.003280500000000001, 0.0006561000000000001]==========\n",
      "========== IN EPOCH 49 the total loss is 12798.737335205078 ==========\n",
      "========== IN EPOCH 49 the vali NLL loss is 6.309887409210205 ==========\n",
      "========== IN EPOCH 49 the GT metric is [[ 6.568752 10.179957]] ==========\n",
      "========== Now this is EPOCH 50 ,lr is [0.002952450000000001, 0.00059049, 0.002952450000000001, 0.00059049]==========\n",
      "========== IN EPOCH 50 the total loss is 12834.819274902344 ==========\n",
      "========== IN EPOCH 50 the vali NLL loss is 6.314210891723633 ==========\n",
      "========== IN EPOCH 50 the GT metric is [[ 6.5687523 10.179957 ]] ==========\n",
      "========== Now this is EPOCH 51 ,lr is [0.002952450000000001, 0.00059049, 0.002952450000000001, 0.00059049]==========\n",
      "========== IN EPOCH 51 the total loss is 12816.607849121094 ==========\n",
      "========== IN EPOCH 51 the vali NLL loss is 6.307675361633301 ==========\n",
      "========== IN EPOCH 51 the GT metric is [[ 6.568752 10.179958]] ==========\n",
      "========== Now this is EPOCH 52 ,lr is [0.002952450000000001, 0.00059049, 0.002952450000000001, 0.00059049]==========\n",
      "========== IN EPOCH 52 the total loss is 12924.595336914062 ==========\n",
      "========== IN EPOCH 52 the vali NLL loss is 6.34205436706543 ==========\n",
      "========== IN EPOCH 52 the GT metric is [[ 6.5687513 10.179958 ]] ==========\n",
      "========== Now this is EPOCH 53 ,lr is [0.002952450000000001, 0.00059049, 0.002952450000000001, 0.00059049]==========\n",
      "========== IN EPOCH 53 the total loss is 12884.193725585938 ==========\n",
      "========== IN EPOCH 53 the vali NLL loss is 6.31958532333374 ==========\n",
      "========== IN EPOCH 53 the GT metric is [[ 6.5687523 10.179958 ]] ==========\n",
      "========== Now this is EPOCH 54 ,lr is [0.002952450000000001, 0.00059049, 0.002952450000000001, 0.00059049]==========\n",
      "========== IN EPOCH 54 the total loss is 12802.416534423828 ==========\n",
      "========== IN EPOCH 54 the vali NLL loss is 6.3017497062683105 ==========\n",
      "========== IN EPOCH 54 the GT metric is [[ 6.568752 10.179958]] ==========\n",
      "========== Now this is EPOCH 55 ,lr is [0.002952450000000001, 0.00059049, 0.002952450000000001, 0.00059049]==========\n",
      "========== IN EPOCH 55 the total loss is 12802.343383789062 ==========\n",
      "========== IN EPOCH 55 the vali NLL loss is 6.301631927490234 ==========\n",
      "========== IN EPOCH 55 the GT metric is [[ 6.568752 10.179958]] ==========\n",
      "========== Now this is EPOCH 56 ,lr is [0.002952450000000001, 0.00059049, 0.002952450000000001, 0.00059049]==========\n",
      "========== IN EPOCH 56 the total loss is 12942.11083984375 ==========\n",
      "========== IN EPOCH 56 the vali NLL loss is 6.30908203125 ==========\n",
      "========== IN EPOCH 56 the GT metric is [[ 6.568752 10.179958]] ==========\n",
      "========== Now this is EPOCH 57 ,lr is [0.002952450000000001, 0.00059049, 0.002952450000000001, 0.00059049]==========\n",
      "========== IN EPOCH 57 the total loss is 12864.873077392578 ==========\n",
      "========== IN EPOCH 57 the vali NLL loss is 6.329512119293213 ==========\n",
      "========== IN EPOCH 57 the GT metric is [[ 6.5687523 10.179957 ]] ==========\n",
      "========== Now this is EPOCH 58 ,lr is [0.002952450000000001, 0.00059049, 0.002952450000000001, 0.00059049]==========\n",
      "========== IN EPOCH 58 the total loss is 12891.135757446289 ==========\n",
      "========== IN EPOCH 58 the vali NLL loss is 6.320839881896973 ==========\n",
      "========== IN EPOCH 58 the GT metric is [[ 6.5687523 10.179957 ]] ==========\n",
      "========== Now this is EPOCH 59 ,lr is [0.002952450000000001, 0.00059049, 0.002952450000000001, 0.00059049]==========\n",
      "========== IN EPOCH 59 the total loss is 12832.021362304688 ==========\n",
      "========== IN EPOCH 59 the vali NLL loss is 6.328600883483887 ==========\n",
      "========== IN EPOCH 59 the GT metric is [[ 6.568752 10.179958]] ==========\n",
      "========== Now this is EPOCH 60 ,lr is [0.002657205000000001, 0.000531441, 0.002657205000000001, 0.000531441]==========\n",
      "========== IN EPOCH 60 the total loss is 12805.181823730469 ==========\n",
      "========== IN EPOCH 60 the vali NLL loss is 6.304382801055908 ==========\n",
      "========== IN EPOCH 60 the GT metric is [[ 6.568753 10.179958]] ==========\n",
      "========== Now this is EPOCH 61 ,lr is [0.002657205000000001, 0.000531441, 0.002657205000000001, 0.000531441]==========\n",
      "========== IN EPOCH 61 the total loss is 12904.225189208984 ==========\n",
      "========== IN EPOCH 61 the vali NLL loss is 6.315837383270264 ==========\n",
      "========== IN EPOCH 61 the GT metric is [[ 6.5687523 10.179958 ]] ==========\n",
      "========== Now this is EPOCH 62 ,lr is [0.002657205000000001, 0.000531441, 0.002657205000000001, 0.000531441]==========\n",
      "========== IN EPOCH 62 the total loss is 12765.532623291016 ==========\n",
      "========== IN EPOCH 62 the vali NLL loss is 6.296954154968262 ==========\n",
      "========== IN EPOCH 62 the GT metric is [[ 6.5687523 10.179958 ]] ==========\n",
      "========== Now this is EPOCH 63 ,lr is [0.002657205000000001, 0.000531441, 0.002657205000000001, 0.000531441]==========\n",
      "========== IN EPOCH 63 the total loss is 12751.942199707031 ==========\n",
      "========== IN EPOCH 63 the vali NLL loss is 6.296604633331299 ==========\n",
      "========== IN EPOCH 63 the GT metric is [[ 6.568752 10.179958]] ==========\n",
      "========== Now this is EPOCH 64 ,lr is [0.002657205000000001, 0.000531441, 0.002657205000000001, 0.000531441]==========\n",
      "========== IN EPOCH 64 the total loss is 12719.789367675781 ==========\n",
      "========== IN EPOCH 64 the vali NLL loss is 6.29997444152832 ==========\n",
      "========== IN EPOCH 64 the GT metric is [[ 6.568752 10.179958]] ==========\n",
      "========== Now this is EPOCH 65 ,lr is [0.002657205000000001, 0.000531441, 0.002657205000000001, 0.000531441]==========\n",
      "========== IN EPOCH 65 the total loss is 12789.445037841797 ==========\n",
      "========== IN EPOCH 65 the vali NLL loss is 6.306156158447266 ==========\n",
      "========== IN EPOCH 65 the GT metric is [[ 6.568752 10.179958]] ==========\n",
      "========== Now this is EPOCH 66 ,lr is [0.002657205000000001, 0.000531441, 0.002657205000000001, 0.000531441]==========\n",
      "========== IN EPOCH 66 the total loss is 12834.036590576172 ==========\n",
      "========== IN EPOCH 66 the vali NLL loss is 6.30100679397583 ==========\n",
      "========== IN EPOCH 66 the GT metric is [[ 6.568752 10.179957]] ==========\n",
      "========== Now this is EPOCH 67 ,lr is [0.002657205000000001, 0.000531441, 0.002657205000000001, 0.000531441]==========\n",
      "========== IN EPOCH 67 the total loss is 12778.945587158203 ==========\n",
      "========== IN EPOCH 67 the vali NLL loss is 6.29717493057251 ==========\n",
      "========== IN EPOCH 67 the GT metric is [[ 6.5687523 10.179958 ]] ==========\n",
      "========== Now this is EPOCH 68 ,lr is [0.002657205000000001, 0.000531441, 0.002657205000000001, 0.000531441]==========\n",
      "========== IN EPOCH 68 the total loss is 12838.605224609375 ==========\n",
      "========== IN EPOCH 68 the vali NLL loss is 6.2996039390563965 ==========\n",
      "========== IN EPOCH 68 the GT metric is [[ 6.5687523 10.179958 ]] ==========\n",
      "========== Now this is EPOCH 69 ,lr is [0.002657205000000001, 0.000531441, 0.002657205000000001, 0.000531441]==========\n",
      "========== IN EPOCH 69 the total loss is 12790.801666259766 ==========\n",
      "========== IN EPOCH 69 the vali NLL loss is 6.290639400482178 ==========\n",
      "========== IN EPOCH 69 the GT metric is [[ 6.568752 10.179957]] ==========\n",
      "Total training time when epoch= *70* is *659.2664291858673 *s\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# filename = \"../log_file.txt\"\n",
    "# f = open(filename,'w')\n",
    "total_train_step = 0\n",
    "EPOCH_NUM = 70\n",
    "\n",
    "train_start_time = time.time()\n",
    "mlp.train()\n",
    "\n",
    "for epoch in range(EPOCH_NUM):\n",
    "    # # Save the net structure\n",
    "    # net_file_name = \"net_before_epoch\"+str(epoch)+\".pth\"\n",
    "    # net_path = opt.net_root_path + net_file_name\n",
    "    # torch.save(mlp.state_dict(), net_path)\n",
    "\n",
    "    epoch_train_loss = 0\n",
    "    # plot_conv(writer,mlp,epoch)\n",
    "    print(f\"========== Now this is EPOCH {epoch} ,lr is {scheduler.get_last_lr()}==========\")\n",
    "    for batch_id,data in enumerate(train_loader):\n",
    "\n",
    "        input_data, target_plot, target_loss, setting, _ = data\n",
    "        # print(f\"---- {batch_id} batch----\")\n",
    "        # Do the inference\n",
    "        input_data = input_data.to(device)\n",
    "        target_loss = target_loss.to(device)\n",
    "        target_plot = target_plot.to(device)\n",
    "        pi, mu, sigma = mlp(input_data)\n",
    "\n",
    "        # Save the params\n",
    "        # params = list(mlp.named_parameters())\n",
    "\n",
    "        # Cal the MLE loss and draw the distrb.\n",
    "        # loss = loss_fn_v2(pi, mu, sigma, target_loss, opt.N_gaussians,opt.TARGET, opt.SAFETY, device)\n",
    "        loss = loss_fn_wei(pi, mu, sigma, target_loss, opt.N_gaussians,opt.TARGET, opt.SAFETY, device)\n",
    "        # loss = loss_fn_v3(pi, mu, sigma, target_loss, opt.N_gaussians,device)\n",
    "        # loss = loss_fn_v5(pi, mu, sigma, target_loss, opt.N_gaussians,device)\n",
    "\n",
    "        # Cal the CDF loss and draw the distrb.\n",
    "        # loss = loss_fn_cdf(pi, mu, sigma, target, opt.N_gaussians)\n",
    "\n",
    "        # Cal the CE or KL loss and draw the distrb.\n",
    "        # loss  = loss_fn_CE(pi, mu, sigma, target_loss, opt.N_gaussians,opt.TARGET, opt.SAFETY, device)\n",
    "\n",
    "        # Cal the WD loss and draw the distrb.\n",
    "        # loss  = loss_fn_WD(pi, mu, sigma, target_loss, opt.N_gaussians,opt.TARGET,device)\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "        draw_loss(total_train_step, loss.item(),win_train_loss_str)\n",
    "\n",
    "        # Optim\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        ########### before step() ###############\n",
    "\n",
    "        # for name, parms in mlp.named_parameters():\n",
    "        #     print(\"=====before step()=====\")\n",
    "        #     print('-->name:', name)\n",
    "        #     print('-->params:', parms.data)\n",
    "        #     # print('-->grad_requirs:',parms.requires_grad)\n",
    "        #     print('-->grad_value:',parms.grad)\n",
    "        #     print(\"===\")\n",
    "\n",
    "        ######\n",
    "\n",
    "        # 反向传播时检测是否有异常值，定位code\n",
    "        # with torch.autograd.detect_anomaly():\n",
    "        #     loss.backward()\n",
    "\n",
    "        ######\n",
    "        # get_dot = register_hooks(loss)\n",
    "        # dot = get_dot()\n",
    "        # dot.save('tmp.dot')   # to get .dot\n",
    "        # dot.render('tmp')     # to get pdf\n",
    "\n",
    "        ######\n",
    "        # Print grad check\n",
    "        # v_n = []    # name\n",
    "        # v_v = []    # value\n",
    "        # v_g = []    # grad\n",
    "        # for name, parameter in mlp.named_parameters():\n",
    "        #     v_n.append(name)\n",
    "        #     v_v.append(parameter.detach().cpu().numpy() if parameter is not None else [0])\n",
    "        #     v_g.append(parameter.grad.detach().cpu().numpy() if parameter.grad is not None else [0])\n",
    "        # for i in range(len(v_n)):\n",
    "        #     # if np.max(v_v[i]).item() - np.min(v_v[i]).item() < 1e-6:\n",
    "        #     #     color = bcolors.FAIL + '*'\n",
    "        #     if np.isnan(v_g[i]).any() or np.isnan(v_v[i]).any():\n",
    "        #         color = bcolors.FAIL + '*'\n",
    "        #     else:\n",
    "        #         color = bcolors.OKGREEN + ' '\n",
    "        #     print('%svalue %s: %.3e ~ %.3e' % (color, v_n[i], np.min(v_v[i]).item(), np.max(v_v[i]).item()))\n",
    "        #     print('%sgrad  %s: %.3e ~ %.3e' % (color, v_n[i], np.min(v_g[i]).item(), np.max(v_g[i]).item()))\n",
    "\n",
    "        ######\n",
    "        optimizer.step()\n",
    "\n",
    "        # for name, parms in mlp.named_parameters():\n",
    "        #     print(\"=====After step()=====\")\n",
    "        #     print('-->name:', name)\n",
    "        #     print('-->params:', parms.data)\n",
    "        #     # print('-->grad_requirs:',parms.requires_grad)\n",
    "        #     print('-->grad_value:',parms.grad)\n",
    "        #     print(\"===\")\n",
    "\n",
    "        # Only draw the 1st result in a training batch (5 in total)\n",
    "        # if total_train_step % 20 == 0:\n",
    "        #     with torch.no_grad():\n",
    "        #         # draw_mdn_2(pi[0,:].clone().detach(), mu[0,:].clone().detach(), sigma[0,:].clone().detach(), target_plot[0].clone().detach(), input_data[0].clone().detach(),total_train_step, opt.N_gaussians)\n",
    "        #         draw_mdn_wei(pi[0,:].clone().detach(), mu[0,:].clone().detach(), sigma[0,:].clone().detach(), target_plot[0].clone().detach(), input_data[0].clone().detach(),total_train_step, opt.N_gaussians)\n",
    "        #         # draw_mdn_cdf(pi[0,:].detach(), m[0], target[0].detach(), total_train_step, loss_list[0],opt.N_gaussians)\n",
    "        total_train_step += 1\n",
    "\n",
    "    ########### Do validation\n",
    "\n",
    "    mlp.eval()\n",
    "    with torch.no_grad():\n",
    "        total_vali_metric, GT_metric = validate(mlp,val_loader,opt.N_gaussians, opt.MIN_LOSS,device)\n",
    "        draw_metric(epoch+1, total_vali_metric.cpu(), GT_metric)\n",
    "        writer.add_scalars(\"metric/\"+opt.tag_str,{\"pred\":total_vali_metric.cpu(),\n",
    "                                     \"GT-1\":GT_metric[0,0],\n",
    "                                     \"GT-2\":GT_metric[0,1]},epoch)\n",
    "    mlp.train()\n",
    "\n",
    "    # Plot the loss in this EPOCH\n",
    "    print(f\"========== IN EPOCH {epoch} the total loss is {epoch_train_loss} ==========\")\n",
    "    print(f\"========== IN EPOCH {epoch} the vali NLL loss is {total_vali_metric.detach().cpu().numpy()} ==========\")\n",
    "    print(f\"========== IN EPOCH {epoch} the GT metric is {GT_metric.detach().cpu().numpy()} ==========\")\n",
    "\n",
    "    # # Save the net structure\n",
    "    # net_file_name = \"net_after_epoch\"+str(epoch)+\".pth\"\n",
    "    # net_path = opt.net_root_path + net_file_name\n",
    "    # torch.save(mlp.state_dict(), net_path)\n",
    "\n",
    "    # Plot loss of this EPOCH\n",
    "    draw_loss(epoch, epoch_train_loss,win_train_epoch_loss_str)\n",
    "    # writer.add_scalars(\"Loss of EPOCH\",{\"Train\":epoch_train_loss},epoch)\n",
    "    # scheduler.step(total_vali_metric)\n",
    "    scheduler.step()\n",
    "\n",
    "    if epoch%3==0:\n",
    "        # Record the weight\n",
    "        plot_conv_weight(writer,mlp,epoch,opt.tag_str)\n",
    "        plot_mu_weight(writer,mlp,epoch,opt.tag_str)\n",
    "        plot_pi_weight(writer,mlp,epoch,opt.tag_str)\n",
    "        plot_sigma_weight(writer,mlp,epoch,opt.tag_str)\n",
    "\n",
    "# f.close()\n",
    "train_end_time = time.time()\n",
    "print(f\"Total training time when epoch= *{EPOCH_NUM}* is *{train_end_time-train_start_time} *s\")\n",
    "writer.close()\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- save"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# model_path_MLE = \"mlp_train=300_MLE.pth\"\n",
    "model_path_MLP = \"mlp_train=300_MLP.pth\"\n",
    "model_path_Conv = \"mlp_train=300_Conv.pth\"\n",
    "model_path_LSE = \"mlp_train=300_LSE.pth\"\n",
    "torch.save(mlp.state_dict(), model_path_Conv)\n",
    "# torch.save(mlp.state_dict(), model_path_MLE)\n",
    "\n",
    "# viz.delete_env(\"001_test\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# Gtmp = pgv.AGraph('tmp.dot')\n",
    "# G = nx.Graph(Gtmp)\n",
    "# nx.draw(G)\n",
    "# # plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "$\\mathcal{L}(y \\vert x) = - \\log\\bigg\\{\\sum_{k=1}^K \\pi_k(x)  \\mathcal{N}\\big(y \\vert \\mu_k(x), \\Sigma_k(x)\\big)\\bigg\\}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Load and test\n",
    "## 7.1 load and init"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "Model_name = \"Conv_N_g=2\"\n",
    "\n",
    "model_path_MLP = \"mlp_train=300_MLP.pth\"\n",
    "model_path_Conv = \"mlp_train=300_Conv.pth\"\n",
    "model_path_init = \"mlp_init.pth\"\n",
    "# mlp = Conv_1_1(opt.N_gaussians,kernel_size=12,stride=3)\n",
    "mlp = Conv_1_4(opt.N_gaussians,kernel_size=9,stride=3)\n",
    "# mlp = MLP_1_4(opt.N_gaussians)\n",
    "model_data = torch.load(model_path_Conv)\n",
    "mlp.load_state_dict(model_data)\n",
    "\n",
    "\n",
    "mlp = mlp.to(device=device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 计算metric"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "\n",
    "win_pi_str = \"pi distrb-\"+Model_name\n",
    "win_mu_str = \"mu distrb-\"+Model_name\n",
    "win_sigma_str = \"sigma distrb-\"+Model_name\n",
    "\n",
    "win_scale_str = \"scale distrb-\"+Model_name\n",
    "win_shape_str = \"shape distrb-\"+Model_name"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== IN Test dataset, NN Model:  6.412590980529785 ==========\n",
      "========== IN Test dataset, the GTs: [[ 6.857092 10.584564]] ==========\n",
      "========== The diff: 0.4445009231567383 ==========\n"
     ]
    }
   ],
   "source": [
    "mlp.eval()\n",
    "with torch.no_grad():\n",
    "    #Compute the metric\n",
    "    total_test_metric, GT_metric = validate(mlp,test_loader,opt.N_gaussians, opt.MIN_LOSS,device)\n",
    "    draw_metric(0, total_test_metric.cpu(), GT_metric)\n",
    "    print(f\"========== IN Test dataset, NN Model:  {total_test_metric.detach().cpu().numpy()} ==========\")\n",
    "    print(f\"========== IN Test dataset, the GTs: {GT_metric.detach().cpu().numpy()} ==========\")\n",
    "    print(f\"========== The diff: {GT_metric.detach().cpu().numpy()[0,0] - total_test_metric.detach().cpu().numpy()} ==========\")\n",
    "\n",
    "    # Save pi and mu sigma\n",
    "    cnt = 0\n",
    "    pi = torch.tensor([])\n",
    "    mu = torch.tensor([])\n",
    "    sigma = torch.tensor([])\n",
    "    for test_batch_id, test_data in enumerate(test_loader):\n",
    "        test_input_data, test_target, _, test_setting , test_metric = test_data\n",
    "        test_input_data = test_input_data.to(device)\n",
    "        test_target = test_target.to(device)\n",
    "        cnt = cnt + len(test_input_data)\n",
    "        test_pi, test_mu, test_sigma = mlp(test_input_data)\n",
    "\n",
    "        # draw_mdn_wei(test_pi[0,:].clone().detach(), test_mu[0,:].clone().detach(), test_sigma[0,:].clone().detach(), test_target[0].clone().detach(), test_input_data[0].clone().detach(),-(test_batch_id+100), opt.N_gaussians)\n",
    "        # draw_mdn_wei(test_pi[1,:].clone().detach(), test_mu[1,:].clone().detach(), test_sigma[1,:].clone().detach(), test_target[1].clone().detach(), test_input_data[1].clone().detach(),-(test_batch_id+101), opt.N_gaussians)\n",
    "        # draw_mdn_wei(test_pi[2,:].clone().detach(), test_mu[2,:].clone().detach(), test_sigma[2,:].clone().detach(), test_target[2].clone().detach(), test_input_data[2].clone().detach(),-(test_batch_id+102), opt.N_gaussians)\n",
    "        # draw_mdn_wei(test_pi[3,:].clone().detach(), test_mu[3,:].clone().detach(), test_sigma[3,:].clone().detach(), test_target[3].clone().detach(), test_input_data[3].clone().detach(),-(test_batch_id+103), opt.N_gaussians)\n",
    "\n",
    "        pi = torch.cat([pi,torch.squeeze(test_pi.detach().cpu())],dim=0)\n",
    "        mu = torch.cat([mu,torch.squeeze(test_mu.detach().cpu())],dim=0)\n",
    "        sigma = torch.cat([sigma,torch.squeeze(test_sigma.detach().cpu())],dim=0)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1864,
   "outputs": [],
   "source": [
    "def plot_scatter(viz,data, win_str, ylabel):\n",
    "    for i in range(len(data)):\n",
    "         # scatter的input shape：N*2或N*3的\n",
    "        viz.scatter(X = np.array([[i],[data[i,0].numpy().item()]]).reshape(1,2), env=\"001\",win=win_str, opts= dict(title=win_str, xlabel=\"idx\", ylabel=ylabel),update=\"append\")\n",
    "        viz.scatter(X = np.array([[i],[data[i,1].numpy().item()]]).reshape(1,2), env=\"001\",win=win_str, opts= dict(title=win_str, xlabel=\"idx\", ylabel=ylabel),update=\"append\")\n",
    "        # viz.scatter(X = np.array([[i],[data[i,2].numpy().item()]]).reshape(1,2), env=\"001\",win=win_str, opts= dict(title=win_str, xlabel=\"idx\", ylabel=ylabel),update=\"append\")\n",
    "\n",
    "        # viz.scatter(X = np.array([[i],[data[i].numpy().item()]]).reshape(1,2), env=\"001\",win=win_str, opts= dict(title=win_str, xlabel=\"idx\", ylabel=ylabel),update=\"append\")\n",
    "\n",
    "    print(\"Done: \"+win_str)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1865,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: scale distrb-Conv_N_g=2\n",
      "Done: shape distrb-Conv_N_g=2\n",
      "Done: pi distrb-Conv_N_g=2\n"
     ]
    }
   ],
   "source": [
    "# plot_scatter(viz,mu,win_mu_str,\"mu\")\n",
    "# plot_scatter(viz,sigma,win_sigma_str,\"sigma\")\n",
    "# plot_scatter(viz,pi,win_pi_str,\"pi\")\n",
    "\n",
    "plot_scatter(viz,mu,win_scale_str,\"scale\")\n",
    "plot_scatter(viz,sigma,win_shape_str,\"shape\")\n",
    "plot_scatter(viz,pi,win_pi_str,\"pi\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
