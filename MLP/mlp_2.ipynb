{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2022/6/7 15:17\n",
    "# @Author  : Wang Yujia\n",
    "# @File    : mlp.ipynb\n",
    "# @Description :\n",
    "# @TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0. what for\n",
    "1. loss部分逻辑重写，添加`no grad`\n",
    "2. 添加hooks\n",
    "3. 使用pad_seq函数取代自己写的pad函数\n",
    "4. 效果：可以train虽然loss不下降，不会出现NaN的问题"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Preparations\n",
    "## 1.1 Import"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import torch.utils.data\n",
    "from mydataset import *\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from tensorboardX import SummaryWriter\n",
    "from visdom import Visdom\n",
    "from graphviz import Digraph\n",
    "import torchvision\n",
    "from torchviz import make_dot\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pygraphviz as pgv\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import geomloss\n",
    "import time\n",
    "from importlib import reload\n",
    "from functools import partial\n",
    "\n",
    "######### Ray Tune\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "import scipy\n",
    "from torch.nn import KLDivLoss\n",
    "from torch.autograd.gradcheck import gradcheck\n",
    "from config import bcolors\n",
    "\n",
    "import config\n",
    "import loss\n",
    "import plot"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Global settings and reloading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "reload(config)  # 必须reload！！\n",
    "reload(loss)    # 必须reload！！\n",
    "reload(plot)\n",
    "from config import DefaultConfig\n",
    "from loss import cal_metric\n",
    "from loss import loss_fn_v2\n",
    "from loss import loss_fn_WD\n",
    "from loss import validate\n",
    "from plot import plot_conv_weight\n",
    "from plot import plot_mu_weight\n",
    "from plot import plot_pi_weight\n",
    "from plot import plot_sigma_weight\n",
    "from plot import plot_net\n",
    "\n",
    "opt = DefaultConfig()\n",
    "\n",
    "total_train_step = 0\n",
    "total_test_step = 0\n",
    "\n",
    "# Range【一般用不到】\n",
    "a = 0\n",
    "b = 255\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Dataset and Dataloader\n",
    "1. DataLoader中的`shuffer=True`表示在每一次epoch中都打乱所有数据的顺序，然后以batch为单位从头到尾按顺序取用数据。这样的结果就是不同epoch中的数据都是乱序的,设置随机种子的作用就是让你的每一次训练都乱的一样，\n",
    "\n",
    "## 2.1 Dataset and spliting\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 设置随机数种子"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup_seed(7)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 读取data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "dataset = myDataset(opt.train_path, opt.target_path_metric, opt.target_path_loss, opt.data_key_path, opt.NLL_metric_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 产生index的乱序排列"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239\n"
     ]
    }
   ],
   "source": [
    "shuffled_indices = []\n",
    "DATA_len = 0\n",
    "# 使用全部的data\n",
    "if not opt.arr_flag:\n",
    "    DATA_len = dataset.__len__()\n",
    "    shuffled_indices = np.random.permutation(DATA_len)\n",
    "\n",
    "# 使用指定的data\n",
    "if opt.arr_flag:\n",
    "    shuffled_indices = np.load(opt.arr_path)\n",
    "    DATA_len = len(shuffled_indices)\n",
    "    np.random.shuffle(shuffled_indices)\n",
    "\n",
    "train_idx = shuffled_indices[:int(opt.train_pct*DATA_len)]\n",
    "# train_idx = shuffled_indices\n",
    "tmp = int((opt.train_pct+opt.vali_pct)*DATA_len)\n",
    "val_idx = shuffled_indices[int(opt.train_pct*DATA_len):tmp]\n",
    "\n",
    "test_idx = shuffled_indices[tmp:]\n",
    "print(train_idx)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 根据这个乱序排列抽取dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# 保存idx\n",
    "np.save('shuffled_indices',shuffled_indices)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Dataloader and collating\n",
    "1. 主要是对label数据进行collate\n",
    "    - 按照batch中的最大target data长度进行padding，padding with 0\n",
    "2. 返回的结果多一个batch dim,比如下面的`5`\n",
    "    - After collating:\n",
    "        - `torch.Size([5, 3, 300]),torch.Size([5, 87, 2])`\n",
    "        - `87`是最长的targets data长度"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_pad_sequence():\n",
    "    seq1 = torch.tensor([[ 2., 0.04761905], [3., 0.14285714], [4., 0.04761905]])\n",
    "    seq2 = torch.tensor([[ 1., 0.04761905]])\n",
    "    seq3 = torch.tensor([[ 3., 0.14285714], [4., 0.04761905]])\n",
    "    ls = list((seq1,seq2,seq3))\n",
    "    ls_length = torch.tensor([3,1,2])\n",
    "    ans = pad_sequence(ls,batch_first=True)\n",
    "    print(ans)\n",
    "    # seq_pak = pack_padded_sequence(ans,ls_length,batch_first=True,enforce_sorted=False)\n",
    "    # seq_unpacked, lens_unpacked = pad_packed_sequence(seq_pak, batch_first=True)\n",
    "    # lens_unpacked"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.1 No rescale"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def my_collate_fn(data):\n",
    "# 这里的data是一个list， list的元素是元组: (self.data, self.label)\n",
    "# collate_fn的作用是把[(data, label),(data, label)...]转化成([data, data...],[label,label...])\n",
    "# 假设self.data的一个data的shape为(channels, length), 每一个channel的length相等,\n",
    "# data[索引到index(batch)][索引到data或者label][索引到channel]\n",
    "\n",
    "    data_list = []              # training data\n",
    "    target_metric_list = []     # target data for computing metric of NN, (TARGET=1)\n",
    "    target_loss_list = []       # target data for computing loss of NN, (TARGET=5)\n",
    "    setting_list = []\n",
    "    metric_list = []\n",
    "\n",
    "    data_len = len(data)        # 读进来的data batch的大小\n",
    "\n",
    "    batch = 0\n",
    "\n",
    "    while batch < data_len:\n",
    "        # print(\"shape:\",data[batch][0].shape) #shape: (3, 300)\n",
    "\n",
    "        # 所有GT model\n",
    "        data_list.append(torch.tensor(data[batch][0]))\n",
    "\n",
    "        target_metric_list.append(torch.tensor(data[batch][1]))\n",
    "        target_loss_list.append(torch.tensor(data[batch][2]))\n",
    "        setting_list.append(torch.tensor(data[batch][3]))\n",
    "        metric_list.append(torch.tensor(data[batch][4]))\n",
    "        batch += 1\n",
    "\n",
    "    # Pad target data with zeros\n",
    "    target_metric_padded = pad_sequence(target_metric_list,batch_first=True)\n",
    "    target_loss_padded = pad_sequence(target_loss_list,batch_first=True)\n",
    "    target_metric_tensor = target_metric_padded.float()\n",
    "    target_loss_padded = target_loss_padded.float()\n",
    "\n",
    "    data_tensor = torch.stack(data_list).float()\n",
    "    setting_tensor = torch.stack(setting_list).float()\n",
    "    metric_tensor = torch.stack(metric_list).float()\n",
    "\n",
    "    return data_tensor, target_metric_tensor, target_loss_padded, setting_tensor, metric_tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.2 Rescale\n",
    "1. padding 太多0不利于学习，因此把padding的0替换一下：替换成`min(min(data[batch][0][0]),opt.SAFETY)`\n",
    "    - 替换`min(data[batch][0][0])`/ 一列中最小的prob，但是如果这个值太大，就替换成`opt.SAFETY`\n",
    "2. 效果: 不全是正向的，效果待定"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "def my_collate_fn_2(data):\n",
    "# 这里的data是一个list， list的元素是元组: (self.data, self.label)\n",
    "# collate_fn的作用是把[(data, label),(data, label)...]转化成([data, data...],[label,label...])\n",
    "# 假设self.data的一个data的shape为(channels, length), 每一个channel的length相等,\n",
    "# data[索引到index(batch)][索引到data或者label][索引到channel]\n",
    "\n",
    "    data_list = []              # training data\n",
    "    target_metric_list = []     # target data for computing metric of NN, (TARGET=1)\n",
    "    target_loss_list = []       # target data for computing loss of NN, (TARGET=5)\n",
    "    setting_list = []\n",
    "    metric_list = []\n",
    "\n",
    "    data_len = len(data)        # 读进来的data batch的大小\n",
    "\n",
    "    batch = 0\n",
    "\n",
    "    while batch < data_len:\n",
    "        # print(\"shape:\",data[batch][0].shape) #shape: (3, 300)\n",
    "\n",
    "        # 对于所有用0填充的data来说，最好用一个数字补齐这些0\n",
    "        # 补齐方法1:\n",
    "        data[batch][0][0,np.where(data[batch][0][0]==0)]= min(min(data[batch][0][0]),opt.SAFETY)\n",
    "        data[batch][0][1,np.where(data[batch][0][1]==0)]= min(min(data[batch][0][1]),opt.SAFETY)\n",
    "\n",
    "        data_list.append(torch.tensor(data[batch][0]))\n",
    "        target_metric_list.append(torch.tensor(data[batch][1]))\n",
    "        target_loss_list.append(torch.tensor(data[batch][2]))\n",
    "        setting_list.append(torch.tensor(data[batch][3]))\n",
    "        metric_list.append(torch.tensor(data[batch][4]))\n",
    "        batch += 1\n",
    "\n",
    "    # Pad target data with zeros\n",
    "    target_metric_padded = pad_sequence(target_metric_list,batch_first=True)\n",
    "    target_loss_padded = pad_sequence(target_loss_list,batch_first=True)\n",
    "    target_metric_tensor = target_metric_padded.float()\n",
    "    target_loss_padded = target_loss_padded.float()\n",
    "\n",
    "    data_tensor = torch.stack(data_list).float()\n",
    "    setting_tensor = torch.stack(setting_list).float()\n",
    "    metric_tensor = torch.stack(metric_list).float()\n",
    "\n",
    "    return data_tensor, target_metric_tensor, target_loss_padded, setting_tensor, metric_tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.3 Rescale-2\n",
    "1. padding 太多0不利于学习，"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [],
   "source": [
    "def my_collate_fn_3(data):\n",
    "# 这里的data是一个list， list的元素是元组: (self.data, self.label)\n",
    "# collate_fn的作用是把[(data, label),(data, label)...]转化成([data, data...],[label,label...])\n",
    "# 假设self.data的一个data的shape为(channels, length), 每一个channel的length相等,\n",
    "# data[索引到index(batch)][索引到data或者label][索引到channel]\n",
    "\n",
    "    data_list = []              # training data\n",
    "    target_metric_list = []     # target data for computing metric of NN, (TARGET=1)\n",
    "    target_loss_list = []       # target data for computing loss of NN, (TARGET=5)\n",
    "    setting_list = []\n",
    "    metric_list = []\n",
    "\n",
    "    data_len = len(data)        # 读进来的data batch的大小\n",
    "\n",
    "    batch = 0\n",
    "    # min_pad_0 = np.min(data[:][0][0])\n",
    "    # min_pad_1 = np.min(data[:][0][1])\n",
    "    min_pad_0 = np.inf\n",
    "    min_pad_1 = np.inf\n",
    "\n",
    "    while batch < data_len:\n",
    "        # print(\"shape:\",data[batch][0].shape) #shape: (3, 300)\n",
    "        # # 补齐方法2: batch中的最小值\n",
    "        # min_pad_0 = min(min_pad_0,min(data[batch][0][0]))\n",
    "        # min_pad_1 = min(min_pad_1,min(data[batch][0][0]))\n",
    "        # assert np.min(data[batch][0][0])>=0,\"<0!\"\n",
    "        # assert np.min(data[batch][0][1])>=0,\"<0!!\"\n",
    "\n",
    "        data_list.append(torch.tensor(data[batch][0]))\n",
    "        target_metric_list.append(torch.tensor(data[batch][1]))\n",
    "        target_loss_list.append(torch.tensor(data[batch][2]))\n",
    "        setting_list.append(torch.tensor(data[batch][3]))\n",
    "        metric_list.append(torch.tensor(data[batch][4]))\n",
    "        batch += 1\n",
    "\n",
    "    # Pad target data with zeros\n",
    "    target_metric_padded = pad_sequence(target_metric_list,batch_first=True)\n",
    "    target_loss_padded = pad_sequence(target_loss_list,batch_first=True)\n",
    "    target_metric_tensor = target_metric_padded.float()\n",
    "    target_loss_padded = target_loss_padded.float()\n",
    "\n",
    "    data_tensor = torch.stack(data_list).float()\n",
    "\n",
    "    data_tensor[data_tensor == 0] = torch.min(data_tensor)\n",
    "    setting_tensor = torch.stack(setting_list).float()\n",
    "    metric_tensor = torch.stack(metric_list).float()\n",
    "\n",
    "    return data_tensor, target_metric_tensor, target_loss_padded, setting_tensor, metric_tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.4 DataLoader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = dataset,batch_size = opt.batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(train_idx), collate_fn = my_collate_fn_2)\n",
    "\n",
    "val_loader = DataLoader(dataset = dataset,batch_size = opt.batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(val_idx),collate_fn = my_collate_fn_2)\n",
    "\n",
    "# 注意test_loader的batch size\n",
    "test_loader = DataLoader(dataset = dataset,batch_size = opt.batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(test_idx),collate_fn = my_collate_fn_2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. The Net and Init\n",
    "1. BatchNorm1d: The mean and std are calculated per-dimension over the mini-batches\n",
    "2."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### BatchNorm2d测试\n",
    "def test_BN():\n",
    "    m = nn.BatchNorm2d(3, affine=False)  # affine: With Learnable Parameters or not\n",
    "    print('m:', m)\n",
    "    # The mean and std are calculated per-dimension over the mini-batches\n",
    "    input = torch.tensor([\n",
    "        [[1.,2.,3.,4.],[1.,2.,3.,4.],[-1.,-2.,-3.,-4.]],\n",
    "        [[0.,0.,0.,0.],[0.,0.,0.,0.],[0.,0.,0.,0.]]\n",
    "    ], requires_grad=True)\n",
    "\n",
    "    print('input:', input.shape)\n",
    "    input = input.unsqueeze(dim=2)\n",
    "    print('input:', input.shape)\n",
    "    output = m(input) # 归一化\n",
    "    print('output:', output.shape)\n",
    "    print('output:', output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### 复现batchNorm2d在input shape为3维的情况\n",
    "input = torch.tensor([[[1.,2.,3.,4.]],[[0.,0.,0.,0.]]])\n",
    "# print(input.shape)\n",
    "# torch.mean(input),torch.var(input,unbiased = False)\n",
    "(input-torch.mean(input))/ torch.sqrt(torch.var(input,unbiased = False))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 Init, Printer, Hook\n",
    "- 设置网络初始权重: 不太work"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [],
   "source": [
    "\n",
    "class model_param_init(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        assert isinstance(model, nn.Module), 'model not a class nn.Module'\n",
    "        self.net = model\n",
    "        self.initParam()\n",
    "\n",
    "    def forward(self):\n",
    "        for param in self.net.parameters():\n",
    "            # nn.init.zeros_(param)\n",
    "            # nn.init.ones_(param)\n",
    "            # nn.init.normal_(param, mean=0, std=1)\n",
    "            # nn.init.uniform_(param, a=0, b=1)\n",
    "            # nn.init.constant_(param, val=1)   # 将所有权重初始化为1\n",
    "            # nn.init.eye_(param)  # 只能将二维的tensor初始化为单位矩阵\n",
    "            # nn.init.xavier_uniform_(param, gain=1)  # Glorot初始化  得到的张量是从-a——a中采用的\n",
    "            # print(param)  x.unsqueeze(0)\n",
    "            nn.init.xavier_normal_(param.unsqueeze(0), gain=1)   # 得到的张量是从0-std采样的\n",
    "            # nn.init.kaiming_normal_(param, a=0, mode='fan_in', nonlinearity='relu') # he初始化方法\n",
    "            # nn.init.kaiming_uniform_(param)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- print网络每层结构"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "class PrintLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if(DEBUG):\n",
    "            print(\"This layer: \")\n",
    "            print(x)      #print(x.shape)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- hook_backward_fn: 输入端的grad，输出端的grad，这里称呼的**输入与输出是站在前向传播的角度的**。如果模块有多个输入与输出的话， 其`grad_input`和`grad_output`可以是tuple类型。\n",
    "- 与forward不同的是，backward传播的时候，**不仅反向传递input和output的grad，还会传递模块Parameter的grad**：\n",
    "    - 比如fc模块，其`grad_input`是一个三元组的tuple，（对bias的梯度，对输入的梯度，对w的梯度）；\n",
    "    - conv模块`grad_input`也是一个三元组tuple，为（对输入的梯度，对w的梯度，对bias的梯度"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# hook functions have to take these 3 input\n",
    "def hook_forward_fn(module, input, output):\n",
    "    print(\"It's forward: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"input: {input}\")\n",
    "    print(f\"output: {output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn(module, grad_input, grad_output):\n",
    "    print(\"It's backward: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn_pi(module, grad_input, grad_output):\n",
    "    print(\"It's backward in pi: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn_sigma(module, grad_input, grad_output):\n",
    "    print(\"It's backward in sigma: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn_mu(module, grad_input, grad_output):\n",
    "    print(\"It's backward in mu: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Conv结构-1\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "outputs": [],
   "source": [
    "# Not Sequential\n",
    "# 573, 588, 1581\n",
    "class MLP_1_1(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.kernel_size = (3,12)\n",
    "        self.stride = (3,3)\n",
    "        self.ln_in = int((300-self.kernel_size[1])/self.stride[1]+1)\n",
    "\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=3,affine=True)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=self.kernel_size, stride=self.stride, padding=0,bias=True)\n",
    "        self.conv2 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=self.kernel_size, stride=self.stride, padding=0,bias=True)\n",
    "        self.conv3 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=self.kernel_size, stride=self.stride, padding=0,bias=True)\n",
    "\n",
    "        self.BN_aff1 = nn.BatchNorm1d(num_features=self.ln_in,affine=True)\n",
    "        self.BN_aff2 = nn.BatchNorm1d(num_features=self.ln_in,affine=True)\n",
    "        self.BN_aff3 = nn.BatchNorm1d(num_features=self.ln_in,affine=True)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "        self.ac_func1 = nn.PReLU()\n",
    "        self.ac_func2 = nn.PReLU()\n",
    "        self.ac_func3 = nn.PReLU()\n",
    "\n",
    "        self.ac_func4 = nn.PReLU()\n",
    "        self.ac_func5 = nn.PReLU()\n",
    "        self.ac_func6 = nn.PReLU()\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(self.ln_in, n_gaussians),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(self.ln_in, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(self.ln_in, n_gaussians)\n",
    "        #\n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, nn.Conv2d):\n",
    "        #         # nn.init.xavier_uniform_(m.weight,gain=1)\n",
    "        #         nn.init.uniform_(m.weight,a=-0.5,b=0.5)\n",
    "        #     elif isinstance(m, nn.Linear):\n",
    "        #         nn.init.uniform_(m.weight,a=-1,b=1)\n",
    "        #         # m.weight.data.normal_(0, 0.02)\n",
    "        #         # nn.init.xavier_normal_(m.weight,gain=1)\n",
    "        #         # nn.init.xavier_uniform_(m.weight,gain=1)\n",
    "        #         # nn.init.orthogonal_(m.weight)\n",
    "        #         # nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.BN1(x)\n",
    "        x = torch.unsqueeze(x,dim=1)                     # torch.Size([B, 1, 3, 300])\n",
    "\n",
    "        x1 = F.leaky_relu(self.conv1(x))                   # [B, 1, 1, ln_in]\n",
    "        x2 = F.leaky_relu(self.conv2(x))                   # [B, 1, 1, ln_in]\n",
    "        x3 = F.leaky_relu(self.conv3(x))                   # [B, 1, 1, ln_in]\n",
    "\n",
    "        x1_1 = torch.squeeze(x1)\n",
    "        x2_1 = torch.squeeze(x2)\n",
    "        x3_1 = torch.squeeze(x3)\n",
    "\n",
    "        x1_2 = self.BN_aff1(x1_1)\n",
    "        x2_2 = self.BN_aff2(x2_1)\n",
    "        x3_2 = self.BN_aff3(x3_1)\n",
    "\n",
    "        x1_3 = F.leaky_relu(x1_2)\n",
    "        x2_3 = F.leaky_relu(x2_2)\n",
    "        x3_3 = F.leaky_relu(x3_2)\n",
    "        # x3_3 = self.ac_func2(x3_2)\n",
    "\n",
    "        pi = self.z_pi(x1_3)\n",
    "        mu = self.z_mu(x2_3)\n",
    "\n",
    "        # sigma = torch.exp(self.z_sigma(x3_3))\n",
    "        sigma = F.relu(self.z_sigma(x3_3))\n",
    "        sigma = torch.clamp(sigma,1e-4)\n",
    "\n",
    "        # return x1_3\n",
    "        return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "       BatchNorm1d-1               [-1, 3, 300]               6\n",
      "            Conv2d-2             [-1, 1, 1, 97]              37\n",
      "            Conv2d-3             [-1, 1, 1, 97]              37\n",
      "            Conv2d-4             [-1, 1, 1, 97]              37\n",
      "       BatchNorm1d-5                   [-1, 97]             194\n",
      "       BatchNorm1d-6                   [-1, 97]             194\n",
      "       BatchNorm1d-7                   [-1, 97]             194\n",
      "            Linear-8                    [-1, 3]             294\n",
      "           Softmax-9                    [-1, 3]               0\n",
      "           Linear-10                    [-1, 3]             294\n",
      "           Linear-11                    [-1, 3]             294\n",
      "================================================================\n",
      "Total params: 1,581\n",
      "Trainable params: 1,581\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.02\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP_1_1(opt.N_gaussians)\n",
    "mlp = mlp.to(device=device)\n",
    "summary(mlp, (3,300))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3 Conv结构-2\n",
    "1. 换了一下conv的结构"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# 参数量747\n",
    "class MLP_1_2(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=3,affine=False)\n",
    "        self.BN2 = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "        self.BN3 = nn.BatchNorm2d(num_features=6,affine=True)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=(1,6), stride=(1,3), padding=0,bias=False)\n",
    "        self.ac_func = nn.Softplus()\n",
    "        # self.ac_func = nn.ReLU()\n",
    "\n",
    "        # 3, 3, 99\n",
    "        self.conv2 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=(3,6), stride=6, padding=0,bias=False)\n",
    "        # 6,1,33\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=6, out_channels=6, kernel_size=(1,3), stride=3, padding=0,bias=False)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        #self.linear1 = nn.Linear(48, 18)\n",
    "\n",
    "        #无clip\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(30, n_gaussians),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        # 有clip\n",
    "        # self.z_pi = nn.Linear(30, n_gaussians)\n",
    "        self.z_mu = nn.Linear(30, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(30, n_gaussians)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.BN1(x)\n",
    "        # 加一个height维度\n",
    "\n",
    "        x = torch.unsqueeze(x,dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.BN2(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.BN3(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        # x = torch.unsqueeze(x,dim=2)\n",
    "        x = self.conv3(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        # x = self.linear1(x)\n",
    "        # x = self.ac_func(self.drop(x))\n",
    "        mu = self.z_mu(x)\n",
    "        pi = self.z_pi(x)\n",
    "\n",
    "        sigma = torch.exp(self.z_sigma(x))\n",
    "        sigma = torch.clamp(sigma,1e-4)\n",
    "\n",
    "        # return x\n",
    "        return pi,mu,sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4 MLP结构\n",
    "### 3.4.1 4层"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# 参数量304,000+\n",
    "# 4层\n",
    "class MLP_1_3(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=3,affine=True)\n",
    "        self.BN2 = nn.BatchNorm1d(num_features=300,affine=True)\n",
    "        self.BN3 = nn.BatchNorm1d(num_features=100,affine=True)\n",
    "        self.BN4 = nn.BatchNorm1d(num_features=30,affine=True)\n",
    "        self.BN5 = nn.BatchNorm1d(num_features=12,affine=True)\n",
    "        self.BN6 = nn.BatchNorm1d(num_features=12,affine=True)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "        self.linear1 = nn.Linear(900, 300)\n",
    "        self.linear2 = nn.Linear(300, 100)\n",
    "        self.linear3 = nn.Linear(100, 30)\n",
    "        self.linear4 = nn.Linear(30, 12)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "        self.ac_func2 = nn.SELU()\n",
    "        self.ac_func3 = nn.LeakyReLU()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(12, n_gaussians),  # 30个params要learn\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(12, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(12, n_gaussians)\n",
    "\n",
    "        # SELU Init\n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, (nn.Linear)):\n",
    "        #         nn.init.kaiming_normal_(m.weight, nonlinearity='linear')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.BN1(x)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.BN2(x)\n",
    "        x = self.ac_func3(self.drop(x))\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = self.BN3(x)\n",
    "        x = self.ac_func3(self.drop(x))\n",
    "\n",
    "        x = self.linear3(x)\n",
    "        x = self.BN4(x)\n",
    "        x = self.ac_func3(self.drop(x))\n",
    "\n",
    "        x = self.linear4(x)\n",
    "        x = self.BN5(x)\n",
    "        x = self.ac_func3(self.drop(x))\n",
    "\n",
    "        x = self.BN6(x)\n",
    "        pi = self.z_pi(x)\n",
    "        # pi = self.z_pi(torch.clamp(x,1e-3))\n",
    "        mu = self.z_mu(x)\n",
    "        # sigma = torch.exp(self.z_sigma(x))\n",
    "        sigma = F.elu(self.z_sigma(x)) + 1\n",
    "        sigma = torch.clamp(sigma,1e-4)\n",
    "\n",
    "        # return x\n",
    "        return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.2 2层"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "# 参数量91,000+\n",
    "# 2层MLP+MDN\n",
    "class MLP_1_4(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=3,affine=True)\n",
    "        self.BN2 = nn.BatchNorm1d(num_features=100,affine=True)\n",
    "        self.BN3 = nn.BatchNorm1d(num_features=12,affine=True)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "        self.linear1 = nn.Linear(900, 100)\n",
    "        self.linear2 = nn.Linear(100, 12)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "        self.ac_func2 = nn.SELU()\n",
    "        self.ac_func3 = nn.LeakyReLU()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(12, n_gaussians),  # 30个params要learn\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(12, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(12, n_gaussians)\n",
    "\n",
    "        # SELU Init\n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, (nn.Linear)):\n",
    "        #         nn.init.kaiming_normal_(m.weight, nonlinearity='linear')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"x.shape: \",x.shape)  # torch.Size([40, 3, 300])\n",
    "        x = self.BN1(x)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        # x = self.BN2(x)\n",
    "        x = self.ac_func3(self.drop(x))\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        # x = self.BN3(x)\n",
    "        x = self.ac_func3(self.drop(x))\n",
    "\n",
    "        pi = self.z_pi(x)\n",
    "        # pi = self.z_pi(torch.clamp(x,1e-3))\n",
    "        mu = self.z_mu(x)\n",
    "\n",
    "        sigma = torch.exp(self.z_sigma(x))\n",
    "        # sigma = F.elu(self.z_sigma(x)) + 1\n",
    "        sigma = torch.clamp(sigma,1e-4)\n",
    "\n",
    "        return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.4 1层+avgpool"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "outputs": [],
   "source": [
    "# 参数量2823\n",
    "# 1层MLP+MDN\n",
    "class MLP_1_5(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=3,affine=True)\n",
    "        self.BN2 = nn.BatchNorm1d(num_features=9,affine=True)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=(1,3))  # kernel_size=(1,Channel)\n",
    "        self.linear1 = nn.Linear(300, 9)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(9, n_gaussians),  # 30个params要learn\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(9, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(9, n_gaussians)\n",
    "\n",
    "        # SELU Init\n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, (nn.Linear)):\n",
    "        #         nn.init.kaiming_normal_(m.weight, nonlinearity='linear')\n",
    "\n",
    "        # 初始化权重\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                # m.weight.data.normal_(0, 0.02)\n",
    "                # nn.init.xavier_normal_(m.weight,gain=1)\n",
    "                nn.init.orthogonal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "        # nn.init.xavier_normal_(self.z_mu.weight,gain=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.BN1(x)\n",
    "        # print(\"before transpose: \",x.shape)\n",
    "        x = torch.transpose(x, 1, 2)        # [B,N,C]\n",
    "        # print(\"before avgpool: \",x.shape)\n",
    "        x = self.avgpool(x)\n",
    "        # print(\"after avgpool: \",x.shape)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.BN2(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        pi = self.z_pi(x)\n",
    "        # pi = self.z_pi(torch.clamp(x,1e-3))\n",
    "        mu = self.z_mu(x)\n",
    "\n",
    "        sigma = torch.exp(self.z_sigma(x))\n",
    "        # sigma = F.elu(self.z_sigma(x)) + 1\n",
    "        sigma = torch.clamp(sigma,1e-4)\n",
    "\n",
    "        return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "outputs": [],
   "source": [
    "def test_pool():\n",
    "    # torch.Size([40, 3, 300])\n",
    "    a = torch.tensor([[[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5]]])\n",
    "    # print(a.shape)\n",
    "    a = torch.transpose(a, 1, 2)\n",
    "    print(a)\n",
    "    ans = F.avg_pool2d(a,(1,3))\n",
    "    print(ans.shape)    # [B,N,C]\n",
    "# test_pool()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.5 1层+无avgpool"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "outputs": [],
   "source": [
    "# 参数量8233\n",
    "# 1层MLP+MDN\n",
    "class MLP_1_6(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=3,affine=True)\n",
    "        self.BN2 = nn.BatchNorm1d(num_features=9,affine=True)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "        self.linear1 = nn.Linear(900, 9)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(9, n_gaussians),  # 30个params要learn\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(9, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(9, n_gaussians)\n",
    "\n",
    "        # SELU Init\n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, (nn.Linear)):\n",
    "        #         nn.init.kaiming_normal_(m.weight, nonlinearity='linear')\n",
    "\n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, nn.BatchNorm1d):\n",
    "        #         m.weight.data.fill_(1)\n",
    "        #         m.bias.data.zero_()\n",
    "        #     elif isinstance(m, nn.Linear):\n",
    "        #         # m.weight.data.normal_(0, 0.02)\n",
    "        #         # nn.init.xavier_normal_(m.weight,gain=1)\n",
    "        #         nn.init.orthogonal_(m.weight)\n",
    "        #         nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.BN1(x)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.BN2(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        pi = self.z_pi(x)\n",
    "        # pi = self.z_pi(torch.clamp(x,1e-3))\n",
    "        mu = self.z_mu(x)\n",
    "\n",
    "        sigma = torch.exp(self.z_sigma(x))\n",
    "        # sigma = F.elu(self.z_sigma(x)) + 1\n",
    "        sigma = torch.clamp(sigma,1e-4)\n",
    "\n",
    "        return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.5 (3,100)的结构\n",
    "1. 适配(3,100)的training data长度而不是(3,300)\n",
    "### 3.5.1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # 参数量：228\n",
    "# class MLP_2_2(nn.Module):\n",
    "#     # code->generate->override methods\n",
    "#     def __init__(self, n_gaussians) -> None:\n",
    "#         super().__init__()\n",
    "#         self.BN = nn.BatchNorm2d(num_features=3,affine=False)\n",
    "#         self.BN_aff = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "#         self.drop = nn.Dropout(0.2)\n",
    "#\n",
    "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#         # self.ac_func = nn.Softplus()\n",
    "#         self.ac_func = nn.LeakyReLU()\n",
    "#         self.ac_func2 = nn.ReLU()\n",
    "#\n",
    "#         # self.ac_func = nn.Sigmoid()\n",
    "#         # self.ac_func = nn.Tanh()\n",
    "#\n",
    "#         self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,2), stride=(1,2), padding=0,bias=False)\n",
    "#\n",
    "#         self.conv3 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#\n",
    "#         self.flatten = nn.Flatten()\n",
    "#\n",
    "#         self.z_pi = nn.Sequential(\n",
    "#             nn.Linear(15, n_gaussians),\n",
    "#             nn.Softmax(dim=1)\n",
    "#         )\n",
    "#\n",
    "#         self.z_mu = nn.Linear(15, n_gaussians)\n",
    "#         self.z_sigma = nn.Linear(15, n_gaussians)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         # 加一个height维度\n",
    "#         x = torch.unsqueeze(x,dim=2)\n",
    "#         x = self.BN(x)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         x = self.BN_aff(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         x = self.BN_aff(x)\n",
    "#         x = self.conv3(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#         #\n",
    "#         x = self.flatten(x)\n",
    "#\n",
    "#         pi = self.z_pi(torch.clamp(x,1e-3))\n",
    "#         # pi = self.z_pi(x)\n",
    "#         mu = self.z_mu(x)\n",
    "#         sigma = torch.exp(self.z_sigma(x))\n",
    "#\n",
    "#         # return x\n",
    "#         return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- MLP结构+长度=100"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.5.2 MLP-2层"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "# 参数量 18699\n",
    "class MLP_2_3(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=3,affine=True)\n",
    "        self.BN2 = nn.BatchNorm1d(num_features=60,affine=True)\n",
    "        self.BN3 = nn.BatchNorm1d(num_features=9,affine=True)\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "\n",
    "        self.linear1 = nn.Linear(300, 60)\n",
    "        self.linear2 = nn.Linear(60, 9)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(9, n_gaussians),  # 30个params要learn\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(9, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(9, n_gaussians)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.BN1(x)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.BN2(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = self.BN3(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        pi = self.z_pi(x)\n",
    "        # pi = self.z_pi(torch.clamp(x,1e-3))\n",
    "        mu = self.z_mu(x)\n",
    "        sigma = torch.exp(self.z_sigma(x))\n",
    "        sigma = torch.clamp(sigma,1e-4)\n",
    "\n",
    "        # return x\n",
    "        return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.5.3 MLP-1层"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [
    "# 参数量2823\n",
    "class MLP_2_4(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=3,affine=True)\n",
    "        self.BN2 = nn.BatchNorm1d(num_features=9,affine=True)\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "\n",
    "        self.linear1 = nn.Linear(300, 9)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(9, n_gaussians),  # 30个params要learn\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(9, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(9, n_gaussians)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.BN1(x)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.BN2(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        pi = self.z_pi(x)\n",
    "        # pi = self.z_pi(torch.clamp(x,1e-3))\n",
    "        mu = self.z_mu(x)\n",
    "        sigma = torch.exp(self.z_sigma(x))\n",
    "        sigma = torch.clamp(sigma,1e-4)\n",
    "\n",
    "        # return x\n",
    "        return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.6 (3,60)的结构\n",
    "1. 适配(3,60)的training data长度而不是(3,300)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # 参数量：174\n",
    "# class MLP_3_1(nn.Module):\n",
    "#     # code->generate->override methods\n",
    "#     def __init__(self, n_gaussians) -> None:\n",
    "#         super().__init__()\n",
    "#         self.BN = nn.BatchNorm2d(num_features=3,affine=False)\n",
    "#         self.BN_aff = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "#         self.drop = nn.Dropout(0.2)\n",
    "#\n",
    "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#         # self.ac_func = nn.Softplus()\n",
    "#         self.ac_func = nn.LeakyReLU()\n",
    "#         self.ac_func2 = nn.ReLU()\n",
    "#\n",
    "#         # self.ac_func = nn.Sigmoid()\n",
    "#         # self.ac_func = nn.Tanh()\n",
    "#\n",
    "#         self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,2), stride=(1,2), padding=0,bias=False)\n",
    "#\n",
    "#         self.conv3 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.linear = nn.Linear(24, 9)\n",
    "#\n",
    "#         self.z_pi = nn.Sequential(\n",
    "#             nn.Linear(9, n_gaussians),\n",
    "#             nn.Softmax(dim=1)\n",
    "#         )\n",
    "#\n",
    "#         self.z_mu = nn.Linear(9, n_gaussians)\n",
    "#         self.z_sigma = nn.Linear(9, n_gaussians)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         # 加一个height维度\n",
    "#         x = torch.unsqueeze(x,dim=2)\n",
    "#         x = self.BN(x)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.ac_func(self.drop(x))      # 3, 1, 20\n",
    "#\n",
    "#         x = self.BN_aff(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.ac_func(self.drop(x))      # 3, 1, 10\n",
    "#\n",
    "#         x = self.BN_aff(x)\n",
    "#         x = self.conv3(x)\n",
    "#         x = self.ac_func(self.drop(x))      # 3, 1, 8\n",
    "#\n",
    "#         x = self.flatten(x)\n",
    "#         # print(x.shape)\n",
    "#         # x = self.linear(x)\n",
    "#         #\n",
    "#         pi = self.z_pi(x)\n",
    "#         mu = self.z_mu(x)\n",
    "#         sigma = torch.exp(self.z_sigma(x))\n",
    "#\n",
    "#         # return x\n",
    "#         return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- MLP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # 参数量：5799\n",
    "# class MLP_3_2(nn.Module):\n",
    "#     # code->generate->override methods\n",
    "#     def __init__(self, n_gaussians) -> None:\n",
    "#         super().__init__()\n",
    "#         self.BN = nn.BatchNorm2d(num_features=3,affine=False)\n",
    "#         self.BN_aff = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "#         self.drop = nn.Dropout(0.2)\n",
    "#\n",
    "#         self.linear1 = nn.Linear(180,30)\n",
    "#         self.linear2 = nn.Linear(30,9)\n",
    "#         # self.ac_func = nn.Softplus()\n",
    "#         self.ac_func = nn.LeakyReLU()\n",
    "#         self.ac_func2 = nn.ReLU()\n",
    "#\n",
    "#         # self.ac_func = nn.Sigmoid()\n",
    "#         # self.ac_func = nn.Tanh()\n",
    "#\n",
    "#         self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,2), stride=(1,2), padding=0,bias=False)\n",
    "#\n",
    "#         self.conv3 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.linear = nn.Linear(24, 9)\n",
    "#\n",
    "#         self.z_pi = nn.Sequential(\n",
    "#             nn.Linear(9, n_gaussians),\n",
    "#             nn.Softmax(dim=1)\n",
    "#         )\n",
    "#\n",
    "#         self.z_mu = nn.Linear(9, n_gaussians)\n",
    "#         self.z_sigma = nn.Linear(9, n_gaussians)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         # 加一个height维度\n",
    "#         x = torch.unsqueeze(x,dim=2)\n",
    "#         x = self.BN(x)\n",
    "#         x = self.flatten(x)\n",
    "#\n",
    "#         x = self.linear1(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         x = self.linear2(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         pi = self.z_pi(x)\n",
    "#         mu = self.z_mu(x)\n",
    "#         sigma = torch.exp(self.z_sigma(x))\n",
    "#\n",
    "#         # return x\n",
    "#         return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. The Loss\n",
    "\n",
    "## 4.0 The metric\n",
    "1. 以NLL作为metric\n",
    "2. 理论模型的metric已经提前算好，读进来.\n",
    "3. 注意这里的metric比较的是target还是target_5:\n",
    "    - 建议使用target data而不是target_5，因为后者是为了方便NN的learning，前者才是真正的比较NLL\n",
    "    - 使用前者也要用cdf进行比较吧。。比如GT的4实际上是我们这里的[3.5,4.5]？是高斯分布在[a-0.5,a+0.5]上的cdf作为a的“单点分布”"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# from loss import cal_metric"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1 NLL loss\n",
    "### 4.1.1 original version"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# from loss import loss_fn_v2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1.2 version 3 [LogSumExp]\n",
    "1. $ exponent_i = \\log_{}{\\alpha_i } -1/2*\\log_{}{2\\pi } -\\log_{}{\\sigma_i }- \\frac{(x-\\mu_i )^2}{2\\sigma_i^2} $\n",
    "2. $ NLL =-\\sum_{j}^{N_S}LogSumExp= -\\sum_{j}^{N_S} \\log\\sum_{i}^{N_G} \\exp \\{exponent_i\\}$\n",
    "3. 不稳定！lr不要设超过1e-2；而且loss会变成负值。。最开始x_max是负值，后面会变成正值 which means prob值开始>1了"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "from loss import log_sum_exp\n",
    "from loss import loss_fn_v3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "    ## 4.2 cdf loss\n",
    "1. 注意把targets data的路径改一下"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# cdf loss\n",
    "from loss import loss_fn_cdf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.3 交叉熵/ KL\n",
    "1. 自己写的，没有用torch的函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from loss import loss_fn_CE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def loss_test():\n",
    "    loss_2 = torch.tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "    loss_3 = loss_2[torch.nonzero(loss_2)].view(-1,1)         # 去掉所有的log(0)\n",
    "    print(\"loss_3:\",loss_3.shape)\n",
    "\n",
    "    MIN_LOSS = torch.min(loss_2) if torch.min(loss_2)>0 else 1e-20\n",
    "\n",
    "    print(\"MIN_LOSS：\",MIN_LOSS)\n",
    "    # 用MIN_loss替代loss_2为0的部分\n",
    "    loss_2[loss_2==0] = MIN_LOSS\n",
    "\n",
    "\n",
    "    loss_padded = [MIN_LOSS]* (loss_2.shape[0]-loss_3.shape[0])\n",
    "    loss_padded_ts = torch.tensor(loss_padded) # ,device=device\n",
    "    loss_3 = torch.cat((loss_3[:,0],loss_padded_ts))\n",
    "    loss_3.shape, torch.log(loss_3)\n",
    "    print(\"data: \",loss_3.data)\n",
    "    print(\"data: \",type(loss_3))\n",
    "# loss_test()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.4 W-distance\n",
    "1. 拿到target data的两列，按照第一列去predict\n",
    "2. Ref: https://www.kernel-operations.io/geomloss/api/pytorch-api.html"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5 Plot\n",
    "1. draw:\n",
    "    - loss: training data的loss和test data的loss趋势\n",
    "    - MLP的网络结构（.png）\n",
    "    - target distrb.和pred. distrb.\n",
    "    - 所有target data的分布图\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "viz = Visdom(env=\"001\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.1 draw 所有的target data\n",
    "1. 实际上在`i < 50`里决定画前50 or what target data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def draw_all_target_data():\n",
    "    # target data\n",
    "    # target_path = r\"../data/targets\"\n",
    "    target_path = r\"../data/targets_5\"\n",
    "    # target_path里有全部的target data地址\n",
    "    target_all_path = os.listdir(target_path)\n",
    "    len_target = len(target_all_path)\n",
    "    print(f\"一共有 *{len_target}* 组 target data\")\n",
    "\n",
    "    # Init\n",
    "    # viz_env_str = \"target_bar\"\n",
    "    # viz_env_str = \"target_line\"\n",
    "    # viz_env_str = \"target_hist\"\n",
    "    viz_env_str = \"target_scatter\"\n",
    "    viz = Visdom(env = viz_env_str)\n",
    "\n",
    "    for i in range(len_target):\n",
    "        target_path_i_path = os.path.join(target_path,target_all_path[i])\n",
    "        target_df = pd.read_csv(target_path_i_path,encoding=\"utf-8\")\n",
    "\n",
    "        # Init\n",
    "        win_str = str(target_all_path[i])\n",
    "        title_str = \"Target Distrb. of \"+win_str\n",
    "\n",
    "        if(i < 50):\n",
    "            # Hist plot which is not that great\n",
    "            if viz_env_str == \"target_hist\" :\n",
    "                viz.histogram(X = np.array(target_df.N), env=viz_env_str, win=win_str,\n",
    "                            opts= dict(title=title_str,numbins = 50))\n",
    "\n",
    "            # Line plot\n",
    "            if viz_env_str == \"target_line\" :\n",
    "                target_df.drop_duplicates(inplace=True)\n",
    "                viz.line(X = np.array(target_df.N),Y= np.array(target_df.P), env=viz_env_str, win=win_str,opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "            # Bar plot. 这个看起来是最准确的一个\n",
    "            if viz_env_str == \"target_bar\" :\n",
    "                target_df.drop_duplicates(inplace=True)\n",
    "                arr_str = [str(i) for i in np.array(target_df.N)]\n",
    "                viz.bar(X=np.array(target_df.P), env=viz_env_str, win=win_str,\n",
    "                        opts=dict(title=title_str,rownames=arr_str,stacked = False))\n",
    "\n",
    "            # Scatter plot\n",
    "            if viz_env_str == \"target_scatter\" :\n",
    "                target_df.drop_duplicates(inplace=True)\n",
    "                viz.scatter(X=np.array(target_df),env=viz_env_str, win=win_str,opts=dict(title=title_str,markersize = 3))\n",
    "    print(\"Done\")\n",
    "\n",
    "# draw_all_target_data()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 画target data的cdf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def draw_all_target_data_cdf():\n",
    "    # target data\n",
    "    # target_path = r\"../data/targets\"\n",
    "    target_path = r\"../data/targets_5_cdf\"\n",
    "    # target_path里有全部的target data地址\n",
    "    target_all_path = os.listdir(target_path)\n",
    "    len_target = len(target_all_path)\n",
    "    print(f\"一共有 *{len_target}* 组 target data\")\n",
    "\n",
    "    # Init，只需要改这个str就ok\n",
    "    # viz_env_str = \"target_barCDF\"\n",
    "    # viz_env_str = \"target_lineCDF\"\n",
    "    # viz_env_str = \"target_histCDF\"\n",
    "    viz_env_str = \"target_scatterCDF\"\n",
    "    viz = Visdom(env = viz_env_str)\n",
    "\n",
    "    for i in range(len_target):\n",
    "        target_path_i_path = os.path.join(target_path,target_all_path[i])\n",
    "        target_df = pd.read_csv(target_path_i_path,encoding=\"utf-8\")\n",
    "\n",
    "        # Init\n",
    "        win_str = str(target_all_path[i])\n",
    "        title_str = \"Target Distrb. of \"+win_str\n",
    "\n",
    "        if(i < 50):\n",
    "            # Hist plot which is not that great\n",
    "            if viz_env_str == \"target_histCDF\" :\n",
    "                viz.histogram(X = np.array(target_df.N), env=viz_env_str, win=win_str,\n",
    "                            opts= dict(title=title_str,numbins = 50))\n",
    "\n",
    "            # Line plot\n",
    "            if viz_env_str == \"target_lineCDF\" :\n",
    "                target_df.drop_duplicates(inplace=True)\n",
    "                viz.line(X = np.array(target_df.N),Y= np.array(target_df.P), env=viz_env_str, win=win_str,opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "            # Bar plot. 这个看起来是最准确的一个\n",
    "            if viz_env_str == \"target_barCDF\" :\n",
    "                target_df.drop_duplicates(inplace=True)\n",
    "                arr_str = [str(i) for i in np.array(target_df.N)]\n",
    "                viz.bar(X=np.array(target_df.P), env=viz_env_str, win=win_str,\n",
    "                        opts=dict(title=title_str,rownames=arr_str,stacked = False))\n",
    "\n",
    "            # Scatter plot\n",
    "            if viz_env_str == \"target_scatterCDF\" :\n",
    "                target_df.drop_duplicates(inplace=True)\n",
    "                viz.scatter(X=np.array(target_df),env=viz_env_str, win=win_str,opts=dict(title=title_str,markersize = 3))\n",
    "    print(\"Done\")\n",
    "\n",
    "# draw_all_target_data_cdf()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 也是画target data，但是是平滑处理\n",
    "- `savgol_filter`参数解析:\n",
    "    - y：代表曲线点坐标（x,y）中的y\n",
    "    - window_length：窗口长度，该值需为正奇整数。值越小，曲线越贴近真实曲线；值越大，平滑效果越厉害\n",
    "    - polyorder: 对窗口内的数据点进行k阶多项式拟合，k的值需要小于window_length。值越大，曲线越贴近真实曲线；值越小，平滑效果越厉害\n",
    "    - mode：确定了要应用滤波器的填充信号的扩展类型。（This determines the type of extension to use for the padded signal to which the filter is applied. ）\n",
    "- **实际上没用到这个函数**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def draw_smooth_target_data():\n",
    "    # target data\n",
    "    target_path = r\"../data/targets\"\n",
    "    # target_path里有全部的target data地址\n",
    "    target_all_path = os.listdir(target_path)\n",
    "    len_target = len(target_all_path)\n",
    "    print(f\"一共有 *{len_target}* 组 target data\")\n",
    "\n",
    "    # Init\n",
    "    viz_env_str = \"target_bar\"\n",
    "    # viz_env_str = \"target_distrb\"\n",
    "    # viz_env_str = \"target_hist\"\n",
    "    # viz_env_str = \"target_scatter\"\n",
    "    viz = Visdom(env = viz_env_str)\n",
    "\n",
    "    for i in range(len_target):\n",
    "        target_path_i_path = os.path.join(target_path,target_all_path[i])\n",
    "        target_df = pd.read_csv(target_path_i_path,encoding=\"utf-8\")\n",
    "\n",
    "        # Init\n",
    "        win_str = str(target_all_path[i])\n",
    "        title_str = \"Target Distrb. of \"+win_str\n",
    "\n",
    "        if(i < 100):\n",
    "            # Smoothen the data\n",
    "            y_smooth = scipy.signal.savgol_filter(target_df.P,window_length=53,polyorder=3)\n",
    "            # y_smooth = scipy.signal.savgol_filter(target_df.P, 99, 1, mode= 'nearest')\n",
    "            target_df.P = y_smooth\n",
    "\n",
    "            # Hist plot which is not that great\n",
    "            # viz.histogram(X = np.array(target_df.N), env=viz_env_str, win=win_str,\n",
    "            #             opts= dict(title=title_str,numbins = 50))\n",
    "\n",
    "            # Line plot\n",
    "            # target_df.drop_duplicates(inplace=True)\n",
    "            # viz.line(X = np.array(target_df.N),Y= np.array(target_df.P), env=viz_env_str, win=win_str,opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "            # Bar plot. 这个看起来是最准确的一个\n",
    "            target_df.drop_duplicates(inplace=True)\n",
    "            arr_str = [str(i) for i in np.array(target_df.N)]\n",
    "            viz.bar(X=np.array(target_df.P), env=viz_env_str, win=win_str,\n",
    "                    opts=dict(title=title_str,rownames=arr_str))\n",
    "\n",
    "            # Scatter plot\n",
    "            # target_df.drop_duplicates(inplace=True)\n",
    "            # viz.scatter(X=np.array(target_df),env=viz_env_str, win=win_str,opts=dict(title=title_str,markersize = 3))\n",
    "    print(\"Done\")\n",
    "\n",
    "# draw_smooth_target_data()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#### Test for drawing\n",
    "def test_draw():\n",
    "    viz = Visdom(env=\"001\")\n",
    "\n",
    "    mu = torch.tensor([0,10,20])\n",
    "    sigma = torch.tensor([1,1,1])\n",
    "    duration = torch.tensor([0,1,2,0])\n",
    "    duration = torch.repeat_interleave(duration.unsqueeze(dim=1), repeats=3, dim=1)\n",
    "    m = torch.distributions.Normal(loc=mu, scale=sigma)\n",
    "    pi = torch.tensor([0.2,0.3,0.5])\n",
    "\n",
    "    # draw\n",
    "    x_0 = torch.tensor(np.arange(0,1000))\n",
    "    x = torch.repeat_interleave(x_0.unsqueeze(dim=1), repeats=3, dim=1)\n",
    "    y = torch.exp(m.log_prob(x))\n",
    "    y_sum = torch.unsqueeze(torch.sum(pi*y,dim=1),dim=1)\n",
    "    viz.line(X = x_0,Y= torch.cat([y,y_sum],dim = 1), env=\"001\", win=\"test_draw_2\",\n",
    "            opts= dict(title='test_draw', legend=['N1', 'N2', 'N3','NNN']))\n",
    "# test_draw()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.2 plot mdn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2.1 mdn pdf的图像[with input data]\n",
    "1. 求解[1,6,11...]上的cdf：\n",
    "    - 求解[1,6,11..]的cdf，再求解[6,11,..]，两者相减"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# smoothed_y[t] = average(y[t-k], y[t-k+1], ..., y[t+k-1], y[t+k])\n",
    "# sm表示滑动窗口大小,为2*k+1,\n",
    "def smooth(data, sm=1):\n",
    "    if sm > 1:\n",
    "        smooth_data = []\n",
    "        for d in data:\n",
    "            y = np.ones(sm)*1.0/sm\n",
    "            d = np.convolve(y, d, \"same\")\n",
    "\n",
    "            smooth_data.append(d)\n",
    "\n",
    "    return smooth_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def draw_mdn_2(pi, mu, sigma, target, input, total_train_step, opt.N_gaussians):\n",
    "    \"\"\"\n",
    "    画两条曲线，pred-1表示归一化前，pred-2表示归一化后\n",
    "\n",
    "    :param pi:\n",
    "    :param mu:\n",
    "    :param sigma:\n",
    "    :param target:\n",
    "    :param input:\n",
    "    :param total_train_step:\n",
    "    :param opt.N_gaussians:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 只画一个batch中的第一个\n",
    "\n",
    "    # The target distrb.\n",
    "    target = torch.unique(target,dim=0)    # drop the duplicate\n",
    "    n_target = target[:,0]\n",
    "    non_zero_idx = torch.nonzero(n_target)\n",
    "    n = n_target[non_zero_idx]\n",
    "\n",
    "    p_target = target[:,1]\n",
    "    p = p_target[non_zero_idx]\n",
    "    max_n = int(max(n).item())\n",
    "\n",
    "    # The predicted distrb.\n",
    "    m = torch.distributions.Normal(mu,sigma)          # Gaussian\n",
    "    # m = torch.distributions.Laplace(mu,sigma)           # Laplace\n",
    "\n",
    "    x_0 = torch.arange(1,(max_n+opt.TARGET),opt.TARGET).to(device=device)\n",
    "\n",
    "    x = torch.repeat_interleave(x_0.unsqueeze(dim=1), repeats=opt.N_gaussians, dim=1)   # expand dim\n",
    "    # 求解每个长度为TARGET的区间上的cdf\n",
    "    y = (m.cdf(x+opt.TARGET) - m.cdf(x)).to(device=device)\n",
    "\n",
    "    # 方法一：\n",
    "    y_pred_1 = torch.sum(pi*y,dim=1)\n",
    "    # 方法二：做一下(0,1)上的归一化\n",
    "    y_pred_2 = y_pred_1/y_pred_1.sum()\n",
    "\n",
    "    # The input distrb.\n",
    "    input_data = input[0:2,:]\n",
    "    x_input = torch.arange(1,(300+1),opt.SCALE).to(device=device)\n",
    "    y_input_0 = (input_data[0,:]).to(device=device)\n",
    "    y_input_1 = (input_data[1,:]).to(device=device)\n",
    "\n",
    "    # Init\n",
    "    win_str = \"total_train_step = \"+str(total_train_step)\n",
    "    title_str = \"Distrb. in \"+win_str\n",
    "    viz.line(X = [0.],Y = [0.], env=\"001\", win=win_str, opts= dict(title=title_str))\n",
    "\n",
    "    # Visdom本身不能把hist和line画在一个window中\n",
    "    # 如果想画一起只能是两条line\n",
    "    # Plot y_target\n",
    "    # viz.histogram(X = n, env=\"001\", win=win_str,\n",
    "    #         opts= dict(title=title_str))\n",
    "    viz.line(X = n,Y= p, env=\"001\", win=win_str, update=\"append\", name='target',\n",
    "            opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "    # Plot y_pred\n",
    "    viz.line(X = x_0,Y= y_pred_1, env=\"001\", win=win_str, update=\"append\", name='pred-1',\n",
    "            opts= dict(title=title_str))\n",
    "\n",
    "    # Plot y_pred_2\n",
    "    viz.line(X = x_0,Y= y_pred_2, env=\"001\", win=win_str, update=\"append\", name='pred-2',\n",
    "            opts= dict(title=title_str))\n",
    "\n",
    "    # Plot y_input\n",
    "    # 如果input data长度短，全部画完\n",
    "    if(len(x_input) <= max_n):\n",
    "        viz.line(X = x_input,Y= y_input_0, env=\"001\", win=win_str, update=\"append\", name='GT-1', opts= dict(title=title_str))\n",
    "        viz.line(X = x_input,Y= y_input_1, env=\"001\", win=win_str, update=\"append\", name='GT-2', opts= dict(title=title_str))\n",
    "    # 如果input data长度长，则截断\n",
    "    else:\n",
    "        x_input_0 = x_input[0:max_n]\n",
    "        y_input_2 = y_input_0[0:max_n]/y_input_0[0:max_n].sum()\n",
    "        y_input_3 = y_input_1[0:max_n]/y_input_1[0:max_n].sum()\n",
    "        viz.line(X = x_input_0,Y= y_input_2, env=\"001\", win=win_str, update=\"append\", name='GT-1', opts= dict(title=title_str))\n",
    "        viz.line(X = x_input_0,Y= y_input_3, env=\"001\", win=win_str, update=\"append\", name='GT-2', opts= dict(title=title_str))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2.2 plot mdn cdf[无input data]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def draw_mdn_cdf(pi, mu, sigma, target, input, total_train_step, opt.N_gaussians):\n",
    "    # 只画一个batch中的第一个\n",
    "\n",
    "    # The target distrb.\n",
    "    target = torch.unique(target,dim=0)    # drop the duplicate\n",
    "    n_target = target[:,0]\n",
    "    non_zero_idx = torch.nonzero(n_target)\n",
    "    n = n_target[non_zero_idx]\n",
    "\n",
    "    p_target = target[:,1]\n",
    "    p = p_target[non_zero_idx]\n",
    "    max_n = max(n).item()               # 横轴长度\n",
    "\n",
    "    # The predicted distrb.\n",
    "    m = torch.distributions.Normal(mu,sigma)\n",
    "    x_0 = torch.arange(1,max_n).to(device=device)\n",
    "    x = torch.repeat_interleave(x_0.unsqueeze(dim=1), repeats=opt.N_gaussians, dim=1)\n",
    "    y = m.cdf(x).to(device=device)                        # y:多条高斯曲线; y_pred: 一条GMM曲线\n",
    "    # y_pred = torch.unsqueeze(torch.sum(pi*y,dim=1),dim=1)                 # 维度相等才能cat\n",
    "    y_pred = torch.sum(pi*y,dim=1)\n",
    "\n",
    "    # Init\n",
    "    win_str = \"total_train_step = \"+str(total_train_step)\n",
    "    title_str = \"Distrb. in \"+win_str\n",
    "    viz.line(X = [0.],Y = [0.], env=\"001\", win=win_str, opts= dict(title=title_str))\n",
    "\n",
    "    # Visdom本身不能把hist和line画在一个window中\n",
    "    # 如果想画一起只能是两条line\n",
    "    # Plot y_target\n",
    "    # viz.histogram(X = n, env=\"001\", win=win_str,\n",
    "    #         opts= dict(title=title_str))\n",
    "    viz.line(X = n,Y= p, env=\"001\", win=win_str, update=\"append\", name='target',\n",
    "            opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "    # Plot y_pred\n",
    "    viz.line(X = x_0,Y= y_pred, env=\"001\", win=win_str, update=\"append\", name='pred',\n",
    "            opts= dict(title=title_str))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2.3 plot mdn pdf in TEST\n",
    "1. 读入2个model时，在test set上比较效果"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_env = \"003\"\n",
    "def draw_mdn_3(Pi1, Mu1, Sigma1, Pi2, Mu2, Sigma2, Target, Input, test_batch, opt.N_gaussians):\n",
    "    for i in range(len(Pi1)):\n",
    "        # The target distrb.\n",
    "        target = Target[i]\n",
    "        target = torch.unique(target,dim=0)    # drop the duplicate\n",
    "        n_target = target[:,0]\n",
    "        non_zero_idx = torch.nonzero(n_target)\n",
    "        n = n_target[non_zero_idx]\n",
    "\n",
    "        p_target = target[:,1]\n",
    "        p = p_target[non_zero_idx]\n",
    "        max_n = int(max(n).item())\n",
    "\n",
    "        # The predicted distrb.\n",
    "        mu1 = Mu1[i]\n",
    "        mu2 = Mu2[i]\n",
    "        sigma1 = Sigma1[i]\n",
    "        sigma2 = Sigma2[i]\n",
    "        m1 = torch.distributions.Normal(mu1,sigma1)          # Gaussian\n",
    "        m2 = torch.distributions.Normal(mu2,sigma2)          # Gaussian\n",
    "        # m = torch.distributions.Laplace(mu,sigma)           # Laplace\n",
    "\n",
    "        x_0 = torch.arange(1,(max_n+opt.TARGET),opt.TARGET).to(device=device)\n",
    "\n",
    "        x = torch.repeat_interleave(x_0.unsqueeze(dim=1), repeats=opt.N_gaussians, dim=1)   # expand dim\n",
    "        # 求解每个长度为TARGET的区间上的cdf\n",
    "        y1 = (m1.cdf(x+opt.TARGET) - m1.cdf(x)).to(device=device)\n",
    "        y2 = (m2.cdf(x+opt.TARGET) - m2.cdf(x)).to(device=device)\n",
    "\n",
    "        # 方法一：\n",
    "        pi1 = Pi1[i]\n",
    "        pi2 = Pi2[i]\n",
    "        y_mdn_1 = torch.sum(pi1*y1,dim=1)\n",
    "        y_mdn_2 = torch.sum(pi2*y2,dim=1)\n",
    "        # 方法二：做一下(0,1)上的归一化\n",
    "        y_pred_1 = y_mdn_1/y_mdn_1.sum()\n",
    "        y_pred_2 = y_mdn_2/y_mdn_2.sum()\n",
    "\n",
    "        # The input distrb.\n",
    "        input = Input[i]\n",
    "        input_data = input[0:2,:]\n",
    "        x_input = torch.arange(1,(300+1),opt.SCALE).to(device=device)\n",
    "        y_input_0 = (input_data[0,:]).to(device=device)\n",
    "        y_input_1 = (input_data[1,:]).to(device=device)\n",
    "\n",
    "        # Init\n",
    "        win_str = \"Test batch = \"+str(test_batch)+\" idx = \"+str(i)\n",
    "        title_str = \"Distrb. of \"+win_str\n",
    "        viz.line(X = [0.],Y = [0.], env=test_env, win=win_str, opts= dict(title=title_str))\n",
    "\n",
    "        # Visdom本身不能把hist和line画在一个window中\n",
    "        # 如果想画一起只能是两条line\n",
    "        # Plot y_target\n",
    "        # viz.histogram(X = n, env=\"001\", win=win_str,\n",
    "        #         opts= dict(title=title_str))\n",
    "        viz.line(X = n,Y= p, env=test_env, win=win_str, update=\"append\", name='target',\n",
    "                opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "        # Plot y_pred\n",
    "        viz.line(X = x_0,Y= y_pred_1, env=test_env, win=win_str, update=\"append\", name='pred-MLE',opts= dict(title=title_str))\n",
    "        viz.line(X = x_0,Y= y_pred_2, env=test_env, win=win_str, update=\"append\", name='pred-WD',\n",
    "                opts= dict(title=title_str))\n",
    "\n",
    "        # Plot y_input\n",
    "        # 如果input data长度短，全部画完\n",
    "        if(len(x_input) <= max_n):\n",
    "            viz.line(X = x_input,Y= y_input_0, env=test_env, win=win_str, update=\"append\", name='GT-1', opts= dict(title=title_str))\n",
    "            viz.line(X = x_input,Y= y_input_1, env=test_env, win=win_str, update=\"append\", name='GT-2', opts= dict(title=title_str))\n",
    "        # 如果input data长度长，则截断\n",
    "        else:\n",
    "            x_input_0 = x_input[0:max_n]\n",
    "            y_input_2 = y_input_0[0:max_n]/y_input_0[0:max_n].sum()\n",
    "            y_input_3 = y_input_1[0:max_n]/y_input_1[0:max_n].sum()\n",
    "            viz.line(X = x_input_0,Y= y_input_2, env=test_env, win=win_str, update=\"append\", name='GT-1', opts= dict(title=title_str))\n",
    "            viz.line(X = x_input_0,Y= y_input_3, env=test_env, win=win_str, update=\"append\", name='GT-2', opts= dict(title=title_str))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.3 png格式的net structure"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def draw_the_net_png():\n",
    "\n",
    "    x = torch.randn([1, 3, 300])  # 定义网络的输入值\n",
    "    mlp = MLP_1_1(opt.N_gaussians)\n",
    "    y = mlp(x)                    # 获取网络的预测值\n",
    "\n",
    "    MyConvNetVis = make_dot(y, params=dict(list(mlp.named_parameters()) + [('x', x)]))\n",
    "    MyConvNetVis.format = \"png\"\n",
    "    # 指定文件生成的文件夹\n",
    "    MyConvNetVis.directory = \"data_pic\"\n",
    "    # 生成文件\n",
    "    MyConvNetVis.view()\n",
    "# draw_the_net_png()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.4 plot the loss\n",
    "1. 直接根据参数决定draw什么loss，反正loss都画在一个window里面"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "win_train_loss_str = \"The Loss of BATCH in the Training Data\"\n",
    "win_vali_loss_str = \"The Loss in the Vali Data\"\n",
    "win_train_epoch_loss_str = \"The Loss of EPOCH in the Training Data\"\n",
    "\n",
    "def draw_loss(X_step, loss, win_str):\n",
    "    viz.line(X = [X_step], Y = [loss],win=win_str, update=\"append\",\n",
    "        opts= dict(title=win_str))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.5 plot the metric"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "win_vali_metric_str = \"The NLL of ALL vali data\"\n",
    "def draw_metric(X_step, total_vali_metric, GT_metric):\n",
    "\n",
    "    viz.line(X = [X_step], Y = [[total_vali_metric,GT_metric[0,0],GT_metric[0,1]]],win=win_vali_metric_str, update=\"append\", opts= dict(title=win_vali_metric_str, legend=['pred','GT-1','GT-2'], showlegend=True,xlabel=\"epoch\", ylabel=\"NLL\"))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.6 grad graph\n",
    "https://github.com/t-vi/pytorch-tvmisc/blob/master/visualize/bad_grad_viz.ipynb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "outputs": [],
   "source": [
    "\n",
    "def iter_graph(root, callback):\n",
    "    queue = [root]\n",
    "    seen = set()\n",
    "    while queue:\n",
    "        # 队头出fn然后判断fn是否seen\n",
    "        fn = queue.pop()\n",
    "        if fn in seen:\n",
    "            continue\n",
    "        # unseen加入queue，并且继续沿着fn向下递归（DFS？）\n",
    "        seen.add(fn)\n",
    "        for next_fn, _ in fn.next_functions:\n",
    "            # Alert：只有非none的才会加入queue，然后在register_grad里被fn_dict记录下grad_input\n",
    "            if next_fn is not None:\n",
    "                queue.append(next_fn)\n",
    "        callback(fn)\n",
    "\n",
    "def register_hooks(var):\n",
    "    fn_dict = {}  # 记录了grad名字和值（grad_input）\n",
    "    def hook_cb(fn):\n",
    "        def register_grad(grad_input, grad_output):\n",
    "            fn_dict[fn] = grad_input\n",
    "        fn.register_hook(register_grad)\n",
    "\n",
    "    # 1. 递归注册grad_input\n",
    "    iter_graph(var.grad_fn, hook_cb)\n",
    "\n",
    "    # def is_bad_grad(grad_output):\n",
    "    #     if grad_output is None:\n",
    "    #         return False\n",
    "    #     return grad_output.isnan().any() or (grad_output.abs() >= 1e6).any()\n",
    "    def is_bad_grad(grad_output):\n",
    "        if grad_output is None:\n",
    "                return True\n",
    "        grad_output = grad_output.data\n",
    "        return grad_output.ne(grad_output).any() or grad_output.gt(1e6).any()\n",
    "\n",
    "    # 2. graph已经从grad input构建完了，还有grad output的判断以及颜色的选择\n",
    "    def make_dot():\n",
    "        node_attr = dict(style='filled',\n",
    "                        shape='box',\n",
    "                        align='left',\n",
    "                        fontsize='12',\n",
    "                        ranksep='0.1',\n",
    "                        height='0.2')\n",
    "        dot = Digraph(node_attr=node_attr, graph_attr=dict(size=\"12,12\"))\n",
    "\n",
    "        def size_to_str(size):\n",
    "            return '('+(', ').join(map(str, size))+')'\n",
    "\n",
    "        def build_graph(fn):\n",
    "            if hasattr(fn, 'variable'):  # if GradAccumulator\n",
    "                u = fn.variable\n",
    "                node_name = 'Variable\\n ' + size_to_str(u.size())\n",
    "                dot.node(str(id(u)), node_name, fillcolor='lightblue')\n",
    "            else:  # 不是variable属性，白色\n",
    "                assert fn in fn_dict, fn   # 判断fn在不在fn_dict里面！\n",
    "                fillcolor = 'white'\n",
    "                if any(is_bad_grad(gi) for gi in fn_dict[fn]):\n",
    "                    fillcolor = 'red'\n",
    "                dot.node(str(id(fn)), str(type(fn).__name__), fillcolor=fillcolor)\n",
    "            for next_fn, _ in fn.next_functions:\n",
    "                if next_fn is not None:\n",
    "                    next_id = id(getattr(next_fn, 'variable', next_fn))\n",
    "                    dot.edge(str(next_id), str(id(fn)))\n",
    "        iter_graph(var.grad_fn, build_graph)\n",
    "\n",
    "        return dot\n",
    "\n",
    "    return make_dot\n",
    "\n",
    "# x = torch.randn(10, 10, requires_grad=True)\n",
    "# y = torch.randn(10, 10, requires_grad=True)\n",
    "#\n",
    "# z = x / (y * 0)\n",
    "# z = z.sum() * 2\n",
    "# get_dot = register_hooks(z)\n",
    "\n",
    "\n",
    "# with torch.autograd.detect_anomaly():\n",
    "#     z.backward()\n",
    "#     dot = get_dot()\n",
    "#     dot.save('tmp.dot') # to get .dot\n",
    "#     dot.render('tmp') # to get SVG\n",
    "#     # dot # in Jupyter, you can just render the variable"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6 Training\n",
    "## 6.1 Init"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "       BatchNorm1d-1               [-1, 3, 300]               6\n",
      "            Conv2d-2             [-1, 1, 1, 97]              37\n",
      "            Conv2d-3             [-1, 1, 1, 97]              37\n",
      "            Conv2d-4             [-1, 1, 1, 97]              37\n",
      "       BatchNorm1d-5                   [-1, 97]             194\n",
      "       BatchNorm1d-6                   [-1, 97]             194\n",
      "       BatchNorm1d-7                   [-1, 97]             194\n",
      "            Linear-8                    [-1, 3]             294\n",
      "           Softmax-9                    [-1, 3]               0\n",
      "           Linear-10                    [-1, 3]             294\n",
      "           Linear-11                    [-1, 3]             294\n",
      "================================================================\n",
      "Total params: 1,581\n",
      "Trainable params: 1,581\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.02\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "mlp = MLP_1_1(opt.N_gaussians)\n",
    "\n",
    "# Init the params\n",
    "# mlp = model_param_init(mlp)\n",
    "\n",
    "# Save the init params\n",
    "# torch.save(mlp.state_dict(), 'mlp_init.pth')\n",
    "\n",
    "# Load the saved model\n",
    "# model_path_WD = \"mlp_train=300_WD.pth\"\n",
    "# model_path_LSE = \"mlp_train=300_LSE.pth\"\n",
    "# model_path_MLE = \"mlp_train=300_MLE.pth\"\n",
    "#\n",
    "# model_data = torch.load(model_path_MLE)\n",
    "# mlp.load_state_dict(model_data)\n",
    "\n",
    "mlp = mlp.to(device=device)\n",
    "summary(mlp, (3,300))\n",
    "\n",
    "############ learning rate strategy ############\n",
    "# 1.\n",
    "# Set different lr for params\n",
    "# id: id() 函数返回对象的唯一标识符，标识符是一个整数。返回对象的内存地址。\n",
    "mu_params = list(map(id, mlp.z_mu.parameters()))\n",
    "sigma_params = list(map(id, mlp.z_sigma.parameters()))\n",
    "pi_params = list(map(id, mlp.z_pi.parameters()))\n",
    "conv2_params = list(map(id, mlp.conv2.parameters()))\n",
    "\n",
    "params_id = mu_params + sigma_params + pi_params + conv2_params\n",
    "\n",
    "base_params = filter(lambda p: id(p) not in params_id, mlp.parameters())\n",
    "params = [{'params': base_params},         # 如果对某个参数不指定学习率，就使用最外层的默认学习率\n",
    "          {'params': mlp.z_pi.parameters(), 'lr': opt.learning_rate},\n",
    "        {'params': mlp.z_mu.parameters(), 'lr': opt.learning_rate},\n",
    "        {'params': mlp.conv2.parameters(), 'lr': opt.learning_rate},\n",
    "          {'params': mlp.z_sigma.parameters(), 'lr': opt.lr_for_sigma}]\n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr=opt.learning_rate,weight_decay=opt.weight_decay)\n",
    "\n",
    "# 2.\n",
    "# Or set lr decay\n",
    "# StepLR为步进，每step_size个epoch，lr*gamma\n",
    "scheduler  = torch.optim.lr_scheduler.StepLR(optimizer,step_size=opt.StepLR_step_size,gamma=opt.StepLR_gamma)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=5, verbose=False, threshold=1e-3, threshold_mode='abs', cooldown=0, min_lr=1e-7, eps=1e-7)\n",
    "# threshold:只关注超过阈值的显著变化；cooldown：触发一次条件后，等待一定epoch再进行检测，避免lr下降过速；\n",
    "\n",
    "# # Set the hooks\n",
    "# #mlp.conv1.register_forward_hook(hook_forward_fn)\n",
    "# mlp.conv1.register_full_backward_hook(hook_backward_fn)\n",
    "#\n",
    "# #mlp.linear1.register_forward_hook(hook_forward_fn)\n",
    "# #mlp.linear1.register_full_backward_hook(hook_backward_fn)\n",
    "#\n",
    "# #mlp.z_pi.register_forward_hook(hook_forward_fn)\n",
    "# mlp.z_pi.register_full_backward_hook(hook_backward_fn_pi)\n",
    "#\n",
    "# #mlp.z_mu.register_forward_hook(hook_forward_fn)\n",
    "# mlp.z_mu.register_full_backward_hook(hook_backward_fn_mu)\n",
    "#\n",
    "# mlp.z_sigma.register_forward_hook(hook_forward_fn)\n",
    "# mlp.z_sigma.register_full_[2, 3, 300]backward_hook(hook_backward_fn_sigma)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir=\"logs-MLP/\"+opt.logs_str,flush_secs=60)\n",
    "\n",
    "# Init the vis win\n",
    "viz.line(X = [0.],Y = [0.], env=\"001\", win=win_train_loss_str, opts= dict(title=win_train_loss_str))\n",
    "viz.line(X = [0.],Y = [0.], env=\"001\", win=win_train_epoch_loss_str, opts= dict(title=win_train_epoch_loss_str))\n",
    "viz.line(X = [0.],Y = [[0.,0.,0.]], env=\"001\", win=win_vali_metric_str, opts= dict(title=win_vali_metric_str,legend=['pred','GT-1','GT-2'], showlegend=True,xlabel=\"epoch\", ylabel=\"NLL\"))\n",
    "\n",
    "plot_net(writer,mlp,torch.randn((2,3,300),device=device))\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.2 grad check\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "outputs": [],
   "source": [
    "# # grad check\n",
    "# input_test = torch.randn((2,3,300), requires_grad=True,device=device)\n",
    "# test_ans = gradcheck(mlp.to(device), input_test, eps=1e-3)  #, eps=1e-6, atol=1e-4\n",
    "# print(\"Are the gradients correct: \", test_ans)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.3 START HERE\n",
    "1. 使用Laplace分布改：\n",
    "    - loss function和draw"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- 11 batch----\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Now this is EPOCH 0 ==========\n",
      "========== IN EPOCH 0 the total loss is 101710.92944335938 ==========\n",
      "========== IN EPOCH 0 the vali NLL loss is 66.32331848144531 ==========\n",
      "========== IN EPOCH 0 the GT metric is [[ 6.6724715 10.343235 ]] ==========\n",
      "========== Now this is EPOCH 1 ==========\n",
      "========== IN EPOCH 1 the total loss is 73219.93115234375 ==========\n",
      "========== IN EPOCH 1 the vali NLL loss is 24.93205451965332 ==========\n",
      "========== IN EPOCH 1 the GT metric is [[ 6.672472 10.343234]] ==========\n",
      "========== Now this is EPOCH 2 ==========\n",
      "========== IN EPOCH 2 the total loss is 58309.97009277344 ==========\n",
      "========== IN EPOCH 2 the vali NLL loss is 21.414222717285156 ==========\n",
      "========== IN EPOCH 2 the GT metric is [[ 6.6724715 10.343234 ]] ==========\n",
      "========== Now this is EPOCH 3 ==========\n",
      "========== IN EPOCH 3 the total loss is 46383.664001464844 ==========\n",
      "========== IN EPOCH 3 the vali NLL loss is 17.076507568359375 ==========\n",
      "========== IN EPOCH 3 the GT metric is [[ 6.672472 10.343235]] ==========\n",
      "========== Now this is EPOCH 4 ==========\n",
      "========== IN EPOCH 4 the total loss is 36196.960388183594 ==========\n",
      "========== IN EPOCH 4 the vali NLL loss is 15.574339866638184 ==========\n",
      "========== IN EPOCH 4 the GT metric is [[ 6.6724715 10.343235 ]] ==========\n",
      "========== Now this is EPOCH 5 ==========\n",
      "========== IN EPOCH 5 the total loss is 28219.53790283203 ==========\n",
      "========== IN EPOCH 5 the vali NLL loss is 15.825189590454102 ==========\n",
      "========== IN EPOCH 5 the GT metric is [[ 6.6724715 10.343234 ]] ==========\n",
      "========== Now this is EPOCH 6 ==========\n",
      "========== IN EPOCH 6 the total loss is 23524.774627685547 ==========\n",
      "========== IN EPOCH 6 the vali NLL loss is 12.534377098083496 ==========\n",
      "========== IN EPOCH 6 the GT metric is [[ 6.6724715 10.343235 ]] ==========\n",
      "========== Now this is EPOCH 7 ==========\n",
      "========== IN EPOCH 7 the total loss is 20429.996215820312 ==========\n",
      "========== IN EPOCH 7 the vali NLL loss is 12.375661849975586 ==========\n",
      "========== IN EPOCH 7 the GT metric is [[ 6.672472 10.343235]] ==========\n",
      "========== Now this is EPOCH 8 ==========\n",
      "========== IN EPOCH 8 the total loss is 19035.489532470703 ==========\n",
      "========== IN EPOCH 8 the vali NLL loss is 12.31892204284668 ==========\n",
      "========== IN EPOCH 8 the GT metric is [[ 6.6724715 10.343234 ]] ==========\n",
      "========== Now this is EPOCH 9 ==========\n",
      "========== IN EPOCH 9 the total loss is 17969.926208496094 ==========\n",
      "========== IN EPOCH 9 the vali NLL loss is 12.351572036743164 ==========\n",
      "========== IN EPOCH 9 the GT metric is [[ 6.6724715 10.343234 ]] ==========\n",
      "========== Now this is EPOCH 10 ==========\n",
      "========== IN EPOCH 10 the total loss is 17146.461303710938 ==========\n",
      "========== IN EPOCH 10 the vali NLL loss is 12.167020797729492 ==========\n",
      "========== IN EPOCH 10 the GT metric is [[ 6.672472 10.343234]] ==========\n",
      "========== Now this is EPOCH 11 ==========\n",
      "========== IN EPOCH 11 the total loss is 16565.038360595703 ==========\n",
      "========== IN EPOCH 11 the vali NLL loss is 10.721718788146973 ==========\n",
      "========== IN EPOCH 11 the GT metric is [[ 6.672472 10.343233]] ==========\n",
      "========== Now this is EPOCH 12 ==========\n",
      "========== IN EPOCH 12 the total loss is 16007.92398071289 ==========\n",
      "========== IN EPOCH 12 the vali NLL loss is 10.875876426696777 ==========\n",
      "========== IN EPOCH 12 the GT metric is [[ 6.672472 10.343235]] ==========\n",
      "========== Now this is EPOCH 13 ==========\n",
      "========== IN EPOCH 13 the total loss is 15486.840087890625 ==========\n",
      "========== IN EPOCH 13 the vali NLL loss is 10.977347373962402 ==========\n",
      "========== IN EPOCH 13 the GT metric is [[ 6.6724715 10.343234 ]] ==========\n",
      "========== Now this is EPOCH 14 ==========\n",
      "========== IN EPOCH 14 the total loss is 15024.438537597656 ==========\n",
      "========== IN EPOCH 14 the vali NLL loss is 10.468318939208984 ==========\n",
      "========== IN EPOCH 14 the GT metric is [[ 6.6724715 10.343235 ]] ==========\n",
      "========== Now this is EPOCH 15 ==========\n",
      "========== IN EPOCH 15 the total loss is 14747.79360961914 ==========\n",
      "========== IN EPOCH 15 the vali NLL loss is 9.735737800598145 ==========\n",
      "========== IN EPOCH 15 the GT metric is [[ 6.6724715 10.343235 ]] ==========\n",
      "========== Now this is EPOCH 16 ==========\n",
      "========== IN EPOCH 16 the total loss is 14217.364013671875 ==========\n",
      "========== IN EPOCH 16 the vali NLL loss is 10.201936721801758 ==========\n",
      "========== IN EPOCH 16 the GT metric is [[ 6.6724715 10.343234 ]] ==========\n",
      "========== Now this is EPOCH 17 ==========\n",
      "========== IN EPOCH 17 the total loss is 14077.948486328125 ==========\n",
      "========== IN EPOCH 17 the vali NLL loss is 9.73934555053711 ==========\n",
      "========== IN EPOCH 17 the GT metric is [[ 6.6724715 10.343235 ]] ==========\n",
      "========== Now this is EPOCH 18 ==========\n",
      "========== IN EPOCH 18 the total loss is 13904.603546142578 ==========\n",
      "========== IN EPOCH 18 the vali NLL loss is 9.866209030151367 ==========\n",
      "========== IN EPOCH 18 the GT metric is [[ 6.6724715 10.343235 ]] ==========\n",
      "========== Now this is EPOCH 19 ==========\n",
      "========== IN EPOCH 19 the total loss is 13610.524505615234 ==========\n",
      "========== IN EPOCH 19 the vali NLL loss is 9.794970512390137 ==========\n",
      "========== IN EPOCH 19 the GT metric is [[ 6.6724715 10.343235 ]] ==========\n",
      "========== Now this is EPOCH 20 ==========\n",
      "========== IN EPOCH 20 the total loss is 13222.069244384766 ==========\n",
      "========== IN EPOCH 20 the vali NLL loss is 9.768789291381836 ==========\n",
      "========== IN EPOCH 20 the GT metric is [[ 6.6724715 10.343235 ]] ==========\n",
      "========== Now this is EPOCH 21 ==========\n",
      "========== IN EPOCH 21 the total loss is 13130.859771728516 ==========\n",
      "========== IN EPOCH 21 the vali NLL loss is 9.145715713500977 ==========\n",
      "========== IN EPOCH 21 the GT metric is [[ 6.672472 10.343235]] ==========\n",
      "========== Now this is EPOCH 22 ==========\n",
      "========== IN EPOCH 22 the total loss is 13126.803314208984 ==========\n",
      "========== IN EPOCH 22 the vali NLL loss is 9.593729972839355 ==========\n",
      "========== IN EPOCH 22 the GT metric is [[ 6.6724715 10.343234 ]] ==========\n",
      "========== Now this is EPOCH 23 ==========\n",
      "========== IN EPOCH 23 the total loss is 13017.60205078125 ==========\n",
      "========== IN EPOCH 23 the vali NLL loss is 8.99666976928711 ==========\n",
      "========== IN EPOCH 23 the GT metric is [[ 6.6724725 10.343234 ]] ==========\n",
      "========== Now this is EPOCH 24 ==========\n",
      "========== IN EPOCH 24 the total loss is 12887.71418762207 ==========\n",
      "========== IN EPOCH 24 the vali NLL loss is 9.419596672058105 ==========\n",
      "========== IN EPOCH 24 the GT metric is [[ 6.672472 10.343234]] ==========\n",
      "========== Now this is EPOCH 25 ==========\n",
      "========== IN EPOCH 25 the total loss is 12849.925201416016 ==========\n",
      "========== IN EPOCH 25 the vali NLL loss is 8.967007637023926 ==========\n",
      "========== IN EPOCH 25 the GT metric is [[ 6.672472 10.343235]] ==========\n",
      "========== Now this is EPOCH 26 ==========\n",
      "========== IN EPOCH 26 the total loss is 12818.337341308594 ==========\n",
      "========== IN EPOCH 26 the vali NLL loss is 9.204774856567383 ==========\n",
      "========== IN EPOCH 26 the GT metric is [[ 6.6724715 10.343235 ]] ==========\n",
      "========== Now this is EPOCH 27 ==========\n",
      "========== IN EPOCH 27 the total loss is 12548.37255859375 ==========\n",
      "========== IN EPOCH 27 the vali NLL loss is 9.062026023864746 ==========\n",
      "========== IN EPOCH 27 the GT metric is [[ 6.6724715 10.343235 ]] ==========\n",
      "========== Now this is EPOCH 28 ==========\n",
      "========== IN EPOCH 28 the total loss is 12534.52407836914 ==========\n",
      "========== IN EPOCH 28 the vali NLL loss is 9.022661209106445 ==========\n",
      "========== IN EPOCH 28 the GT metric is [[ 6.6724715 10.343234 ]] ==========\n",
      "========== Now this is EPOCH 29 ==========\n",
      "========== IN EPOCH 29 the total loss is 12364.116241455078 ==========\n",
      "========== IN EPOCH 29 the vali NLL loss is 9.156414985656738 ==========\n",
      "========== IN EPOCH 29 the GT metric is [[ 6.6724725 10.343235 ]] ==========\n",
      "========== Now this is EPOCH 30 ==========\n",
      "========== IN EPOCH 30 the total loss is 12461.622528076172 ==========\n",
      "========== IN EPOCH 30 the vali NLL loss is 8.787543296813965 ==========\n",
      "========== IN EPOCH 30 the GT metric is [[ 6.6724715 10.343233 ]] ==========\n",
      "========== Now this is EPOCH 31 ==========\n",
      "========== IN EPOCH 31 the total loss is 12132.66455078125 ==========\n",
      "========== IN EPOCH 31 the vali NLL loss is 8.867209434509277 ==========\n",
      "========== IN EPOCH 31 the GT metric is [[ 6.6724725 10.343235 ]] ==========\n",
      "========== Now this is EPOCH 32 ==========\n",
      "========== IN EPOCH 32 the total loss is 12048.344207763672 ==========\n",
      "========== IN EPOCH 32 the vali NLL loss is 8.555998802185059 ==========\n",
      "========== IN EPOCH 32 the GT metric is [[ 6.6724715 10.343234 ]] ==========\n",
      "========== Now this is EPOCH 33 ==========\n",
      "========== IN EPOCH 33 the total loss is 11939.165496826172 ==========\n",
      "========== IN EPOCH 33 the vali NLL loss is 8.556422233581543 ==========\n",
      "========== IN EPOCH 33 the GT metric is [[ 6.6724715 10.343234 ]] ==========\n",
      "========== Now this is EPOCH 34 ==========\n",
      "========== IN EPOCH 34 the total loss is 11976.74838256836 ==========\n",
      "========== IN EPOCH 34 the vali NLL loss is 8.61742877960205 ==========\n",
      "========== IN EPOCH 34 the GT metric is [[ 6.6724715 10.343234 ]] ==========\n",
      "========== Now this is EPOCH 35 ==========\n",
      "========== IN EPOCH 35 the total loss is 11851.933166503906 ==========\n",
      "========== IN EPOCH 35 the vali NLL loss is 8.717606544494629 ==========\n",
      "========== IN EPOCH 35 the GT metric is [[ 6.6724715 10.343235 ]] ==========\n",
      "========== Now this is EPOCH 36 ==========\n",
      "========== IN EPOCH 36 the total loss is 11858.709228515625 ==========\n",
      "========== IN EPOCH 36 the vali NLL loss is 8.222517967224121 ==========\n",
      "========== IN EPOCH 36 the GT metric is [[ 6.6724715 10.343235 ]] ==========\n",
      "========== Now this is EPOCH 37 ==========\n",
      "========== IN EPOCH 37 the total loss is 11617.526504516602 ==========\n",
      "========== IN EPOCH 37 the vali NLL loss is 8.625228881835938 ==========\n",
      "========== IN EPOCH 37 the GT metric is [[ 6.6724715 10.343234 ]] ==========\n",
      "========== Now this is EPOCH 38 ==========\n",
      "========== IN EPOCH 38 the total loss is 11636.495849609375 ==========\n",
      "========== IN EPOCH 38 the vali NLL loss is 8.463051795959473 ==========\n",
      "========== IN EPOCH 38 the GT metric is [[ 6.6724715 10.343234 ]] ==========\n",
      "========== Now this is EPOCH 39 ==========\n",
      "========== IN EPOCH 39 the total loss is 11513.352508544922 ==========\n",
      "========== IN EPOCH 39 the vali NLL loss is 8.322466850280762 ==========\n",
      "========== IN EPOCH 39 the GT metric is [[ 6.672472 10.343235]] ==========\n",
      "========== Now this is EPOCH 40 ==========\n",
      "========== IN EPOCH 40 the total loss is 11620.50422668457 ==========\n",
      "========== IN EPOCH 40 the vali NLL loss is 7.876972675323486 ==========\n",
      "========== IN EPOCH 40 the GT metric is [[ 6.672472 10.343234]] ==========\n",
      "========== Now this is EPOCH 41 ==========\n",
      "========== IN EPOCH 41 the total loss is 11352.013046264648 ==========\n",
      "========== IN EPOCH 41 the vali NLL loss is 8.51319408416748 ==========\n",
      "========== IN EPOCH 41 the GT metric is [[ 6.672472 10.343234]] ==========\n",
      "========== Now this is EPOCH 42 ==========\n",
      "========== IN EPOCH 42 the total loss is 11406.759185791016 ==========\n",
      "========== IN EPOCH 42 the vali NLL loss is 8.176029205322266 ==========\n",
      "========== IN EPOCH 42 the GT metric is [[ 6.6724715 10.343234 ]] ==========\n",
      "========== Now this is EPOCH 43 ==========\n",
      "========== IN EPOCH 43 the total loss is 11351.200897216797 ==========\n",
      "========== IN EPOCH 43 the vali NLL loss is 8.634208679199219 ==========\n",
      "========== IN EPOCH 43 the GT metric is [[ 6.6724715 10.343235 ]] ==========\n",
      "========== Now this is EPOCH 44 ==========\n",
      "========== IN EPOCH 44 the total loss is 11242.217987060547 ==========\n",
      "========== IN EPOCH 44 the vali NLL loss is 8.084063529968262 ==========\n",
      "========== IN EPOCH 44 the GT metric is [[ 6.6724715 10.343234 ]] ==========\n",
      "========== Now this is EPOCH 45 ==========\n",
      "========== IN EPOCH 45 the total loss is 11401.520751953125 ==========\n",
      "========== IN EPOCH 45 the vali NLL loss is 7.979298114776611 ==========\n",
      "========== IN EPOCH 45 the GT metric is [[ 6.6724715 10.343234 ]] ==========\n",
      "========== Now this is EPOCH 46 ==========\n",
      "========== IN EPOCH 46 the total loss is 11119.23046875 ==========\n",
      "========== IN EPOCH 46 the vali NLL loss is 8.338981628417969 ==========\n",
      "========== IN EPOCH 46 the GT metric is [[ 6.6724715 10.343234 ]] ==========\n",
      "========== Now this is EPOCH 47 ==========\n",
      "========== IN EPOCH 47 the total loss is 11274.398406982422 ==========\n",
      "========== IN EPOCH 47 the vali NLL loss is 8.132678985595703 ==========\n",
      "========== IN EPOCH 47 the GT metric is [[ 6.6724715 10.343235 ]] ==========\n",
      "========== Now this is EPOCH 48 ==========\n",
      "========== IN EPOCH 48 the total loss is 11007.168258666992 ==========\n",
      "========== IN EPOCH 48 the vali NLL loss is 7.822937965393066 ==========\n",
      "========== IN EPOCH 48 the GT metric is [[ 6.6724715 10.343234 ]] ==========\n",
      "========== Now this is EPOCH 49 ==========\n",
      "========== IN EPOCH 49 the total loss is 11115.76724243164 ==========\n",
      "========== IN EPOCH 49 the vali NLL loss is 8.186270713806152 ==========\n",
      "========== IN EPOCH 49 the GT metric is [[ 6.6724715 10.343235 ]] ==========\n",
      "========== Now this is EPOCH 50 ==========\n",
      "========== IN EPOCH 50 the total loss is 10897.411834716797 ==========\n",
      "========== IN EPOCH 50 the vali NLL loss is 8.222883224487305 ==========\n",
      "========== IN EPOCH 50 the GT metric is [[ 6.6724715 10.343234 ]] ==========\n",
      "========== Now this is EPOCH 51 ==========\n",
      "========== IN EPOCH 51 the total loss is 11067.207733154297 ==========\n",
      "========== IN EPOCH 51 the vali NLL loss is 7.9788994789123535 ==========\n",
      "========== IN EPOCH 51 the GT metric is [[ 6.6724725 10.343235 ]] ==========\n",
      "========== Now this is EPOCH 52 ==========\n",
      "========== IN EPOCH 52 the total loss is 11001.036041259766 ==========\n",
      "========== IN EPOCH 52 the vali NLL loss is 8.069597244262695 ==========\n",
      "========== IN EPOCH 52 the GT metric is [[ 6.672472 10.343234]] ==========\n",
      "========== Now this is EPOCH 53 ==========\n",
      "========== IN EPOCH 53 the total loss is 10944.465942382812 ==========\n",
      "========== IN EPOCH 53 the vali NLL loss is 7.957643985748291 ==========\n",
      "========== IN EPOCH 53 the GT metric is [[ 6.6724715 10.343234 ]] ==========\n",
      "========== Now this is EPOCH 54 ==========\n",
      "========== IN EPOCH 54 the total loss is 10841.122940063477 ==========\n",
      "========== IN EPOCH 54 the vali NLL loss is 8.05783462524414 ==========\n",
      "========== IN EPOCH 54 the GT metric is [[ 6.6724715 10.343235 ]] ==========\n",
      "========== Now this is EPOCH 55 ==========\n",
      "========== IN EPOCH 55 the total loss is 10879.987335205078 ==========\n",
      "========== IN EPOCH 55 the vali NLL loss is 7.906063556671143 ==========\n",
      "========== IN EPOCH 55 the GT metric is [[ 6.6724715 10.343234 ]] ==========\n",
      "========== Now this is EPOCH 56 ==========\n",
      "========== IN EPOCH 56 the total loss is 10805.155380249023 ==========\n",
      "========== IN EPOCH 56 the vali NLL loss is 7.874752998352051 ==========\n",
      "========== IN EPOCH 56 the GT metric is [[ 6.672472 10.343234]] ==========\n",
      "========== Now this is EPOCH 57 ==========\n",
      "========== IN EPOCH 57 the total loss is 10727.791915893555 ==========\n",
      "========== IN EPOCH 57 the vali NLL loss is 7.809457778930664 ==========\n",
      "========== IN EPOCH 57 the GT metric is [[ 6.672472 10.343234]] ==========\n",
      "========== Now this is EPOCH 58 ==========\n",
      "========== IN EPOCH 58 the total loss is 10688.847610473633 ==========\n",
      "========== IN EPOCH 58 the vali NLL loss is 7.911788463592529 ==========\n",
      "========== IN EPOCH 58 the GT metric is [[ 6.6724715 10.343235 ]] ==========\n",
      "========== Now this is EPOCH 59 ==========\n",
      "========== IN EPOCH 59 the total loss is 10795.664001464844 ==========\n",
      "========== IN EPOCH 59 the vali NLL loss is 7.976593017578125 ==========\n",
      "========== IN EPOCH 59 the GT metric is [[ 6.6724715 10.343235 ]] ==========\n",
      "========== Now this is EPOCH 60 ==========\n",
      "========== IN EPOCH 60 the total loss is 10704.062316894531 ==========\n",
      "========== IN EPOCH 60 the vali NLL loss is 7.77227258682251 ==========\n",
      "========== IN EPOCH 60 the GT metric is [[ 6.672472 10.343234]] ==========\n",
      "========== Now this is EPOCH 61 ==========\n",
      "========== IN EPOCH 61 the total loss is 10821.549514770508 ==========\n",
      "========== IN EPOCH 61 the vali NLL loss is 7.746389389038086 ==========\n",
      "========== IN EPOCH 61 the GT metric is [[ 6.6724715 10.343234 ]] ==========\n",
      "========== Now this is EPOCH 62 ==========\n",
      "========== IN EPOCH 62 the total loss is 10785.670043945312 ==========\n",
      "========== IN EPOCH 62 the vali NLL loss is 7.741364002227783 ==========\n",
      "========== IN EPOCH 62 the GT metric is [[ 6.672472 10.343235]] ==========\n",
      "========== Now this is EPOCH 63 ==========\n",
      "========== IN EPOCH 63 the total loss is 10747.276596069336 ==========\n",
      "========== IN EPOCH 63 the vali NLL loss is 7.845995903015137 ==========\n",
      "========== IN EPOCH 63 the GT metric is [[ 6.6724715 10.343235 ]] ==========\n",
      "========== Now this is EPOCH 64 ==========\n",
      "========== IN EPOCH 64 the total loss is 10667.605712890625 ==========\n",
      "========== IN EPOCH 64 the vali NLL loss is 7.768442153930664 ==========\n",
      "========== IN EPOCH 64 the GT metric is [[ 6.672472 10.343235]] ==========\n",
      "========== Now this is EPOCH 65 ==========\n",
      "========== IN EPOCH 65 the total loss is 10520.24478149414 ==========\n",
      "========== IN EPOCH 65 the vali NLL loss is 7.83859920501709 ==========\n",
      "========== IN EPOCH 65 the GT metric is [[ 6.672472 10.343234]] ==========\n",
      "========== Now this is EPOCH 66 ==========\n",
      "========== IN EPOCH 66 the total loss is 10570.974014282227 ==========\n",
      "========== IN EPOCH 66 the vali NLL loss is 7.819655418395996 ==========\n",
      "========== IN EPOCH 66 the GT metric is [[ 6.672472 10.343235]] ==========\n",
      "========== Now this is EPOCH 67 ==========\n",
      "========== IN EPOCH 67 the total loss is 10554.391159057617 ==========\n",
      "========== IN EPOCH 67 the vali NLL loss is 7.712350845336914 ==========\n",
      "========== IN EPOCH 67 the GT metric is [[ 6.672472 10.343235]] ==========\n",
      "========== Now this is EPOCH 68 ==========\n",
      "========== IN EPOCH 68 the total loss is 10521.233200073242 ==========\n",
      "========== IN EPOCH 68 the vali NLL loss is 7.64367151260376 ==========\n",
      "========== IN EPOCH 68 the GT metric is [[ 6.672472 10.343235]] ==========\n",
      "========== Now this is EPOCH 69 ==========\n",
      "========== IN EPOCH 69 the total loss is 10558.769836425781 ==========\n",
      "========== IN EPOCH 69 the vali NLL loss is 7.799426555633545 ==========\n",
      "========== IN EPOCH 69 the GT metric is [[ 6.6724715 10.343235 ]] ==========\n",
      "========== Now this is EPOCH 70 ==========\n",
      "========== IN EPOCH 70 the total loss is 10647.828857421875 ==========\n",
      "========== IN EPOCH 70 the vali NLL loss is 7.718120098114014 ==========\n",
      "========== IN EPOCH 70 the GT metric is [[ 6.6724715 10.343234 ]] ==========\n",
      "========== Now this is EPOCH 71 ==========\n",
      "========== IN EPOCH 71 the total loss is 10573.995178222656 ==========\n",
      "========== IN EPOCH 71 the vali NLL loss is 7.594570636749268 ==========\n",
      "========== IN EPOCH 71 the GT metric is [[ 6.6724715 10.343235 ]] ==========\n",
      "========== Now this is EPOCH 72 ==========\n",
      "========== IN EPOCH 72 the total loss is 10535.402923583984 ==========\n",
      "========== IN EPOCH 72 the vali NLL loss is 7.660183429718018 ==========\n",
      "========== IN EPOCH 72 the GT metric is [[ 6.672472 10.343235]] ==========\n",
      "========== Now this is EPOCH 73 ==========\n",
      "========== IN EPOCH 73 the total loss is 10541.056823730469 ==========\n",
      "========== IN EPOCH 73 the vali NLL loss is 7.68120002746582 ==========\n",
      "========== IN EPOCH 73 the GT metric is [[ 6.6724715 10.343235 ]] ==========\n",
      "========== Now this is EPOCH 74 ==========\n",
      "========== IN EPOCH 74 the total loss is 10473.850128173828 ==========\n",
      "========== IN EPOCH 74 the vali NLL loss is 7.6176371574401855 ==========\n",
      "========== IN EPOCH 74 the GT metric is [[ 6.6724715 10.343235 ]] ==========\n",
      "========== Now this is EPOCH 75 ==========\n",
      "========== IN EPOCH 75 the total loss is 10312.69595336914 ==========\n",
      "========== IN EPOCH 75 the vali NLL loss is 7.708623886108398 ==========\n",
      "========== IN EPOCH 75 the GT metric is [[ 6.6724715 10.343234 ]] ==========\n",
      "========== Now this is EPOCH 76 ==========\n",
      "========== IN EPOCH 76 the total loss is 10483.75064086914 ==========\n",
      "========== IN EPOCH 76 the vali NLL loss is 7.657787322998047 ==========\n",
      "========== IN EPOCH 76 the GT metric is [[ 6.6724715 10.343234 ]] ==========\n",
      "========== Now this is EPOCH 77 ==========\n",
      "========== IN EPOCH 77 the total loss is 10501.417572021484 ==========\n",
      "========== IN EPOCH 77 the vali NLL loss is 7.573507308959961 ==========\n",
      "========== IN EPOCH 77 the GT metric is [[ 6.672472 10.343234]] ==========\n",
      "========== Now this is EPOCH 78 ==========\n",
      "========== IN EPOCH 78 the total loss is 10513.407104492188 ==========\n",
      "========== IN EPOCH 78 the vali NLL loss is 7.53364372253418 ==========\n",
      "========== IN EPOCH 78 the GT metric is [[ 6.6724715 10.343234 ]] ==========\n",
      "========== Now this is EPOCH 79 ==========\n",
      "========== IN EPOCH 79 the total loss is 10458.671005249023 ==========\n",
      "========== IN EPOCH 79 the vali NLL loss is 7.687352657318115 ==========\n",
      "========== IN EPOCH 79 the GT metric is [[ 6.6724725 10.343234 ]] ==========\n",
      "========== Now this is EPOCH 80 ==========\n",
      "========== IN EPOCH 80 the total loss is 10423.55712890625 ==========\n",
      "========== IN EPOCH 80 the vali NLL loss is 7.624232769012451 ==========\n",
      "========== IN EPOCH 80 the GT metric is [[ 6.672472 10.343235]] ==========\n",
      "========== Now this is EPOCH 81 ==========\n",
      "========== IN EPOCH 81 the total loss is 10318.897079467773 ==========\n",
      "========== IN EPOCH 81 the vali NLL loss is 7.580054759979248 ==========\n",
      "========== IN EPOCH 81 the GT metric is [[ 6.672472 10.343234]] ==========\n",
      "========== Now this is EPOCH 82 ==========\n",
      "========== IN EPOCH 82 the total loss is 10344.874084472656 ==========\n",
      "========== IN EPOCH 82 the vali NLL loss is 7.574422836303711 ==========\n",
      "========== IN EPOCH 82 the GT metric is [[ 6.6724715 10.343235 ]] ==========\n",
      "========== Now this is EPOCH 83 ==========\n",
      "========== IN EPOCH 83 the total loss is 10333.85678100586 ==========\n",
      "========== IN EPOCH 83 the vali NLL loss is 7.527091026306152 ==========\n",
      "========== IN EPOCH 83 the GT metric is [[ 6.672472 10.343235]] ==========\n",
      "========== Now this is EPOCH 84 ==========\n",
      "========== IN EPOCH 84 the total loss is 10268.654724121094 ==========\n",
      "========== IN EPOCH 84 the vali NLL loss is 7.616769313812256 ==========\n",
      "========== IN EPOCH 84 the GT metric is [[ 6.672472 10.343235]] ==========\n",
      "========== Now this is EPOCH 85 ==========\n",
      "========== IN EPOCH 85 the total loss is 10323.922393798828 ==========\n",
      "========== IN EPOCH 85 the vali NLL loss is 7.600831031799316 ==========\n",
      "========== IN EPOCH 85 the GT metric is [[ 6.672472 10.343235]] ==========\n",
      "========== Now this is EPOCH 86 ==========\n",
      "========== IN EPOCH 86 the total loss is 10386.000244140625 ==========\n",
      "========== IN EPOCH 86 the vali NLL loss is 7.621613502502441 ==========\n",
      "========== IN EPOCH 86 the GT metric is [[ 6.6724715 10.343235 ]] ==========\n",
      "========== Now this is EPOCH 87 ==========\n",
      "========== IN EPOCH 87 the total loss is 10347.147659301758 ==========\n",
      "========== IN EPOCH 87 the vali NLL loss is 7.490235328674316 ==========\n",
      "========== IN EPOCH 87 the GT metric is [[ 6.672472 10.343235]] ==========\n",
      "========== Now this is EPOCH 88 ==========\n",
      "========== IN EPOCH 88 the total loss is 10314.785140991211 ==========\n",
      "========== IN EPOCH 88 the vali NLL loss is 7.66750431060791 ==========\n",
      "========== IN EPOCH 88 the GT metric is [[ 6.672472 10.343235]] ==========\n",
      "========== Now this is EPOCH 89 ==========\n",
      "========== IN EPOCH 89 the total loss is 10387.709838867188 ==========\n",
      "========== IN EPOCH 89 the vali NLL loss is 7.605730056762695 ==========\n",
      "========== IN EPOCH 89 the GT metric is [[ 6.672472 10.343235]] ==========\n",
      "========== Now this is EPOCH 90 ==========\n",
      "========== IN EPOCH 90 the total loss is 10301.074951171875 ==========\n",
      "========== IN EPOCH 90 the vali NLL loss is 7.559336185455322 ==========\n",
      "========== IN EPOCH 90 the GT metric is [[ 6.672472 10.343234]] ==========\n",
      "========== Now this is EPOCH 91 ==========\n",
      "========== IN EPOCH 91 the total loss is 10270.95053100586 ==========\n",
      "========== IN EPOCH 91 the vali NLL loss is 7.543718338012695 ==========\n",
      "========== IN EPOCH 91 the GT metric is [[ 6.6724715 10.343235 ]] ==========\n",
      "========== Now this is EPOCH 92 ==========\n",
      "========== IN EPOCH 92 the total loss is 10376.162490844727 ==========\n",
      "========== IN EPOCH 92 the vali NLL loss is 7.490835189819336 ==========\n",
      "========== IN EPOCH 92 the GT metric is [[ 6.6724715 10.343235 ]] ==========\n",
      "========== Now this is EPOCH 93 ==========\n",
      "========== IN EPOCH 93 the total loss is 10377.162887573242 ==========\n",
      "========== IN EPOCH 93 the vali NLL loss is 7.567009449005127 ==========\n",
      "========== IN EPOCH 93 the GT metric is [[ 6.672472 10.343235]] ==========\n",
      "========== Now this is EPOCH 94 ==========\n",
      "========== IN EPOCH 94 the total loss is 10300.065307617188 ==========\n",
      "========== IN EPOCH 94 the vali NLL loss is 7.5495195388793945 ==========\n",
      "========== IN EPOCH 94 the GT metric is [[ 6.6724715 10.343235 ]] ==========\n",
      "========== Now this is EPOCH 95 ==========\n",
      "========== IN EPOCH 95 the total loss is 10303.03628540039 ==========\n",
      "========== IN EPOCH 95 the vali NLL loss is 7.502524375915527 ==========\n",
      "========== IN EPOCH 95 the GT metric is [[ 6.672472 10.343235]] ==========\n",
      "========== Now this is EPOCH 96 ==========\n",
      "========== IN EPOCH 96 the total loss is 10365.304107666016 ==========\n",
      "========== IN EPOCH 96 the vali NLL loss is 7.481761932373047 ==========\n",
      "========== IN EPOCH 96 the GT metric is [[ 6.6724715 10.343234 ]] ==========\n",
      "========== Now this is EPOCH 97 ==========\n",
      "========== IN EPOCH 97 the total loss is 10295.559173583984 ==========\n",
      "========== IN EPOCH 97 the vali NLL loss is 7.550564289093018 ==========\n",
      "========== IN EPOCH 97 the GT metric is [[ 6.6724715 10.343235 ]] ==========\n",
      "========== Now this is EPOCH 98 ==========\n",
      "========== IN EPOCH 98 the total loss is 10211.809844970703 ==========\n",
      "========== IN EPOCH 98 the vali NLL loss is 7.4452314376831055 ==========\n",
      "========== IN EPOCH 98 the GT metric is [[ 6.672472 10.343234]] ==========\n",
      "========== Now this is EPOCH 99 ==========\n",
      "========== IN EPOCH 99 the total loss is 10205.489379882812 ==========\n",
      "========== IN EPOCH 99 the vali NLL loss is 7.648772716522217 ==========\n",
      "========== IN EPOCH 99 the GT metric is [[ 6.672472 10.343233]] ==========\n",
      "Total training time when epoch= *100* is *903.8044703006744 *s\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# filename = \"../log_file.txt\"\n",
    "# f = open(filename,'w')\n",
    "total_train_step = 0\n",
    "EPOCH_NUM = 100\n",
    "\n",
    "train_start_time = time.time()\n",
    "mlp.train()\n",
    "\n",
    "for epoch in range(EPOCH_NUM):\n",
    "    # # Save the net structure\n",
    "    # net_file_name = \"net_before_epoch\"+str(epoch)+\".pth\"\n",
    "    # net_path = opt.net_root_path + net_file_name\n",
    "    # torch.save(mlp.state_dict(), net_path)\n",
    "\n",
    "    epoch_train_loss = 0\n",
    "    # plot_conv(writer,mlp,epoch)\n",
    "    print(f\"========== Now this is EPOCH {epoch} ==========\")\n",
    "    for batch_id,data in enumerate(train_loader):\n",
    "\n",
    "        input_data, _, target_loss, setting, _ = data\n",
    "        # print(f\"---- {batch_id} batch----\")\n",
    "        # Do the inference\n",
    "        input_data = input_data.to(device)\n",
    "        target_loss = target_loss.to(device)\n",
    "        pi, mu, sigma = mlp(input_data)\n",
    "\n",
    "        # Save the params\n",
    "        # params = list(mlp.named_parameters())\n",
    "\n",
    "        # Cal the MLE loss and draw the distrb.\n",
    "        # loss = loss_fn_v2(pi, mu, sigma, target, opt.N_gaussians,device)\n",
    "        loss = loss_fn_v2(pi, mu, sigma, target_loss, opt.N_gaussians,opt.TARGET, opt.SAFETY, device)\n",
    "        # loss = loss_fn_v3(pi, mu, sigma, target_loss, opt.N_gaussians,device)\n",
    "        # loss = loss_fn_v5(pi, mu, sigma, target_loss, opt.N_gaussians,device)\n",
    "\n",
    "        # Cal the CDF loss and draw the distrb.\n",
    "        # loss = loss_fn_cdf(pi, mu, sigma, target, opt.N_gaussians)\n",
    "\n",
    "        # Cal the CE or KL loss and draw the distrb.\n",
    "        # loss  = loss_fn_CE(pi, mu, sigma, target, opt.N_gaussians)\n",
    "\n",
    "        # Cal the WD loss and draw the distrb.\n",
    "        # loss  = loss_fn_WD(pi, mu, sigma, target_loss, opt.N_gaussians,opt.TARGET,device)\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "        draw_loss(total_train_step, loss.item(),win_train_loss_str)\n",
    "\n",
    "        # Optim\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        ########### before step() ###############\n",
    "\n",
    "        # for name, parms in mlp.named_parameters():\n",
    "        #     print(\"=====before step()=====\")\n",
    "        #     print('-->name:', name)\n",
    "        #     print('-->params:', parms.data)\n",
    "        #     # print('-->grad_requirs:',parms.requires_grad)\n",
    "        #     print('-->grad_value:',parms.grad)\n",
    "        #     print(\"===\")\n",
    "\n",
    "        ######\n",
    "\n",
    "        # 反向传播时检测是否有异常值，定位code\n",
    "        # with torch.autograd.detect_anomaly():\n",
    "        #     loss.backward()\n",
    "\n",
    "        ######\n",
    "        # get_dot = register_hooks(loss)\n",
    "        # dot = get_dot()\n",
    "        # dot.save('tmp.dot')   # to get .dot\n",
    "        # dot.render('tmp')     # to get pdf\n",
    "\n",
    "        ######\n",
    "        # Print grad check\n",
    "        # v_n = []    # name\n",
    "        # v_v = []    # value\n",
    "        # v_g = []    # grad\n",
    "        # for name, parameter in mlp.named_parameters():\n",
    "        #     v_n.append(name)\n",
    "        #     v_v.append(parameter.detach().cpu().numpy() if parameter is not None else [0])\n",
    "        #     v_g.append(parameter.grad.detach().cpu().numpy() if parameter.grad is not None else [0])\n",
    "        # for i in range(len(v_n)):\n",
    "        #     # if np.max(v_v[i]).item() - np.min(v_v[i]).item() < 1e-6:\n",
    "        #     #     color = bcolors.FAIL + '*'\n",
    "        #     if np.isnan(v_g[i]).any() or np.isnan(v_v[i]).any():\n",
    "        #         color = bcolors.FAIL + '*'\n",
    "        #     else:\n",
    "        #         color = bcolors.OKGREEN + ' '\n",
    "        #     print('%svalue %s: %.3e ~ %.3e' % (color, v_n[i], np.min(v_v[i]).item(), np.max(v_v[i]).item()))\n",
    "        #     print('%sgrad  %s: %.3e ~ %.3e' % (color, v_n[i], np.min(v_g[i]).item(), np.max(v_g[i]).item()))\n",
    "\n",
    "        ######\n",
    "        optimizer.step()\n",
    "\n",
    "        # for name, parms in mlp.named_parameters():\n",
    "        #     print(\"=====After step()=====\")\n",
    "        #     print('-->name:', name)\n",
    "        #     print('-->params:', parms.data)\n",
    "        #     # print('-->grad_requirs:',parms.requires_grad)\n",
    "        #     print('-->grad_value:',parms.grad)\n",
    "        #     print(\"===\")\n",
    "\n",
    "        # Only draw the 1st result in a training batch (5 in total)\n",
    "        # if total_train_step % 20 == 0:\n",
    "        #     with torch.no_grad():\n",
    "        #         draw_mdn_2(pi[0,:].clone().detach(), mu[0,:].clone().detach(), sigma[0,:].clone().detach(), target[0].clone().detach(), input_data[0].clone().detach(),total_train_step, opt.N_gaussians)\n",
    "        #         # draw_mdn_cdf(pi[0,:].detach(), m[0], target[0].detach(), total_train_step, loss_list[0],opt.N_gaussians)\n",
    "        total_train_step += 1\n",
    "\n",
    "    ########### Do validation\n",
    "\n",
    "    mlp.eval()\n",
    "    with torch.no_grad():\n",
    "        total_vali_metric, GT_metric = validate(mlp,val_loader,opt.N_gaussians, opt.MIN_LOSS,device)\n",
    "        draw_metric(epoch, total_vali_metric.cpu(), GT_metric)\n",
    "        writer.add_scalars(\"metric/\"+opt.tag_str,{\"pred\":total_vali_metric.cpu(),\n",
    "                                     \"GT-1\":GT_metric[0,0],\n",
    "                                     \"GT-2\":GT_metric[0,1]},epoch)\n",
    "    mlp.train()\n",
    "\n",
    "    # Plot the loss in this EPOCH\n",
    "    print(f\"========== IN EPOCH {epoch} the total loss is {epoch_train_loss} ==========\")\n",
    "    print(f\"========== IN EPOCH {epoch} the vali NLL loss is {total_vali_metric.detach().cpu().numpy()} ==========\")\n",
    "    print(f\"========== IN EPOCH {epoch} the GT metric is {GT_metric.detach().cpu().numpy()} ==========\")\n",
    "\n",
    "    # # Save the net structure\n",
    "    # net_file_name = \"net_after_epoch\"+str(epoch)+\".pth\"\n",
    "    # net_path = opt.net_root_path + net_file_name\n",
    "    # torch.save(mlp.state_dict(), net_path)\n",
    "\n",
    "    # Plot loss of this EPOCH\n",
    "    draw_loss(epoch, epoch_train_loss,win_train_epoch_loss_str)\n",
    "    # writer.add_scalars(\"Loss of EPOCH\",{\"Train\":epoch_train_loss},epoch)\n",
    "    # scheduler.step(total_vali_metric)\n",
    "    scheduler.step()\n",
    "\n",
    "    if epoch%3==0:\n",
    "        # Record the weight\n",
    "        plot_conv_weight(writer,mlp,epoch,opt.tag_str)\n",
    "        plot_mu_weight(writer,mlp,epoch,opt.tag_str)\n",
    "        plot_pi_weight(writer,mlp,epoch,opt.tag_str)\n",
    "        plot_sigma_weight(writer,mlp,epoch,opt.tag_str)\n",
    "\n",
    "# f.close()\n",
    "train_end_time = time.time()\n",
    "print(f\"Total training time when epoch= *{EPOCH_NUM}* is *{train_end_time-train_start_time} *s\")\n",
    "writer.close()\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- save"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "outputs": [],
   "source": [
    "model_path_MLE = \"mlp_train=300_MLE.pth\"\n",
    "# model_path_WD = \"mlp_train=300_WD_epoch=80.pth\"\n",
    "model_path_WD = \"mlp_train=300_WD.pth\"\n",
    "model_path_LSE = \"mlp_train=300_LSE.pth\"\n",
    "torch.save(mlp.state_dict(), model_path_MLE)\n",
    "# torch.save(mlp.state_dict(), model_path_MLE)\n",
    "\n",
    "# viz.delete_env(\"001_test\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Gtmp = pgv.AGraph('tmp.dot')\n",
    "# G = nx.Graph(Gtmp)\n",
    "# nx.draw(G)\n",
    "# # plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "$\\mathcal{L}(y \\vert x) = - \\log\\bigg\\{\\sum_{k=1}^K \\pi_k(x)  \\mathcal{N}\\big(y \\vert \\mu_k(x), \\Sigma_k(x)\\big)\\bigg\\}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Load and test\n",
    "## 7.1 small batch\n",
    "- 对某个小batch画图，batch size可以=1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "small_batch_size = 1\n",
    "\n",
    "small_test_idx = train_idx[3:4]\n",
    "small_test_loader = DataLoader(dataset = dataset,batch_size = small_batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(small_test_idx),collate_fn = my_collate_fn)\n",
    "small_test_idx"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot\n",
    "test_env = \"002\"\n",
    "def draw_mdn_test(pi, m, target, input, epoch, idx,opt.N_gaussians,flag):\n",
    "    # 只画一个batch中的第一个\n",
    "\n",
    "    # The target distrb.\n",
    "    target = torch.unique(target,dim=0)    # drop the duplicate\n",
    "    n_target = target[:,0]\n",
    "    non_zero_idx = torch.nonzero(n_target)\n",
    "    n = n_target[non_zero_idx]\n",
    "\n",
    "    p_target = target[:,1]\n",
    "    p = p_target[non_zero_idx]\n",
    "    max_n = max(n).item()               # 横轴长度\n",
    "\n",
    "    # The predicted distrb.\n",
    "    x_0 = torch.arange(1,max_n).to(device=device)\n",
    "    x = torch.repeat_interleave(x_0.unsqueeze(dim=1), repeats=opt.N_gaussians, dim=1)\n",
    "    y = torch.exp(m.log_prob(x)).to(device=device)                          # y:多条高斯曲线; y_pred: 一条GMM曲线\n",
    "    # y_pred = torch.unsqueeze(torch.sum(pi*y,dim=1),dim=1)                 # 维度相等才能cat\n",
    "    y_pred = torch.sum(pi*y,dim=1)\n",
    "\n",
    "    # The input distrb.\n",
    "    input_data = input[0:2,:]\n",
    "    x_input = torch.arange(1,input_data.shape[1]+1).to(device=device)\n",
    "    y_input_0 = input_data[0,:].to(device=device)\n",
    "    y_input_1 = input_data[1,:].to(device=device)\n",
    "\n",
    "    # Init\n",
    "    win_str = flag+ \" epoch=\"+str(epoch)+\" | idx=\"+str(idx)\n",
    "    title_str = \"Distrb. \"+win_str\n",
    "    viz.line(X = [0.],Y = [0.], env=test_env, win=win_str, opts= dict(title=title_str))\n",
    "\n",
    "    # Visdom本身不能把hist和line画在一个window中\n",
    "    # 如果想画一起只能是两条line\n",
    "    # Plot y_target\n",
    "    # viz.histogram(X = n, env=\"001\", win=win_str,\n",
    "    #         opts= dict(title=title_str))\n",
    "    viz.line(X = n,Y= p, env=test_env, win=win_str, update=\"append\", name='target',\n",
    "            opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "    # Plot y_pred\n",
    "    viz.line(X = x_0,Y= y_pred, env=test_env, win=win_str, update=\"append\", name='pred',\n",
    "            opts= dict(title=title_str))\n",
    "\n",
    "    # Plot y_input\n",
    "    viz.line(X = x_input,Y= y_input_0, env=test_env, win=win_str, update=\"append\", name='GT-1', opts= dict(title=title_str))\n",
    "    viz.line(X = x_input,Y= y_input_1, env=test_env, win=win_str, update=\"append\", name='GT-2', opts= dict(title=title_str))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(EPOCH_NUM):\n",
    "\n",
    "    net_file_name_0 = \"net_before_epoch\"+str(epoch)+\".pth\"\n",
    "    net_file_name_1 = \"net_after_epoch\"+str(epoch)+\".pth\"\n",
    "    net_path0 = opt.net_root_path + net_file_name_0\n",
    "    net_path1 = opt.net_root_path + net_file_name_1\n",
    "\n",
    "    # Load the model\n",
    "    mlp0 = MLP(opt.N_gaussians)\n",
    "    mlp1 = MLP(opt.N_gaussians)\n",
    "    mlp0.load_state_dict(torch.load(net_path0))\n",
    "    mlp1.load_state_dict(torch.load(net_path1))\n",
    "    mlp0.to(device)\n",
    "    mlp1.to(device)\n",
    "\n",
    "    # 其实每个epoch只会执行一个batch\n",
    "    for batch_id,data in enumerate(small_test_loader):\n",
    "\n",
    "        # Predict\n",
    "        input_data, target = data\n",
    "        input_data = input_data.to(device)\n",
    "        target = target.to(device)\n",
    "        pi0, mu0, sigma0 = mlp0(input_data)\n",
    "        pi1, mu1, sigma1 = mlp1(input_data)\n",
    "        # Get the mdn\n",
    "        _,m0  = loss_preparation(pi0.detach(), mu0.detach(), sigma0.detach(), target.detach())\n",
    "        _,m1  = loss_preparation(pi1.detach(), mu1.detach(), sigma1.detach(), target.detach())\n",
    "\n",
    "        # Plot for 'small_batch_size' times\n",
    "        for j in range(small_batch_size):\n",
    "            draw_mdn_test(pi0[j,:].detach(), m0[j], target[j].detach(), input_data[j].detach(), epoch, j,opt.N_gaussians,flag=\"before\")\n",
    "            draw_mdn_test(pi1[j,:].detach(), m1[j], target[j].detach(), input_data[j].detach(), epoch, j,opt.N_gaussians,flag=\"after\")\n",
    "\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7.2 Test dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "# model_path_WD_2 = \"mlp_train=300_WD_epoch=80.pth\"\n",
    "model_path_WD = \"mlp_train=300_WD.pth\"\n",
    "model_path_MLE = \"mlp_train=300_MLE.pth\"\n",
    "\n",
    "\n",
    "mlp_1 = MLP_1_3(opt.N_gaussians)\n",
    "mlp_2 = MLP_1_3(opt.N_gaussians)\n",
    "\n",
    "model_data_1 = torch.load(model_path_MLE)\n",
    "# model_data_1 = torch.load(model_path_WD_2)\n",
    "model_data_2 = torch.load(model_path_WD)\n",
    "\n",
    "mlp_1.load_state_dict(model_data_1)\n",
    "mlp_2.load_state_dict(model_data_2)\n",
    "\n",
    "mlp_1.to(device)\n",
    "mlp_2.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total_test_loss_1 = 0\n",
    "total_test_loss_2 = 0\n",
    "i = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_id,data in enumerate(test_loader):\n",
    "            input_data, target = data\n",
    "            input_data = input_data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            pi_1, mu_1, sigma_1 = mlp_1(input_data)\n",
    "            pi_2, mu_2, sigma_2 = mlp_2(input_data)\n",
    "\n",
    "            draw_mdn_3(pi_1, mu_1, sigma_1, pi_2, mu_2, sigma_2, target, input_data,i, opt.N_gaussians)\n",
    "\n",
    "            loss_1 = loss_fn_v2(pi_1, mu_1, sigma_1, target, opt.N_gaussians)\n",
    "            loss_2 = loss_fn_v2(pi_2, mu_2, sigma_2, target, opt.N_gaussians)\n",
    "\n",
    "            total_test_loss_1 += loss_1.item()\n",
    "            total_test_loss_2 += loss_2.item()\n",
    "\n",
    "            i += 1\n",
    "    # 这里print的loss可以视作平均到每个sample上的loss\n",
    "    print(\"total_test_loss_1 MLE:\",total_test_loss_1/len(test_loader))\n",
    "    print(\"total_test_loss_2 WD :\",total_test_loss_2/len(test_loader))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8. Tuning pipeline\n",
    "## 8.1 training wrapper"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8.2 main"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
