{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2022/6/7 15:17\n",
    "# @Author  : Wang Yujia\n",
    "# @File    : mlp.ipynb\n",
    "# @Description : 非Sequential结构\n",
    "# @TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0. what for\n",
    "1. loss部分逻辑重写，添加`no grad`\n",
    "2. 添加hooks\n",
    "3. 使用pad_seq函数取代自己写的pad函数\n",
    "4. 效果：可以train虽然loss不下降，不会出现NaN的问题"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Preparations\n",
    "## 1.1 global settings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# nums of Gaussian kernels\n",
    "N_gaussians = 3\n",
    "\n",
    "# dataset划分\n",
    "batch_size = 5\n",
    "train_pct = 0.7\n",
    "vali_pct = 0.2\n",
    "test_pct = 0.1\n",
    "\n",
    "# train and optim.\n",
    "learning_rate = 0.0001\n",
    "total_train_step = 0\n",
    "total_test_step = 0\n",
    "EPOCH_NUM = 5\n",
    "MIN_LOSS = 1e-7\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.utils.data\n",
    "from mydataset import *\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from tensorboardX import SummaryWriter\n",
    "from visdom import Visdom\n",
    "from torchviz import make_dot\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import math\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 the data path\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# training data\n",
    "train_path = r\"../data/train\"\n",
    "# target data\n",
    "target_path = r\"../data/targets\"\n",
    "# data keys\n",
    "data_key_path = \"../data/target_datakey.csv\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Dataset and Dataloader\n",
    "1. DataLoader中的shuffer=True表示在每一次epoch中都打乱所有数据的顺序，然后以batch为单位从头到尾按顺序取用数据。这样的结果就是不同epoch中的数据都是乱序的,设置随机种子的作用就是让你的每一次训练都乱的一样，\n",
    "\n",
    "## 2.1 Dataset and spliting\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 设置随机数种子"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup_seed(7)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 读取data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "dataset = myDataset(train_path, target_path, data_key_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 产生index的乱序排列"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 200  133  110 1026  613  676  822  587  268  682  669  627  121  827\n",
      " 1012 1068  861  206 1046   38   26    5 1158  358  514  720  570  362\n",
      "  563   53 1030  789 1169  667 1048  179  849  423 1087    2 1189  106\n",
      "  759 1067   34  171  895  256  992  954  150  894  891 1083  739  562\n",
      "  499  981 1001  399  853  982  880  141  232  156 1100   11  946  234\n",
      "  295  339 1076  910  194  190  145  651  848   72  672  484  490  283\n",
      "  915  371 1113  989  375  147 1188  786  544  865 1018  717  873 1040\n",
      "  635  795  620 1060  745 1035  583  262  875  196  767  814   89   57\n",
      " 1102 1187 1020 1129  854   93  443  755  626  386  270  376  856 1022\n",
      "  416  396  217  685  329  405  908  301  400  693   86 1106  662  240\n",
      "  356  382  198  444  395  615 1005 1002  239  643  743  956 1006  647\n",
      "  560  753  996  525  884  820  460 1097  467  916 1177  192  186   51\n",
      "  658  883  988  480  296 1111  456    0   30  419  204 1095  513 1011\n",
      "  212  962  516  803   94 1124  341  983  998 1159 1019   99  271  881\n",
      "  520  545 1105  278   43    7   32  335  770  898  796  242  221  751\n",
      "  345  457  551  142 1091  535  603  373  876  475  211  225  340  576\n",
      " 1089  684  752  135  494  517  678  241 1174  746  785  804  619  935\n",
      "  151  652  703  427  365  305  124  665 1123  971   60  757  369  325\n",
      "   12  328  766  938  433  886  237  273  765  912 1025 1182  539 1070\n",
      "  999   54  762 1029 1121  184  169  398  495  176  690   46  420 1127\n",
      "  852  801  687  986    6  318  100  769  543  628  390  633 1108  549\n",
      "  596  546  463  208  245 1043  468  401   97  750  637   28  666  695\n",
      "  826  383   14 1154  191 1131  737 1066  729  292  821  609 1114  878\n",
      "  259 1168  644  350  267  933  372  835  614  713  704  960  432  165\n",
      "  465  602  247   36  968  474   58  158  534  213  168  414  862  503\n",
      "  374 1064  407  580 1047  976  597    9  512  928  366   24  404   52\n",
      "  418  251  640 1080 1014   16 1003  472  526  840  936   71  195  554\n",
      "   20  279  673  914  172  639  523   18  735  119  889 1078  426  734\n",
      "  760  708  415  349  233 1069  568 1024 1186  911  367 1143  582  787\n",
      "  464   79  850  969   82  140  483  947  879  634   37   44 1000  834\n",
      "   29  932 1013  481  388  210  733  377  681  412 1193  406  450  740\n",
      "  488  300   87  547 1074  842   49 1058  502  721  845  715 1180  393\n",
      "  691 1142  509  901  679  170  314 1172 1054  137  173  253  807  439\n",
      " 1041 1062  471  638  522  595 1052  299  479 1021  203  832  727  922\n",
      " 1061 1038  716  773  229  661 1049  846 1145  183  802  978  841  482\n",
      "  763  441  117  584  741  157  129  431  214  844  107   33  344  537\n",
      " 1016  496 1044  447 1073  718 1161  178   70  519 1053 1190  291  792\n",
      "  127  284  709  470 1118  342  125  868  336 1031  913   31  994  462\n",
      "  504  308  899  363  756  108  507  831  559  995  312  591  188   47\n",
      "  532   76  612  705  768  280  149  664  790  485  205 1072  353  561\n",
      "  421   10  579  569  761  781 1034  361  174  890 1140  892  162  838\n",
      " 1119  590  185  459  697  160  749 1151 1137  263  368  808 1099  598\n",
      "  791  618  231  333 1084  919  694  380  958  636  477  990  744 1138\n",
      "  148  748  257  660  578  207  610 1167  309 1042  143  556  622  839\n",
      " 1139  430  833  417  548  993  805 1110  811  799  104  950  564  461\n",
      "  707  631 1059   83  904  146  736  608  394  674  941   45  197  553\n",
      "  836  655  264  987  111  588  317 1010  379  454  592  136  966  653\n",
      "  338 1176  920  851  343 1032  316  541  506  440  220 1045  732  688\n",
      "   69  712  533  896  882   74  855 1120  298  552  929  113 1194 1096\n",
      "  518 1148 1178  641 1055   95   35  702 1147  347  863 1023  116  824\n",
      "  530  772  858  902    4 1135 1075  646  473  306 1027 1136  223  939\n",
      "  227  977  508 1057  424  974 1175   81  422  959  452  668   62  951\n",
      "  453 1004  557  435  429  515  330  670  109  780  650 1098   40  680\n",
      "  985  354  230  527 1134   64  311  607  943 1116   19  402  357  360\n",
      "  105  738  857  289  392  810 1149  847  222 1184  723 1163  623  930\n",
      " 1179  397  909  313  287  593  202  701 1093 1192 1128  332  438  326\n",
      "  806  900  387   17 1086  163  281  961  351  331  731 1051 1081   42\n",
      " 1017  775  101  275  286  446 1141  249  134  574  859  594  128  747\n",
      " 1085  572  103 1028  410  793  154  337  604  209   66  940  903   56\n",
      "  970  711  324  778   78  540  355  493  818  359  815]\n"
     ]
    }
   ],
   "source": [
    "shuffled_indices = np.random.permutation(dataset.__len__())\n",
    "# shuffled_indices = np.arange(0,dataset.__len__())\n",
    "train_idx = shuffled_indices[:int(train_pct*dataset.__len__())]\n",
    "# train_idx = shuffled_indices\n",
    "tmp = int((train_pct+vali_pct)*dataset.__len__())\n",
    "val_idx = shuffled_indices[int(train_pct*dataset.__len__()):tmp]\n",
    "\n",
    "test_idx = shuffled_indices[tmp:]\n",
    "print(train_idx)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 根据这个乱序排列抽取dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Dataloader and collating\n",
    "1. 主要是对label数据进行collate\n",
    "    - 按照batch中的最大target data长度进行padding，padding with 0\n",
    "2. 返回的结果多一个batch dim,比如下面的`5`\n",
    "    - After collating:\n",
    "        - `torch.Size([5, 3, 300]),torch.Size([5, 87, 2])`\n",
    "        - `87`是最长的targets data长度"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[2.0000, 0.0476],\n         [3.0000, 0.1429],\n         [4.0000, 0.0476]],\n\n        [[1.0000, 0.0476],\n         [0.0000, 0.0000],\n         [0.0000, 0.0000]],\n\n        [[3.0000, 0.1429],\n         [4.0000, 0.0476],\n         [0.0000, 0.0000]]])"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq1 = torch.tensor([[ 2., 0.04761905], [3., 0.14285714], [4., 0.04761905]])\n",
    "seq2 = torch.tensor([[ 1., 0.04761905]])\n",
    "seq3 = torch.tensor([[ 3., 0.14285714], [4., 0.04761905]])\n",
    "ls = list((seq1,seq2,seq3))\n",
    "ls_length = torch.tensor([3,1,2])\n",
    "ans = pad_sequence(ls,batch_first=True)\n",
    "ans\n",
    "# seq_pak = pack_padded_sequence(ans,ls_length,batch_first=True,enforce_sorted=False)\n",
    "# seq_unpacked, lens_unpacked = pad_packed_sequence(seq_pak, batch_first=True)\n",
    "# lens_unpacked"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [],
   "source": [
    "def my_collate_fn(data):\n",
    "# 这里的data是一个list， list的元素是元组: (self.data, self.label)\n",
    "# collate_fn的作用是把[(data, label),(data, label)...]转化成([data, data...],[label,label...])\n",
    "# 假设self.data的一个data的shape为(channels, length), 每一个channel的length相等,\n",
    "# data[索引到index(batch)][索引到data或者label][索引到channel]\n",
    "\n",
    "    data_list = []\n",
    "    target_list = []\n",
    "    data_len = len(data)      # 读进来的data batch的大小\n",
    "\n",
    "    batch = 0\n",
    "    while batch < data_len:\n",
    "        data_list.append(torch.tensor(data[batch][0]))\n",
    "        target_list.append(torch.tensor(data[batch][1]))\n",
    "        batch += 1\n",
    "\n",
    "    # pad with zeros\n",
    "    target_padded = pad_sequence(target_list,batch_first=True)\n",
    "\n",
    "    data_tensor = torch.stack(data_list).float()\n",
    "    target_tensor = target_padded.float()\n",
    "\n",
    "    return data_tensor, target_tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = dataset,batch_size = batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(train_idx), collate_fn = my_collate_fn)\n",
    "\n",
    "val_loader = DataLoader(dataset = dataset,batch_size = batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(val_idx),collate_fn = my_collate_fn)\n",
    "\n",
    "test_loader = DataLoader(dataset = dataset,batch_size = batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(test_idx),collate_fn = my_collate_fn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. The Net and Init\n",
    "1. BatchNorm1d: The mean and std are calculated per-dimension over the mini-batches\n",
    "2."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "### BatchNorm2d测试\n",
    "def test_BN():\n",
    "    m = nn.BatchNorm2d(3, affine=False)  # affine: With Learnable Parameters or not\n",
    "    print('m:', m)\n",
    "    # The mean and std are calculated per-dimension over the mini-batches\n",
    "    input = torch.tensor([\n",
    "        [[1.,2.,3.,4.],[1.,2.,3.,4.],[-1.,-2.,-3.,-4.]],\n",
    "        [[0.,0.,0.,0.],[0.,0.,0.,0.],[0.,0.,0.,0.]]\n",
    "    ], requires_grad=True)\n",
    "\n",
    "    print('input:', input.shape)\n",
    "    input = input.unsqueeze(dim=2)\n",
    "    print('input:', input.shape)\n",
    "    output = m(input) # 归一化\n",
    "    print('output:', output.shape)\n",
    "    print('output:', output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-0.1690,  0.5071,  1.1832,  1.8593]],\n\n        [[-0.8452, -0.8452, -0.8452, -0.8452]]])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 复现batchNorm2d在input shape为3维的情况\n",
    "input = torch.tensor([[[1.,2.,3.,4.]],[[0.,0.,0.,0.]]])\n",
    "# print(input.shape)\n",
    "# torch.mean(input),torch.var(input,unbiased = False)\n",
    "(input-torch.mean(input))/ torch.sqrt(torch.var(input,unbiased = False))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 设置网络初始权重: 不太work"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class model_param_init(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        assert isinstance(model, nn.Module), 'model not a class nn.Module'\n",
    "        self.net = model\n",
    "        self.initParam()\n",
    "\n",
    "    def initParam(self):\n",
    "        for param in self.net.parameters():\n",
    "            # nn.init.zeros_(param)\n",
    "            # nn.init.ones_(param)\n",
    "            # nn.init.normal_(param, mean=0, std=1)\n",
    "            # nn.init.uniform_(param, a=0, b=1)\n",
    "            # nn.init.constant_(param, val=1)   # 将所有权重初始化为1\n",
    "            # nn.init.eye_(param)  # 只能将二维的tensor初始化为单位矩阵\n",
    "            # nn.init.xavier_uniform_(param, gain=1)  # Glorot初始化  得到的张量是从-a——a中采用的\n",
    "            # nn.init.xavier_normal_(param, gain=1)   # 得到的张量是从0-std采样的\n",
    "            nn.init.kaiming_normal_(param, a=0, mode='fan_in', nonlinearity='relu') # he初始化方法\n",
    "            # nn.init.kaiming_uniform_(param)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- print网络每层结构"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "class PrintLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if(DEBUG):\n",
    "            print(\"This layer: \")\n",
    "            print(x)      #print(x.shape)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Sequential结构\n",
    "- 最后输出mu的时候求了mean，不太好？"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- hook_backward_fn: 输入端的grad，输出端的grad，这里称呼的**输入与输出是站在前向传播的角度的**。如果模块有多个输入与输出的话， 其`grad_input`和`grad_output`可以是tuple类型。\n",
    "- 与forward不同的是，backward传播的时候，**不仅反向传递input和output的grad，还会传递模块Parameter的grad**：\n",
    "    - 比如fc模块，其`grad_input`是一个三元组的tuple，（对bias的梯度，对输入的梯度，对w的梯度）；\n",
    "    - conv模块`grad_input`也是一个三元组tuple，为（对输入的梯度，对w的梯度，对bias的梯度"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# hook functions have to take these 3 input\n",
    "def hook_forward_fn(module, input, output):\n",
    "    print(\"It's forward: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"input: {input}\")\n",
    "    print(f\"output: {output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn(module, grad_input, grad_output):\n",
    "    print(\"It's backward: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn_pi(module, grad_input, grad_output):\n",
    "    print(\"It's backward in pi: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn_sigma(module, grad_input, grad_output):\n",
    "    print(\"It's backward in sigma: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn_mu(module, grad_input, grad_output):\n",
    "    print(\"It's backward in mu: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [],
   "source": [
    "# Not Sequential\n",
    "class MLP(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "        self.BN = nn.BatchNorm2d(num_features=3,affine=False)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "        self.ac_func1 = nn.Softplus()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,2), stride=(1,2), padding=0,bias=False)\n",
    "        #nn.ReLU(inplace=True),\n",
    "\n",
    "        self.ac_func2 = nn.Softplus()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,5), stride=(1,5), padding=0,bias=False)\n",
    "        # nn.ReLU(inplace=True),\n",
    "\n",
    "        self.ac_func3 = nn.Softplus()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(30, 9)\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(9, n_gaussians),  # 30个params要learn\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(9, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(9, n_gaussians)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 加一个height维度\n",
    "        x = torch.unsqueeze(x,dim=2)\n",
    "        x = self.BN(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.ac_func1(x)\n",
    "\n",
    "        x = self.BN(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.ac_func2(x)\n",
    "\n",
    "        x = self.BN(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.ac_func3(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "\n",
    "        pi = self.z_pi(x)\n",
    "        mu = self.z_mu(x)\n",
    "        sigma = torch.exp(self.z_sigma(x))\n",
    "\n",
    "        return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input's shape is torch.Size([2, 3, 1, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": "torch.Size([2, 12])"
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([\n",
    "    [[1.,2.,3.,4.],[1.,2.,3.,4.],[-1.,-2.,-3.,-4.]],\n",
    "    [[0.,0.,0.,0.],[0.,0.,0.,0.],[0.,0.,0.,0.]]\n",
    "], requires_grad=True)\n",
    "input = input.unsqueeze(dim=2)\n",
    "print(f\"input's shape is {input.shape}\")\n",
    "flt = nn.Flatten(start_dim=1)\n",
    "flt(input).shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. The Loss\n",
    "- `loss_preparation`用来做loss的前期data准备：\n",
    "    - 计算混合模型的分布`m`以及target data中的`duration`\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "# 当input的shape是[50,3]时，输出应该是50个GMM\n",
    "def loss_preparation(pi, mu, sigma, target):\n",
    "\n",
    "    m=[]\n",
    "    for i in range(pi.shape[0]):\n",
    "        m.append(torch.distributions.Normal(loc=mu[i,:].T, scale=sigma[i,:].T))\n",
    "\n",
    "    # target_packed = pack_padded_sequence(target,target_len,batch_first=True,enforce_sorted=False)  # 去掉padded 0并拉成一个vector\n",
    "    # seq_unpacked, lens_unpacked = pad_packed_sequence(target_packed, batch_first=True)             # 和上面互为逆运算\n",
    "\n",
    "    duration = target[:,:,0]\n",
    "\n",
    "    return duration,m"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [],
   "source": [
    "# # 当input的shape是[50,3]时，输出应该是50个GMM\n",
    "# # 对这50个GMM看能生成什么output\n",
    "\n",
    "def loss_fn(Pi,duration,m):\n",
    "    loss_list = []\n",
    "    with torch.no_grad():\n",
    "        # for each GMM\n",
    "        # 后期肯定要矩阵化这个计算！\n",
    "        for i in range(len(m)):\n",
    "            target = duration[i,:]\n",
    "            pi = Pi[i,:]\n",
    "\n",
    "            # repeat and copy target data\n",
    "            target_nonzero = target[torch.nonzero(target)].squeeze_()\n",
    "            target_nonzero = torch.repeat_interleave(target_nonzero.unsqueeze(dim=1), repeats=3, dim=1).to(device)\n",
    "\n",
    "            # loss_1 是高斯分布的概率密度value\n",
    "            loss_1 = torch.exp(m[i].log_prob(target_nonzero))\n",
    "\n",
    "            # loss_2是MDN的概率密度value\n",
    "            loss_2 = torch.sum(loss_1 * pi, dim=1)\n",
    "\n",
    "            # loss_3是非0的prob value\n",
    "            loss_3 = loss_2[torch.nonzero(loss_2)].view(-1,1)                # 再去掉所有的log(0)\n",
    "\n",
    "            MIN_LOSS = torch.min(loss_3) if torch.min(loss_2)>0 else 1e-20   # 如果loss_2全是0则赋值为1e-20，否则赋值为loss的最小值\n",
    "            loss_padded = [MIN_LOSS]* (loss_2.shape[0]-loss_3.shape[0])\n",
    "            loss_padded_ts = torch.tensor(loss_padded,device=device)\n",
    "            loss_3 = torch.cat((loss_3[:,0],loss_padded_ts))\n",
    "            torch._assert(len(loss_3) == len(loss_2),\"cat不正确\")\n",
    "\n",
    "            loss_4 = torch.log(loss_3)\n",
    "            loss_list.append(-torch.mean((loss_4)).item())\n",
    "\n",
    "        # 最后处理loss\n",
    "        loss = np.sum(loss_list)\n",
    "\n",
    "    loss_ts = torch.tensor(loss,device=device,requires_grad=True)\n",
    "    return loss_ts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [],
   "source": [
    "def loss_test():\n",
    "    loss_2 = torch.tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "    loss_3 = loss_2[torch.nonzero(loss_2)].view(-1,1)         # 去掉所有的log(0)\n",
    "    print(\"loss_3:\",loss_3.shape)\n",
    "\n",
    "    MIN_LOSS = torch.min(loss_3) if torch.min(loss_2)>0 else 1e-20\n",
    "\n",
    "    print(\"MIN_LOSS：\",MIN_LOSS)\n",
    "    loss_padded = [MIN_LOSS]* (loss_2.shape[0]-loss_3.shape[0])\n",
    "    loss_padded_ts = torch.tensor(loss_padded) # ,device=device\n",
    "    loss_3 = torch.cat((loss_3[:,0],loss_padded_ts))\n",
    "    loss_3.shape, torch.log(loss_3)\n",
    "    print(\"data: \",loss_3.data)\n",
    "    print(\"data: \",type(loss_3))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Training\n",
    "## 5.1 preparations\n",
    "1. 初始化Visdom环境\n",
    "2.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# writer = SummaryWriter(\"logs-MLP\")\n",
    "viz = Visdom(env=\"001\")\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.2 Draw\n",
    "1. draw:\n",
    "    - mdn的图（visdom）以及mdn的test draw\n",
    "    - loss图以及初始化（visdom）\n",
    "    - MLP的网络结构（.png）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "#### Test for drawing\n",
    "def test_draw():\n",
    "    viz = Visdom(env=\"001\")\n",
    "\n",
    "    mu = torch.tensor([0,10,20])\n",
    "    sigma = torch.tensor([1,1,1])\n",
    "    duration = torch.tensor([0,1,2,0])\n",
    "    duration = torch.repeat_interleave(duration.unsqueeze(dim=1), repeats=3, dim=1)\n",
    "    m = torch.distributions.Normal(loc=mu, scale=sigma)\n",
    "    pi = torch.tensor([0.2,0.3,0.5])\n",
    "\n",
    "    # draw\n",
    "    x_0 = torch.tensor(np.arange(0,1000))\n",
    "    x = torch.repeat_interleave(x_0.unsqueeze(dim=1), repeats=3, dim=1)\n",
    "    y = torch.exp(m.log_prob(x))\n",
    "    y_sum = torch.unsqueeze(torch.sum(pi*y,dim=1),dim=1)\n",
    "    viz.line(X = x_0,Y= torch.cat([y,y_sum],dim = 1), env=\"001\", win=\"test_draw_2\",\n",
    "            opts= dict(title='test_draw', legend=['N1', 'N2', 'N3','NNN']))\n",
    "# test_draw()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "def draw_mdn(pi,duration,m,total_train_step):\n",
    "    # draw the distrb.\n",
    "    x_0 = torch.arange(0,torch.max(duration).item()).to(device)\n",
    "    x = torch.repeat_interleave(x_0.unsqueeze(dim=1), repeats=3, dim=1)\n",
    "    y = torch.exp(m.log_prob(x)).to(device)\n",
    "    y_sum = torch.unsqueeze(torch.sum(pi*y,dim=1),dim=1)   # 维度相等才能cat\n",
    "    win_str = \"total_train_step-\"+str(total_train_step)\n",
    "    viz.line(X = x_0,Y= torch.cat([y,y_sum],dim = 1), env=\"001\", win=win_str,\n",
    "        opts= dict(title=win_str, legend=['N1', 'N2', 'N3','NNN']))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [],
   "source": [
    "def draw_the_net():\n",
    "\n",
    "    x = torch.randn([5, 3, 300])  # 定义网络的输入值\n",
    "    mlp = MLP(N_gaussians)\n",
    "    y = mlp(x)                    # 获取网络的预测值\n",
    "\n",
    "    MyConvNetVis = make_dot(y, params=dict(list(mlp.named_parameters()) + [('x', x)]))\n",
    "    MyConvNetVis.format = \"png\"\n",
    "    # 指定文件生成的文件夹\n",
    "    MyConvNetVis.directory = \"data_pic\"\n",
    "    # 生成文件\n",
    "    MyConvNetVis.view()\n",
    "# draw_the_net()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [],
   "source": [
    "viz.line(X = [0.],Y = [0.], env=\"001\", win=\"The Loss\", opts= dict(title=\"The Loss\"))\n",
    "def draw_loss(total_train_step, loss):\n",
    "    viz.line(X = [total_train_step], Y = [loss],win=\"The Loss\", update=\"append\",\n",
    "        opts= dict(title=\"The Loss\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.3 Training\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "       BatchNorm2d-1            [-1, 3, 1, 300]               0\n",
      "            Conv2d-2            [-1, 3, 1, 100]              27\n",
      "          Softplus-3            [-1, 3, 1, 100]               0\n",
      "       BatchNorm2d-4            [-1, 3, 1, 100]               0\n",
      "            Conv2d-5             [-1, 3, 1, 50]              18\n",
      "          Softplus-6             [-1, 3, 1, 50]               0\n",
      "       BatchNorm2d-7             [-1, 3, 1, 50]               0\n",
      "            Conv2d-8             [-1, 3, 1, 10]              45\n",
      "          Softplus-9             [-1, 3, 1, 10]               0\n",
      "          Flatten-10                   [-1, 30]               0\n",
      "           Linear-11                    [-1, 9]             279\n",
      "           Linear-12                    [-1, 3]              30\n",
      "          Softmax-13                    [-1, 3]               0\n",
      "           Linear-14                    [-1, 3]              30\n",
      "           Linear-15                    [-1, 3]              30\n",
      "================================================================\n",
      "Total params: 459\n",
      "Trainable params: 459\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.02\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP(N_gaussians)\n",
    "\n",
    "# Init the params\n",
    "# # mlp = model_param_init(mlp)\n",
    "\n",
    "# save the init params\n",
    "torch.save(mlp.state_dict(), 'mlp_init.pth')\n",
    "\n",
    "# read the saved model\n",
    "# model_data = torch.load('mlp_init_loss_17.pth')\n",
    "# mlp.load_state_dict(model_data)\n",
    "\n",
    "mlp = mlp.to(device=device)\n",
    "summary(mlp, (3,300))\n",
    "# optimizer = torch.optim.Adagrad(mlp.parameters(),lr=learning_rate, lr_decay=learning_rate, weight_decay=learning_rate)\n",
    "optimizer = torch.optim.Adam(mlp.parameters(),lr=learning_rate)\n",
    "\n",
    "# # hooks\n",
    "# #mlp.conv1.register_forward_hook(hook_forward_fn)\n",
    "# mlp.conv1.register_full_backward_hook(hook_backward_fn)\n",
    "#\n",
    "# #mlp.linear1.register_forward_hook(hook_forward_fn)\n",
    "# #mlp.linear1.register_full_backward_hook(hook_backward_fn)\n",
    "#\n",
    "# #mlp.z_pi.register_forward_hook(hook_forward_fn)\n",
    "# mlp.z_pi.register_full_backward_hook(hook_backward_fn_pi)\n",
    "#\n",
    "# #mlp.z_mu.register_forward_hook(hook_forward_fn)\n",
    "# mlp.z_mu.register_full_backward_hook(hook_backward_fn_mu)\n",
    "#\n",
    "# #mlp.z_sigma.register_forward_hook(hook_forward_fn)\n",
    "# mlp.z_sigma.register_full_backward_hook(hook_backward_fn_sigma)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- 0 batch----\n",
      "训练次数：0，Loss：221.59072494506836\n",
      "tensor([[0.3250, 0.5029, 0.1721],\n",
      "        [0.3033, 0.5260, 0.1707],\n",
      "        [0.2663, 0.5663, 0.1674],\n",
      "        [0.3144, 0.4975, 0.1881],\n",
      "        [0.2356, 0.6176, 0.1469]], device='cuda:0', grad_fn=<SoftmaxBackward0>) \n",
      " tensor([[ 0.0310,  0.2524, -0.3290],\n",
      "        [ 0.0406,  0.3048, -0.3535],\n",
      "        [ 0.0655,  0.2938, -0.1311],\n",
      "        [ 0.1070,  0.3186, -0.1775],\n",
      "        [ 0.2466,  0.5058, -0.2540]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) \n",
      " tensor([[0.6617, 1.4068, 0.8348],\n",
      "        [0.7332, 1.0673, 0.8976],\n",
      "        [0.6526, 1.3692, 0.7989],\n",
      "        [0.7210, 1.4413, 0.8803],\n",
      "        [0.4027, 1.7673, 0.7591]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "---- 1 batch----\n",
      "训练次数：1，Loss：189.5836057662964\n",
      "---- 2 batch----\n",
      "训练次数：2，Loss：228.55752563476562\n",
      "---- 3 batch----\n",
      "训练次数：3，Loss：218.70854568481445\n",
      "---- 4 batch----\n",
      "训练次数：4，Loss：219.95672225952148\n",
      "---- 5 batch----\n",
      "训练次数：5，Loss：177.62089443206787\n",
      "---- 6 batch----\n",
      "训练次数：6，Loss：217.6673355102539\n",
      "---- 7 batch----\n",
      "训练次数：7，Loss：216.54985427856445\n",
      "---- 8 batch----\n",
      "训练次数：8，Loss：230.90175247192383\n",
      "---- 9 batch----\n",
      "训练次数：9，Loss：208.02791786193848\n",
      "---- 10 batch----\n",
      "训练次数：10，Loss：222.2039337158203\n",
      "tensor([[0.3105, 0.5132, 0.1763],\n",
      "        [0.3290, 0.4980, 0.1730],\n",
      "        [0.3544, 0.4736, 0.1720],\n",
      "        [0.2816, 0.5639, 0.1545],\n",
      "        [0.2984, 0.5184, 0.1833]], device='cuda:0', grad_fn=<SoftmaxBackward0>) \n",
      " tensor([[-0.0100,  0.2353, -0.3356],\n",
      "        [ 0.0662,  0.2635, -0.1390],\n",
      "        [-0.0226,  0.2484, -0.3910],\n",
      "        [ 0.0090,  0.2816, -0.1556],\n",
      "        [-0.0158,  0.2533, -0.2835]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) \n",
      " tensor([[0.7264, 1.3800, 0.9237],\n",
      "        [0.8505, 1.3132, 0.8804],\n",
      "        [0.8813, 1.0134, 0.9144],\n",
      "        [0.7177, 1.2862, 0.8668],\n",
      "        [0.6599, 1.3728, 0.8724]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "---- 11 batch----\n",
      "训练次数：11，Loss：217.55974578857422\n",
      "---- 12 batch----\n",
      "训练次数：12，Loss：208.90213012695312\n",
      "---- 13 batch----\n",
      "训练次数：13，Loss：210.04978942871094\n",
      "---- 14 batch----\n",
      "训练次数：14，Loss：221.84711074829102\n",
      "---- 15 batch----\n",
      "训练次数：15，Loss：224.56001663208008\n",
      "---- 16 batch----\n",
      "训练次数：16，Loss：219.9219970703125\n",
      "---- 17 batch----\n",
      "训练次数：17，Loss：200.2045726776123\n",
      "---- 18 batch----\n",
      "训练次数：18，Loss：230.07334518432617\n",
      "---- 19 batch----\n",
      "训练次数：19，Loss：233.1119155883789\n",
      "---- 20 batch----\n",
      "训练次数：20，Loss：230.9699363708496\n",
      "tensor([[0.3045, 0.5267, 0.1688],\n",
      "        [0.3364, 0.4820, 0.1816],\n",
      "        [0.2443, 0.5815, 0.1743],\n",
      "        [0.2835, 0.5470, 0.1695],\n",
      "        [0.3256, 0.5024, 0.1719]], device='cuda:0', grad_fn=<SoftmaxBackward0>) \n",
      " tensor([[-0.0580,  0.2273, -0.2561],\n",
      "        [ 0.0331,  0.2185, -0.1094],\n",
      "        [ 0.0664,  0.3997, -0.2121],\n",
      "        [ 0.1878,  0.4100, -0.1176],\n",
      "        [ 0.0347,  0.2859, -0.3708]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) \n",
      " tensor([[0.7641, 1.2277, 0.9244],\n",
      "        [0.8649, 1.3560, 0.8792],\n",
      "        [0.6106, 1.1835, 0.9568],\n",
      "        [0.5813, 1.4317, 0.7819],\n",
      "        [0.8107, 1.0863, 0.9361]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "---- 21 batch----\n",
      "训练次数：21，Loss：214.26745223999023\n",
      "---- 22 batch----\n",
      "训练次数：22，Loss：226.35689544677734\n",
      "---- 23 batch----\n",
      "训练次数：23，Loss：231.1762924194336\n",
      "---- 24 batch----\n",
      "训练次数：24，Loss：223.90145111083984\n",
      "---- 25 batch----\n",
      "训练次数：25，Loss：226.85532760620117\n",
      "---- 26 batch----\n",
      "训练次数：26，Loss：219.8785629272461\n",
      "---- 27 batch----\n",
      "训练次数：27，Loss：202.20867538452148\n",
      "---- 28 batch----\n",
      "训练次数：28，Loss：230.6693878173828\n",
      "---- 29 batch----\n",
      "训练次数：29，Loss：228.54931640625\n",
      "---- 30 batch----\n",
      "训练次数：30，Loss：232.3498878479004\n",
      "tensor([[0.3077, 0.5206, 0.1717],\n",
      "        [0.2712, 0.5626, 0.1663],\n",
      "        [0.3235, 0.5092, 0.1674],\n",
      "        [0.2763, 0.5588, 0.1649],\n",
      "        [0.3459, 0.4849, 0.1691]], device='cuda:0', grad_fn=<SoftmaxBackward0>) \n",
      " tensor([[ 0.1204,  0.3796, -0.2431],\n",
      "        [-0.0307,  0.2385, -0.1355],\n",
      "        [ 0.1817,  0.3632, -0.0724],\n",
      "        [-0.0094,  0.2505, -0.1533],\n",
      "        [-0.0353,  0.1950, -0.3204]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) \n",
      " tensor([[0.6771, 1.2820, 0.8764],\n",
      "        [0.7817, 1.1909, 0.9182],\n",
      "        [0.7263, 1.4552, 0.7813],\n",
      "        [0.7480, 1.1630, 0.8585],\n",
      "        [0.7987, 1.2243, 0.8491]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "---- 31 batch----\n",
      "训练次数：31，Loss：217.09871673583984\n",
      "---- 32 batch----\n",
      "训练次数：32，Loss：226.90464401245117\n",
      "---- 33 batch----\n",
      "训练次数：33，Loss：205.56078338623047\n",
      "---- 34 batch----\n",
      "训练次数：34，Loss：224.0511703491211\n",
      "---- 35 batch----\n",
      "训练次数：35，Loss：194.30687141418457\n",
      "---- 36 batch----\n",
      "训练次数：36，Loss：230.4289207458496\n",
      "---- 37 batch----\n",
      "训练次数：37，Loss：194.53125\n",
      "---- 38 batch----\n",
      "训练次数：38，Loss：175.55859088897705\n",
      "---- 39 batch----\n",
      "训练次数：39，Loss：207.54918670654297\n",
      "---- 40 batch----\n",
      "训练次数：40，Loss：199.18292999267578\n",
      "tensor([[0.2669, 0.5584, 0.1747],\n",
      "        [0.2753, 0.5465, 0.1782],\n",
      "        [0.2788, 0.5516, 0.1697],\n",
      "        [0.2370, 0.6345, 0.1285],\n",
      "        [0.3130, 0.5229, 0.1641]], device='cuda:0', grad_fn=<SoftmaxBackward0>) \n",
      " tensor([[ 0.0599,  0.3301, -0.1375],\n",
      "        [ 0.0224,  0.2759, -0.1170],\n",
      "        [ 0.0009,  0.2665, -0.1499],\n",
      "        [-0.0087,  0.2728, -0.3756],\n",
      "        [ 0.0468,  0.2618, -0.1300]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) \n",
      " tensor([[0.6268, 1.4335, 0.8583],\n",
      "        [0.7897, 1.1433, 0.9195],\n",
      "        [0.7341, 1.2315, 0.8914],\n",
      "        [0.5898, 1.5856, 0.9317],\n",
      "        [0.6437, 1.4959, 0.7378]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "---- 41 batch----\n",
      "训练次数：41，Loss：211.3124485015869\n",
      "---- 42 batch----\n",
      "训练次数：42，Loss：226.25749969482422\n",
      "---- 43 batch----\n",
      "训练次数：43，Loss：223.08533477783203\n",
      "---- 44 batch----\n",
      "训练次数：44，Loss：226.3411102294922\n",
      "---- 45 batch----\n",
      "训练次数：45，Loss：235.59761428833008\n",
      "---- 46 batch----\n",
      "训练次数：46，Loss：215.8629493713379\n",
      "---- 47 batch----\n",
      "训练次数：47，Loss：195.26930618286133\n",
      "---- 48 batch----\n",
      "训练次数：48，Loss：204.52635955810547\n",
      "---- 49 batch----\n",
      "训练次数：49，Loss：231.23722457885742\n",
      "---- 50 batch----\n",
      "训练次数：50，Loss：231.06815719604492\n",
      "tensor([[0.2753, 0.5595, 0.1652],\n",
      "        [0.2305, 0.5834, 0.1861],\n",
      "        [0.3497, 0.4721, 0.1782],\n",
      "        [0.2875, 0.5410, 0.1715],\n",
      "        [0.2802, 0.5503, 0.1695]], device='cuda:0', grad_fn=<SoftmaxBackward0>) \n",
      " tensor([[ 0.0851,  0.3613, -0.1801],\n",
      "        [ 0.0290,  0.3811, -0.1956],\n",
      "        [ 0.0074,  0.2010, -0.1945],\n",
      "        [ 0.1655,  0.3905, -0.1275],\n",
      "        [ 0.0144,  0.2547, -0.0958]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) \n",
      " tensor([[0.5646, 1.5803, 0.8497],\n",
      "        [0.6228, 1.0568, 0.9916],\n",
      "        [0.6924, 1.4602, 0.7340],\n",
      "        [0.6116, 1.3709, 0.7987],\n",
      "        [0.7264, 1.3005, 0.8459]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "---- 51 batch----\n",
      "训练次数：51，Loss：216.5546760559082\n",
      "---- 52 batch----\n",
      "训练次数：52，Loss：230.8869400024414\n",
      "---- 53 batch----\n",
      "训练次数：53，Loss：226.26538848876953\n",
      "---- 54 batch----\n",
      "训练次数：54，Loss：227.53711318969727\n",
      "---- 55 batch----\n",
      "训练次数：55，Loss：210.77313041687012\n",
      "---- 56 batch----\n",
      "训练次数：56，Loss：210.95022773742676\n",
      "---- 57 batch----\n",
      "训练次数：57，Loss：229.12290954589844\n",
      "---- 58 batch----\n",
      "训练次数：58，Loss：231.15715789794922\n",
      "---- 59 batch----\n",
      "训练次数：59，Loss：228.5649528503418\n",
      "---- 60 batch----\n",
      "训练次数：60，Loss：232.0169334411621\n",
      "tensor([[0.3072, 0.5341, 0.1588],\n",
      "        [0.2828, 0.5493, 0.1679],\n",
      "        [0.3019, 0.5200, 0.1782],\n",
      "        [0.3096, 0.5103, 0.1801],\n",
      "        [0.2793, 0.5542, 0.1665]], device='cuda:0', grad_fn=<SoftmaxBackward0>) \n",
      " tensor([[-0.0121,  0.2396, -0.1063],\n",
      "        [ 0.1762,  0.4020, -0.1278],\n",
      "        [ 0.0542,  0.2909, -0.2002],\n",
      "        [ 0.0111,  0.2582, -0.2997],\n",
      "        [ 0.0674,  0.3400, -0.1856]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) \n",
      " tensor([[0.8107, 1.3227, 0.8709],\n",
      "        [0.5978, 1.3827, 0.7911],\n",
      "        [0.7609, 1.3351, 0.9152],\n",
      "        [0.7032, 1.1695, 0.8276],\n",
      "        [0.5742, 1.4561, 0.8010]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "---- 61 batch----\n",
      "训练次数：61，Loss：213.35659980773926\n",
      "---- 62 batch----\n",
      "训练次数：62，Loss：223.13903427124023\n",
      "---- 63 batch----\n",
      "训练次数：63，Loss：221.84134674072266\n",
      "---- 64 batch----\n",
      "训练次数：64，Loss：233.22216796875\n",
      "---- 65 batch----\n",
      "训练次数：65，Loss：223.5303955078125\n",
      "---- 66 batch----\n",
      "训练次数：66，Loss：217.65950393676758\n",
      "---- 67 batch----\n",
      "训练次数：67，Loss：223.58914947509766\n",
      "---- 68 batch----\n",
      "训练次数：68，Loss：224.86340713500977\n",
      "---- 69 batch----\n",
      "训练次数：69，Loss：226.9012908935547\n",
      "---- 70 batch----\n",
      "训练次数：70，Loss：226.8719367980957\n",
      "tensor([[0.2686, 0.5596, 0.1719],\n",
      "        [0.2838, 0.5347, 0.1815],\n",
      "        [0.3261, 0.5090, 0.1650],\n",
      "        [0.3064, 0.5280, 0.1657],\n",
      "        [0.3070, 0.5191, 0.1739]], device='cuda:0', grad_fn=<SoftmaxBackward0>) \n",
      " tensor([[ 0.0512,  0.3101, -0.1185],\n",
      "        [ 0.0450,  0.2843, -0.1596],\n",
      "        [ 0.0258,  0.2376, -0.2191],\n",
      "        [-0.0133,  0.2223, -0.1751],\n",
      "        [ 0.1232,  0.3823, -0.2380]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) \n",
      " tensor([[0.6752, 1.3511, 0.8863],\n",
      "        [0.7009, 1.4515, 0.9014],\n",
      "        [0.8091, 1.2846, 0.8554],\n",
      "        [0.6854, 1.5348, 0.8172],\n",
      "        [0.6728, 1.2863, 0.8757]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "---- 71 batch----\n",
      "训练次数：71，Loss：230.7125358581543\n",
      "---- 72 batch----\n",
      "训练次数：72，Loss：228.0511703491211\n",
      "---- 73 batch----\n",
      "训练次数：73，Loss：205.6765308380127\n",
      "---- 74 batch----\n",
      "训练次数：74，Loss：221.6581916809082\n",
      "---- 75 batch----\n",
      "训练次数：75，Loss：223.9028778076172\n",
      "---- 76 batch----\n",
      "训练次数：76，Loss：217.5295295715332\n",
      "---- 77 batch----\n",
      "训练次数：77，Loss：205.31626319885254\n",
      "---- 78 batch----\n",
      "训练次数：78，Loss：224.31775665283203\n",
      "---- 79 batch----\n",
      "训练次数：79，Loss：216.93600463867188\n",
      "---- 80 batch----\n",
      "训练次数：80，Loss：189.6620979309082\n",
      "tensor([[0.2177, 0.6878, 0.0944],\n",
      "        [0.2683, 0.5753, 0.1564],\n",
      "        [0.2135, 0.6221, 0.1644],\n",
      "        [0.2961, 0.5098, 0.1941],\n",
      "        [0.3029, 0.5389, 0.1582]], device='cuda:0', grad_fn=<SoftmaxBackward0>) \n",
      " tensor([[ 0.0720,  0.2973, -0.3632],\n",
      "        [-0.0068,  0.2386, -0.1504],\n",
      "        [ 0.0265,  0.3698, -0.1305],\n",
      "        [ 0.1302,  0.3426, -0.2430],\n",
      "        [ 0.0050,  0.2296, -0.1465]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) \n",
      " tensor([[0.4371, 2.4009, 0.7597],\n",
      "        [0.6991, 1.5549, 0.8973],\n",
      "        [0.5983, 1.2631, 0.9758],\n",
      "        [0.6749, 1.0938, 0.8185],\n",
      "        [0.8151, 1.2908, 0.8440]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "---- 81 batch----\n",
      "训练次数：81，Loss：224.08334732055664\n",
      "---- 82 batch----\n",
      "训练次数：82，Loss：228.76483917236328\n",
      "---- 83 batch----\n",
      "训练次数：83，Loss：213.36236953735352\n",
      "---- 84 batch----\n",
      "训练次数：84，Loss：222.02521514892578\n",
      "---- 85 batch----\n",
      "训练次数：85，Loss：208.7664909362793\n",
      "---- 86 batch----\n",
      "训练次数：86，Loss：230.9701919555664\n",
      "---- 87 batch----\n",
      "训练次数：87，Loss：229.1787872314453\n",
      "---- 88 batch----\n",
      "训练次数：88，Loss：220.3014678955078\n",
      "---- 89 batch----\n",
      "训练次数：89，Loss：228.83976364135742\n",
      "---- 90 batch----\n",
      "训练次数：90，Loss：216.16693496704102\n",
      "tensor([[0.2717, 0.5526, 0.1756],\n",
      "        [0.2988, 0.5378, 0.1634],\n",
      "        [0.2617, 0.5870, 0.1513],\n",
      "        [0.3136, 0.5161, 0.1704],\n",
      "        [0.3165, 0.5167, 0.1668]], device='cuda:0', grad_fn=<SoftmaxBackward0>) \n",
      " tensor([[-0.0121,  0.2426, -0.1133],\n",
      "        [ 0.1998,  0.3862, -0.1878],\n",
      "        [-0.0151,  0.2436, -0.2835],\n",
      "        [-0.0281,  0.2119, -0.2703],\n",
      "        [-0.0460,  0.1875, -0.1825]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) \n",
      " tensor([[0.7297, 1.2960, 0.8994],\n",
      "        [0.5789, 1.5433, 0.7321],\n",
      "        [0.6162, 1.5098, 0.8602],\n",
      "        [0.8179, 1.3934, 0.9618],\n",
      "        [0.7909, 1.4533, 0.8756]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "---- 91 batch----\n",
      "训练次数：91，Loss：216.78309631347656\n",
      "---- 92 batch----\n",
      "训练次数：92，Loss：226.4495964050293\n",
      "---- 93 batch----\n",
      "训练次数：93，Loss：227.11275100708008\n",
      "---- 94 batch----\n",
      "训练次数：94，Loss：231.0734405517578\n",
      "---- 95 batch----\n",
      "训练次数：95，Loss：216.25852584838867\n",
      "---- 96 batch----\n",
      "训练次数：96，Loss：153.58598518371582\n",
      "---- 97 batch----\n",
      "训练次数：97，Loss：212.82373809814453\n",
      "---- 98 batch----\n",
      "训练次数：98，Loss：226.87343978881836\n",
      "---- 99 batch----\n",
      "训练次数：99，Loss：221.48833847045898\n",
      "---- 100 batch----\n",
      "训练次数：100，Loss：187.6020107269287\n",
      "tensor([[0.3385, 0.5089, 0.1526],\n",
      "        [0.2942, 0.5642, 0.1416],\n",
      "        [0.3002, 0.5092, 0.1907],\n",
      "        [0.3109, 0.5094, 0.1797],\n",
      "        [0.2746, 0.5604, 0.1649]], device='cuda:0', grad_fn=<SoftmaxBackward0>) \n",
      " tensor([[ 0.1415,  0.3173, -0.2067],\n",
      "        [ 0.0694,  0.2899, -0.2508],\n",
      "        [ 0.0239,  0.2686, -0.1666],\n",
      "        [ 0.0593,  0.2859, -0.2297],\n",
      "        [ 0.0342,  0.2980, -0.2221]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) \n",
      " tensor([[0.7557, 1.3761, 0.7927],\n",
      "        [0.5411, 1.7785, 0.7292],\n",
      "        [0.7601, 1.4788, 0.9656],\n",
      "        [0.6710, 1.3367, 0.8096],\n",
      "        [0.6858, 1.1210, 0.8533]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "---- 101 batch----\n",
      "训练次数：101，Loss：226.287109375\n",
      "---- 102 batch----\n",
      "训练次数：102，Loss：225.38943099975586\n",
      "---- 103 batch----\n",
      "训练次数：103，Loss：224.66845321655273\n",
      "---- 104 batch----\n",
      "训练次数：104，Loss：229.67041778564453\n",
      "---- 105 batch----\n",
      "训练次数：105，Loss：233.13331604003906\n",
      "---- 106 batch----\n",
      "训练次数：106，Loss：224.2981185913086\n",
      "---- 107 batch----\n",
      "训练次数：107，Loss：219.3418846130371\n",
      "---- 108 batch----\n",
      "训练次数：108，Loss：215.82198333740234\n",
      "---- 109 batch----\n",
      "训练次数：109，Loss：223.51839447021484\n",
      "---- 110 batch----\n",
      "训练次数：110，Loss：218.78623580932617\n",
      "tensor([[0.2776, 0.5581, 0.1644],\n",
      "        [0.3491, 0.4779, 0.1730],\n",
      "        [0.3702, 0.4731, 0.1566],\n",
      "        [0.3207, 0.5087, 0.1706],\n",
      "        [0.3111, 0.5131, 0.1757]], device='cuda:0', grad_fn=<SoftmaxBackward0>) \n",
      " tensor([[-0.0249,  0.2128, -0.1402],\n",
      "        [-0.0449,  0.1888, -0.2807],\n",
      "        [-0.0169,  0.1853, -0.2871],\n",
      "        [ 0.0589,  0.2865, -0.3302],\n",
      "        [ 0.0536,  0.2892, -0.0833]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) \n",
      " tensor([[0.7251, 1.5094, 0.9082],\n",
      "        [0.8233, 1.3064, 0.8573],\n",
      "        [0.8769, 1.3376, 0.8575],\n",
      "        [0.6227, 1.3909, 0.7658],\n",
      "        [0.7434, 1.3043, 0.8291]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "---- 111 batch----\n",
      "训练次数：111，Loss：229.7689323425293\n",
      "---- 112 batch----\n",
      "训练次数：112，Loss：236.8114776611328\n",
      "---- 113 batch----\n",
      "训练次数：113，Loss：226.39701080322266\n",
      "---- 114 batch----\n",
      "训练次数：114，Loss：204.27092361450195\n",
      "---- 115 batch----\n",
      "训练次数：115，Loss：230.20244216918945\n",
      "---- 116 batch----\n",
      "训练次数：116，Loss：233.91771697998047\n",
      "---- 117 batch----\n",
      "训练次数：117，Loss：223.5949821472168\n",
      "---- 118 batch----\n",
      "训练次数：118，Loss：202.3309726715088\n",
      "---- 119 batch----\n",
      "训练次数：119，Loss：209.7812213897705\n",
      "---- 120 batch----\n",
      "训练次数：120，Loss：218.59868621826172\n",
      "tensor([[0.2752, 0.5623, 0.1625],\n",
      "        [0.3245, 0.4948, 0.1807],\n",
      "        [0.2830, 0.5469, 0.1701],\n",
      "        [0.2853, 0.5406, 0.1741],\n",
      "        [0.2926, 0.5280, 0.1795]], device='cuda:0', grad_fn=<SoftmaxBackward0>) \n",
      " tensor([[-0.0731,  0.1992, -0.1919],\n",
      "        [-0.0660,  0.1942, -0.3367],\n",
      "        [ 0.1642,  0.3957, -0.1337],\n",
      "        [ 0.0172,  0.2752, -0.1596],\n",
      "        [-0.0952,  0.2155, -0.3997]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) \n",
      " tensor([[0.7406, 1.3787, 0.9355],\n",
      "        [0.7702, 1.4462, 0.9750],\n",
      "        [0.6070, 1.3520, 0.8026],\n",
      "        [0.7563, 1.1946, 0.9011],\n",
      "        [0.7223, 1.2858, 1.0202]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "---- 121 batch----\n",
      "训练次数：121，Loss：229.90026473999023\n",
      "---- 122 batch----\n",
      "训练次数：122，Loss：226.96669006347656\n",
      "---- 123 batch----\n",
      "训练次数：123，Loss：237.09566116333008\n",
      "---- 124 batch----\n",
      "训练次数：124，Loss：229.01107025146484\n",
      "---- 125 batch----\n",
      "训练次数：125，Loss：225.82676315307617\n",
      "---- 126 batch----\n",
      "训练次数：126，Loss：213.2672119140625\n",
      "---- 127 batch----\n",
      "训练次数：127，Loss：183.58803176879883\n",
      "---- 128 batch----\n",
      "训练次数：128，Loss：206.07583808898926\n",
      "---- 129 batch----\n",
      "训练次数：129，Loss：230.55809020996094\n",
      "---- 130 batch----\n",
      "训练次数：130，Loss：224.71308517456055\n",
      "tensor([[0.2706, 0.5490, 0.1804],\n",
      "        [0.2624, 0.5636, 0.1739],\n",
      "        [0.3106, 0.5186, 0.1708],\n",
      "        [0.3506, 0.4794, 0.1700],\n",
      "        [0.2895, 0.5453, 0.1652]], device='cuda:0', grad_fn=<SoftmaxBackward0>) \n",
      " tensor([[ 0.0639,  0.2975, -0.1450],\n",
      "        [ 0.0753,  0.3328, -0.1487],\n",
      "        [-0.0075,  0.2385, -0.2926],\n",
      "        [ 0.1166,  0.3078, -0.1713],\n",
      "        [ 0.1466,  0.3721, -0.2120]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) \n",
      " tensor([[0.6514, 1.3924, 0.8565],\n",
      "        [0.6600, 1.2780, 0.8746],\n",
      "        [0.8767, 1.3186, 1.0371],\n",
      "        [0.7320, 1.5463, 0.8058],\n",
      "        [0.7216, 1.3236, 0.9139]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "---- 131 batch----\n",
      "训练次数：131，Loss：226.11008071899414\n",
      "---- 132 batch----\n",
      "训练次数：132，Loss：225.0373306274414\n",
      "---- 133 batch----\n",
      "训练次数：133，Loss：211.94445419311523\n",
      "---- 134 batch----\n",
      "训练次数：134，Loss：199.05168437957764\n",
      "---- 135 batch----\n",
      "训练次数：135，Loss：212.83645248413086\n",
      "---- 136 batch----\n",
      "训练次数：136，Loss：229.3285255432129\n",
      "---- 137 batch----\n",
      "训练次数：137，Loss：231.6589698791504\n",
      "---- 138 batch----\n",
      "训练次数：138，Loss：226.19270706176758\n",
      "---- 139 batch----\n",
      "训练次数：139，Loss：221.2114486694336\n",
      "---- 140 batch----\n",
      "训练次数：140，Loss：217.9930877685547\n",
      "tensor([[0.2986, 0.5244, 0.1769],\n",
      "        [0.3447, 0.4710, 0.1842],\n",
      "        [0.3440, 0.4863, 0.1697],\n",
      "        [0.3026, 0.5337, 0.1637],\n",
      "        [0.3247, 0.5074, 0.1678]], device='cuda:0', grad_fn=<SoftmaxBackward0>) \n",
      " tensor([[ 0.0480,  0.2549, -0.0968],\n",
      "        [ 0.0797,  0.2668, -0.2328],\n",
      "        [-0.0509,  0.1889, -0.2830],\n",
      "        [ 0.0667,  0.2893, -0.0300],\n",
      "        [ 0.0129,  0.2255, -0.2328]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) \n",
      " tensor([[0.7430, 1.3305, 0.8200],\n",
      "        [0.8029, 1.2370, 0.8308],\n",
      "        [0.8193, 1.2806, 0.8549],\n",
      "        [0.6989, 1.4420, 0.7653],\n",
      "        [0.7792, 1.4096, 0.8929]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "---- 141 batch----\n",
      "训练次数：141，Loss：224.3913917541504\n",
      "---- 142 batch----\n",
      "训练次数：142，Loss：225.0041389465332\n",
      "---- 143 batch----\n",
      "训练次数：143，Loss：238.2121696472168\n",
      "---- 144 batch----\n",
      "训练次数：144，Loss：200.8999366760254\n",
      "---- 145 batch----\n",
      "训练次数：145，Loss：227.00766372680664\n",
      "---- 146 batch----\n",
      "训练次数：146，Loss：228.99951934814453\n",
      "---- 147 batch----\n",
      "训练次数：147，Loss：185.3455104827881\n",
      "---- 148 batch----\n",
      "训练次数：148，Loss：231.9159698486328\n",
      "---- 149 batch----\n",
      "训练次数：149，Loss：229.10724258422852\n",
      "---- 150 batch----\n",
      "训练次数：150，Loss：216.16290664672852\n",
      "tensor([[0.3510, 0.4723, 0.1768],\n",
      "        [0.3443, 0.4679, 0.1877],\n",
      "        [0.3055, 0.5190, 0.1754],\n",
      "        [0.2759, 0.5532, 0.1709],\n",
      "        [0.3226, 0.5033, 0.1741]], device='cuda:0', grad_fn=<SoftmaxBackward0>) \n",
      " tensor([[ 0.0813,  0.2819, -0.2548],\n",
      "        [-0.0264,  0.2400, -0.3847],\n",
      "        [-0.0022,  0.2314, -0.2683],\n",
      "        [-0.0097,  0.2327, -0.1016],\n",
      "        [-0.0658,  0.1813, -0.2673]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) \n",
      " tensor([[0.7740, 1.2843, 0.8350],\n",
      "        [0.8935, 1.1371, 1.0511],\n",
      "        [0.7211, 1.5169, 0.9071],\n",
      "        [0.7006, 1.2814, 0.8251],\n",
      "        [0.8255, 1.3115, 0.9060]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "---- 151 batch----\n",
      "训练次数：151，Loss：226.85666275024414\n",
      "---- 152 batch----\n",
      "训练次数：152，Loss：197.5565357208252\n",
      "---- 153 batch----\n",
      "训练次数：153，Loss：210.51263046264648\n",
      "---- 154 batch----\n",
      "训练次数：154，Loss：218.1857147216797\n",
      "---- 155 batch----\n",
      "训练次数：155，Loss：230.68616485595703\n",
      "---- 156 batch----\n",
      "训练次数：156，Loss：201.13966751098633\n",
      "---- 157 batch----\n",
      "训练次数：157，Loss：225.77465438842773\n",
      "---- 158 batch----\n",
      "训练次数：158，Loss：227.9372215270996\n",
      "---- 159 batch----\n",
      "训练次数：159，Loss：209.34404373168945\n",
      "---- 160 batch----\n",
      "训练次数：160，Loss：217.42630004882812\n",
      "tensor([[0.3405, 0.4731, 0.1864],\n",
      "        [0.3028, 0.5260, 0.1712],\n",
      "        [0.3121, 0.5198, 0.1681],\n",
      "        [0.3186, 0.5044, 0.1770],\n",
      "        [0.2885, 0.5627, 0.1487]], device='cuda:0', grad_fn=<SoftmaxBackward0>) \n",
      " tensor([[ 0.0073,  0.2659, -0.3539],\n",
      "        [-0.1179,  0.1826, -0.2574],\n",
      "        [ 0.0262,  0.2542, -0.1938],\n",
      "        [ 0.0314,  0.2436, -0.1795],\n",
      "        [ 0.1616,  0.3749, -0.2404]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) \n",
      " tensor([[0.8632, 1.1340, 1.0144],\n",
      "        [0.8117, 1.3838, 1.0117],\n",
      "        [0.6725, 1.3535, 0.7580],\n",
      "        [0.8340, 1.3741, 0.9097],\n",
      "        [0.5264, 1.7117, 0.7304]], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "---- 161 batch----\n",
      "训练次数：161，Loss：194.15559768676758\n",
      "---- 162 batch----\n",
      "训练次数：162，Loss：210.98466682434082\n",
      "---- 163 batch----\n",
      "训练次数：163，Loss：228.14379119873047\n",
      "---- 164 batch----\n",
      "训练次数：164，Loss：221.58766555786133\n",
      "---- 165 batch----\n",
      "训练次数：165，Loss：219.1892318725586\n",
      "---- 166 batch----\n",
      "训练次数：166，Loss：219.6545066833496\n",
      "---- 167 batch----\n",
      "训练次数：167，Loss：93.91007995605469\n"
     ]
    }
   ],
   "source": [
    "# filename = \"../log_file.txt\"\n",
    "# f = open(filename,'w')\n",
    "total_train_step = 0\n",
    "mlp.train()\n",
    "for epoch in range(0,1):\n",
    "    for batch_id,data in enumerate(train_loader):\n",
    "\n",
    "        input_data, target = data\n",
    "        print(f\"---- {batch_id} batch----\")\n",
    "\n",
    "        # do the inference\n",
    "        input_data = input_data.to(device)\n",
    "        pi, mu, sigma = mlp(input_data)\n",
    "        # print(f\"The [pi,mu,sigma] is : \\n\")\n",
    "        # print(pi,\"\\n\",mu,\"\\n\",sigma)\n",
    "\n",
    "        # save the params\n",
    "        # params = list(mlp.named_parameters())\n",
    "\n",
    "        # cal the loss and draw the MDN\n",
    "        duration,m  = loss_preparation(pi.detach(), mu.detach(), sigma.detach(), target)\n",
    "        # draw_mdn(pi,duration,m,total_train_step)\n",
    "        loss = loss_fn(pi,duration,m)\n",
    "        draw_loss(total_train_step, loss.item())\n",
    "        print(\"训练次数：{}，Loss：{}\".format(total_train_step, loss.item()))\n",
    "\n",
    "\n",
    "        ########### before step() ###############\n",
    "\n",
    "        # for name, parms in mlp.named_parameters():\n",
    "        #     print(\"=====before step()=====\")\n",
    "        #     print('-->name:', name)\n",
    "        #     print('-->para:', parms)\n",
    "        #     print('-->grad_requirs:',parms.requires_grad)\n",
    "        #     print('-->grad_value:',parms.grad)\n",
    "        #     print(\"===\")\n",
    "\n",
    "        # optim\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        # for name, parms in mlp.named_parameters():\n",
    "        #     print(\"=====After step()=====\")\n",
    "        #     print('-->name:', name)\n",
    "        #     print('-->para:', parms)\n",
    "        #     print('-->grad_requirs:',parms.requires_grad)\n",
    "        #     print('-->grad_value:',parms.grad)\n",
    "        #     print(\"===\")\n",
    "\n",
    "        # print(\"训练次数：{}，Loss：{}, Loss's grad: {}\".format(total_train_step, loss.item(), loss.grad))\n",
    "\n",
    "        if total_train_step % 10 == 0:\n",
    "            print(pi,\"\\n\",mu,\"\\n\",sigma)\n",
    "\n",
    "        total_train_step += 1\n",
    "\n",
    "# f.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [],
   "source": [
    "# torch.save(mlp.state_dict(), 'mlp_init_loss_17.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "$\\mathcal{L}(y \\vert x) = - \\log\\bigg\\{\\sum_{k=1}^K \\pi_k(x)  \\mathcal{N}\\big(y \\vert \\mu_k(x), \\Sigma_k(x)\\big)\\bigg\\}$"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
