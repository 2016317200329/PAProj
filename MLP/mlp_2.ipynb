{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2022/6/7 15:17\n",
    "# @Author  : Wang Yujia\n",
    "# @File    : mlp.ipynb\n",
    "# @Description :\n",
    "# @TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0. what for\n",
    "1. loss部分逻辑重写，添加`no grad`\n",
    "2. 添加hooks\n",
    "3. 使用pad_seq函数取代自己写的pad函数\n",
    "4. 效果：可以train虽然loss不下降，不会出现NaN的问题"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Preparations\n",
    "## 1.1 Import"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import torch.utils.data\n",
    "from mydataset import *\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from tensorboardX import SummaryWriter\n",
    "from visdom import Visdom\n",
    "from graphviz import Digraph\n",
    "import torchvision\n",
    "from torchviz import make_dot\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pygraphviz as pgv\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import geomloss\n",
    "import time\n",
    "from importlib import reload\n",
    "from functools import partial\n",
    "\n",
    "######### Ray Tune\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "import scipy\n",
    "from torch.nn import KLDivLoss\n",
    "from torch.autograd.gradcheck import gradcheck\n",
    "from config import bcolors\n",
    "\n",
    "import config\n",
    "import loss\n",
    "import plot"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Global settings and reloading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "reload(config)  # 必须reload！！\n",
    "reload(loss)    # 必须reload！！\n",
    "reload(plot)\n",
    "from config import DefaultConfig\n",
    "from loss import cal_metric\n",
    "from loss import loss_fn_v2\n",
    "from plot import plot_conv_weight\n",
    "from plot import plot_mu_weight\n",
    "from plot import plot_pi_weight\n",
    "from plot import plot_sigma_weight\n",
    "from plot import plot_net\n",
    "\n",
    "opt = DefaultConfig()\n",
    "\n",
    "total_train_step = 0\n",
    "total_test_step = 0\n",
    "\n",
    "# Range【一般用不到】\n",
    "a = 0\n",
    "b = 255\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Dataset and Dataloader\n",
    "1. DataLoader中的`shuffer=True`表示在每一次epoch中都打乱所有数据的顺序，然后以batch为单位从头到尾按顺序取用数据。这样的结果就是不同epoch中的数据都是乱序的,设置随机种子的作用就是让你的每一次训练都乱的一样，\n",
    "\n",
    "## 2.1 Dataset and spliting\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 设置随机数种子"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup_seed(7)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 读取data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "dataset = myDataset(opt.train_path, opt.target_path_metric, opt.target_path_loss, opt.data_key_path, opt.NLL_metric_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 产生index的乱序排列"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 106  983  924  803  472   26 1136  810  171  358  831  234  930    2\n",
      "  204   34  947  674 1027   38  110  270  494  946  179  563 1031 1125\n",
      " 1173  373  979  522  763   11  796  414  341  622  318  736  156  200\n",
      "   72  644   89  366  814  133  640  212  994  568    5  570  211  752\n",
      "  390  610  597 1174  192  989  634  426  484  938  968 1007  865  121\n",
      "  839  647  415  896 1189 1162  420  217  147  416  395   57  618  770\n",
      " 1033  795 1082  339  678  637  325  826  513  741  862  844  237  382\n",
      "  481  365  682  857  479  667  350 1096   53  889  206  240   93   86\n",
      "  851  196  296 1085  278  301  335  404  418  328  262  465 1058  853\n",
      "  651  295  732  145  268  981  739  141  150  749  633 1068 1061  547\n",
      "  735  463  966 1025   51  914  890  221  198  861  241  873    0   30\n",
      "  964  743  982  457 1181  750  456   94  439 1160  959 1017  995   99\n",
      " 1187  755  672  495  225   43    7   32  427  242  876  992  190 1167\n",
      "  534  186  142  480  850  652  854 1006  194  693  561 1074  666  974\n",
      "  135  669  369  916  305  432  727  232  785  545  977  151  910  935\n",
      "  374  834  124 1063  849   60  496  664  886   12 1099  467  471  407\n",
      "  603  535  733  838  932  560  832  941  766 1144  383  256  746  362\n",
      "  271 1001  811  904 1055 1046  239   54  525  584 1005 1106  184  169\n",
      "  748 1159 1080  433  375  499  503  386   46  396 1035  958 1175  356\n",
      "    6 1139  100  945  549  464 1098 1093  405  858  475  744  208  245\n",
      " 1116  954 1135   97  780  740   28  423  615  579   14  191 1041 1026\n",
      " 1034  767  292  802  609  490  514  259 1153  398  329  267  911  996\n",
      "  587  598  695 1090  962  372  165  371  894  247   36  950  939   58\n",
      "  158  520  213  168  400  840  474  543 1049  681 1002 1032 1000  582\n",
      "  990  482 1091  345   24  502 1191 1134  251 1028  419  562   16 1112\n",
      "  444  512  635  517   71  195  539   20  279  655  892  172  519  660\n",
      "   18  818  119  707  734  715 1065  987  349  233 1054  626  971 1171\n",
      "  627  367  595  859  768  978   79 1133  468   82  140 1184 1003  687\n",
      "   37   44  718  799   29  412  614  709  388  210  856  801  537  639\n",
      "  406  721  460  300   87  532 1059  820   49 1043  690  702  769  697\n",
      "  757  393  673  852  688  661  170  314 1087 1039  137  173  253  559\n",
      "  613  915  488  720  704  580  551  299  943  997  203 1076  590  628\n",
      " 1011 1023  653 1051  229 1020  804  759 1130  183 1044  879  807  636\n",
      "  441  117  569  662 1183  129  431  214  822  107   33  344  523  842\n",
      " 1029  447  516  845 1146  178   70 1038  450  751 1075    9  443  401\n",
      " 1102  884  291  773  399  127  273  284  691 1109  631  377  470  176\n",
      "  685 1103  342  125  483  340  942  846  336 1036  891   31 1013  462\n",
      "  504  308 1170  363  737  108  507  526  544 1154  312  576  188   47\n",
      "   76  596  895  936  280  149  824  998  485  205 1057  353  546  421\n",
      "   10  612  554  847  762  665  361  174  868  705 1119  162 1126  806\n",
      "  900  185  459  679  160 1185  760 1122  263  368  789  929  583  772\n",
      "  602  231  333 1069  897  708  380 1180  619  477  827 1123  148  729\n",
      "  257  676  578  207  790 1152  309  620  143  556  658 1157 1124  430\n",
      " 1101 1190  548  969  786 1095 1143 1195  104  927  564  461  913 1081\n",
      "   83  882  146  717  608  394 1178  919   45  197  553  703  638  264\n",
      " 1037  111  588  317  986  379  454  592  136  765  338 1161  898  792\n",
      "  343 1012  316  541  506  440  220 1193  713  716   69  694  533  712\n",
      " 1045   74  833 1105  298  552  907  113 1179  745  518  808 1163  641\n",
      " 1021   95   35  684 1132  347  841  999  116  805  530  753  836  880\n",
      "    4 1120 1060 1194  473  306  901 1121  223  976  227 1114  508 1042\n",
      "  424 1182   81  422  821  452  668   62  928  453 1052  557  435  429\n",
      "  515  330  670  109  761  650 1083   40  680  961  354  230  527  781\n",
      "   64  311  607  921  848   19  402  357  360  105  738  835  289  392\n",
      "  791 1172 1104  222 1169  723 1148  623  863 1164  397 1108  313  287\n",
      "  593  202  701 1078 1177 1113  332  438  326  787  878  387   17 1084\n",
      "  163  281 1047  351  331 1010  985 1066   42  993  775  101  275  286\n",
      "  446  956  249  134  574  970  594  128  747 1070  572  103 1004  410\n",
      "  793  154  337  604  209   66 1128  881   56  711  324  778   78  540\n",
      "  355  493  359  290  224  307  855  813   41  883  381  788  902  657\n",
      "  931 1100    3 1016  215   98  955  899  434 1018  542]\n"
     ]
    }
   ],
   "source": [
    "shuffled_indices = []\n",
    "DATA_len = 0\n",
    "# 使用全部的data\n",
    "if not opt.arr_flag:\n",
    "    DATA_len = dataset.__len__()\n",
    "    shuffled_indices = np.random.permutation(DATA_len)\n",
    "\n",
    "# 使用指定的data\n",
    "if opt.arr_flag:\n",
    "    shuffled_indices = np.load(opt.arr_path)\n",
    "    DATA_len = len(shuffled_indices)\n",
    "    np.random.shuffle(shuffled_indices)\n",
    "\n",
    "train_idx = shuffled_indices[:int(opt.train_pct*DATA_len)]\n",
    "# train_idx = shuffled_indices\n",
    "tmp = int((opt.train_pct+opt.vali_pct)*DATA_len)\n",
    "val_idx = shuffled_indices[int(opt.train_pct*DATA_len):tmp]\n",
    "\n",
    "test_idx = shuffled_indices[tmp:]\n",
    "print(train_idx)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 根据这个乱序排列抽取dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# 保存idx\n",
    "np.save('shuffled_indices',shuffled_indices)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Dataloader and collating\n",
    "1. 主要是对label数据进行collate\n",
    "    - 按照batch中的最大target data长度进行padding，padding with 0\n",
    "2. 返回的结果多一个batch dim,比如下面的`5`\n",
    "    - After collating:\n",
    "        - `torch.Size([5, 3, 300]),torch.Size([5, 87, 2])`\n",
    "        - `87`是最长的targets data长度"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_pad_sequence():\n",
    "    seq1 = torch.tensor([[ 2., 0.04761905], [3., 0.14285714], [4., 0.04761905]])\n",
    "    seq2 = torch.tensor([[ 1., 0.04761905]])\n",
    "    seq3 = torch.tensor([[ 3., 0.14285714], [4., 0.04761905]])\n",
    "    ls = list((seq1,seq2,seq3))\n",
    "    ls_length = torch.tensor([3,1,2])\n",
    "    ans = pad_sequence(ls,batch_first=True)\n",
    "    print(ans)\n",
    "    # seq_pak = pack_padded_sequence(ans,ls_length,batch_first=True,enforce_sorted=False)\n",
    "    # seq_unpacked, lens_unpacked = pad_packed_sequence(seq_pak, batch_first=True)\n",
    "    # lens_unpacked"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.1 No rescale"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "def my_collate_fn(data):\n",
    "# 这里的data是一个list， list的元素是元组: (self.data, self.label)\n",
    "# collate_fn的作用是把[(data, label),(data, label)...]转化成([data, data...],[label,label...])\n",
    "# 假设self.data的一个data的shape为(channels, length), 每一个channel的length相等,\n",
    "# data[索引到index(batch)][索引到data或者label][索引到channel]\n",
    "\n",
    "    data_list = []\n",
    "    target_metric_list = []\n",
    "    target_loss_list = []\n",
    "    setting_list = []\n",
    "    metric_list = []\n",
    "\n",
    "    data_len = len(data)      # 读进来的data batch的大小\n",
    "\n",
    "    batch = 0\n",
    "\n",
    "    while batch < data_len:\n",
    "        # print(\"shape:\",data[batch][0].shape) #shape: (3, 300)\n",
    "\n",
    "        # 所有GT model\n",
    "        data_list.append(torch.tensor(data[batch][0]))\n",
    "\n",
    "        target_metric_list.append(torch.tensor(data[batch][1]))\n",
    "        target_loss_list.append(torch.tensor(data[batch][2]))\n",
    "        setting_list.append(torch.tensor(data[batch][3]))\n",
    "        metric_list.append(torch.tensor(data[batch][4]))\n",
    "        batch += 1\n",
    "\n",
    "    # Pad target data with zeros\n",
    "    target_metric_padded = pad_sequence(target_metric_list,batch_first=True)\n",
    "    target_loss_padded = pad_sequence(target_loss_list,batch_first=True)\n",
    "    target_metric_tensor = target_metric_padded.float()\n",
    "    target_loss_padded = target_loss_padded.float()\n",
    "\n",
    "    data_tensor = torch.stack(data_list).float()\n",
    "    setting_tensor = torch.stack(setting_list).float()\n",
    "    metric_tensor = torch.stack(metric_list).float()\n",
    "\n",
    "    return data_tensor, target_metric_tensor, target_loss_padded, setting_tensor, metric_tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.2 Rescale"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def my_collate_fn_rescale(data):\n",
    "# 这里的data是一个list， list的元素是元组: (self.data, self.label)\n",
    "# collate_fn的作用是把[(data, label),(data, label)...]转化成([data, data...],[label,label...])\n",
    "# 假设self.data的一个data的shape为(channels, length), 每一个channel的length相等,\n",
    "# data[索引到index(batch)][索引到data或者label][索引到channel]\n",
    "\n",
    "    data_list = []\n",
    "    target_list = []\n",
    "    data_len = len(data)      # 读进来的data batch的大小\n",
    "\n",
    "    batch = 0\n",
    "\n",
    "    while batch < data_len:\n",
    "        data_df = pd.DataFrame(data[batch][0])\n",
    "        data_GT = data_df.iloc[0:2,:]\n",
    "\n",
    "        # Get the max/min in each chanel and broadcast it by hand\n",
    "        data_max = np.tile(pd.DataFrame(np.max(data_GT,axis=1)), (1, data_GT.shape[1]))\n",
    "        data_min = np.tile(pd.DataFrame(np.min(data_GT,axis=1)), (1, data_GT.shape[1]))\n",
    "        # Rescale\n",
    "        data_rescaled = a + (data_GT-data_min)*(b-a) / (data_max - data_min)\n",
    "        data_df.iloc[0:2,:] = data_rescaled\n",
    "\n",
    "        data_list.append(torch.tensor(np.array(data_df,dtype=float)))\n",
    "        # print(\"After scale:\",data_df)\n",
    "        target_list.append(torch.tensor(data[batch][1]))\n",
    "        batch += 1\n",
    "\n",
    "    # Pad target data with zeros\n",
    "    target_padded = pad_sequence(target_list,batch_first=True)\n",
    "\n",
    "    data_tensor = torch.stack(data_list).float()\n",
    "    target_tensor = target_padded.float()\n",
    "\n",
    "    return data_tensor, target_tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.3 log1p"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "outputs": [],
   "source": [
    "def my_collate_fn_log1p(data):\n",
    "# 这里的data是一个list， list的元素是元组: (self.data, self.label)\n",
    "# collate_fn的作用是把[(data, label),(data, label)...]转化成([data, data...],[label,label...])\n",
    "# 假设self.data的一个data的shape为(channels, length), 每一个channel的length相等,\n",
    "# data[索引到index(batch)][索引到data或者label][索引到channel]\n",
    "\n",
    "    data_list = []\n",
    "    target_list = []\n",
    "    data_len = len(data)      # 读进来的data batch的大小\n",
    "\n",
    "    batch = 0\n",
    "\n",
    "    while batch < data_len:\n",
    "        data_df = pd.DataFrame(data[batch][0])\n",
    "        data_GT = data_df.iloc[0:2,:]\n",
    "\n",
    "        data_log1p = -np.log(data_GT+opt.MIN_LOSS)\n",
    "        data_df.iloc[0:2,:] = data_log1p\n",
    "\n",
    "        data_list.append(torch.tensor(np.array(data_df,dtype=float)))\n",
    "        target_list.append(torch.tensor(data[batch][1]))\n",
    "        batch += 1\n",
    "\n",
    "    # Pad target data with zeros\n",
    "    target_padded = pad_sequence(target_list,batch_first=True)\n",
    "\n",
    "    data_tensor = torch.stack(data_list).float()\n",
    "    target_tensor = target_padded.float()\n",
    "\n",
    "    return data_tensor, target_tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.4 DataLoader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = dataset,batch_size = opt.batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(train_idx), collate_fn = my_collate_fn)\n",
    "\n",
    "val_loader = DataLoader(dataset = dataset,batch_size = opt.batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(val_idx),collate_fn = my_collate_fn)\n",
    "\n",
    "# 注意test_loader的batch size\n",
    "test_loader = DataLoader(dataset = dataset,batch_size = opt.batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(test_idx),collate_fn = my_collate_fn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. The Net and Init\n",
    "1. BatchNorm1d: The mean and std are calculated per-dimension over the mini-batches\n",
    "2."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### BatchNorm2d测试\n",
    "def test_BN():\n",
    "    m = nn.BatchNorm2d(3, affine=False)  # affine: With Learnable Parameters or not\n",
    "    print('m:', m)\n",
    "    # The mean and std are calculated per-dimension over the mini-batches\n",
    "    input = torch.tensor([\n",
    "        [[1.,2.,3.,4.],[1.,2.,3.,4.],[-1.,-2.,-3.,-4.]],\n",
    "        [[0.,0.,0.,0.],[0.,0.,0.,0.],[0.,0.,0.,0.]]\n",
    "    ], requires_grad=True)\n",
    "\n",
    "    print('input:', input.shape)\n",
    "    input = input.unsqueeze(dim=2)\n",
    "    print('input:', input.shape)\n",
    "    output = m(input) # 归一化\n",
    "    print('output:', output.shape)\n",
    "    print('output:', output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### 复现batchNorm2d在input shape为3维的情况\n",
    "input = torch.tensor([[[1.,2.,3.,4.]],[[0.,0.,0.,0.]]])\n",
    "# print(input.shape)\n",
    "# torch.mean(input),torch.var(input,unbiased = False)\n",
    "(input-torch.mean(input))/ torch.sqrt(torch.var(input,unbiased = False))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 Init, Printer, Hook\n",
    "- 设置网络初始权重: 不太work"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [],
   "source": [
    "\n",
    "class model_param_init(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        assert isinstance(model, nn.Module), 'model not a class nn.Module'\n",
    "        self.net = model\n",
    "        self.initParam()\n",
    "\n",
    "    def forward(self):\n",
    "        for param in self.net.parameters():\n",
    "            # nn.init.zeros_(param)\n",
    "            # nn.init.ones_(param)\n",
    "            # nn.init.normal_(param, mean=0, std=1)\n",
    "            # nn.init.uniform_(param, a=0, b=1)\n",
    "            # nn.init.constant_(param, val=1)   # 将所有权重初始化为1\n",
    "            # nn.init.eye_(param)  # 只能将二维的tensor初始化为单位矩阵\n",
    "            # nn.init.xavier_uniform_(param, gain=1)  # Glorot初始化  得到的张量是从-a——a中采用的\n",
    "            # print(param)  x.unsqueeze(0)\n",
    "            nn.init.xavier_normal_(param.unsqueeze(0), gain=1)   # 得到的张量是从0-std采样的\n",
    "            # nn.init.kaiming_normal_(param, a=0, mode='fan_in', nonlinearity='relu') # he初始化方法\n",
    "            # nn.init.kaiming_uniform_(param)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- print网络每层结构"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "class PrintLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if(DEBUG):\n",
    "            print(\"This layer: \")\n",
    "            print(x)      #print(x.shape)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- hook_backward_fn: 输入端的grad，输出端的grad，这里称呼的**输入与输出是站在前向传播的角度的**。如果模块有多个输入与输出的话， 其`grad_input`和`grad_output`可以是tuple类型。\n",
    "- 与forward不同的是，backward传播的时候，**不仅反向传递input和output的grad，还会传递模块Parameter的grad**：\n",
    "    - 比如fc模块，其`grad_input`是一个三元组的tuple，（对bias的梯度，对输入的梯度，对w的梯度）；\n",
    "    - conv模块`grad_input`也是一个三元组tuple，为（对输入的梯度，对w的梯度，对bias的梯度"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# hook functions have to take these 3 input\n",
    "def hook_forward_fn(module, input, output):\n",
    "    print(\"It's forward: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"input: {input}\")\n",
    "    print(f\"output: {output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn(module, grad_input, grad_output):\n",
    "    print(\"It's backward: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn_pi(module, grad_input, grad_output):\n",
    "    print(\"It's backward in pi: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn_sigma(module, grad_input, grad_output):\n",
    "    print(\"It's backward in sigma: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn_mu(module, grad_input, grad_output):\n",
    "    print(\"It's backward in mu: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Conv结构-1\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "outputs": [],
   "source": [
    "# Not Sequential\n",
    "# 573, 588, 693\n",
    "class MLP_1_1(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.kernel_size = (3,10)\n",
    "        # (3,15),19\n",
    "        self.stride = (3,5)\n",
    "        self.ln_in = 59\n",
    "\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=3,affine=True)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=self.kernel_size, stride=self.stride, padding=0,bias=True)\n",
    "        self.conv2 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=self.kernel_size, stride=self.stride, padding=0,bias=True)\n",
    "        self.conv3 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=self.kernel_size, stride=self.stride, padding=0,bias=True)\n",
    "\n",
    "        self.BN_aff1 = nn.BatchNorm1d(num_features=self.ln_in,affine=True)\n",
    "        self.BN_aff2 = nn.BatchNorm1d(num_features=self.ln_in,affine=True)\n",
    "        self.BN_aff3 = nn.BatchNorm1d(num_features=self.ln_in,affine=True)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(self.ln_in, n_gaussians),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(self.ln_in, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(self.ln_in, n_gaussians)\n",
    "        #\n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, nn.Conv2d):\n",
    "        #         # nn.init.xavier_uniform_(m.weight,gain=1)\n",
    "        #         nn.init.uniform_(m.weight,a=-0.5,b=0.5)\n",
    "        #     elif isinstance(m, nn.Linear):\n",
    "        #         nn.init.uniform_(m.weight,a=-1,b=1)\n",
    "        #         # m.weight.data.normal_(0, 0.02)\n",
    "        #         # nn.init.xavier_normal_(m.weight,gain=1)\n",
    "        #         # nn.init.xavier_uniform_(m.weight,gain=1)\n",
    "        #         # nn.init.orthogonal_(m.weight)\n",
    "        #         # nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.BN1(x)\n",
    "        # 加一个channel维度\n",
    "        x = torch.unsqueeze(x,dim=1)                    # torch.Size([B, 1, 3, 300])\n",
    "\n",
    "        x1 = F.softplus(self.conv1(x))                   # [B, 1, 1, ln_in]\n",
    "        x2 = F.softplus(self.conv2(x))                   # [B, 1, 1, ln_in]\n",
    "        x3 = F.softplus(self.conv3(x))                   # [B, 1, 1, ln_in]\n",
    "\n",
    "        x1_1 = torch.squeeze(x1)\n",
    "        x2_1 = torch.squeeze(x2)\n",
    "        # x3_1 = torch.squeeze(x2)\n",
    "        x3_1 = torch.squeeze(x3)            # 救命啊怎会如此。。，相当于之前mu和sigma是同一个conv搞出来的\n",
    "\n",
    "        x1_2 = self.BN_aff1(x1_1)\n",
    "        x2_2 = self.BN_aff2(x2_1)\n",
    "        x3_2 = self.BN_aff3(x3_1)\n",
    "\n",
    "        x1_3 = F.softplus(x1_2)\n",
    "        x2_3 = F.softplus(x2_2)\n",
    "        x3_3 = F.softplus(x3_2)\n",
    "\n",
    "        pi = self.z_pi(x1_3)\n",
    "        mu = self.z_mu(x2_3)\n",
    "\n",
    "        sigma = torch.exp(self.z_sigma(x3_3))\n",
    "        sigma = torch.clamp(sigma,1e-4)\n",
    "\n",
    "        # return x1_3\n",
    "        return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "outputs": [],
   "source": [
    "# mlp = MLP_1_1(opt.N_gaussians)\n",
    "# mlp = mlp.to(device=device)\n",
    "# summary(mlp, (3,300))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3 Conv结构-2\n",
    "1. 换了一下conv的结构"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# 参数量747\n",
    "class MLP_1_2(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=3,affine=False)\n",
    "        self.BN2 = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "        self.BN3 = nn.BatchNorm2d(num_features=6,affine=True)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=(1,6), stride=(1,3), padding=0,bias=False)\n",
    "        self.ac_func = nn.Softplus()\n",
    "        # self.ac_func = nn.ReLU()\n",
    "\n",
    "        # 3, 3, 99\n",
    "        self.conv2 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=(3,6), stride=6, padding=0,bias=False)\n",
    "        # 6,1,33\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=6, out_channels=6, kernel_size=(1,3), stride=3, padding=0,bias=False)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        #self.linear1 = nn.Linear(48, 18)\n",
    "\n",
    "        #无clip\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(30, n_gaussians),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        # 有clip\n",
    "        # self.z_pi = nn.Linear(30, n_gaussians)\n",
    "        self.z_mu = nn.Linear(30, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(30, n_gaussians)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.BN1(x)\n",
    "        # 加一个height维度\n",
    "\n",
    "        x = torch.unsqueeze(x,dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.BN2(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.BN3(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        # x = torch.unsqueeze(x,dim=2)\n",
    "        x = self.conv3(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        # x = self.linear1(x)\n",
    "        # x = self.ac_func(self.drop(x))\n",
    "        mu = self.z_mu(x)\n",
    "        pi = self.z_pi(x)\n",
    "\n",
    "        sigma = torch.exp(self.z_sigma(x))\n",
    "        sigma = torch.clamp(sigma,1e-4)\n",
    "\n",
    "        # return x\n",
    "        return pi,mu,sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4 MLP结构\n",
    "### 3.4.1 4层"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# 参数量304,000+\n",
    "# 4层\n",
    "class MLP_1_3(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=3,affine=True)\n",
    "        self.BN2 = nn.BatchNorm1d(num_features=300,affine=True)\n",
    "        self.BN3 = nn.BatchNorm1d(num_features=100,affine=True)\n",
    "        self.BN4 = nn.BatchNorm1d(num_features=30,affine=True)\n",
    "        self.BN5 = nn.BatchNorm1d(num_features=12,affine=True)\n",
    "        self.BN6 = nn.BatchNorm1d(num_features=12,affine=True)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "        self.linear1 = nn.Linear(900, 300)\n",
    "        self.linear2 = nn.Linear(300, 100)\n",
    "        self.linear3 = nn.Linear(100, 30)\n",
    "        self.linear4 = nn.Linear(30, 12)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "        self.ac_func2 = nn.SELU()\n",
    "        self.ac_func3 = nn.LeakyReLU()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(12, n_gaussians),  # 30个params要learn\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(12, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(12, n_gaussians)\n",
    "\n",
    "        # SELU Init\n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, (nn.Linear)):\n",
    "        #         nn.init.kaiming_normal_(m.weight, nonlinearity='linear')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.BN1(x)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.BN2(x)\n",
    "        x = self.ac_func3(self.drop(x))\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = self.BN3(x)\n",
    "        x = self.ac_func3(self.drop(x))\n",
    "\n",
    "        x = self.linear3(x)\n",
    "        x = self.BN4(x)\n",
    "        x = self.ac_func3(self.drop(x))\n",
    "\n",
    "        x = self.linear4(x)\n",
    "        x = self.BN5(x)\n",
    "        x = self.ac_func3(self.drop(x))\n",
    "\n",
    "        x = self.BN6(x)\n",
    "        pi = self.z_pi(x)\n",
    "        # pi = self.z_pi(torch.clamp(x,1e-3))\n",
    "        mu = self.z_mu(x)\n",
    "        # sigma = torch.exp(self.z_sigma(x))\n",
    "        sigma = F.elu(self.z_sigma(x)) + 1\n",
    "        sigma = torch.clamp(sigma,1e-4)\n",
    "\n",
    "        # return x\n",
    "        return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.2 2层"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "# 参数量91,000+\n",
    "# 2层MLP+MDN\n",
    "class MLP_1_4(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=3,affine=True)\n",
    "        self.BN2 = nn.BatchNorm1d(num_features=100,affine=True)\n",
    "        self.BN3 = nn.BatchNorm1d(num_features=12,affine=True)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "        self.linear1 = nn.Linear(900, 100)\n",
    "        self.linear2 = nn.Linear(100, 12)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "        self.ac_func2 = nn.SELU()\n",
    "        self.ac_func3 = nn.LeakyReLU()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(12, n_gaussians),  # 30个params要learn\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(12, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(12, n_gaussians)\n",
    "\n",
    "        # SELU Init\n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, (nn.Linear)):\n",
    "        #         nn.init.kaiming_normal_(m.weight, nonlinearity='linear')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"x.shape: \",x.shape)  # torch.Size([40, 3, 300])\n",
    "        x = self.BN1(x)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        # x = self.BN2(x)\n",
    "        x = self.ac_func3(self.drop(x))\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        # x = self.BN3(x)\n",
    "        x = self.ac_func3(self.drop(x))\n",
    "\n",
    "        pi = self.z_pi(x)\n",
    "        # pi = self.z_pi(torch.clamp(x,1e-3))\n",
    "        mu = self.z_mu(x)\n",
    "\n",
    "        sigma = torch.exp(self.z_sigma(x))\n",
    "        # sigma = F.elu(self.z_sigma(x)) + 1\n",
    "        sigma = torch.clamp(sigma,1e-4)\n",
    "\n",
    "        return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.4 1层+avgpool"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "outputs": [],
   "source": [
    "# 参数量2823\n",
    "# 1层MLP+MDN\n",
    "class MLP_1_5(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=3,affine=True)\n",
    "        self.BN2 = nn.BatchNorm1d(num_features=9,affine=True)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=(1,3))  # kernel_size=(1,Channel)\n",
    "        self.linear1 = nn.Linear(300, 9)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(9, n_gaussians),  # 30个params要learn\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(9, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(9, n_gaussians)\n",
    "\n",
    "        # SELU Init\n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, (nn.Linear)):\n",
    "        #         nn.init.kaiming_normal_(m.weight, nonlinearity='linear')\n",
    "\n",
    "        # 初始化权重\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                # m.weight.data.normal_(0, 0.02)\n",
    "                # nn.init.xavier_normal_(m.weight,gain=1)\n",
    "                nn.init.orthogonal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "        # nn.init.xavier_normal_(self.z_mu.weight,gain=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.BN1(x)\n",
    "        # print(\"before transpose: \",x.shape)\n",
    "        x = torch.transpose(x, 1, 2)        # [B,N,C]\n",
    "        # print(\"before avgpool: \",x.shape)\n",
    "        x = self.avgpool(x)\n",
    "        # print(\"after avgpool: \",x.shape)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.BN2(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        pi = self.z_pi(x)\n",
    "        # pi = self.z_pi(torch.clamp(x,1e-3))\n",
    "        mu = self.z_mu(x)\n",
    "\n",
    "        sigma = torch.exp(self.z_sigma(x))\n",
    "        # sigma = F.elu(self.z_sigma(x)) + 1\n",
    "        sigma = torch.clamp(sigma,1e-4)\n",
    "\n",
    "        return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "outputs": [],
   "source": [
    "def test_pool():\n",
    "    # torch.Size([40, 3, 300])\n",
    "    a = torch.tensor([[[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5]]])\n",
    "    # print(a.shape)\n",
    "    a = torch.transpose(a, 1, 2)\n",
    "    print(a)\n",
    "    ans = F.avg_pool2d(a,(1,3))\n",
    "    print(ans.shape)    # [B,N,C]\n",
    "# test_pool()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.5 1层+无avgpool"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "outputs": [],
   "source": [
    "# 参数量8233\n",
    "# 1层MLP+MDN\n",
    "class MLP_1_6(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=3,affine=True)\n",
    "        self.BN2 = nn.BatchNorm1d(num_features=9,affine=True)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "        self.linear1 = nn.Linear(900, 9)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(9, n_gaussians),  # 30个params要learn\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(9, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(9, n_gaussians)\n",
    "\n",
    "        # SELU Init\n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, (nn.Linear)):\n",
    "        #         nn.init.kaiming_normal_(m.weight, nonlinearity='linear')\n",
    "\n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, nn.BatchNorm1d):\n",
    "        #         m.weight.data.fill_(1)\n",
    "        #         m.bias.data.zero_()\n",
    "        #     elif isinstance(m, nn.Linear):\n",
    "        #         # m.weight.data.normal_(0, 0.02)\n",
    "        #         # nn.init.xavier_normal_(m.weight,gain=1)\n",
    "        #         nn.init.orthogonal_(m.weight)\n",
    "        #         nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.BN1(x)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.BN2(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        pi = self.z_pi(x)\n",
    "        # pi = self.z_pi(torch.clamp(x,1e-3))\n",
    "        mu = self.z_mu(x)\n",
    "\n",
    "        sigma = torch.exp(self.z_sigma(x))\n",
    "        # sigma = F.elu(self.z_sigma(x)) + 1\n",
    "        sigma = torch.clamp(sigma,1e-4)\n",
    "\n",
    "        return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.5 (3,100)的结构\n",
    "1. 适配(3,100)的training data长度而不是(3,300)\n",
    "### 3.5.1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # 参数量：228\n",
    "# class MLP_2_2(nn.Module):\n",
    "#     # code->generate->override methods\n",
    "#     def __init__(self, n_gaussians) -> None:\n",
    "#         super().__init__()\n",
    "#         self.BN = nn.BatchNorm2d(num_features=3,affine=False)\n",
    "#         self.BN_aff = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "#         self.drop = nn.Dropout(0.2)\n",
    "#\n",
    "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#         # self.ac_func = nn.Softplus()\n",
    "#         self.ac_func = nn.LeakyReLU()\n",
    "#         self.ac_func2 = nn.ReLU()\n",
    "#\n",
    "#         # self.ac_func = nn.Sigmoid()\n",
    "#         # self.ac_func = nn.Tanh()\n",
    "#\n",
    "#         self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,2), stride=(1,2), padding=0,bias=False)\n",
    "#\n",
    "#         self.conv3 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#\n",
    "#         self.flatten = nn.Flatten()\n",
    "#\n",
    "#         self.z_pi = nn.Sequential(\n",
    "#             nn.Linear(15, n_gaussians),\n",
    "#             nn.Softmax(dim=1)\n",
    "#         )\n",
    "#\n",
    "#         self.z_mu = nn.Linear(15, n_gaussians)\n",
    "#         self.z_sigma = nn.Linear(15, n_gaussians)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         # 加一个height维度\n",
    "#         x = torch.unsqueeze(x,dim=2)\n",
    "#         x = self.BN(x)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         x = self.BN_aff(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         x = self.BN_aff(x)\n",
    "#         x = self.conv3(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#         #\n",
    "#         x = self.flatten(x)\n",
    "#\n",
    "#         pi = self.z_pi(torch.clamp(x,1e-3))\n",
    "#         # pi = self.z_pi(x)\n",
    "#         mu = self.z_mu(x)\n",
    "#         sigma = torch.exp(self.z_sigma(x))\n",
    "#\n",
    "#         # return x\n",
    "#         return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- MLP结构+长度=100"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.5.2 MLP-2层"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "# 参数量 18699\n",
    "class MLP_2_3(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=3,affine=True)\n",
    "        self.BN2 = nn.BatchNorm1d(num_features=60,affine=True)\n",
    "        self.BN3 = nn.BatchNorm1d(num_features=9,affine=True)\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "\n",
    "        self.linear1 = nn.Linear(300, 60)\n",
    "        self.linear2 = nn.Linear(60, 9)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(9, n_gaussians),  # 30个params要learn\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(9, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(9, n_gaussians)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.BN1(x)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.BN2(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = self.BN3(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        pi = self.z_pi(x)\n",
    "        # pi = self.z_pi(torch.clamp(x,1e-3))\n",
    "        mu = self.z_mu(x)\n",
    "        sigma = torch.exp(self.z_sigma(x))\n",
    "        sigma = torch.clamp(sigma,1e-4)\n",
    "\n",
    "        # return x\n",
    "        return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.5.3 MLP-1层"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [
    "# 参数量2823\n",
    "class MLP_2_4(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=3,affine=True)\n",
    "        self.BN2 = nn.BatchNorm1d(num_features=9,affine=True)\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "\n",
    "        self.linear1 = nn.Linear(300, 9)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(9, n_gaussians),  # 30个params要learn\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(9, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(9, n_gaussians)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.BN1(x)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.BN2(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        pi = self.z_pi(x)\n",
    "        # pi = self.z_pi(torch.clamp(x,1e-3))\n",
    "        mu = self.z_mu(x)\n",
    "        sigma = torch.exp(self.z_sigma(x))\n",
    "        sigma = torch.clamp(sigma,1e-4)\n",
    "\n",
    "        # return x\n",
    "        return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.6 (3,60)的结构\n",
    "1. 适配(3,60)的training data长度而不是(3,300)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # 参数量：174\n",
    "# class MLP_3_1(nn.Module):\n",
    "#     # code->generate->override methods\n",
    "#     def __init__(self, n_gaussians) -> None:\n",
    "#         super().__init__()\n",
    "#         self.BN = nn.BatchNorm2d(num_features=3,affine=False)\n",
    "#         self.BN_aff = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "#         self.drop = nn.Dropout(0.2)\n",
    "#\n",
    "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#         # self.ac_func = nn.Softplus()\n",
    "#         self.ac_func = nn.LeakyReLU()\n",
    "#         self.ac_func2 = nn.ReLU()\n",
    "#\n",
    "#         # self.ac_func = nn.Sigmoid()\n",
    "#         # self.ac_func = nn.Tanh()\n",
    "#\n",
    "#         self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,2), stride=(1,2), padding=0,bias=False)\n",
    "#\n",
    "#         self.conv3 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.linear = nn.Linear(24, 9)\n",
    "#\n",
    "#         self.z_pi = nn.Sequential(\n",
    "#             nn.Linear(9, n_gaussians),\n",
    "#             nn.Softmax(dim=1)\n",
    "#         )\n",
    "#\n",
    "#         self.z_mu = nn.Linear(9, n_gaussians)\n",
    "#         self.z_sigma = nn.Linear(9, n_gaussians)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         # 加一个height维度\n",
    "#         x = torch.unsqueeze(x,dim=2)\n",
    "#         x = self.BN(x)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.ac_func(self.drop(x))      # 3, 1, 20\n",
    "#\n",
    "#         x = self.BN_aff(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.ac_func(self.drop(x))      # 3, 1, 10\n",
    "#\n",
    "#         x = self.BN_aff(x)\n",
    "#         x = self.conv3(x)\n",
    "#         x = self.ac_func(self.drop(x))      # 3, 1, 8\n",
    "#\n",
    "#         x = self.flatten(x)\n",
    "#         # print(x.shape)\n",
    "#         # x = self.linear(x)\n",
    "#         #\n",
    "#         pi = self.z_pi(x)\n",
    "#         mu = self.z_mu(x)\n",
    "#         sigma = torch.exp(self.z_sigma(x))\n",
    "#\n",
    "#         # return x\n",
    "#         return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- MLP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # 参数量：5799\n",
    "# class MLP_3_2(nn.Module):\n",
    "#     # code->generate->override methods\n",
    "#     def __init__(self, n_gaussians) -> None:\n",
    "#         super().__init__()\n",
    "#         self.BN = nn.BatchNorm2d(num_features=3,affine=False)\n",
    "#         self.BN_aff = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "#         self.drop = nn.Dropout(0.2)\n",
    "#\n",
    "#         self.linear1 = nn.Linear(180,30)\n",
    "#         self.linear2 = nn.Linear(30,9)\n",
    "#         # self.ac_func = nn.Softplus()\n",
    "#         self.ac_func = nn.LeakyReLU()\n",
    "#         self.ac_func2 = nn.ReLU()\n",
    "#\n",
    "#         # self.ac_func = nn.Sigmoid()\n",
    "#         # self.ac_func = nn.Tanh()\n",
    "#\n",
    "#         self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,2), stride=(1,2), padding=0,bias=False)\n",
    "#\n",
    "#         self.conv3 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.linear = nn.Linear(24, 9)\n",
    "#\n",
    "#         self.z_pi = nn.Sequential(\n",
    "#             nn.Linear(9, n_gaussians),\n",
    "#             nn.Softmax(dim=1)\n",
    "#         )\n",
    "#\n",
    "#         self.z_mu = nn.Linear(9, n_gaussians)\n",
    "#         self.z_sigma = nn.Linear(9, n_gaussians)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         # 加一个height维度\n",
    "#         x = torch.unsqueeze(x,dim=2)\n",
    "#         x = self.BN(x)\n",
    "#         x = self.flatten(x)\n",
    "#\n",
    "#         x = self.linear1(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         x = self.linear2(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         pi = self.z_pi(x)\n",
    "#         mu = self.z_mu(x)\n",
    "#         sigma = torch.exp(self.z_sigma(x))\n",
    "#\n",
    "#         # return x\n",
    "#         return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. The Loss\n",
    "\n",
    "## 4.0 The metric\n",
    "1. 以NLL作为metric\n",
    "2. 理论模型的metric已经提前算好，读进来.\n",
    "3. 注意这里的metric比较的是target还是target_5:\n",
    "    - 建议使用target data而不是target_5，因为后者是为了方便NN的learning，前者才是真正的比较NLL\n",
    "    - 使用前者也要用cdf进行比较吧。。比如GT的4实际上是我们这里的[3.5,4.5]？是高斯分布在[a-0.5,a+0.5]上的cdf作为a的“单点分布”"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# from loss import cal_metric"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1 NLL loss\n",
    "### 4.1.1 original version"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# from loss import loss_fn_v2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1.2 version 3 [LogSumExp]\n",
    "1. $ exponent_i = \\log_{}{\\alpha_i } -1/2*\\log_{}{2\\pi } -\\log_{}{\\sigma_i }- \\frac{(x-\\mu_i )^2}{2\\sigma_i^2} $\n",
    "2. $ NLL =-\\sum_{j}^{N_S}LogSumExp= -\\sum_{j}^{N_S} \\log\\sum_{i}^{N_G} \\exp \\{exponent_i\\}$\n",
    "3. 不稳定！lr不要设超过1e-2；而且loss会变成负值。。最开始x_max是负值，后面会变成正值 which means prob值开始>1了"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "from loss import log_sum_exp\n",
    "from loss import loss_fn_v3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 cdf loss\n",
    "1. 注意把targets data的路径改一下"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# cdf loss\n",
    "from loss import loss_fn_cdf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.3 交叉熵/ KL\n",
    "1. 自己写的，没有用torch的函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from loss import loss_fn_CE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def loss_test():\n",
    "    loss_2 = torch.tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "    loss_3 = loss_2[torch.nonzero(loss_2)].view(-1,1)         # 去掉所有的log(0)\n",
    "    print(\"loss_3:\",loss_3.shape)\n",
    "\n",
    "    MIN_LOSS = torch.min(loss_2) if torch.min(loss_2)>0 else 1e-20\n",
    "\n",
    "    print(\"MIN_LOSS：\",MIN_LOSS)\n",
    "    # 用MIN_loss替代loss_2为0的部分\n",
    "    loss_2[loss_2==0] = MIN_LOSS\n",
    "\n",
    "\n",
    "    loss_padded = [MIN_LOSS]* (loss_2.shape[0]-loss_3.shape[0])\n",
    "    loss_padded_ts = torch.tensor(loss_padded) # ,device=device\n",
    "    loss_3 = torch.cat((loss_3[:,0],loss_padded_ts))\n",
    "    loss_3.shape, torch.log(loss_3)\n",
    "    print(\"data: \",loss_3.data)\n",
    "    print(\"data: \",type(loss_3))\n",
    "# loss_test()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.4 W-distance\n",
    "1. 拿到target data的两列，按照第一列去predict\n",
    "2. Ref: https://www.kernel-operations.io/geomloss/api/pytorch-api.html"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [],
   "source": [
    "from loss import loss_fn_WD"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5 Plot\n",
    "1. draw:\n",
    "    - loss: training data的loss和test data的loss趋势\n",
    "    - MLP的网络结构（.png）\n",
    "    - target distrb.和pred. distrb.\n",
    "    - 所有target data的分布图\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "viz = Visdom(env=\"001\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.1 draw 所有的target data\n",
    "1. 实际上在`i < 50`里决定画前50 or what target data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def draw_all_target_data():\n",
    "    # target data\n",
    "    # target_path = r\"../data/targets\"\n",
    "    target_path = r\"../data/targets_5\"\n",
    "    # target_path里有全部的target data地址\n",
    "    target_all_path = os.listdir(target_path)\n",
    "    len_target = len(target_all_path)\n",
    "    print(f\"一共有 *{len_target}* 组 target data\")\n",
    "\n",
    "    # Init\n",
    "    # viz_env_str = \"target_bar\"\n",
    "    # viz_env_str = \"target_line\"\n",
    "    # viz_env_str = \"target_hist\"\n",
    "    viz_env_str = \"target_scatter\"\n",
    "    viz = Visdom(env = viz_env_str)\n",
    "\n",
    "    for i in range(len_target):\n",
    "        target_path_i_path = os.path.join(target_path,target_all_path[i])\n",
    "        target_df = pd.read_csv(target_path_i_path,encoding=\"utf-8\")\n",
    "\n",
    "        # Init\n",
    "        win_str = str(target_all_path[i])\n",
    "        title_str = \"Target Distrb. of \"+win_str\n",
    "\n",
    "        if(i < 50):\n",
    "            # Hist plot which is not that great\n",
    "            if viz_env_str == \"target_hist\" :\n",
    "                viz.histogram(X = np.array(target_df.N), env=viz_env_str, win=win_str,\n",
    "                            opts= dict(title=title_str,numbins = 50))\n",
    "\n",
    "            # Line plot\n",
    "            if viz_env_str == \"target_line\" :\n",
    "                target_df.drop_duplicates(inplace=True)\n",
    "                viz.line(X = np.array(target_df.N),Y= np.array(target_df.P), env=viz_env_str, win=win_str,opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "            # Bar plot. 这个看起来是最准确的一个\n",
    "            if viz_env_str == \"target_bar\" :\n",
    "                target_df.drop_duplicates(inplace=True)\n",
    "                arr_str = [str(i) for i in np.array(target_df.N)]\n",
    "                viz.bar(X=np.array(target_df.P), env=viz_env_str, win=win_str,\n",
    "                        opts=dict(title=title_str,rownames=arr_str,stacked = False))\n",
    "\n",
    "            # Scatter plot\n",
    "            if viz_env_str == \"target_scatter\" :\n",
    "                target_df.drop_duplicates(inplace=True)\n",
    "                viz.scatter(X=np.array(target_df),env=viz_env_str, win=win_str,opts=dict(title=title_str,markersize = 3))\n",
    "    print(\"Done\")\n",
    "\n",
    "# draw_all_target_data()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 画target data的cdf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def draw_all_target_data_cdf():\n",
    "    # target data\n",
    "    # target_path = r\"../data/targets\"\n",
    "    target_path = r\"../data/targets_5_cdf\"\n",
    "    # target_path里有全部的target data地址\n",
    "    target_all_path = os.listdir(target_path)\n",
    "    len_target = len(target_all_path)\n",
    "    print(f\"一共有 *{len_target}* 组 target data\")\n",
    "\n",
    "    # Init，只需要改这个str就ok\n",
    "    # viz_env_str = \"target_barCDF\"\n",
    "    # viz_env_str = \"target_lineCDF\"\n",
    "    # viz_env_str = \"target_histCDF\"\n",
    "    viz_env_str = \"target_scatterCDF\"\n",
    "    viz = Visdom(env = viz_env_str)\n",
    "\n",
    "    for i in range(len_target):\n",
    "        target_path_i_path = os.path.join(target_path,target_all_path[i])\n",
    "        target_df = pd.read_csv(target_path_i_path,encoding=\"utf-8\")\n",
    "\n",
    "        # Init\n",
    "        win_str = str(target_all_path[i])\n",
    "        title_str = \"Target Distrb. of \"+win_str\n",
    "\n",
    "        if(i < 50):\n",
    "            # Hist plot which is not that great\n",
    "            if viz_env_str == \"target_histCDF\" :\n",
    "                viz.histogram(X = np.array(target_df.N), env=viz_env_str, win=win_str,\n",
    "                            opts= dict(title=title_str,numbins = 50))\n",
    "\n",
    "            # Line plot\n",
    "            if viz_env_str == \"target_lineCDF\" :\n",
    "                target_df.drop_duplicates(inplace=True)\n",
    "                viz.line(X = np.array(target_df.N),Y= np.array(target_df.P), env=viz_env_str, win=win_str,opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "            # Bar plot. 这个看起来是最准确的一个\n",
    "            if viz_env_str == \"target_barCDF\" :\n",
    "                target_df.drop_duplicates(inplace=True)\n",
    "                arr_str = [str(i) for i in np.array(target_df.N)]\n",
    "                viz.bar(X=np.array(target_df.P), env=viz_env_str, win=win_str,\n",
    "                        opts=dict(title=title_str,rownames=arr_str,stacked = False))\n",
    "\n",
    "            # Scatter plot\n",
    "            if viz_env_str == \"target_scatterCDF\" :\n",
    "                target_df.drop_duplicates(inplace=True)\n",
    "                viz.scatter(X=np.array(target_df),env=viz_env_str, win=win_str,opts=dict(title=title_str,markersize = 3))\n",
    "    print(\"Done\")\n",
    "\n",
    "# draw_all_target_data_cdf()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 也是画target data，但是是平滑处理\n",
    "- `savgol_filter`参数解析:\n",
    "    - y：代表曲线点坐标（x,y）中的y\n",
    "    - window_length：窗口长度，该值需为正奇整数。值越小，曲线越贴近真实曲线；值越大，平滑效果越厉害\n",
    "    - polyorder: 对窗口内的数据点进行k阶多项式拟合，k的值需要小于window_length。值越大，曲线越贴近真实曲线；值越小，平滑效果越厉害\n",
    "    - mode：确定了要应用滤波器的填充信号的扩展类型。（This determines the type of extension to use for the padded signal to which the filter is applied. ）\n",
    "- **实际上没用到这个函数**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def draw_smooth_target_data():\n",
    "    # target data\n",
    "    target_path = r\"../data/targets\"\n",
    "    # target_path里有全部的target data地址\n",
    "    target_all_path = os.listdir(target_path)\n",
    "    len_target = len(target_all_path)\n",
    "    print(f\"一共有 *{len_target}* 组 target data\")\n",
    "\n",
    "    # Init\n",
    "    viz_env_str = \"target_bar\"\n",
    "    # viz_env_str = \"target_distrb\"\n",
    "    # viz_env_str = \"target_hist\"\n",
    "    # viz_env_str = \"target_scatter\"\n",
    "    viz = Visdom(env = viz_env_str)\n",
    "\n",
    "    for i in range(len_target):\n",
    "        target_path_i_path = os.path.join(target_path,target_all_path[i])\n",
    "        target_df = pd.read_csv(target_path_i_path,encoding=\"utf-8\")\n",
    "\n",
    "        # Init\n",
    "        win_str = str(target_all_path[i])\n",
    "        title_str = \"Target Distrb. of \"+win_str\n",
    "\n",
    "        if(i < 100):\n",
    "            # Smoothen the data\n",
    "            y_smooth = scipy.signal.savgol_filter(target_df.P,window_length=53,polyorder=3)\n",
    "            # y_smooth = scipy.signal.savgol_filter(target_df.P, 99, 1, mode= 'nearest')\n",
    "            target_df.P = y_smooth\n",
    "\n",
    "            # Hist plot which is not that great\n",
    "            # viz.histogram(X = np.array(target_df.N), env=viz_env_str, win=win_str,\n",
    "            #             opts= dict(title=title_str,numbins = 50))\n",
    "\n",
    "            # Line plot\n",
    "            # target_df.drop_duplicates(inplace=True)\n",
    "            # viz.line(X = np.array(target_df.N),Y= np.array(target_df.P), env=viz_env_str, win=win_str,opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "            # Bar plot. 这个看起来是最准确的一个\n",
    "            target_df.drop_duplicates(inplace=True)\n",
    "            arr_str = [str(i) for i in np.array(target_df.N)]\n",
    "            viz.bar(X=np.array(target_df.P), env=viz_env_str, win=win_str,\n",
    "                    opts=dict(title=title_str,rownames=arr_str))\n",
    "\n",
    "            # Scatter plot\n",
    "            # target_df.drop_duplicates(inplace=True)\n",
    "            # viz.scatter(X=np.array(target_df),env=viz_env_str, win=win_str,opts=dict(title=title_str,markersize = 3))\n",
    "    print(\"Done\")\n",
    "\n",
    "# draw_smooth_target_data()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#### Test for drawing\n",
    "def test_draw():\n",
    "    viz = Visdom(env=\"001\")\n",
    "\n",
    "    mu = torch.tensor([0,10,20])\n",
    "    sigma = torch.tensor([1,1,1])\n",
    "    duration = torch.tensor([0,1,2,0])\n",
    "    duration = torch.repeat_interleave(duration.unsqueeze(dim=1), repeats=3, dim=1)\n",
    "    m = torch.distributions.Normal(loc=mu, scale=sigma)\n",
    "    pi = torch.tensor([0.2,0.3,0.5])\n",
    "\n",
    "    # draw\n",
    "    x_0 = torch.tensor(np.arange(0,1000))\n",
    "    x = torch.repeat_interleave(x_0.unsqueeze(dim=1), repeats=3, dim=1)\n",
    "    y = torch.exp(m.log_prob(x))\n",
    "    y_sum = torch.unsqueeze(torch.sum(pi*y,dim=1),dim=1)\n",
    "    viz.line(X = x_0,Y= torch.cat([y,y_sum],dim = 1), env=\"001\", win=\"test_draw_2\",\n",
    "            opts= dict(title='test_draw', legend=['N1', 'N2', 'N3','NNN']))\n",
    "# test_draw()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.2 plot mdn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2.1 mdn pdf的图像[with input data]\n",
    "1. 求解[1,6,11...]上的cdf：\n",
    "    - 求解[1,6,11..]的cdf，再求解[6,11,..]，两者相减"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# smoothed_y[t] = average(y[t-k], y[t-k+1], ..., y[t+k-1], y[t+k])\n",
    "# sm表示滑动窗口大小,为2*k+1,\n",
    "def smooth(data, sm=1):\n",
    "    if sm > 1:\n",
    "        smooth_data = []\n",
    "        for d in data:\n",
    "            y = np.ones(sm)*1.0/sm\n",
    "            d = np.convolve(y, d, \"same\")\n",
    "\n",
    "            smooth_data.append(d)\n",
    "\n",
    "    return smooth_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def draw_mdn_2(pi, mu, sigma, target, input, total_train_step, opt.N_gaussians):\n",
    "    \"\"\"\n",
    "    画两条曲线，pred-1表示归一化前，pred-2表示归一化后\n",
    "\n",
    "    :param pi:\n",
    "    :param mu:\n",
    "    :param sigma:\n",
    "    :param target:\n",
    "    :param input:\n",
    "    :param total_train_step:\n",
    "    :param opt.N_gaussians:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 只画一个batch中的第一个\n",
    "\n",
    "    # The target distrb.\n",
    "    target = torch.unique(target,dim=0)    # drop the duplicate\n",
    "    n_target = target[:,0]\n",
    "    non_zero_idx = torch.nonzero(n_target)\n",
    "    n = n_target[non_zero_idx]\n",
    "\n",
    "    p_target = target[:,1]\n",
    "    p = p_target[non_zero_idx]\n",
    "    max_n = int(max(n).item())\n",
    "\n",
    "    # The predicted distrb.\n",
    "    m = torch.distributions.Normal(mu,sigma)          # Gaussian\n",
    "    # m = torch.distributions.Laplace(mu,sigma)           # Laplace\n",
    "\n",
    "    x_0 = torch.arange(1,(max_n+opt.TARGET),opt.TARGET).to(device=device)\n",
    "\n",
    "    x = torch.repeat_interleave(x_0.unsqueeze(dim=1), repeats=opt.N_gaussians, dim=1)   # expand dim\n",
    "    # 求解每个长度为TARGET的区间上的cdf\n",
    "    y = (m.cdf(x+opt.TARGET) - m.cdf(x)).to(device=device)\n",
    "\n",
    "    # 方法一：\n",
    "    y_pred_1 = torch.sum(pi*y,dim=1)\n",
    "    # 方法二：做一下(0,1)上的归一化\n",
    "    y_pred_2 = y_pred_1/y_pred_1.sum()\n",
    "\n",
    "    # The input distrb.\n",
    "    input_data = input[0:2,:]\n",
    "    x_input = torch.arange(1,(300+1),opt.SCALE).to(device=device)\n",
    "    y_input_0 = (input_data[0,:]).to(device=device)\n",
    "    y_input_1 = (input_data[1,:]).to(device=device)\n",
    "\n",
    "    # Init\n",
    "    win_str = \"total_train_step = \"+str(total_train_step)\n",
    "    title_str = \"Distrb. in \"+win_str\n",
    "    viz.line(X = [0.],Y = [0.], env=\"001\", win=win_str, opts= dict(title=title_str))\n",
    "\n",
    "    # Visdom本身不能把hist和line画在一个window中\n",
    "    # 如果想画一起只能是两条line\n",
    "    # Plot y_target\n",
    "    # viz.histogram(X = n, env=\"001\", win=win_str,\n",
    "    #         opts= dict(title=title_str))\n",
    "    viz.line(X = n,Y= p, env=\"001\", win=win_str, update=\"append\", name='target',\n",
    "            opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "    # Plot y_pred\n",
    "    viz.line(X = x_0,Y= y_pred_1, env=\"001\", win=win_str, update=\"append\", name='pred-1',\n",
    "            opts= dict(title=title_str))\n",
    "\n",
    "    # Plot y_pred_2\n",
    "    viz.line(X = x_0,Y= y_pred_2, env=\"001\", win=win_str, update=\"append\", name='pred-2',\n",
    "            opts= dict(title=title_str))\n",
    "\n",
    "    # Plot y_input\n",
    "    # 如果input data长度短，全部画完\n",
    "    if(len(x_input) <= max_n):\n",
    "        viz.line(X = x_input,Y= y_input_0, env=\"001\", win=win_str, update=\"append\", name='GT-1', opts= dict(title=title_str))\n",
    "        viz.line(X = x_input,Y= y_input_1, env=\"001\", win=win_str, update=\"append\", name='GT-2', opts= dict(title=title_str))\n",
    "    # 如果input data长度长，则截断\n",
    "    else:\n",
    "        x_input_0 = x_input[0:max_n]\n",
    "        y_input_2 = y_input_0[0:max_n]/y_input_0[0:max_n].sum()\n",
    "        y_input_3 = y_input_1[0:max_n]/y_input_1[0:max_n].sum()\n",
    "        viz.line(X = x_input_0,Y= y_input_2, env=\"001\", win=win_str, update=\"append\", name='GT-1', opts= dict(title=title_str))\n",
    "        viz.line(X = x_input_0,Y= y_input_3, env=\"001\", win=win_str, update=\"append\", name='GT-2', opts= dict(title=title_str))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2.2 plot mdn cdf[无input data]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def draw_mdn_cdf(pi, mu, sigma, target, input, total_train_step, opt.N_gaussians):\n",
    "    # 只画一个batch中的第一个\n",
    "\n",
    "    # The target distrb.\n",
    "    target = torch.unique(target,dim=0)    # drop the duplicate\n",
    "    n_target = target[:,0]\n",
    "    non_zero_idx = torch.nonzero(n_target)\n",
    "    n = n_target[non_zero_idx]\n",
    "\n",
    "    p_target = target[:,1]\n",
    "    p = p_target[non_zero_idx]\n",
    "    max_n = max(n).item()               # 横轴长度\n",
    "\n",
    "    # The predicted distrb.\n",
    "    m = torch.distributions.Normal(mu,sigma)\n",
    "    x_0 = torch.arange(1,max_n).to(device=device)\n",
    "    x = torch.repeat_interleave(x_0.unsqueeze(dim=1), repeats=opt.N_gaussians, dim=1)\n",
    "    y = m.cdf(x).to(device=device)                        # y:多条高斯曲线; y_pred: 一条GMM曲线\n",
    "    # y_pred = torch.unsqueeze(torch.sum(pi*y,dim=1),dim=1)                 # 维度相等才能cat\n",
    "    y_pred = torch.sum(pi*y,dim=1)\n",
    "\n",
    "    # Init\n",
    "    win_str = \"total_train_step = \"+str(total_train_step)\n",
    "    title_str = \"Distrb. in \"+win_str\n",
    "    viz.line(X = [0.],Y = [0.], env=\"001\", win=win_str, opts= dict(title=title_str))\n",
    "\n",
    "    # Visdom本身不能把hist和line画在一个window中\n",
    "    # 如果想画一起只能是两条line\n",
    "    # Plot y_target\n",
    "    # viz.histogram(X = n, env=\"001\", win=win_str,\n",
    "    #         opts= dict(title=title_str))\n",
    "    viz.line(X = n,Y= p, env=\"001\", win=win_str, update=\"append\", name='target',\n",
    "            opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "    # Plot y_pred\n",
    "    viz.line(X = x_0,Y= y_pred, env=\"001\", win=win_str, update=\"append\", name='pred',\n",
    "            opts= dict(title=title_str))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2.3 plot mdn pdf in TEST\n",
    "1. 读入2个model时，在test set上比较效果"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_env = \"003\"\n",
    "def draw_mdn_3(Pi1, Mu1, Sigma1, Pi2, Mu2, Sigma2, Target, Input, test_batch, opt.N_gaussians):\n",
    "    for i in range(len(Pi1)):\n",
    "        # The target distrb.\n",
    "        target = Target[i]\n",
    "        target = torch.unique(target,dim=0)    # drop the duplicate\n",
    "        n_target = target[:,0]\n",
    "        non_zero_idx = torch.nonzero(n_target)\n",
    "        n = n_target[non_zero_idx]\n",
    "\n",
    "        p_target = target[:,1]\n",
    "        p = p_target[non_zero_idx]\n",
    "        max_n = int(max(n).item())\n",
    "\n",
    "        # The predicted distrb.\n",
    "        mu1 = Mu1[i]\n",
    "        mu2 = Mu2[i]\n",
    "        sigma1 = Sigma1[i]\n",
    "        sigma2 = Sigma2[i]\n",
    "        m1 = torch.distributions.Normal(mu1,sigma1)          # Gaussian\n",
    "        m2 = torch.distributions.Normal(mu2,sigma2)          # Gaussian\n",
    "        # m = torch.distributions.Laplace(mu,sigma)           # Laplace\n",
    "\n",
    "        x_0 = torch.arange(1,(max_n+opt.TARGET),opt.TARGET).to(device=device)\n",
    "\n",
    "        x = torch.repeat_interleave(x_0.unsqueeze(dim=1), repeats=opt.N_gaussians, dim=1)   # expand dim\n",
    "        # 求解每个长度为TARGET的区间上的cdf\n",
    "        y1 = (m1.cdf(x+opt.TARGET) - m1.cdf(x)).to(device=device)\n",
    "        y2 = (m2.cdf(x+opt.TARGET) - m2.cdf(x)).to(device=device)\n",
    "\n",
    "        # 方法一：\n",
    "        pi1 = Pi1[i]\n",
    "        pi2 = Pi2[i]\n",
    "        y_mdn_1 = torch.sum(pi1*y1,dim=1)\n",
    "        y_mdn_2 = torch.sum(pi2*y2,dim=1)\n",
    "        # 方法二：做一下(0,1)上的归一化\n",
    "        y_pred_1 = y_mdn_1/y_mdn_1.sum()\n",
    "        y_pred_2 = y_mdn_2/y_mdn_2.sum()\n",
    "\n",
    "        # The input distrb.\n",
    "        input = Input[i]\n",
    "        input_data = input[0:2,:]\n",
    "        x_input = torch.arange(1,(300+1),opt.SCALE).to(device=device)\n",
    "        y_input_0 = (input_data[0,:]).to(device=device)\n",
    "        y_input_1 = (input_data[1,:]).to(device=device)\n",
    "\n",
    "        # Init\n",
    "        win_str = \"Test batch = \"+str(test_batch)+\" idx = \"+str(i)\n",
    "        title_str = \"Distrb. of \"+win_str\n",
    "        viz.line(X = [0.],Y = [0.], env=test_env, win=win_str, opts= dict(title=title_str))\n",
    "\n",
    "        # Visdom本身不能把hist和line画在一个window中\n",
    "        # 如果想画一起只能是两条line\n",
    "        # Plot y_target\n",
    "        # viz.histogram(X = n, env=\"001\", win=win_str,\n",
    "        #         opts= dict(title=title_str))\n",
    "        viz.line(X = n,Y= p, env=test_env, win=win_str, update=\"append\", name='target',\n",
    "                opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "        # Plot y_pred\n",
    "        viz.line(X = x_0,Y= y_pred_1, env=test_env, win=win_str, update=\"append\", name='pred-MLE',opts= dict(title=title_str))\n",
    "        viz.line(X = x_0,Y= y_pred_2, env=test_env, win=win_str, update=\"append\", name='pred-WD',\n",
    "                opts= dict(title=title_str))\n",
    "\n",
    "        # Plot y_input\n",
    "        # 如果input data长度短，全部画完\n",
    "        if(len(x_input) <= max_n):\n",
    "            viz.line(X = x_input,Y= y_input_0, env=test_env, win=win_str, update=\"append\", name='GT-1', opts= dict(title=title_str))\n",
    "            viz.line(X = x_input,Y= y_input_1, env=test_env, win=win_str, update=\"append\", name='GT-2', opts= dict(title=title_str))\n",
    "        # 如果input data长度长，则截断\n",
    "        else:\n",
    "            x_input_0 = x_input[0:max_n]\n",
    "            y_input_2 = y_input_0[0:max_n]/y_input_0[0:max_n].sum()\n",
    "            y_input_3 = y_input_1[0:max_n]/y_input_1[0:max_n].sum()\n",
    "            viz.line(X = x_input_0,Y= y_input_2, env=test_env, win=win_str, update=\"append\", name='GT-1', opts= dict(title=title_str))\n",
    "            viz.line(X = x_input_0,Y= y_input_3, env=test_env, win=win_str, update=\"append\", name='GT-2', opts= dict(title=title_str))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.3 png格式的net structure"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def draw_the_net_png():\n",
    "\n",
    "    x = torch.randn([1, 3, 300])  # 定义网络的输入值\n",
    "    mlp = MLP_1_1(opt.N_gaussians)\n",
    "    y = mlp(x)                    # 获取网络的预测值\n",
    "\n",
    "    MyConvNetVis = make_dot(y, params=dict(list(mlp.named_parameters()) + [('x', x)]))\n",
    "    MyConvNetVis.format = \"png\"\n",
    "    # 指定文件生成的文件夹\n",
    "    MyConvNetVis.directory = \"data_pic\"\n",
    "    # 生成文件\n",
    "    MyConvNetVis.view()\n",
    "# draw_the_net_png()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.4 plot the loss\n",
    "1. 直接根据参数决定draw什么loss，反正loss都画在一个window里面"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "win_train_loss_str = \"The Loss of BATCH in the Training Data\"\n",
    "win_vali_loss_str = \"The Loss in the Vali Data\"\n",
    "win_train_epoch_loss_str = \"The Loss of EPOCH in the Training Data\"\n",
    "\n",
    "def draw_loss(X_step, loss, win_str):\n",
    "    viz.line(X = [X_step], Y = [loss],win=win_str, update=\"append\",\n",
    "        opts= dict(title=win_str))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.5 plot the metric"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "win_vali_metric_str = \"The NLL of ALL vali data\"\n",
    "def draw_metric(X_step, total_vali_metric, GT_metric):\n",
    "\n",
    "    viz.line(X = [X_step], Y = [[total_vali_metric,GT_metric[0,0],GT_metric[0,1]]],win=win_vali_metric_str, update=\"append\", opts= dict(title=win_vali_metric_str, legend=['pred','GT-1','GT-2'], showlegend=True,xlabel=\"epoch\", ylabel=\"NLL\"))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.6 grad graph\n",
    "https://github.com/t-vi/pytorch-tvmisc/blob/master/visualize/bad_grad_viz.ipynb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "outputs": [],
   "source": [
    "\n",
    "def iter_graph(root, callback):\n",
    "    queue = [root]\n",
    "    seen = set()\n",
    "    while queue:\n",
    "        # 队头出fn然后判断fn是否seen\n",
    "        fn = queue.pop()\n",
    "        if fn in seen:\n",
    "            continue\n",
    "        # unseen加入queue，并且继续沿着fn向下递归（DFS？）\n",
    "        seen.add(fn)\n",
    "        for next_fn, _ in fn.next_functions:\n",
    "            # Alert：只有非none的才会加入queue，然后在register_grad里被fn_dict记录下grad_input\n",
    "            if next_fn is not None:\n",
    "                queue.append(next_fn)\n",
    "        callback(fn)\n",
    "\n",
    "def register_hooks(var):\n",
    "    fn_dict = {}  # 记录了grad名字和值（grad_input）\n",
    "    def hook_cb(fn):\n",
    "        def register_grad(grad_input, grad_output):\n",
    "            fn_dict[fn] = grad_input\n",
    "        fn.register_hook(register_grad)\n",
    "\n",
    "    # 1. 递归注册grad_input\n",
    "    iter_graph(var.grad_fn, hook_cb)\n",
    "\n",
    "    # def is_bad_grad(grad_output):\n",
    "    #     if grad_output is None:\n",
    "    #         return False\n",
    "    #     return grad_output.isnan().any() or (grad_output.abs() >= 1e6).any()\n",
    "    def is_bad_grad(grad_output):\n",
    "        if grad_output is None:\n",
    "                return True\n",
    "        grad_output = grad_output.data\n",
    "        return grad_output.ne(grad_output).any() or grad_output.gt(1e6).any()\n",
    "\n",
    "    # 2. graph已经从grad input构建完了，还有grad output的判断以及颜色的选择\n",
    "    def make_dot():\n",
    "        node_attr = dict(style='filled',\n",
    "                        shape='box',\n",
    "                        align='left',\n",
    "                        fontsize='12',\n",
    "                        ranksep='0.1',\n",
    "                        height='0.2')\n",
    "        dot = Digraph(node_attr=node_attr, graph_attr=dict(size=\"12,12\"))\n",
    "\n",
    "        def size_to_str(size):\n",
    "            return '('+(', ').join(map(str, size))+')'\n",
    "\n",
    "        def build_graph(fn):\n",
    "            if hasattr(fn, 'variable'):  # if GradAccumulator\n",
    "                u = fn.variable\n",
    "                node_name = 'Variable\\n ' + size_to_str(u.size())\n",
    "                dot.node(str(id(u)), node_name, fillcolor='lightblue')\n",
    "            else:  # 不是variable属性，白色\n",
    "                assert fn in fn_dict, fn   # 判断fn在不在fn_dict里面！\n",
    "                fillcolor = 'white'\n",
    "                if any(is_bad_grad(gi) for gi in fn_dict[fn]):\n",
    "                    fillcolor = 'red'\n",
    "                dot.node(str(id(fn)), str(type(fn).__name__), fillcolor=fillcolor)\n",
    "            for next_fn, _ in fn.next_functions:\n",
    "                if next_fn is not None:\n",
    "                    next_id = id(getattr(next_fn, 'variable', next_fn))\n",
    "                    dot.edge(str(next_id), str(id(fn)))\n",
    "        iter_graph(var.grad_fn, build_graph)\n",
    "\n",
    "        return dot\n",
    "\n",
    "    return make_dot\n",
    "\n",
    "# x = torch.randn(10, 10, requires_grad=True)\n",
    "# y = torch.randn(10, 10, requires_grad=True)\n",
    "#\n",
    "# z = x / (y * 0)\n",
    "# z = z.sum() * 2\n",
    "# get_dot = register_hooks(z)\n",
    "\n",
    "\n",
    "# with torch.autograd.detect_anomaly():\n",
    "#     z.backward()\n",
    "#     dot = get_dot()\n",
    "#     dot.save('tmp.dot') # to get .dot\n",
    "#     dot.render('tmp') # to get SVG\n",
    "#     # dot # in Jupyter, you can just render the variable"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6 Training\n",
    "## 6.1 Init"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "       BatchNorm1d-1               [-1, 3, 300]               6\n",
      "            Conv2d-2             [-1, 1, 1, 59]              31\n",
      "            Conv2d-3             [-1, 1, 1, 59]              31\n",
      "            Conv2d-4             [-1, 1, 1, 59]              31\n",
      "       BatchNorm1d-5                   [-1, 59]             118\n",
      "       BatchNorm1d-6                   [-1, 59]             118\n",
      "       BatchNorm1d-7                   [-1, 59]             118\n",
      "            Linear-8                    [-1, 3]             180\n",
      "           Softmax-9                    [-1, 3]               0\n",
      "           Linear-10                    [-1, 3]             180\n",
      "           Linear-11                    [-1, 3]             180\n",
      "================================================================\n",
      "Total params: 993\n",
      "Trainable params: 993\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.02\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "mlp = MLP_1_1(opt.N_gaussians)\n",
    "\n",
    "# Init the params\n",
    "# mlp = model_param_init(mlp)\n",
    "\n",
    "# Save the init params\n",
    "# torch.save(mlp.state_dict(), 'mlp_init.pth')\n",
    "\n",
    "# Load the saved model\n",
    "# model_path_WD = \"mlp_train=300_WD.pth\"\n",
    "# model_path_LSE = \"mlp_train=300_LSE.pth\"\n",
    "# model_path_MLE = \"mlp_train=300_MLE.pth\"\n",
    "#\n",
    "# model_data = torch.load(model_path_MLE)\n",
    "# mlp.load_state_dict(model_data)\n",
    "\n",
    "mlp = mlp.to(device=device)\n",
    "summary(mlp, (3,300))\n",
    "\n",
    "############ learning rate strategy ############\n",
    "# 1.\n",
    "# Set different lr for params\n",
    "# id: id() 函数返回对象的唯一标识符，标识符是一个整数。返回对象的内存地址。\n",
    "mu_params = list(map(id, mlp.z_mu.parameters()))\n",
    "sigma_params = list(map(id, mlp.z_sigma.parameters()))\n",
    "pi_params = list(map(id, mlp.z_pi.parameters()))\n",
    "\n",
    "params_id = mu_params + sigma_params + pi_params\n",
    "\n",
    "base_params = filter(lambda p: id(p) not in params_id, mlp.parameters())\n",
    "params = [{'params': base_params},         # 如果对某个参数不指定学习率，就使用最外层的默认学习率\n",
    "          {'params': mlp.z_pi.parameters(), 'lr': opt.lr_for_pi},\n",
    "        {'params': mlp.z_mu.parameters(), 'lr': opt.lr_for_mu},\n",
    "          {'params': mlp.z_sigma.parameters(), 'lr': opt.lr_for_sigma}]\n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr=opt.learning_rate,weight_decay=opt.weight_decay)\n",
    "\n",
    "# 2.\n",
    "# Or set lr decay\n",
    "# StepLR为步进，每step_size个epoch，lr*gamma\n",
    "scheduler  = torch.optim.lr_scheduler.StepLR(optimizer,step_size=opt.StepLR_step_size,gamma=opt.StepLR_gamma)\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[15,30], gamma=0.8, last_epoch=-1)\n",
    "\n",
    "# # Set the hooks\n",
    "# #mlp.conv1.register_forward_hook(hook_forward_fn)\n",
    "# mlp.conv1.register_full_backward_hook(hook_backward_fn)\n",
    "#\n",
    "# #mlp.linear1.register_forward_hook(hook_forward_fn)\n",
    "# #mlp.linear1.register_full_backward_hook(hook_backward_fn)\n",
    "#\n",
    "# #mlp.z_pi.register_forward_hook(hook_forward_fn)\n",
    "# mlp.z_pi.register_full_backward_hook(hook_backward_fn_pi)\n",
    "#\n",
    "# #mlp.z_mu.register_forward_hook(hook_forward_fn)\n",
    "# mlp.z_mu.register_full_backward_hook(hook_backward_fn_mu)\n",
    "#\n",
    "# mlp.z_sigma.register_forward_hook(hook_forward_fn)\n",
    "# mlp.z_sigma.register_full_[2, 3, 300]backward_hook(hook_backward_fn_sigma)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir=\"logs-MLP/\"+opt.logs_str,flush_secs=60)\n",
    "\n",
    "# Init the vis win\n",
    "viz.line(X = [0.],Y = [0.], env=\"001\", win=win_train_loss_str, opts= dict(title=win_train_loss_str))\n",
    "viz.line(X = [0.],Y = [0.], env=\"001\", win=win_train_epoch_loss_str, opts= dict(title=win_train_epoch_loss_str))\n",
    "viz.line(X = [0.],Y = [[0.,0.,0.]], env=\"001\", win=win_vali_metric_str, opts= dict(title=win_vali_metric_str,legend=['pred','GT-1','GT-2'], showlegend=True,xlabel=\"epoch\", ylabel=\"NLL\"))\n",
    "\n",
    "plot_net(writer,mlp,torch.randn((2,3,300),device=device))\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.2 grad check\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "outputs": [],
   "source": [
    "# # grad check\n",
    "# input_test = torch.randn((2,3,300), requires_grad=True,device=device)\n",
    "# test_ans = gradcheck(mlp.to(device), input_test, eps=1e-3)  #, eps=1e-6, atol=1e-4\n",
    "# print(\"Are the gradients correct: \", test_ans)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.3 START HERE\n",
    "1. 使用Laplace分布改：\n",
    "    - loss function和draw"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- 11 batch----\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Now this is EPOCH 0 ==========\n",
      "========== IN EPOCH 0 the total loss is 59106.34997558594 ==========\n",
      "========== IN EPOCH 0 the vali NLL loss is 8.511602401733398 ==========\n",
      "========== IN EPOCH 0 the GT metric is [[6.45831  9.718622]] ==========\n",
      "========== Now this is EPOCH 1 ==========\n",
      "========== IN EPOCH 1 the total loss is 19417.03485107422 ==========\n",
      "========== IN EPOCH 1 the vali NLL loss is 8.216514587402344 ==========\n",
      "========== IN EPOCH 1 the GT metric is [[6.4583097 9.718622 ]] ==========\n",
      "========== Now this is EPOCH 2 ==========\n",
      "========== IN EPOCH 2 the total loss is 16182.341369628906 ==========\n",
      "========== IN EPOCH 2 the vali NLL loss is 7.425222396850586 ==========\n",
      "========== IN EPOCH 2 the GT metric is [[6.4583097 9.718622 ]] ==========\n",
      "========== Now this is EPOCH 3 ==========\n",
      "========== IN EPOCH 3 the total loss is 14935.10580444336 ==========\n",
      "========== IN EPOCH 3 the vali NLL loss is 6.824872016906738 ==========\n",
      "========== IN EPOCH 3 the GT metric is [[6.45831  9.718623]] ==========\n",
      "========== Now this is EPOCH 4 ==========\n",
      "========== IN EPOCH 4 the total loss is 14444.368591308594 ==========\n",
      "========== IN EPOCH 4 the vali NLL loss is 6.692056179046631 ==========\n",
      "========== IN EPOCH 4 the GT metric is [[6.45831  9.718623]] ==========\n",
      "========== Now this is EPOCH 5 ==========\n",
      "========== IN EPOCH 5 the total loss is 14182.330139160156 ==========\n",
      "========== IN EPOCH 5 the vali NLL loss is 6.651177883148193 ==========\n",
      "========== IN EPOCH 5 the GT metric is [[6.45831  9.718622]] ==========\n",
      "========== Now this is EPOCH 6 ==========\n",
      "========== IN EPOCH 6 the total loss is 13824.766052246094 ==========\n",
      "========== IN EPOCH 6 the vali NLL loss is 6.510138034820557 ==========\n",
      "========== IN EPOCH 6 the GT metric is [[6.458309 9.718622]] ==========\n",
      "========== Now this is EPOCH 7 ==========\n",
      "========== IN EPOCH 7 the total loss is 13788.931243896484 ==========\n",
      "========== IN EPOCH 7 the vali NLL loss is 6.424419403076172 ==========\n",
      "========== IN EPOCH 7 the GT metric is [[6.45831  9.718622]] ==========\n",
      "========== Now this is EPOCH 8 ==========\n",
      "========== IN EPOCH 8 the total loss is 13988.418640136719 ==========\n",
      "========== IN EPOCH 8 the vali NLL loss is 6.445719242095947 ==========\n",
      "========== IN EPOCH 8 the GT metric is [[6.4583097 9.718622 ]] ==========\n",
      "========== Now this is EPOCH 9 ==========\n",
      "========== IN EPOCH 9 the total loss is 13871.874816894531 ==========\n",
      "========== IN EPOCH 9 the vali NLL loss is 6.374908924102783 ==========\n",
      "========== IN EPOCH 9 the GT metric is [[6.45831  9.718622]] ==========\n",
      "========== Now this is EPOCH 10 ==========\n",
      "========== IN EPOCH 10 the total loss is 13654.642822265625 ==========\n",
      "========== IN EPOCH 10 the vali NLL loss is 6.375721454620361 ==========\n",
      "========== IN EPOCH 10 the GT metric is [[6.45831  9.718622]] ==========\n",
      "========== Now this is EPOCH 11 ==========\n",
      "========== IN EPOCH 11 the total loss is 13604.90576171875 ==========\n",
      "========== IN EPOCH 11 the vali NLL loss is 6.363311290740967 ==========\n",
      "========== IN EPOCH 11 the GT metric is [[6.45831  9.718623]] ==========\n",
      "========== Now this is EPOCH 12 ==========\n",
      "========== IN EPOCH 12 the total loss is 13569.130859375 ==========\n",
      "========== IN EPOCH 12 the vali NLL loss is 6.3870649337768555 ==========\n",
      "========== IN EPOCH 12 the GT metric is [[6.4583097 9.718623 ]] ==========\n",
      "========== Now this is EPOCH 13 ==========\n",
      "========== IN EPOCH 13 the total loss is 13577.09097290039 ==========\n",
      "========== IN EPOCH 13 the vali NLL loss is 6.500011920928955 ==========\n",
      "========== IN EPOCH 13 the GT metric is [[6.4583097 9.718622 ]] ==========\n",
      "========== Now this is EPOCH 14 ==========\n",
      "========== IN EPOCH 14 the total loss is 13509.972412109375 ==========\n",
      "========== IN EPOCH 14 the vali NLL loss is 6.353594779968262 ==========\n",
      "========== IN EPOCH 14 the GT metric is [[6.45831  9.718622]] ==========\n",
      "========== Now this is EPOCH 15 ==========\n",
      "========== IN EPOCH 15 the total loss is 13711.491271972656 ==========\n",
      "========== IN EPOCH 15 the vali NLL loss is 6.3803935050964355 ==========\n",
      "========== IN EPOCH 15 the GT metric is [[6.4583097 9.718622 ]] ==========\n",
      "========== Now this is EPOCH 16 ==========\n",
      "========== IN EPOCH 16 the total loss is 13381.373458862305 ==========\n",
      "========== IN EPOCH 16 the vali NLL loss is 6.361225128173828 ==========\n",
      "========== IN EPOCH 16 the GT metric is [[6.45831  9.718623]] ==========\n",
      "========== Now this is EPOCH 17 ==========\n",
      "========== IN EPOCH 17 the total loss is 13374.705261230469 ==========\n",
      "========== IN EPOCH 17 the vali NLL loss is 6.313528060913086 ==========\n",
      "========== IN EPOCH 17 the GT metric is [[6.4583097 9.718622 ]] ==========\n",
      "========== Now this is EPOCH 18 ==========\n",
      "========== IN EPOCH 18 the total loss is 13541.559509277344 ==========\n",
      "========== IN EPOCH 18 the vali NLL loss is 6.363868236541748 ==========\n",
      "========== IN EPOCH 18 the GT metric is [[6.4583097 9.718622 ]] ==========\n",
      "========== Now this is EPOCH 19 ==========\n",
      "========== IN EPOCH 19 the total loss is 13455.9375 ==========\n",
      "========== IN EPOCH 19 the vali NLL loss is 6.325493335723877 ==========\n",
      "========== IN EPOCH 19 the GT metric is [[6.45831  9.718622]] ==========\n",
      "========== Now this is EPOCH 20 ==========\n",
      "========== IN EPOCH 20 the total loss is 13226.396240234375 ==========\n",
      "========== IN EPOCH 20 the vali NLL loss is 6.357431888580322 ==========\n",
      "========== IN EPOCH 20 the GT metric is [[6.45831  9.718623]] ==========\n",
      "========== Now this is EPOCH 21 ==========\n",
      "========== IN EPOCH 21 the total loss is 13448.009002685547 ==========\n",
      "========== IN EPOCH 21 the vali NLL loss is 6.326253890991211 ==========\n",
      "========== IN EPOCH 21 the GT metric is [[6.4583106 9.718623 ]] ==========\n",
      "========== Now this is EPOCH 22 ==========\n",
      "========== IN EPOCH 22 the total loss is 13324.144256591797 ==========\n",
      "========== IN EPOCH 22 the vali NLL loss is 6.307315826416016 ==========\n",
      "========== IN EPOCH 22 the GT metric is [[6.45831  9.718623]] ==========\n",
      "========== Now this is EPOCH 23 ==========\n",
      "========== IN EPOCH 23 the total loss is 13266.761840820312 ==========\n",
      "========== IN EPOCH 23 the vali NLL loss is 6.327352523803711 ==========\n",
      "========== IN EPOCH 23 the GT metric is [[6.45831  9.718622]] ==========\n",
      "========== Now this is EPOCH 24 ==========\n",
      "========== IN EPOCH 24 the total loss is 13219.20166015625 ==========\n",
      "========== IN EPOCH 24 the vali NLL loss is 6.313704013824463 ==========\n",
      "========== IN EPOCH 24 the GT metric is [[6.45831  9.718622]] ==========\n",
      "========== Now this is EPOCH 25 ==========\n",
      "========== IN EPOCH 25 the total loss is 13254.95248413086 ==========\n",
      "========== IN EPOCH 25 the vali NLL loss is 6.453692436218262 ==========\n",
      "========== IN EPOCH 25 the GT metric is [[6.4583106 9.718622 ]] ==========\n",
      "========== Now this is EPOCH 26 ==========\n",
      "========== IN EPOCH 26 the total loss is 13389.405517578125 ==========\n",
      "========== IN EPOCH 26 the vali NLL loss is 6.326905250549316 ==========\n",
      "========== IN EPOCH 26 the GT metric is [[6.4583097 9.718622 ]] ==========\n",
      "========== Now this is EPOCH 27 ==========\n",
      "========== IN EPOCH 27 the total loss is 13229.283264160156 ==========\n",
      "========== IN EPOCH 27 the vali NLL loss is 6.321071624755859 ==========\n",
      "========== IN EPOCH 27 the GT metric is [[6.4583097 9.718623 ]] ==========\n",
      "========== Now this is EPOCH 28 ==========\n",
      "========== IN EPOCH 28 the total loss is 13243.778289794922 ==========\n",
      "========== IN EPOCH 28 the vali NLL loss is 6.304008960723877 ==========\n",
      "========== IN EPOCH 28 the GT metric is [[6.45831  9.718622]] ==========\n",
      "========== Now this is EPOCH 29 ==========\n",
      "========== IN EPOCH 29 the total loss is 13412.260772705078 ==========\n",
      "========== IN EPOCH 29 the vali NLL loss is 6.369342803955078 ==========\n",
      "========== IN EPOCH 29 the GT metric is [[6.4583106 9.718623 ]] ==========\n",
      "========== Now this is EPOCH 30 ==========\n",
      "========== IN EPOCH 30 the total loss is 13233.619934082031 ==========\n",
      "========== IN EPOCH 30 the vali NLL loss is 6.305267333984375 ==========\n",
      "========== IN EPOCH 30 the GT metric is [[6.45831  9.718622]] ==========\n",
      "========== Now this is EPOCH 31 ==========\n",
      "========== IN EPOCH 31 the total loss is 13437.602478027344 ==========\n",
      "========== IN EPOCH 31 the vali NLL loss is 6.408943176269531 ==========\n",
      "========== IN EPOCH 31 the GT metric is [[6.4583106 9.718622 ]] ==========\n",
      "========== Now this is EPOCH 32 ==========\n",
      "========== IN EPOCH 32 the total loss is 13432.974700927734 ==========\n",
      "========== IN EPOCH 32 the vali NLL loss is 6.298133850097656 ==========\n",
      "========== IN EPOCH 32 the GT metric is [[6.45831  9.718621]] ==========\n",
      "========== Now this is EPOCH 33 ==========\n",
      "========== IN EPOCH 33 the total loss is 13140.790313720703 ==========\n",
      "========== IN EPOCH 33 the vali NLL loss is 6.2785539627075195 ==========\n",
      "========== IN EPOCH 33 the GT metric is [[6.45831  9.718622]] ==========\n",
      "========== Now this is EPOCH 34 ==========\n",
      "========== IN EPOCH 34 the total loss is 13340.686248779297 ==========\n",
      "========== IN EPOCH 34 the vali NLL loss is 6.283144950866699 ==========\n",
      "========== IN EPOCH 34 the GT metric is [[6.45831  9.718622]] ==========\n",
      "========== Now this is EPOCH 35 ==========\n",
      "========== IN EPOCH 35 the total loss is 13152.863525390625 ==========\n",
      "========== IN EPOCH 35 the vali NLL loss is 6.286050319671631 ==========\n",
      "========== IN EPOCH 35 the GT metric is [[6.4583097 9.718622 ]] ==========\n",
      "========== Now this is EPOCH 36 ==========\n",
      "========== IN EPOCH 36 the total loss is 13151.588470458984 ==========\n",
      "========== IN EPOCH 36 the vali NLL loss is 6.289886474609375 ==========\n",
      "========== IN EPOCH 36 the GT metric is [[6.4583097 9.718622 ]] ==========\n",
      "========== Now this is EPOCH 37 ==========\n",
      "========== IN EPOCH 37 the total loss is 13177.176300048828 ==========\n",
      "========== IN EPOCH 37 the vali NLL loss is 6.315608978271484 ==========\n",
      "========== IN EPOCH 37 the GT metric is [[6.4583097 9.718622 ]] ==========\n",
      "========== Now this is EPOCH 38 ==========\n",
      "========== IN EPOCH 38 the total loss is 13267.351806640625 ==========\n",
      "========== IN EPOCH 38 the vali NLL loss is 6.383612632751465 ==========\n",
      "========== IN EPOCH 38 the GT metric is [[6.45831  9.718622]] ==========\n",
      "========== Now this is EPOCH 39 ==========\n",
      "========== IN EPOCH 39 the total loss is 13177.585723876953 ==========\n",
      "========== IN EPOCH 39 the vali NLL loss is 6.271857738494873 ==========\n",
      "========== IN EPOCH 39 the GT metric is [[6.4583097 9.718623 ]] ==========\n",
      "========== Now this is EPOCH 40 ==========\n",
      "========== IN EPOCH 40 the total loss is 13447.576080322266 ==========\n",
      "========== IN EPOCH 40 the vali NLL loss is 6.286736488342285 ==========\n",
      "========== IN EPOCH 40 the GT metric is [[6.45831  9.718622]] ==========\n",
      "========== Now this is EPOCH 41 ==========\n",
      "========== IN EPOCH 41 the total loss is 13267.314163208008 ==========\n",
      "========== IN EPOCH 41 the vali NLL loss is 6.275778770446777 ==========\n",
      "========== IN EPOCH 41 the GT metric is [[6.45831  9.718622]] ==========\n",
      "========== Now this is EPOCH 42 ==========\n",
      "========== IN EPOCH 42 the total loss is 13051.913543701172 ==========\n",
      "========== IN EPOCH 42 the vali NLL loss is 6.26331901550293 ==========\n",
      "========== IN EPOCH 42 the GT metric is [[6.4583097 9.718622 ]] ==========\n",
      "========== Now this is EPOCH 43 ==========\n",
      "========== IN EPOCH 43 the total loss is 13270.52587890625 ==========\n",
      "========== IN EPOCH 43 the vali NLL loss is 6.29568338394165 ==========\n",
      "========== IN EPOCH 43 the GT metric is [[6.4583097 9.718622 ]] ==========\n",
      "========== Now this is EPOCH 44 ==========\n",
      "========== IN EPOCH 44 the total loss is 13449.443054199219 ==========\n",
      "========== IN EPOCH 44 the vali NLL loss is 6.2668070793151855 ==========\n",
      "========== IN EPOCH 44 the GT metric is [[6.45831  9.718623]] ==========\n",
      "========== Now this is EPOCH 45 ==========\n",
      "========== IN EPOCH 45 the total loss is 13155.439422607422 ==========\n",
      "========== IN EPOCH 45 the vali NLL loss is 6.282742977142334 ==========\n",
      "========== IN EPOCH 45 the GT metric is [[6.45831  9.718622]] ==========\n",
      "========== Now this is EPOCH 46 ==========\n",
      "========== IN EPOCH 46 the total loss is 13024.7197265625 ==========\n",
      "========== IN EPOCH 46 the vali NLL loss is 6.264163970947266 ==========\n",
      "========== IN EPOCH 46 the GT metric is [[6.45831  9.718622]] ==========\n",
      "========== Now this is EPOCH 47 ==========\n",
      "========== IN EPOCH 47 the total loss is 13193.210479736328 ==========\n",
      "========== IN EPOCH 47 the vali NLL loss is 6.316561222076416 ==========\n",
      "========== IN EPOCH 47 the GT metric is [[6.45831  9.718622]] ==========\n",
      "========== Now this is EPOCH 48 ==========\n",
      "========== IN EPOCH 48 the total loss is 13122.615234375 ==========\n",
      "========== IN EPOCH 48 the vali NLL loss is 6.286317348480225 ==========\n",
      "========== IN EPOCH 48 the GT metric is [[6.45831  9.718622]] ==========\n",
      "========== Now this is EPOCH 49 ==========\n",
      "========== IN EPOCH 49 the total loss is 13301.625183105469 ==========\n",
      "========== IN EPOCH 49 the vali NLL loss is 6.365116119384766 ==========\n",
      "========== IN EPOCH 49 the GT metric is [[6.45831  9.718621]] ==========\n",
      "========== Now this is EPOCH 50 ==========\n",
      "========== IN EPOCH 50 the total loss is 13183.525573730469 ==========\n",
      "========== IN EPOCH 50 the vali NLL loss is 6.273589611053467 ==========\n",
      "========== IN EPOCH 50 the GT metric is [[6.45831  9.718623]] ==========\n",
      "========== Now this is EPOCH 51 ==========\n",
      "========== IN EPOCH 51 the total loss is 13032.662689208984 ==========\n",
      "========== IN EPOCH 51 the vali NLL loss is 6.289270877838135 ==========\n",
      "========== IN EPOCH 51 the GT metric is [[6.4583097 9.718622 ]] ==========\n",
      "========== Now this is EPOCH 52 ==========\n",
      "========== IN EPOCH 52 the total loss is 13214.290573120117 ==========\n",
      "========== IN EPOCH 52 the vali NLL loss is 6.275418281555176 ==========\n",
      "========== IN EPOCH 52 the GT metric is [[6.4583097 9.718622 ]] ==========\n",
      "========== Now this is EPOCH 53 ==========\n",
      "========== IN EPOCH 53 the total loss is 13020.628326416016 ==========\n",
      "========== IN EPOCH 53 the vali NLL loss is 6.273924827575684 ==========\n",
      "========== IN EPOCH 53 the GT metric is [[6.45831  9.718622]] ==========\n",
      "========== Now this is EPOCH 54 ==========\n",
      "========== IN EPOCH 54 the total loss is 13192.160247802734 ==========\n",
      "========== IN EPOCH 54 the vali NLL loss is 6.279153823852539 ==========\n",
      "========== IN EPOCH 54 the GT metric is [[6.4583106 9.718622 ]] ==========\n",
      "========== Now this is EPOCH 55 ==========\n",
      "========== IN EPOCH 55 the total loss is 13289.333282470703 ==========\n",
      "========== IN EPOCH 55 the vali NLL loss is 6.270239353179932 ==========\n",
      "========== IN EPOCH 55 the GT metric is [[6.4583097 9.718622 ]] ==========\n",
      "========== Now this is EPOCH 56 ==========\n",
      "========== IN EPOCH 56 the total loss is 13131.273468017578 ==========\n",
      "========== IN EPOCH 56 the vali NLL loss is 6.272173881530762 ==========\n",
      "========== IN EPOCH 56 the GT metric is [[6.45831  9.718622]] ==========\n",
      "========== Now this is EPOCH 57 ==========\n",
      "========== IN EPOCH 57 the total loss is 13136.893264770508 ==========\n",
      "========== IN EPOCH 57 the vali NLL loss is 6.291494369506836 ==========\n",
      "========== IN EPOCH 57 the GT metric is [[6.4583097 9.718622 ]] ==========\n",
      "========== Now this is EPOCH 58 ==========\n",
      "========== IN EPOCH 58 the total loss is 13149.889556884766 ==========\n",
      "========== IN EPOCH 58 the vali NLL loss is 6.309292316436768 ==========\n",
      "========== IN EPOCH 58 the GT metric is [[6.45831  9.718623]] ==========\n",
      "========== Now this is EPOCH 59 ==========\n",
      "========== IN EPOCH 59 the total loss is 12999.30599975586 ==========\n",
      "========== IN EPOCH 59 the vali NLL loss is 6.279691219329834 ==========\n",
      "========== IN EPOCH 59 the GT metric is [[6.45831  9.718623]] ==========\n",
      "Total training time when epoch= *60* is *838.5887277126312 *s\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# filename = \"../log_file.txt\"\n",
    "# f = open(filename,'w')\n",
    "total_train_step = 0\n",
    "EPOCH_NUM = 60\n",
    "\n",
    "train_start_time = time.time()\n",
    "mlp.train()\n",
    "\n",
    "for epoch in range(EPOCH_NUM):\n",
    "    # # Save the net structure\n",
    "    # net_file_name = \"net_before_epoch\"+str(epoch)+\".pth\"\n",
    "    # net_path = opt.net_root_path + net_file_name\n",
    "    # torch.save(mlp.state_dict(), net_path)\n",
    "\n",
    "    epoch_train_loss = 0\n",
    "    # plot_conv(writer,mlp,epoch)\n",
    "    print(f\"========== Now this is EPOCH {epoch} ==========\")\n",
    "    for batch_id,data in enumerate(train_loader):\n",
    "\n",
    "        input_data, _, target_loss, setting, _ = data\n",
    "        # print(f\"---- {batch_id} batch----\")\n",
    "        # Do the inference\n",
    "        input_data = input_data.to(device)\n",
    "        target_loss = target_loss.to(device)\n",
    "        pi, mu, sigma = mlp(input_data)\n",
    "\n",
    "        # Save the params\n",
    "        # params = list(mlp.named_parameters())\n",
    "\n",
    "        # Cal the MLE loss and draw the distrb.\n",
    "        # loss = loss_fn_v2(pi, mu, sigma, target, opt.N_gaussians,device)\n",
    "        loss = loss_fn_v2(pi, mu, sigma, target_loss, opt.N_gaussians,opt.TARGET, opt.SAFETY, device)\n",
    "        # loss = loss_fn_v3(pi, mu, sigma, target_da, opt.N_gaussians,device)\n",
    "        # loss = loss_fn_v5(pi, mu, sigma, target_da, opt.N_gaussians,device)\n",
    "\n",
    "        # Cal the CDF loss and draw the distrb.\n",
    "        # loss = loss_fn_cdf(pi, mu, sigma, target, opt.N_gaussians)\n",
    "\n",
    "        # Cal the CE or KL loss and draw the distrb.\n",
    "        # loss  = loss_fn_CE(pi, mu, sigma, target, opt.N_gaussians)\n",
    "\n",
    "        # Cal the WD loss and draw the distrb.\n",
    "        # loss  = loss_fn_WD(pi, mu, sigma, target, opt.N_gaussians)\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "        draw_loss(total_train_step, loss.item(),win_train_loss_str)\n",
    "\n",
    "        # Optim\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        ########### before step() ###############\n",
    "\n",
    "        # for name, parms in mlp.named_parameters():\n",
    "        #     print(\"=====before step()=====\")\n",
    "        #     print('-->name:', name)\n",
    "        #     print('-->params:', parms.data)\n",
    "        #     # print('-->grad_requirs:',parms.requires_grad)\n",
    "        #     print('-->grad_value:',parms.grad)\n",
    "        #     print(\"===\")\n",
    "\n",
    "        ######\n",
    "\n",
    "        # 反向传播时检测是否有异常值，定位code\n",
    "        # with torch.autograd.detect_anomaly():\n",
    "        #     loss.backward()\n",
    "\n",
    "        ######\n",
    "        # get_dot = register_hooks(loss)\n",
    "        # dot = get_dot()\n",
    "        # dot.save('tmp.dot')   # to get .dot\n",
    "        # dot.render('tmp')     # to get pdf\n",
    "\n",
    "        ######\n",
    "        # Print grad check\n",
    "        # v_n = []    # name\n",
    "        # v_v = []    # value\n",
    "        # v_g = []    # grad\n",
    "        # for name, parameter in mlp.named_parameters():\n",
    "        #     v_n.append(name)\n",
    "        #     v_v.append(parameter.detach().cpu().numpy() if parameter is not None else [0])\n",
    "        #     v_g.append(parameter.grad.detach().cpu().numpy() if parameter.grad is not None else [0])\n",
    "        # for i in range(len(v_n)):\n",
    "        #     # if np.max(v_v[i]).item() - np.min(v_v[i]).item() < 1e-6:\n",
    "        #     #     color = bcolors.FAIL + '*'\n",
    "        #     if np.isnan(v_g[i]).any() or np.isnan(v_v[i]).any():\n",
    "        #         color = bcolors.FAIL + '*'\n",
    "        #     else:\n",
    "        #         color = bcolors.OKGREEN + ' '\n",
    "        #     print('%svalue %s: %.3e ~ %.3e' % (color, v_n[i], np.min(v_v[i]).item(), np.max(v_v[i]).item()))\n",
    "        #     print('%sgrad  %s: %.3e ~ %.3e' % (color, v_n[i], np.min(v_g[i]).item(), np.max(v_g[i]).item()))\n",
    "\n",
    "        ######\n",
    "        optimizer.step()\n",
    "\n",
    "        # for name, parms in mlp.named_parameters():\n",
    "        #     print(\"=====After step()=====\")\n",
    "        #     print('-->name:', name)\n",
    "        #     print('-->params:', parms.data)\n",
    "        #     # print('-->grad_requirs:',parms.requires_grad)\n",
    "        #     print('-->grad_value:',parms.grad)\n",
    "        #     print(\"===\")\n",
    "\n",
    "        # Only draw the 1st result in a training batch (5 in total)\n",
    "        # if total_train_step % 20 == 0:\n",
    "        #     with torch.no_grad():\n",
    "        #         draw_mdn_2(pi[0,:].clone().detach(), mu[0,:].clone().detach(), sigma[0,:].clone().detach(), target[0].clone().detach(), input_data[0].clone().detach(),total_train_step, opt.N_gaussians)\n",
    "        #         # draw_mdn_cdf(pi[0,:].detach(), m[0], target[0].detach(), total_train_step, loss_list[0],opt.N_gaussians)\n",
    "        total_train_step += 1\n",
    "\n",
    "    ########### Do validation\n",
    "    with torch.no_grad():\n",
    "        mlp.eval()\n",
    "        total_vali_metric = 0\n",
    "        GT_metric = torch.tensor([0.,0.]).reshape(1,2)\n",
    "\n",
    "        for vali_batch_id, vali_data in enumerate(val_loader):\n",
    "            vali_input_data, vali_target, _, vali_setting , vali_metric = vali_data\n",
    "            vali_input_data = vali_input_data.to(device)\n",
    "            vali_target = vali_target.to(device)\n",
    "            # vali_metric = vali_metric\n",
    "\n",
    "            vali_pi, vali_mu, vali_sigma = mlp(vali_input_data)\n",
    "\n",
    "            # Compute the error/ metric\n",
    "            vali_nll = cal_metric(vali_pi, vali_mu, vali_sigma, vali_target, opt.N_gaussians, vali_setting,opt.MIN_LOSS,device)\n",
    "            total_vali_metric += vali_nll\n",
    "\n",
    "            # Sum up NLL of all vali data\n",
    "            GT_metric += torch.sum(vali_metric,dim=0)\n",
    "        # total_vali_metric = total_vali_metric/len(val_idx)\n",
    "\n",
    "        # Get metric of GT model\n",
    "        GT_metric = GT_metric/len(val_idx)\n",
    "        total_vali_metric = total_vali_metric/len(val_idx)\n",
    "\n",
    "        draw_metric(epoch, total_vali_metric.cpu(), GT_metric)\n",
    "        mlp.train()\n",
    "\n",
    "    # Plot the loss in this EPOCH\n",
    "    print(f\"========== IN EPOCH {epoch} the total loss is {epoch_train_loss} ==========\")\n",
    "    print(f\"========== IN EPOCH {epoch} the vali NLL loss is {total_vali_metric.detach().cpu().numpy()} ==========\")\n",
    "    print(f\"========== IN EPOCH {epoch} the GT metric is {GT_metric.detach().cpu().numpy()} ==========\")\n",
    "\n",
    "    # # Save the net structure\n",
    "    # net_file_name = \"net_after_epoch\"+str(epoch)+\".pth\"\n",
    "    # net_path = opt.net_root_path + net_file_name\n",
    "    # torch.save(mlp.state_dict(), net_path)\n",
    "\n",
    "    # Plot loss of this EPOCH\n",
    "    draw_loss(epoch, epoch_train_loss,win_train_epoch_loss_str)\n",
    "    # writer.add_scalars(\"Loss of EPOCH\",{\"Train\":epoch_train_loss},epoch)\n",
    "    scheduler.step()\n",
    "\n",
    "    # Record the weight\n",
    "    plot_conv_weight(writer,mlp,epoch,opt.tag_str)\n",
    "    plot_mu_weight(writer,mlp,epoch,opt.tag_str)\n",
    "    plot_pi_weight(writer,mlp,epoch,opt.tag_str)\n",
    "    plot_sigma_weight(writer,mlp,epoch,opt.tag_str)\n",
    "\n",
    "# f.close()\n",
    "train_end_time = time.time()\n",
    "print(f\"Total training time when epoch= *{EPOCH_NUM}* is *{train_end_time-train_start_time} *s\")\n",
    "writer.close()\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- save"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "outputs": [],
   "source": [
    "model_path_MLE = \"mlp_train=300_MLE.pth\"\n",
    "# model_path_WD = \"mlp_train=300_WD_epoch=80.pth\"\n",
    "model_path_WD = \"mlp_train=300_WD.pth\"\n",
    "model_path_LSE = \"mlp_train=300_LSE.pth\"\n",
    "torch.save(mlp.state_dict(), model_path_MLE)\n",
    "# torch.save(mlp.state_dict(), model_path_MLE)\n",
    "\n",
    "# viz.delete_env(\"001_test\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Gtmp = pgv.AGraph('tmp.dot')\n",
    "# G = nx.Graph(Gtmp)\n",
    "# nx.draw(G)\n",
    "# # plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "$\\mathcal{L}(y \\vert x) = - \\log\\bigg\\{\\sum_{k=1}^K \\pi_k(x)  \\mathcal{N}\\big(y \\vert \\mu_k(x), \\Sigma_k(x)\\big)\\bigg\\}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Load and test\n",
    "## 7.1 small batch\n",
    "- 对某个小batch画图，batch size可以=1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "small_batch_size = 1\n",
    "\n",
    "small_test_idx = train_idx[3:4]\n",
    "small_test_loader = DataLoader(dataset = dataset,batch_size = small_batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(small_test_idx),collate_fn = my_collate_fn)\n",
    "small_test_idx"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot\n",
    "test_env = \"002\"\n",
    "def draw_mdn_test(pi, m, target, input, epoch, idx,opt.N_gaussians,flag):\n",
    "    # 只画一个batch中的第一个\n",
    "\n",
    "    # The target distrb.\n",
    "    target = torch.unique(target,dim=0)    # drop the duplicate\n",
    "    n_target = target[:,0]\n",
    "    non_zero_idx = torch.nonzero(n_target)\n",
    "    n = n_target[non_zero_idx]\n",
    "\n",
    "    p_target = target[:,1]\n",
    "    p = p_target[non_zero_idx]\n",
    "    max_n = max(n).item()               # 横轴长度\n",
    "\n",
    "    # The predicted distrb.\n",
    "    x_0 = torch.arange(1,max_n).to(device=device)\n",
    "    x = torch.repeat_interleave(x_0.unsqueeze(dim=1), repeats=opt.N_gaussians, dim=1)\n",
    "    y = torch.exp(m.log_prob(x)).to(device=device)                          # y:多条高斯曲线; y_pred: 一条GMM曲线\n",
    "    # y_pred = torch.unsqueeze(torch.sum(pi*y,dim=1),dim=1)                 # 维度相等才能cat\n",
    "    y_pred = torch.sum(pi*y,dim=1)\n",
    "\n",
    "    # The input distrb.\n",
    "    input_data = input[0:2,:]\n",
    "    x_input = torch.arange(1,input_data.shape[1]+1).to(device=device)\n",
    "    y_input_0 = input_data[0,:].to(device=device)\n",
    "    y_input_1 = input_data[1,:].to(device=device)\n",
    "\n",
    "    # Init\n",
    "    win_str = flag+ \" epoch=\"+str(epoch)+\" | idx=\"+str(idx)\n",
    "    title_str = \"Distrb. \"+win_str\n",
    "    viz.line(X = [0.],Y = [0.], env=test_env, win=win_str, opts= dict(title=title_str))\n",
    "\n",
    "    # Visdom本身不能把hist和line画在一个window中\n",
    "    # 如果想画一起只能是两条line\n",
    "    # Plot y_target\n",
    "    # viz.histogram(X = n, env=\"001\", win=win_str,\n",
    "    #         opts= dict(title=title_str))\n",
    "    viz.line(X = n,Y= p, env=test_env, win=win_str, update=\"append\", name='target',\n",
    "            opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "    # Plot y_pred\n",
    "    viz.line(X = x_0,Y= y_pred, env=test_env, win=win_str, update=\"append\", name='pred',\n",
    "            opts= dict(title=title_str))\n",
    "\n",
    "    # Plot y_input\n",
    "    viz.line(X = x_input,Y= y_input_0, env=test_env, win=win_str, update=\"append\", name='GT-1', opts= dict(title=title_str))\n",
    "    viz.line(X = x_input,Y= y_input_1, env=test_env, win=win_str, update=\"append\", name='GT-2', opts= dict(title=title_str))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(EPOCH_NUM):\n",
    "\n",
    "    net_file_name_0 = \"net_before_epoch\"+str(epoch)+\".pth\"\n",
    "    net_file_name_1 = \"net_after_epoch\"+str(epoch)+\".pth\"\n",
    "    net_path0 = opt.net_root_path + net_file_name_0\n",
    "    net_path1 = opt.net_root_path + net_file_name_1\n",
    "\n",
    "    # Load the model\n",
    "    mlp0 = MLP(opt.N_gaussians)\n",
    "    mlp1 = MLP(opt.N_gaussians)\n",
    "    mlp0.load_state_dict(torch.load(net_path0))\n",
    "    mlp1.load_state_dict(torch.load(net_path1))\n",
    "    mlp0.to(device)\n",
    "    mlp1.to(device)\n",
    "\n",
    "    # 其实每个epoch只会执行一个batch\n",
    "    for batch_id,data in enumerate(small_test_loader):\n",
    "\n",
    "        # Predict\n",
    "        input_data, target = data\n",
    "        input_data = input_data.to(device)\n",
    "        target = target.to(device)\n",
    "        pi0, mu0, sigma0 = mlp0(input_data)\n",
    "        pi1, mu1, sigma1 = mlp1(input_data)\n",
    "        # Get the mdn\n",
    "        _,m0  = loss_preparation(pi0.detach(), mu0.detach(), sigma0.detach(), target.detach())\n",
    "        _,m1  = loss_preparation(pi1.detach(), mu1.detach(), sigma1.detach(), target.detach())\n",
    "\n",
    "        # Plot for 'small_batch_size' times\n",
    "        for j in range(small_batch_size):\n",
    "            draw_mdn_test(pi0[j,:].detach(), m0[j], target[j].detach(), input_data[j].detach(), epoch, j,opt.N_gaussians,flag=\"before\")\n",
    "            draw_mdn_test(pi1[j,:].detach(), m1[j], target[j].detach(), input_data[j].detach(), epoch, j,opt.N_gaussians,flag=\"after\")\n",
    "\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7.2 Test dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "# model_path_WD_2 = \"mlp_train=300_WD_epoch=80.pth\"\n",
    "model_path_WD = \"mlp_train=300_WD.pth\"\n",
    "model_path_MLE = \"mlp_train=300_MLE.pth\"\n",
    "\n",
    "\n",
    "mlp_1 = MLP_1_3(opt.N_gaussians)\n",
    "mlp_2 = MLP_1_3(opt.N_gaussians)\n",
    "\n",
    "model_data_1 = torch.load(model_path_MLE)\n",
    "# model_data_1 = torch.load(model_path_WD_2)\n",
    "model_data_2 = torch.load(model_path_WD)\n",
    "\n",
    "mlp_1.load_state_dict(model_data_1)\n",
    "mlp_2.load_state_dict(model_data_2)\n",
    "\n",
    "mlp_1.to(device)\n",
    "mlp_2.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total_test_loss_1 = 0\n",
    "total_test_loss_2 = 0\n",
    "i = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_id,data in enumerate(test_loader):\n",
    "            input_data, target = data\n",
    "            input_data = input_data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            pi_1, mu_1, sigma_1 = mlp_1(input_data)\n",
    "            pi_2, mu_2, sigma_2 = mlp_2(input_data)\n",
    "\n",
    "            draw_mdn_3(pi_1, mu_1, sigma_1, pi_2, mu_2, sigma_2, target, input_data,i, opt.N_gaussians)\n",
    "\n",
    "            loss_1 = loss_fn_v2(pi_1, mu_1, sigma_1, target, opt.N_gaussians)\n",
    "            loss_2 = loss_fn_v2(pi_2, mu_2, sigma_2, target, opt.N_gaussians)\n",
    "\n",
    "            total_test_loss_1 += loss_1.item()\n",
    "            total_test_loss_2 += loss_2.item()\n",
    "\n",
    "            i += 1\n",
    "    # 这里print的loss可以视作平均到每个sample上的loss\n",
    "    print(\"total_test_loss_1 MLE:\",total_test_loss_1/len(test_loader))\n",
    "    print(\"total_test_loss_2 WD :\",total_test_loss_2/len(test_loader))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8. Tuning pipeline\n",
    "## 8.1 training wrapper"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Wrap整个training 过程\n",
    "def train_cifar(config, checkpoint_dir=None):\n",
    "\n",
    "    net = MLP_1_1(opt.N_gaussians)\n",
    "\n",
    "    EPOCH_NUM = 100\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "\n",
    "    trainloader = DataLoader(dataset = dataset,batch_size = config[\"batch_size\"], shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(train_idx), collate_fn = my_collate_fn)\n",
    "    valloader = DataLoader(dataset = dataset,batch_size = config[\"batch_size\"], shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(val_idx),collate_fn = my_collate_fn)\n",
    "    testloader = DataLoader(dataset = dataset,batch_size = config[\"batch_size\"], shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(test_idx),collate_fn = my_collate_fn)\n",
    "\n",
    "    scheduler  = torch.optim.lr_scheduler.StepLR(optimizer,step_size=opt.StepLR_step_size,gamma=opt.StepLR_gamma)\n",
    "\n",
    "    # 设置checkpoint\n",
    "    if checkpoint_dir:\n",
    "        model_state, optimizer_state = torch.load(\n",
    "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        net.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    mlp.train()\n",
    "    total_train_step = 0\n",
    "    for epoch in range(EPOCH_NUM):\n",
    "        epoch_train_loss = 0\n",
    "        print(f\"========== Now this is EPOCH {epoch} ==========\")\n",
    "        for batch_id,data in enumerate(trainloader):\n",
    "\n",
    "            input_data, _, target_loss, setting, _ = data\n",
    "            input_data = input_data.to(device)\n",
    "            target_loss = target_loss.to(device)\n",
    "            pi, mu, sigma = mlp(input_data)\n",
    "\n",
    "            # Cal the MLE loss and draw the distrb.\n",
    "            loss = loss_fn_v2(pi, mu, sigma, target_loss, opt.N_gaussians,opt.TARGET, opt.SAFETY, device)\n",
    "\n",
    "            epoch_train_loss += loss.item()\n",
    "            draw_loss(total_train_step, loss.item(),win_train_loss_str)\n",
    "\n",
    "            # Optim\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_step += 1\n",
    "\n",
    "        ########### Do validation\n",
    "        with torch.no_grad():\n",
    "            mlp.eval()\n",
    "            total_vali_metric = 0\n",
    "            GT_metric = torch.tensor([0.,0.]).reshape(1,2)\n",
    "\n",
    "            for vali_batch_id, vali_data in enumerate(valloader):\n",
    "                vali_input_data, vali_target, _, vali_setting , vali_metric = vali_data\n",
    "                vali_input_data = vali_input_data.to(device)\n",
    "                vali_target = vali_target.to(device)\n",
    "                # vali_metric = vali_metric\n",
    "\n",
    "                vali_pi, vali_mu, vali_sigma = mlp(vali_input_data)\n",
    "\n",
    "                # Compute the error/ metric\n",
    "                vali_nll = cal_metric(vali_pi, vali_mu, vali_sigma, vali_target, opt.N_gaussians, vali_setting,opt.MIN_LOSS,device)\n",
    "                total_vali_metric += vali_nll\n",
    "\n",
    "                # Sum up NLL of all vali data\n",
    "                GT_metric += torch.sum(vali_metric,dim=0)\n",
    "\n",
    "            # Get metric of GT model\n",
    "            GT_metric = GT_metric/len(val_idx)\n",
    "            total_vali_metric = total_vali_metric/len(val_idx)\n",
    "\n",
    "            draw_metric(epoch, total_vali_metric.cpu(), GT_metric)\n",
    "            mlp.train()\n",
    "\n",
    "        # Plot loss of this EPOCH\n",
    "        draw_loss(epoch, epoch_train_loss,win_train_epoch_loss_str)\n",
    "        # writer.add_scalars(\"Loss of EPOCH\",{\"Train\":epoch_train_loss},epoch)\n",
    "        scheduler.step()\n",
    "\n",
    "        # Record the weight\n",
    "        if epoch % 3 == 0:\n",
    "            plot_conv_weight(writer,mlp,epoch,opt.tag_str)\n",
    "            plot_mu_weight(writer,mlp,epoch,opt.tag_str)\n",
    "            plot_pi_weight(writer,mlp,epoch,opt.tag_str)\n",
    "            plot_sigma_weight(writer,mlp,epoch,opt.tag_str)\n",
    "\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "        tune.report(loss=(total_vali_metric), GT_metric = GT_metric)\n",
    "    print(\"Finished Training\")\n",
    "    writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8.2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=2):\n",
    "\n",
    "    # 要调的参数放这里，其他的在opt里\n",
    "    config = {\n",
    "        \"l1\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
    "        \"l2\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"learning_rate\" : tune.grid_search([1e-1,5e-2,1e-2,5e-3,1e-3]),\n",
    "        \"lr_for_pi\" : tune.grid_search([1e-1,5e-2,1e-2,5e-3]),   # 给pi单独设置learning rate\n",
    "        \"lr_for_mu\" : tune.grid_search([1e-1,5e-2,1e-2,5e-3]),   # 给mu单独设置learning rate\n",
    "        \"batch_size\": tune.grid_search([20, 30, 40, 50, 60])\n",
    "    }\n",
    "\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "    reporter = CLIReporter(\n",
    "        # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n",
    "        metric_columns=[\"loss\", \"GT_metric\", \"training_iteration\"])\n",
    "\n",
    "    result = tune.run(\n",
    "        partial(train_cifar, checkpoint_dir = \"/checkpoint\"),\n",
    "        resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter)\n",
    "\n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_trial.last_result[\"accuracy\"]))\n",
    "\n",
    "    best_trained_model = MLP_1_1(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if gpus_per_trial > 1:\n",
    "            best_trained_model = nn.DataParallel(best_trained_model)\n",
    "    best_trained_model.to(device)\n",
    "\n",
    "    best_checkpoint_dir = best_trial.checkpoint.value\n",
    "    model_state, optimizer_state = torch.load(os.path.join(\n",
    "        best_checkpoint_dir, \"checkpoint\"))\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "    test_acc = test_accuracy(best_trained_model, device)\n",
    "    print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
    "\n",
    "\n",
    "main(num_samples=10, max_num_epochs=10, gpus_per_trial=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
