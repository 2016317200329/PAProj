{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2022/6/7 15:17\n",
    "# @Author  : Wang Yujia\n",
    "# @File    : mlp.ipynb\n",
    "# @Description :\n",
    "# @TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0. what for\n",
    "1. loss部分逻辑重写，添加`no grad`\n",
    "2. 添加hooks\n",
    "3. 使用pad_seq函数取代自己写的pad函数\n",
    "4. 效果：可以train虽然loss不下降，不会出现NaN的问题"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Preparations\n",
    "## 1.1 global settings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# nums of Gaussian kernels\n",
    "N_gaussians = 3\n",
    "\n",
    "# dataset划分\n",
    "batch_size = 30\n",
    "train_pct = 0.7\n",
    "vali_pct = 0.2\n",
    "test_pct = 0.1\n",
    "\n",
    "# train and optim.\n",
    "learning_rate = 1e-2\n",
    "lr_for_mu = 1e-2   # 给mu单独设置learning rate\n",
    "total_train_step = 0\n",
    "total_test_step = 0\n",
    "EPOCH_NUM = 5\n",
    "MIN_LOSS = 1e-8\n",
    "\n",
    "# Range,【一般用不到】\n",
    "a = 0\n",
    "b = 255\n",
    "\n",
    "# training data的粒度，画图会使用到\n",
    "SCALE = 1\n",
    "# target data是target_5时\n",
    "TARGET = 5\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import torch.utils.data\n",
    "from mydataset import *\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from tensorboardX import SummaryWriter\n",
    "from visdom import Visdom\n",
    "from graphviz import Digraph\n",
    "from torchviz import make_dot\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pygraphviz as pgv\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import geomloss\n",
    "import time\n",
    "\n",
    "import math\n",
    "import scipy\n",
    "from torch.nn import KLDivLoss\n",
    "from torch.autograd.gradcheck import gradcheck\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 the data path\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [],
   "source": [
    "# Training data\n",
    "# train_path = r\"../data/train_100\"\n",
    "# train_path = r\"../data/train_60\"\n",
    "train_path = r\"../data/train_300_v1\"\n",
    "\n",
    "# Target data\n",
    "# target_path = r\"../data/targets\"\n",
    "target_path = r\"../data/targets_5\"\n",
    "# target_path = r\"../data/targets_5_cdf\"\n",
    "# data keys\n",
    "data_key_path = \"../data/target_datakey.csv\"\n",
    "\n",
    "\n",
    "# Net path\n",
    "net_root_path = \"../net_saved/\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Dataset and Dataloader\n",
    "1. DataLoader中的shuffer=True表示在每一次epoch中都打乱所有数据的顺序，然后以batch为单位从头到尾按顺序取用数据。这样的结果就是不同epoch中的数据都是乱序的,设置随机种子的作用就是让你的每一次训练都乱的一样，\n",
    "\n",
    "## 2.1 Dataset and spliting\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 设置随机数种子"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup_seed(7)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 读取data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "dataset = myDataset(train_path, target_path, data_key_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 产生index的乱序排列"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 807  305  455  939  508  594  835 1082  598 1102   46  761  841  141\n",
      "  407  334  253  500  734  936  698  446  907 1087 1009  140  463  547\n",
      " 1155  856   34 1156  703 1121  751  587 1100  509  473  128  788   97\n",
      "  471  385 1085  525  679 1135  284 1146  186  318 1088 1113    9  457\n",
      " 1047  451  569 1120 1074  326  377  809  109   98  620 1194  825  828\n",
      "   83  113  556  674  568  853  351  558   54  656  804 1149  101  344\n",
      "  851  544  955   40 1021  489  626  664  657  868 1169  151  179 1130\n",
      "  564  713  743 1014  966  861 1132  146  610  408  662  551  172 1020\n",
      "  982 1148  431  517  270  858  170  374  816  618  205   17   53 1003\n",
      "  263  857  716  843  498  228  339  725  752  278  649 1017  108  642\n",
      " 1195 1174   99  530  632  888  189  961  358 1078  663  757 1051  204\n",
      "  409  283  562   23  619  216  474  921  950 1162  123  785  769  621\n",
      "  262  586 1178  541  795   70 1189  396  171  845  168 1125  361  224\n",
      "  125  706  164  231  264   42  746  872  998  132 1035  430   38  522\n",
      "  880  325  285  367  870  418   13  166  513  901  617  740  680 1126\n",
      "   20  372   71 1038  355   60  220  110    4 1115  572  415  336  316\n",
      "  449  412 1066  768 1055  704  869   94   95 1163 1145  648  884  885\n",
      " 1170  848   58  207  306 1067  729   24  644  633  252  832  244   25\n",
      " 1094   63  222 1070  996 1063  130  153 1180  922  399  997   75  894\n",
      " 1048  308  280  934  721  199  820  134  289  259 1159    3  510  492\n",
      "  196  565  563   26  913  483 1141  653   61  623  984  652  609  739\n",
      "  824 1018  891  467  842    8  503  714 1015  447  783  992   74 1184\n",
      "   57  978  484  297 1039  217  681 1027  960  350 1129 1045 1183  906\n",
      "  813  660  266    6  379  526  611  138  163  701 1031 1050   43  733\n",
      "  720  319 1158 1153  877  294  432  531  578  523   19  426  830  927\n",
      "  781 1033 1013  420  137  488  185  920  702 1134 1124  454  327  317\n",
      "  799  655  878  699  981  806  953  750  148  591  915  731  937  365\n",
      "  557  397 1173  779 1185  459  232  389 1147 1011  158  315  256  863\n",
      "   96 1157  445  384  511  817  328  314  357  347  452  298  899  887\n",
      " 1034  839  778  747  249  766   15  159  696  585  360   48  478  330\n",
      "  893  112  338  307  528  422 1142  971  506  504 1049  791  395  669\n",
      "  540  127 1160  902  507  980  237  167  602  983   80  414  640  300\n",
      "  977  324 1117  215 1058   30  242  697  860  951  209  671  742 1072\n",
      "  435   27 1060  518  122 1086  462 1028 1118  603  929  732  722  493\n",
      "  202  403 1089 1179  362  271  466  641 1061  744  272  277 1177   12\n",
      "  597  668  210  651  952  147  442  299  115  375  635  767  748 1080\n",
      "  755  177  472  685   51  810 1079  571  388   22  650  965  423  938\n",
      " 1112  643 1193  142   50  666 1143   81  482  321  948  687 1138   66\n",
      "  909  534  738  417 1005   64  895  886  918  491  700  790   65  968\n",
      "  627  999  296  419  438  261  837  281  631  677  188  881  251  694\n",
      "  514  590  487  273  943 1073  448  411 1131  933 1191  935  149  218\n",
      "  239  718  192  191  945  754  665  705  595  926  708  582 1046  323\n",
      " 1026 1071  658  437  770  833  341    2  229 1098 1152   76 1103  371\n",
      "  180  889  577  736  758 1019 1105 1010  589 1167  812  956  882   37\n",
      "  353 1095  236  928  499  553   55  302  286  800  219  789  387  737\n",
      "  390  352  193  235 1110  480  567  116 1057  957 1106  890  777  120\n",
      "  815  990  773   62  667  453  515  570  223  613  221  673  794  465\n",
      "   52 1093  485 1140 1053  155  912  600 1023  248  441  975 1164  542\n",
      "  117  724  628  712  916  291 1041  381  954  200  596  976 1137 1187\n",
      "  691  505  348 1190  340  615  154 1104  879  995  287  322  819  818\n",
      "  796   82  793  464  797  756 1168  771  333   10  470   32  782  413\n",
      "  580  382  930  760 1133  840 1139  378   89  792  601  213  332  133\n",
      "  346  855    7  941  320 1044  187  972  552  304   31  831  684 1065\n",
      "  240  993  174  245   56  630  583 1025  145   90 1114  392  682  310\n",
      "  359   93  947 1024  376  243 1076   87  214  444  173  425 1040  118\n",
      "  119 1151  905    0  394  401  404  836 1091  162  114  670  150  798\n",
      "  875 1064  368  543  477 1128  532  625  967  512  801   72  258  624\n",
      "  234  135  616  516  866  802  592 1056  501  538  897  728  581  343\n",
      "  925 1002  293  364  883  659   14  686   86  370  165  131  335   59\n",
      "  709  676  354  959  424  292   41  329  614  429   79]\n"
     ]
    }
   ],
   "source": [
    "shuffled_indices = np.random.permutation(dataset.__len__())\n",
    "\n",
    "train_idx = shuffled_indices[:int(train_pct*dataset.__len__())]\n",
    "# train_idx = shuffled_indices\n",
    "tmp = int((train_pct+vali_pct)*dataset.__len__())\n",
    "val_idx = shuffled_indices[int(train_pct*dataset.__len__()):tmp]\n",
    "\n",
    "test_idx = shuffled_indices[tmp:]\n",
    "print(train_idx)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 根据这个乱序排列抽取dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Dataloader and collating\n",
    "1. 主要是对label数据进行collate\n",
    "    - 按照batch中的最大target data长度进行padding，padding with 0\n",
    "2. 返回的结果多一个batch dim,比如下面的`5`\n",
    "    - After collating:\n",
    "        - `torch.Size([5, 3, 300]),torch.Size([5, 87, 2])`\n",
    "        - `87`是最长的targets data长度"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "def test_pad_sequence():\n",
    "    seq1 = torch.tensor([[ 2., 0.04761905], [3., 0.14285714], [4., 0.04761905]])\n",
    "    seq2 = torch.tensor([[ 1., 0.04761905]])\n",
    "    seq3 = torch.tensor([[ 3., 0.14285714], [4., 0.04761905]])\n",
    "    ls = list((seq1,seq2,seq3))\n",
    "    ls_length = torch.tensor([3,1,2])\n",
    "    ans = pad_sequence(ls,batch_first=True)\n",
    "    print(ans)\n",
    "    # seq_pak = pack_padded_sequence(ans,ls_length,batch_first=True,enforce_sorted=False)\n",
    "    # seq_unpacked, lens_unpacked = pad_packed_sequence(seq_pak, batch_first=True)\n",
    "    # lens_unpacked"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.1 No rescale"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "def my_collate_fn(data):\n",
    "# 这里的data是一个list， list的元素是元组: (self.data, self.label)\n",
    "# collate_fn的作用是把[(data, label),(data, label)...]转化成([data, data...],[label,label...])\n",
    "# 假设self.data的一个data的shape为(channels, length), 每一个channel的length相等,\n",
    "# data[索引到index(batch)][索引到data或者label][索引到channel]\n",
    "\n",
    "    data_list = []\n",
    "    target_list = []\n",
    "    data_len = len(data)      # 读进来的data batch的大小\n",
    "\n",
    "    batch = 0\n",
    "\n",
    "    while batch < data_len:\n",
    "        data_list.append(torch.tensor(data[batch][0]))\n",
    "        target_list.append(torch.tensor(data[batch][1]))\n",
    "        batch += 1\n",
    "\n",
    "    # Pad target data with zeros\n",
    "    target_padded = pad_sequence(target_list,batch_first=True)\n",
    "\n",
    "    data_tensor = torch.stack(data_list).float()\n",
    "    target_tensor = target_padded.float()\n",
    "\n",
    "    return data_tensor, target_tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.2 Rescale"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "def my_collate_fn_rescale(data):\n",
    "# 这里的data是一个list， list的元素是元组: (self.data, self.label)\n",
    "# collate_fn的作用是把[(data, label),(data, label)...]转化成([data, data...],[label,label...])\n",
    "# 假设self.data的一个data的shape为(channels, length), 每一个channel的length相等,\n",
    "# data[索引到index(batch)][索引到data或者label][索引到channel]\n",
    "\n",
    "    data_list = []\n",
    "    target_list = []\n",
    "    data_len = len(data)      # 读进来的data batch的大小\n",
    "\n",
    "    batch = 0\n",
    "\n",
    "    while batch < data_len:\n",
    "        data_df = pd.DataFrame(data[batch][0])\n",
    "        data_GT = data_df.iloc[0:2,:]\n",
    "\n",
    "        # Get the max/min in each chanel and broadcast it by hand\n",
    "        data_max = np.tile(pd.DataFrame(np.max(data_GT,axis=1)), (1, data_GT.shape[1]))\n",
    "        data_min = np.tile(pd.DataFrame(np.min(data_GT,axis=1)), (1, data_GT.shape[1]))\n",
    "        # Rescale\n",
    "        data_rescaled = a + (data_GT-data_min)*(b-a) / (data_max - data_min)\n",
    "        data_df.iloc[0:2,:] = data_rescaled\n",
    "\n",
    "        data_list.append(torch.tensor(np.array(data_df,dtype=float)))\n",
    "        # print(\"After scale:\",data_df)\n",
    "        target_list.append(torch.tensor(data[batch][1]))\n",
    "        batch += 1\n",
    "\n",
    "    # Pad target data with zeros\n",
    "    target_padded = pad_sequence(target_list,batch_first=True)\n",
    "\n",
    "    data_tensor = torch.stack(data_list).float()\n",
    "    target_tensor = target_padded.float()\n",
    "\n",
    "    return data_tensor, target_tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.3 log1p"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "def my_collate_fn_log1p(data):\n",
    "# 这里的data是一个list， list的元素是元组: (self.data, self.label)\n",
    "# collate_fn的作用是把[(data, label),(data, label)...]转化成([data, data...],[label,label...])\n",
    "# 假设self.data的一个data的shape为(channels, length), 每一个channel的length相等,\n",
    "# data[索引到index(batch)][索引到data或者label][索引到channel]\n",
    "\n",
    "    data_list = []\n",
    "    target_list = []\n",
    "    data_len = len(data)      # 读进来的data batch的大小\n",
    "\n",
    "    batch = 0\n",
    "\n",
    "    while batch < data_len:\n",
    "        data_df = pd.DataFrame(data[batch][0])\n",
    "        data_GT = data_df.iloc[0:2,:]\n",
    "\n",
    "        data_log1p = -np.log(data_GT+MIN_LOSS)\n",
    "        data_df.iloc[0:2,:] = data_log1p\n",
    "\n",
    "        data_list.append(torch.tensor(np.array(data_df,dtype=float)))\n",
    "        target_list.append(torch.tensor(data[batch][1]))\n",
    "        batch += 1\n",
    "\n",
    "    # Pad target data with zeros\n",
    "    target_padded = pad_sequence(target_list,batch_first=True)\n",
    "\n",
    "    data_tensor = torch.stack(data_list).float()\n",
    "    target_tensor = target_padded.float()\n",
    "\n",
    "    return data_tensor, target_tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.4 DataLoader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = dataset,batch_size = batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(train_idx), collate_fn = my_collate_fn)\n",
    "\n",
    "val_loader = DataLoader(dataset = dataset,batch_size = batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(val_idx),collate_fn = my_collate_fn)\n",
    "\n",
    "# 注意test_loader的batch size\n",
    "test_loader = DataLoader(dataset = dataset,batch_size = batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(test_idx),collate_fn = my_collate_fn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. The Net and Init\n",
    "1. BatchNorm1d: The mean and std are calculated per-dimension over the mini-batches\n",
    "2."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "### BatchNorm2d测试\n",
    "def test_BN():\n",
    "    m = nn.BatchNorm2d(3, affine=False)  # affine: With Learnable Parameters or not\n",
    "    print('m:', m)\n",
    "    # The mean and std are calculated per-dimension over the mini-batches\n",
    "    input = torch.tensor([\n",
    "        [[1.,2.,3.,4.],[1.,2.,3.,4.],[-1.,-2.,-3.,-4.]],\n",
    "        [[0.,0.,0.,0.],[0.,0.,0.,0.],[0.,0.,0.,0.]]\n",
    "    ], requires_grad=True)\n",
    "\n",
    "    print('input:', input.shape)\n",
    "    input = input.unsqueeze(dim=2)\n",
    "    print('input:', input.shape)\n",
    "    output = m(input) # 归一化\n",
    "    print('output:', output.shape)\n",
    "    print('output:', output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-0.1690,  0.5071,  1.1832,  1.8593]],\n\n        [[-0.8452, -0.8452, -0.8452, -0.8452]]])"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 复现batchNorm2d在input shape为3维的情况\n",
    "input = torch.tensor([[[1.,2.,3.,4.]],[[0.,0.,0.,0.]]])\n",
    "# print(input.shape)\n",
    "# torch.mean(input),torch.var(input,unbiased = False)\n",
    "(input-torch.mean(input))/ torch.sqrt(torch.var(input,unbiased = False))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 Init, Printer, Hook\n",
    "- 设置网络初始权重: 不太work"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "class model_param_init(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        assert isinstance(model, nn.Module), 'model not a class nn.Module'\n",
    "        self.net = model\n",
    "        self.initParam()\n",
    "\n",
    "    def initParam(self):\n",
    "        for param in self.net.parameters():\n",
    "            # nn.init.zeros_(param)\n",
    "            # nn.init.ones_(param)\n",
    "            # nn.init.normal_(param, mean=0, std=1)\n",
    "            # nn.init.uniform_(param, a=0, b=1)\n",
    "            # nn.init.constant_(param, val=1)   # 将所有权重初始化为1\n",
    "            # nn.init.eye_(param)  # 只能将二维的tensor初始化为单位矩阵\n",
    "            # nn.init.xavier_uniform_(param, gain=1)  # Glorot初始化  得到的张量是从-a——a中采用的\n",
    "            # nn.init.xavier_normal_(param, gain=1)   # 得到的张量是从0-std采样的\n",
    "            nn.init.kaiming_normal_(param, a=0, mode='fan_in', nonlinearity='relu') # he初始化方法\n",
    "            # nn.init.kaiming_uniform_(param)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- print网络每层结构"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "class PrintLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if(DEBUG):\n",
    "            print(\"This layer: \")\n",
    "            print(x)      #print(x.shape)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- hook_backward_fn: 输入端的grad，输出端的grad，这里称呼的**输入与输出是站在前向传播的角度的**。如果模块有多个输入与输出的话， 其`grad_input`和`grad_output`可以是tuple类型。\n",
    "- 与forward不同的是，backward传播的时候，**不仅反向传递input和output的grad，还会传递模块Parameter的grad**：\n",
    "    - 比如fc模块，其`grad_input`是一个三元组的tuple，（对bias的梯度，对输入的梯度，对w的梯度）；\n",
    "    - conv模块`grad_input`也是一个三元组tuple，为（对输入的梯度，对w的梯度，对bias的梯度"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "# hook functions have to take these 3 input\n",
    "def hook_forward_fn(module, input, output):\n",
    "    print(\"It's forward: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"input: {input}\")\n",
    "    print(f\"output: {output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn(module, grad_input, grad_output):\n",
    "    print(\"It's backward: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn_pi(module, grad_input, grad_output):\n",
    "    print(\"It's backward in pi: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn_sigma(module, grad_input, grad_output):\n",
    "    print(\"It's backward in sigma: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn_mu(module, grad_input, grad_output):\n",
    "    print(\"It's backward in mu: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Conv结构-1\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "# # Not Sequential\n",
    "# class MLP_1_1(nn.Module):\n",
    "#     # code->generate->override methods\n",
    "#     def __init__(self, n_gaussians) -> None:\n",
    "#         super().__init__()\n",
    "#         self.BN = nn.BatchNorm2d(num_features=3,affine=False)\n",
    "#         self.BN_aff = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "#         self.drop = nn.Dropout(0.3)\n",
    "#\n",
    "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#         self.ac_func = nn.Softplus()\n",
    "#\n",
    "#         self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,2), stride=(1,2), padding=0,bias=False)\n",
    "#\n",
    "#         self.conv3 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,5), stride=(1,5), padding=0,bias=False)\n",
    "#\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.linear1 = nn.Linear(30, 9)\n",
    "#\n",
    "#         self.z_pi = nn.Sequential(\n",
    "#             nn.Linear(9, n_gaussians),  # 30个params要learn\n",
    "#             nn.Softmax(dim=1)\n",
    "#         )\n",
    "#\n",
    "#         self.z_mu = nn.Linear(9, n_gaussians)\n",
    "#         self.z_sigma = nn.Linear(9, n_gaussians)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         # 加一个height维度\n",
    "#         x = torch.unsqueeze(x,dim=2)\n",
    "#         x = self.BN(x)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         x = self.BN_aff(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         x = self.BN_aff(x)\n",
    "#         x = self.conv3(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         x = self.flatten(x)\n",
    "#         x = self.linear1(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         pi = self.z_pi(x)\n",
    "#         mu = self.z_mu(x)\n",
    "#         sigma = torch.exp(self.z_sigma(x))\n",
    "#\n",
    "#         return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3 Conv结构-2\n",
    "1. 换了一下conv的结构"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [],
   "source": [
    "# 参数量747\n",
    "class MLP_1_2(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=3,affine=False)\n",
    "        self.BN2 = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "        self.BN3 = nn.BatchNorm2d(num_features=6,affine=True)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=(1,6), stride=(1,3), padding=0,bias=False)\n",
    "        # self.ac_func = nn.Softplus()\n",
    "        self.ac_func = nn.ReLU()\n",
    "        # self.ac_func = nn.Tanh()\n",
    "        # 3, 3, 99\n",
    "        self.conv2 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=(3,6), stride=6, padding=0,bias=False)\n",
    "        # 6,1,33\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=6, out_channels=6, kernel_size=(1,3), stride=3, padding=0,bias=False)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        #self.linear1 = nn.Linear(48, 18)\n",
    "\n",
    "        #无clip\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(30, n_gaussians),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        # 有clip\n",
    "        # self.z_pi = nn.Linear(30, n_gaussians)\n",
    "        self.z_mu = nn.Linear(30, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(30, n_gaussians)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.BN1(x)\n",
    "        # 加一个height维度\n",
    "\n",
    "        x = torch.unsqueeze(x,dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.BN2(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.BN3(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        # x = torch.unsqueeze(x,dim=2)\n",
    "        x = self.conv3(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        # x = self.linear1(x)\n",
    "        # x = self.ac_func(self.drop(x))\n",
    "        mu = self.z_mu(x)\n",
    "        sigma = torch.exp(self.z_sigma(x))\n",
    "\n",
    "        # 加入clip\n",
    "        # pi = self.z_pi(torch.clamp(x,1e-3))\n",
    "        # 无clip\n",
    "        pi = self.z_pi(x)\n",
    "\n",
    "        # mu = torch.maximum(torch.tensor(30.0),mu)\n",
    "\n",
    "        # return x\n",
    "        return pi,mu,sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- MLP结构"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "# 参数量303,919\n",
    "class MLP_1_3(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=3,affine=False)\n",
    "        self.BN2 = nn.BatchNorm1d(num_features=1,affine=False)\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "\n",
    "        self.linear1 = nn.Linear(900, 300)\n",
    "        self.linear2 = nn.Linear(300, 100)\n",
    "        self.linear3 = nn.Linear(100, 30)\n",
    "        self.linear4 = nn.Linear(30, 12)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(12, n_gaussians),  # 30个params要learn\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(12, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(12, n_gaussians)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.BN1(x)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        # x = self.BN2(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        # x = self.BN2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        x = self.linear4(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        pi = self.z_pi(x)\n",
    "        # pi = self.z_pi(torch.clamp(x,1e-3))\n",
    "        mu = self.z_mu(x)\n",
    "        sigma = torch.exp(self.z_sigma(x))\n",
    "\n",
    "        # return x\n",
    "        return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4 (3,100)的结构-3\n",
    "1. 适配(3,100)的training data长度而不是(3,300)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "# # 参数量：498\n",
    "# class MLP_2_1(nn.Module):\n",
    "#     # code->generate->override methods\n",
    "#     def __init__(self, n_gaussians) -> None:\n",
    "#         super().__init__()\n",
    "#         self.BN = nn.BatchNorm2d(num_features=3,affine=False)\n",
    "#         self.BN_aff1 = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "#         self.BN_aff2 = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "#         self.BN_aff3 = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "#         self.drop = nn.Dropout(0.3)\n",
    "#\n",
    "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#         # self.ac_func = nn.Softplus()\n",
    "#         # self.ac_func = nn.Sigmoid()\n",
    "#         self.ac_func = nn.Tanh()\n",
    "#\n",
    "#         self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,2), stride=(1,2), padding=0,bias=False)\n",
    "#\n",
    "#         self.conv3 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         # self.linear1 = nn.Linear(30, 9)\n",
    "#\n",
    "#         self.z_pi = nn.Sequential(\n",
    "#             nn.Linear(48, n_gaussians),  # 30个params要learn\n",
    "#             nn.Softmax(dim=1)\n",
    "#         )\n",
    "#\n",
    "#         self.z_mu = nn.Linear(48, n_gaussians)\n",
    "#         self.z_sigma = nn.Linear(48, n_gaussians)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         # 加一个height维度\n",
    "#         x = torch.unsqueeze(x,dim=2)\n",
    "#         x = self.BN(x)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.BN_aff1(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         # x = self.BN_aff2(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.BN_aff2(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         x = self.flatten(x)\n",
    "#\n",
    "#         pi = self.z_pi(torch.clamp(x,1e-3))\n",
    "#         # pi = self.z_pi(x)\n",
    "#         mu = self.z_mu(x)\n",
    "#         # sigma = torch.exp(self.z_sigma(x))\n",
    "#         sigma = F.elu(self.z_sigma(x))+1\n",
    "#\n",
    "#         # pi.retain_grad()\n",
    "#         # mu.retain_grad()\n",
    "#         # sigma.retain_grad()\n",
    "#         # print(\"grad:\")\n",
    "#         # print(\"pi.grad:\",pi.grad)\n",
    "#         # print(\"mu.grad:\",mu.grad)\n",
    "#         # print(\"sigma.grad:\",sigma.grad)\n",
    "#\n",
    "#         # return x\n",
    "#         return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "# # 参数量：228\n",
    "# class MLP_2_2(nn.Module):\n",
    "#     # code->generate->override methods\n",
    "#     def __init__(self, n_gaussians) -> None:\n",
    "#         super().__init__()\n",
    "#         self.BN = nn.BatchNorm2d(num_features=3,affine=False)\n",
    "#         self.BN_aff = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "#         self.drop = nn.Dropout(0.2)\n",
    "#\n",
    "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#         # self.ac_func = nn.Softplus()\n",
    "#         self.ac_func = nn.LeakyReLU()\n",
    "#         self.ac_func2 = nn.ReLU()\n",
    "#\n",
    "#         # self.ac_func = nn.Sigmoid()\n",
    "#         # self.ac_func = nn.Tanh()\n",
    "#\n",
    "#         self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,2), stride=(1,2), padding=0,bias=False)\n",
    "#\n",
    "#         self.conv3 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#\n",
    "#         self.flatten = nn.Flatten()\n",
    "#\n",
    "#         self.z_pi = nn.Sequential(\n",
    "#             nn.Linear(15, n_gaussians),\n",
    "#             nn.Softmax(dim=1)\n",
    "#         )\n",
    "#\n",
    "#         self.z_mu = nn.Linear(15, n_gaussians)\n",
    "#         self.z_sigma = nn.Linear(15, n_gaussians)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         # 加一个height维度\n",
    "#         x = torch.unsqueeze(x,dim=2)\n",
    "#         x = self.BN(x)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         x = self.BN_aff(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         x = self.BN_aff(x)\n",
    "#         x = self.conv3(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#         #\n",
    "#         x = self.flatten(x)\n",
    "#\n",
    "#         pi = self.z_pi(torch.clamp(x,1e-3))\n",
    "#         # pi = self.z_pi(x)\n",
    "#         mu = self.z_mu(x)\n",
    "#         sigma = torch.exp(self.z_sigma(x))\n",
    "#\n",
    "#         # return x\n",
    "#         return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- MLP结构+长度=100"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "# # Not Sequential\n",
    "# class MLP_2_3(nn.Module):\n",
    "#     # code->generate->override methods\n",
    "#     def __init__(self, n_gaussians) -> None:\n",
    "#         super().__init__()\n",
    "#         self.BN1 = nn.BatchNorm1d(num_features=3,affine=False)\n",
    "#         self.BN2 = nn.BatchNorm1d(num_features=1,affine=False)\n",
    "#         self.drop = nn.Dropout(0.5)\n",
    "#\n",
    "#         self.linear1 = nn.Linear(300, 100)\n",
    "#         self.linear2 = nn.Linear(100, 30)\n",
    "#         self.linear3 = nn.Linear(30, 12)\n",
    "#\n",
    "#         self.ac_func = nn.Softplus()\n",
    "#\n",
    "#         self.flatten = nn.Flatten()\n",
    "#\n",
    "#         self.z_pi = nn.Sequential(\n",
    "#             nn.Linear(12, n_gaussians),  # 30个params要learn\n",
    "#             nn.Softmax(dim=1)\n",
    "#         )\n",
    "#\n",
    "#         self.z_mu = nn.Linear(12, n_gaussians)\n",
    "#         self.z_sigma = nn.Linear(12, n_gaussians)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         x = self.BN1(x)\n",
    "#         x = self.flatten(x)\n",
    "#\n",
    "#         x = self.linear1(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         # x = self.BN2(x)\n",
    "#         x = self.linear2(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         # x = self.BN2(x)\n",
    "#         x = self.linear3(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         # pi = self.z_pi(x)\n",
    "#         pi = self.z_pi(torch.clamp(x,1e-3))\n",
    "#         mu = self.z_mu(x)\n",
    "#         sigma = torch.exp(self.z_sigma(x))\n",
    "#\n",
    "#         # return x\n",
    "#         return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.5 (3,60)的结构-3\n",
    "1. 适配(3,60)的training data长度而不是(3,300)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "# # 参数量：174\n",
    "# class MLP_3_1(nn.Module):\n",
    "#     # code->generate->override methods\n",
    "#     def __init__(self, n_gaussians) -> None:\n",
    "#         super().__init__()\n",
    "#         self.BN = nn.BatchNorm2d(num_features=3,affine=False)\n",
    "#         self.BN_aff = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "#         self.drop = nn.Dropout(0.2)\n",
    "#\n",
    "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#         # self.ac_func = nn.Softplus()\n",
    "#         self.ac_func = nn.LeakyReLU()\n",
    "#         self.ac_func2 = nn.ReLU()\n",
    "#\n",
    "#         # self.ac_func = nn.Sigmoid()\n",
    "#         # self.ac_func = nn.Tanh()\n",
    "#\n",
    "#         self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,2), stride=(1,2), padding=0,bias=False)\n",
    "#\n",
    "#         self.conv3 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.linear = nn.Linear(24, 9)\n",
    "#\n",
    "#         self.z_pi = nn.Sequential(\n",
    "#             nn.Linear(9, n_gaussians),\n",
    "#             nn.Softmax(dim=1)\n",
    "#         )\n",
    "#\n",
    "#         self.z_mu = nn.Linear(9, n_gaussians)\n",
    "#         self.z_sigma = nn.Linear(9, n_gaussians)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         # 加一个height维度\n",
    "#         x = torch.unsqueeze(x,dim=2)\n",
    "#         x = self.BN(x)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.ac_func(self.drop(x))      # 3, 1, 20\n",
    "#\n",
    "#         x = self.BN_aff(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.ac_func(self.drop(x))      # 3, 1, 10\n",
    "#\n",
    "#         x = self.BN_aff(x)\n",
    "#         x = self.conv3(x)\n",
    "#         x = self.ac_func(self.drop(x))      # 3, 1, 8\n",
    "#\n",
    "#         x = self.flatten(x)\n",
    "#         # print(x.shape)\n",
    "#         # x = self.linear(x)\n",
    "#         #\n",
    "#         pi = self.z_pi(x)\n",
    "#         mu = self.z_mu(x)\n",
    "#         sigma = torch.exp(self.z_sigma(x))\n",
    "#\n",
    "#         # return x\n",
    "#         return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- MLP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "# # 参数量：5799\n",
    "# class MLP_3_2(nn.Module):\n",
    "#     # code->generate->override methods\n",
    "#     def __init__(self, n_gaussians) -> None:\n",
    "#         super().__init__()\n",
    "#         self.BN = nn.BatchNorm2d(num_features=3,affine=False)\n",
    "#         self.BN_aff = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "#         self.drop = nn.Dropout(0.2)\n",
    "#\n",
    "#         self.linear1 = nn.Linear(180,30)\n",
    "#         self.linear2 = nn.Linear(30,9)\n",
    "#         # self.ac_func = nn.Softplus()\n",
    "#         self.ac_func = nn.LeakyReLU()\n",
    "#         self.ac_func2 = nn.ReLU()\n",
    "#\n",
    "#         # self.ac_func = nn.Sigmoid()\n",
    "#         # self.ac_func = nn.Tanh()\n",
    "#\n",
    "#         self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,2), stride=(1,2), padding=0,bias=False)\n",
    "#\n",
    "#         self.conv3 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.linear = nn.Linear(24, 9)\n",
    "#\n",
    "#         self.z_pi = nn.Sequential(\n",
    "#             nn.Linear(9, n_gaussians),\n",
    "#             nn.Softmax(dim=1)\n",
    "#         )\n",
    "#\n",
    "#         self.z_mu = nn.Linear(9, n_gaussians)\n",
    "#         self.z_sigma = nn.Linear(9, n_gaussians)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         # 加一个height维度\n",
    "#         x = torch.unsqueeze(x,dim=2)\n",
    "#         x = self.BN(x)\n",
    "#         x = self.flatten(x)\n",
    "#\n",
    "#         x = self.linear1(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         x = self.linear2(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         pi = self.z_pi(x)\n",
    "#         mu = self.z_mu(x)\n",
    "#         sigma = torch.exp(self.z_sigma(x))\n",
    "#\n",
    "#         # return x\n",
    "#         return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input's shape is torch.Size([2, 3, 1, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": "torch.Size([2, 12])"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([\n",
    "    [[1.,2.,3.,4.],[1.,2.,3.,4.],[-1.,-2.,-3.,-4.]],\n",
    "    [[0.,0.,0.,0.],[0.,0.,0.,0.],[0.,0.,0.,0.]]\n",
    "], requires_grad=True)\n",
    "input = input.unsqueeze(dim=2)\n",
    "print(f\"input's shape is {input.shape}\")\n",
    "flt = nn.Flatten(start_dim=1)\n",
    "flt(input).shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. The Loss\n",
    "- `loss_preparation`用来做loss的前期data准备：\n",
    "    - 计算混合模型的分布`m`以及target data中的`duration`\n",
    "- `loss_fn`用来计算loss值\n",
    "\n",
    "## 4.1 初始版本\n",
    "### 4.1.1 version 1 [FAILED]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "# # 当input的shape是[50,3]时，输出应该是50个GMM\n",
    "# # 对这50个GMM看能生成什么output\n",
    "\n",
    "def loss_fn(Pi,duration,m,N_gaussians):\n",
    "    loss_list = []\n",
    "    with torch.no_grad():\n",
    "        # for each GMM\n",
    "        for i in range(len(Pi)):\n",
    "            target = duration[i,:]\n",
    "            pi = Pi[i,:]\n",
    "\n",
    "            # Drop padded data and Expanded to the same dim\n",
    "            target_nonzero = target[torch.nonzero(target)].squeeze_()\n",
    "            target_nonzero = torch.repeat_interleave(target_nonzero.unsqueeze(dim=1), repeats=N_gaussians, dim=1).to(device)\n",
    "            # len_target = len(target_nonzero)       # The length of target data\n",
    "\n",
    "            # loss_1 是高斯分布的概率密度value\n",
    "            loss_1 = torch.exp(m[i].log_prob(target_nonzero))\n",
    "\n",
    "            # loss_2 是MDN的概率密度value\n",
    "            loss_2 = torch.sum(loss_1 * pi, dim=1)\n",
    "\n",
    "            # loss_3 是值非0的概率密度value\n",
    "            loss_3 = loss_2[torch.nonzero(loss_2)].view(-1,1)                # 再去掉所有的log(0)\n",
    "\n",
    "            # 如果loss_2全是0则赋值为1e-40，否则赋值为loss的最小值\n",
    "            MIN_LOSS = torch.min(loss_3) if torch.min(loss_2)>0 else 1e-40\n",
    "            loss_padded = [MIN_LOSS]* (loss_2.shape[0]-loss_3.shape[0])\n",
    "            loss_padded_ts = torch.tensor(loss_padded,device=device)\n",
    "\n",
    "            # loss_4是用MIN_LOSS填充后的loss_3，和loss_2等长\n",
    "            loss_4 = torch.cat((loss_3[:,0],loss_padded_ts))\n",
    "            torch._assert(len(loss_4) == len(loss_2),\"cat不正确\")\n",
    "\n",
    "            # loss_5是log likelihood loss\n",
    "            loss_5 = torch.log(loss_4)\n",
    "            loss_list.append(-torch.mean((loss_5)).item())\n",
    "            # loss_list.append(-torch.sum((loss_5)).item()) # loss值会比较大，\n",
    "\n",
    "    # 最后loss求一下平均\n",
    "    loss = np.mean(loss_list)\n",
    "    loss_ts = torch.tensor(loss,device=device,requires_grad=True)\n",
    "    return loss_ts,loss_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1.2 version 2 [WORK]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "def loss_fn_v2(Pi, Mu, Sigma, Duration, N_gaussians):\n",
    "\n",
    "    loss_sum = torch.tensor(0.,device=device,requires_grad=True)\n",
    "\n",
    "    for i in range(len(Pi)):\n",
    "\n",
    "        target = Duration[i,:,0]\n",
    "        pi = Pi[i,:]\n",
    "        m = torch.distributions.Normal(loc=Mu[i,:], scale=Sigma[i,:])\n",
    "\n",
    "        # Drop padded data and Expanded to the same dim\n",
    "        idx = torch.nonzero(target)\n",
    "        target_nonzero = target[idx]\n",
    "        # target_nonzero_2 = torch.repeat_interleave(target_nonzero.unsqueeze(dim=1), repeats=N_gaussians, dim=1).to(device)\n",
    "        target_nonzero_2 = target_nonzero.repeat(1,N_gaussians).to(device)\n",
    "\n",
    "        # loss_1 是高斯分布的概率密度value\n",
    "        loss_1 = torch.exp(m.log_prob(target_nonzero_2))\n",
    "\n",
    "        # loss_2 是MDN的概率密度value\n",
    "        loss_2 = torch.sum(loss_1 * pi, dim=1)\n",
    "\n",
    "        ## 两种截断的方式都ok：\n",
    "        ## 方法一：在MDN的prob上设定最小值；\n",
    "        loss_3 = torch.clamp(loss_2,min=MIN_LOSS)\n",
    "        loss_4 = torch.log(loss_3)\n",
    "        loss_sum = -torch.mean(loss_4) + loss_sum\n",
    "\n",
    "        ## 方法二：最后加一个safety数 MIN_LOSS\n",
    "        # loss_3 = torch.log(loss_2+MIN_LOSS)\n",
    "        # loss_sum = -torch.mean(loss_3) + loss_sum\n",
    "\n",
    "    # 最后loss求一下平均\n",
    "    # return loss_sum\n",
    "    return loss_sum/batch_size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1.3 version 3 [work but why]\n",
    "1. 效果不好，不能照搬，loss不同"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "def log_sum_exp(x, axis=None):\n",
    "    \"\"\"Log-sum-exp trick implementation\"\"\"\n",
    "    x_max = torch.max(x, dim=axis, keepdim=True)[0]\n",
    "    # print(\"x_max. shape:\",x_max.shape)\n",
    "    return torch.log(torch.sum(torch.exp(x - x_max), dim=axis, keepdim=True))+x_max"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "def loss_fn_v3(Pi, Mu, Sigma, Target, N_gaussians):\n",
    "\n",
    "    loss_sum = torch.tensor(0.,device=device,requires_grad=True)\n",
    "\n",
    "    for i in range(len(Pi)):\n",
    "\n",
    "        target = Target[i,:,0]\n",
    "        pi = Pi[i,:]\n",
    "\n",
    "        # Drop padded data and Expanded to the same dim\n",
    "        #### 写法一：work。无原地变换\n",
    "        idx = torch.nonzero(target)\n",
    "        target_nonzero = target[idx]\n",
    "        target_nonzero_2 = target_nonzero.repeat(1,N_gaussians)\n",
    "\n",
    "        # loss_1 是高斯分布的概率密度value\n",
    "\n",
    "        # loss_1 = gaussian_pdf(target_nonzero_2,Mu[i,:],Sigma[i,:],C)\n",
    "        # print(\"loss_0:\",gaussian_pdf(target_nonzero_2,Mu[i,:],Sigma[i,:],0))\n",
    "        # print(\"loss_1:\",loss_1)\n",
    "###########\n",
    "        # exponent?\n",
    "        v1 = .5 * torch.log(2 * torch.tensor(np.pi)) -torch.log(Sigma[i,:])\n",
    "        v2 = (target_nonzero_2 - Mu[i,:])\n",
    "        # v1 torch.Size([3])\n",
    "        # v2 torch.Size([44, 3])\n",
    "\n",
    "        exponent = torch.log(Pi[i,:]) -  v1\\\n",
    "                   - (v2**2)/(2*(Sigma[i,:])**2)\n",
    "        log_gauss = log_sum_exp(exponent, axis=1)\n",
    "\n",
    "        loss_sum = -torch.mean(log_gauss) + loss_sum\n",
    "\n",
    "    loss_ts = loss_sum/len(Pi)      # 太大了\n",
    "    return loss_ts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1.4 version 4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "def loss_fn_v4(Pi, Mu, Sigma, Duration, N_gaussians):\n",
    "\n",
    "    loss_sum = torch.tensor(0.,device=device,requires_grad=True)\n",
    "\n",
    "    for i in range(len(Pi)):\n",
    "\n",
    "        target = Duration[i,:,0]\n",
    "        pi = Pi[i,:]\n",
    "        m = torch.distributions.Normal(loc=Mu[i,:], scale=Sigma[i,:])\n",
    "\n",
    "        # Drop padded data and Expanded to the same dim\n",
    "        idx = torch.nonzero(target)\n",
    "        target_nonzero = target[idx]\n",
    "        # target_nonzero_2 = torch.repeat_interleave(target_nonzero.unsqueeze(dim=1), repeats=N_gaussians, dim=1).to(device)\n",
    "        target_nonzero_2 = target_nonzero.repeat(1,N_gaussians).to(device)\n",
    "\n",
    "        # loss_1 是高斯分布的概率密度value\n",
    "        loss_1 = torch.exp(m.log_prob(target_nonzero_2))\n",
    "\n",
    "        # loss_2 是MDN的概率密度value\n",
    "        loss_2 = torch.sum(loss_1 * pi, dim=1)\n",
    "\n",
    "        ## 两种截断的方式都ok：一种是在MDN的prob上设定最小值；一种是最后加一个safety数 MIN_LOSS\n",
    "        # loss_4 = torch.clamp(loss_2,min=MIN_LOSS)\n",
    "        Min = torch.min(loss_2).data\n",
    "        eps = torch.where(torch.isnan(torch.log(Min)),MIN_LOSS,0)\n",
    "        # loss_3 = torch.log(loss_2+MIN_LOSS)\n",
    "        loss_3 = torch.log(loss_2+eps)  # +1e-8\n",
    "        loss_sum = -torch.mean(loss_3) + loss_sum\n",
    "\n",
    "    # 最后loss求一下平均\n",
    "    return loss_sum"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5])\n",
      "tensor([[4, 4, 4, 4, 4, 4]])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor(4.5528)"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def argmax(vec):\n",
    "    # return the argmax as a python int\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    print(idx)\n",
    "    return idx.item()\n",
    "\n",
    "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "def log_sum_exp_2(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    print(max_score_broadcast)\n",
    "    return max_score + \\\n",
    "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "\n",
    "input_test = torch.tensor([[1,2,3],[2,1,4]]).view(1, -1)\n",
    "log_sum_exp_2(input_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1.5 version 5-Laplace\n",
    "1.version 2 但是是Laplace"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "def loss_fn_v5(Pi, Mu, Sigma, Duration, N_gaussians):\n",
    "\n",
    "    loss_sum = torch.tensor(0.,device=device,requires_grad=True)\n",
    "\n",
    "    for i in range(len(Pi)):\n",
    "\n",
    "        target = Duration[i,:,0]\n",
    "        pi = Pi[i,:]\n",
    "        m = torch.distributions.Laplace(loc=Mu[i,:], scale=Sigma[i,:])\n",
    "\n",
    "        # Drop padded data and Expanded to the same dim\n",
    "        idx = torch.nonzero(target)\n",
    "        target_nonzero = target[idx]\n",
    "        # target_nonzero_2 = torch.repeat_interleave(target_nonzero.unsqueeze(dim=1), repeats=N_gaussians, dim=1).to(device)\n",
    "        target_nonzero_2 = target_nonzero.repeat(1,N_gaussians).to(device)\n",
    "\n",
    "        # loss_1 是高斯分布的概率密度value\n",
    "        loss_1 = torch.exp(m.log_prob(target_nonzero_2))\n",
    "\n",
    "        # loss_2 是MDN的概率密度value\n",
    "        loss_2 = torch.sum(loss_1 * pi, dim=1)\n",
    "\n",
    "        ## 两种截断的方式都ok：\n",
    "        ## 方法一：在MDN的prob上设定最小值；\n",
    "        loss_3 = torch.clamp(loss_2,min=MIN_LOSS)\n",
    "        loss_4 = torch.log(loss_3)\n",
    "        loss_sum = -torch.mean(loss_4) + loss_sum\n",
    "\n",
    "        ## 方法二：最后加一个safety数 MIN_LOSS\n",
    "        # loss_3 = torch.log(loss_2+MIN_LOSS)\n",
    "        # loss_sum = -torch.mean(loss_3) + loss_sum\n",
    "\n",
    "    # 最后loss求一下平均\n",
    "    return loss_sum/batch_size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 cdf loss\n",
    "1. 注意把targets data的路径改一下"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "# cdf loss\n",
    "def loss_fn_cdf(Pi, Mu, Sigma, Target, N_gaussians):\n",
    "\n",
    "    loss_sum = torch.tensor(0.,device=device,requires_grad=True)\n",
    "\n",
    "    # for each GMM\n",
    "    for i in range(len(Pi)):\n",
    "        target = Target[i,:,0]\n",
    "        prob_target = Target[i,:,1]\n",
    "        pi = Pi[i,:]\n",
    "        m = torch.distributions.Normal(loc=Mu[i,:], scale=Sigma[i,:])\n",
    "\n",
    "        # Drop padded data and Expanded to the same dim\n",
    "        #### 写法一：work。无原地变换\n",
    "        idx = torch.nonzero(target)\n",
    "        target_nonzero = torch.squeeze(target[idx])\n",
    "        prob_target_nonzero = torch.squeeze(prob_target[idx])\n",
    "        target_nonzero_2 = torch.repeat_interleave(target_nonzero.unsqueeze(dim=1), repeats=N_gaussians, dim=1).to(device)\n",
    "\n",
    "        # loss_1 是高斯分布的概率密度value\n",
    "        loss_1 = torch.exp(m.log_prob(target_nonzero_2))\n",
    "\n",
    "        # loss_2 是MDN的概率密度value\n",
    "        loss_2 = torch.sum(loss_1 * pi, dim=1)\n",
    "\n",
    "        # loss_3 是cdf的差值的一范数or二范数，表示的是MDN和target之间的总体差异\n",
    "        loss_3 = torch.norm((loss_2 - prob_target_nonzero), 1)\n",
    "\n",
    "        # loss_4 = -torch.log(loss_3)   # +MIN_LOSS\n",
    "        loss_sum = torch.log(loss_3) + loss_sum\n",
    "\n",
    "    # Batch平均loss\n",
    "\n",
    "    return loss_sum\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.3 交叉熵/ KL\n",
    "1. 自己写的，没有用torch的函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "# 当input的shape是[50,3]时，输出应该是50个GMM\n",
    "def loss_preparation_CE(pi, mu, sigma):\n",
    "\n",
    "    m=[]\n",
    "    for i in range(pi.shape[0]):\n",
    "        m.append(torch.distributions.Normal(loc=mu[i,:], scale=sigma[i,:]))\n",
    "\n",
    "    # target_packed = pack_padded_sequence(target,target_len,batch_first=True,enforce_sorted=False)  # 去掉padded 0并拉成一个vector\n",
    "    # seq_unpacked, lens_unpacked = pad_packed_sequence(target_packed, batch_first=True)             # 和上面互为逆运算\n",
    "\n",
    "    return m"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "# # 当input的shape是[50,3]时，输出应该是50个GMM\n",
    "# # 对这50个GMM看能生成什么output\n",
    "\n",
    "def loss_fn_CE(Pi, Mu, Sigma, Target, N_gaussians):\n",
    "\n",
    "    loss_sum = torch.tensor(0.,device=device,requires_grad=True)\n",
    "\n",
    "    # for each GMM\n",
    "    for i in range(len(Pi)):\n",
    "        target = Target[i,:,0]\n",
    "        prob_target = Target[i,:,1]\n",
    "        pi = Pi[i,:]\n",
    "        m = torch.distributions.Normal(loc=Mu[i,:], scale=Sigma[i,:])\n",
    "\n",
    "        # Drop padded data and Expanded to the same dim\n",
    "        #### 写法一：work。无原地变换\n",
    "        idx = torch.nonzero(target)\n",
    "        target_nonzero = torch.squeeze(target[idx])\n",
    "        prob_target_nonzero = torch.squeeze(prob_target[idx])\n",
    "\n",
    "        target_nonzero_2 = torch.repeat_interleave(target_nonzero.unsqueeze(dim=1), repeats=N_gaussians, dim=1).to(device)\n",
    "\n",
    "        # loss_1 是高斯分布的概率密度value\n",
    "        loss_1 = torch.exp(m.log_prob(target_nonzero_2))\n",
    "\n",
    "        # loss_2 是MDN的概率密度value\n",
    "        loss_2 = torch.sum(loss_1 * pi, dim=1)\n",
    "\n",
    "        # loss_4是KL loss\n",
    "        # loss_4 = -prob_target_nonzero*torch.log(loss_2) + prob_target_nonzero*torch.log(prob_target_nonzero)\n",
    "        # print(\"prob_target_nonzero shape:\",prob_target_nonzero.shape)\n",
    "        # print(\"loss_2 shape:\",loss_2.shape)\n",
    "\n",
    "        loss_4 = -prob_target_nonzero*torch.log(loss_2+MIN_LOSS)+ prob_target_nonzero*torch.log(prob_target_nonzero)\n",
    "        loss_sum = torch.sum(loss_4)+loss_sum\n",
    "\n",
    "    loss_ts = loss_sum/len(Pi)\n",
    "    return loss_ts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.4 log sum exp\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "# 使用log_sum_exp的loss\n",
    "def loss_fn_lse(Pi,duration,m,N_gaussians):\n",
    "    loss_list = []\n",
    "    # with torch.no_grad():\n",
    "    # for each GMM\n",
    "    # 后期肯定要矩阵化这个计算！\n",
    "    for i in range(len(m)):\n",
    "        target = duration[i,:]\n",
    "        pi = Pi[i,:]\n",
    "\n",
    "        # Drop padded data and Expanded to the same dim\n",
    "        target_nonzero = target[torch.nonzero(target)].squeeze_()\n",
    "        target_nonzero = torch.repeat_interleave(target_nonzero.unsqueeze(dim=1), repeats=N_gaussians, dim=1).to(device)\n",
    "        # len_target = len(target_nonzero)       # The length of target data\n",
    "\n",
    "        # loss_1 是高斯分布的log prob\n",
    "        loss_1 = m[i].log_prob(target_nonzero)\n",
    "\n",
    "        # loss_2 是MDN的log prob\n",
    "        loss_2 = torch.sum(loss_1, dim=1) + torch.log(pi)\n",
    "\n",
    "        # loss_3 是值非0的概率密度value\n",
    "        loss_3 = loss_2[torch.nonzero(loss_2)].view(-1,1)                # 再去掉所有的log(0)\n",
    "\n",
    "        # 如果loss_2全是0则赋值为1e-40，否则赋值为loss的最小值\n",
    "        MIN_LOSS = torch.min(loss_3) if torch.min(loss_2)>0 else 1e-40\n",
    "        loss_padded = [MIN_LOSS]* (loss_2.shape[0]-loss_3.shape[0])\n",
    "        loss_padded_ts = torch.tensor(loss_padded,device=device)\n",
    "\n",
    "        # loss_4是用MIN_LOSS填充后的loss_3，和loss_2等长\n",
    "        loss_4 = torch.cat((loss_3[:,0],loss_padded_ts))\n",
    "        torch._assert(len(loss_4) == len(loss_2),\"cat不正确\")\n",
    "\n",
    "        # loss_5是log likelihood loss\n",
    "        loss_5 = torch.log(loss_4)\n",
    "        loss_list.append(-torch.mean((loss_5)).item())\n",
    "        # loss_list.append(-torch.sum((loss_5)).item()) # loss值会比较大，\n",
    "\n",
    "    # 最后loss求一下平均\n",
    "    loss = np.mean(loss_list)\n",
    "\n",
    "    loss_ts = torch.tensor(loss,device=device,requires_grad=True)\n",
    "    return loss_ts,loss_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "def loss_test():\n",
    "    loss_2 = torch.tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "    loss_3 = loss_2[torch.nonzero(loss_2)].view(-1,1)         # 去掉所有的log(0)\n",
    "    print(\"loss_3:\",loss_3.shape)\n",
    "\n",
    "    MIN_LOSS = torch.min(loss_2) if torch.min(loss_2)>0 else 1e-20\n",
    "\n",
    "    print(\"MIN_LOSS：\",MIN_LOSS)\n",
    "    # 用MIN_loss替代loss_2为0的部分\n",
    "    loss_2[loss_2==0] = MIN_LOSS\n",
    "\n",
    "\n",
    "    loss_padded = [MIN_LOSS]* (loss_2.shape[0]-loss_3.shape[0])\n",
    "    loss_padded_ts = torch.tensor(loss_padded) # ,device=device\n",
    "    loss_3 = torch.cat((loss_3[:,0],loss_padded_ts))\n",
    "    loss_3.shape, torch.log(loss_3)\n",
    "    print(\"data: \",loss_3.data)\n",
    "    print(\"data: \",type(loss_3))\n",
    "# loss_test()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.5 W-distance\n",
    "1. 拿到target data的两列，按照第一列去predict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "outputs": [],
   "source": [
    "p = 2\n",
    "entreg = .1 # entropy regularization factor for Sinkhorn\n",
    "factor = 1  # prob的放大系数\n",
    "# 若以欧式距离为metric，则cost function可以直接用geomloss提供的 Sinkhorn快速解\n",
    "OTLoss = geomloss.SamplesLoss(\n",
    "    loss='sinkhorn', p=p, blur=entreg**(1/p), backend='tensorized', verbose=False)\n",
    "\n",
    "def loss_fn_WD(Pi, Mu, Sigma, Duration, N_gaussians):\n",
    "\n",
    "    loss_sum = torch.tensor(0.,device=device,requires_grad=True)\n",
    "\n",
    "    for i in range(len(Pi)):\n",
    "\n",
    "        target = torch.unique(Duration[i],dim=0)\n",
    "        n_target = target[:,0]\n",
    "        non_zero_idx = torch.nonzero(n_target)\n",
    "        n = n_target[non_zero_idx]\n",
    "        p_target = target[:,1]\n",
    "        p = p_target[non_zero_idx]*factor\n",
    "\n",
    "        # 放大“p”\n",
    "        y_target = torch.cat([n,p],dim=1)\n",
    "\n",
    "        pi = Pi[i,:]\n",
    "        m = torch.distributions.Normal(loc=Mu[i,:], scale=Sigma[i,:])\n",
    "        x = torch.repeat_interleave(n, repeats=N_gaussians, dim=1)   # expand dim\n",
    "        y = (m.cdf(x+TARGET) - m.cdf(x)).to(device=device)\n",
    "        y_cdf = torch.sum(pi*y,dim=1).unsqueeze(dim=1)*factor\n",
    "        y_pred = torch.cat([n,y_cdf],dim=1)\n",
    "\n",
    "        loss_sum = OTLoss(y_pred,y_target) + loss_sum\n",
    "\n",
    "    return loss_sum/batch_size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "from geomloss import SamplesLoss\n",
    "loss_fn_2 = SamplesLoss(\"sinkhorn\", p=1, blur=0.01)\n",
    "\n",
    "a, b = torch.randn((100, 512)), torch.randn((100, 512))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5 Plot\n",
    "1. draw:\n",
    "    - loss: training data的loss和test data的loss趋势\n",
    "    - MLP的网络结构（.png）\n",
    "    - target distrb.和pred. distrb.\n",
    "    - 所有target data的分布图\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(\"logs-MLP\")\n",
    "viz = Visdom(env=\"001\")\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.1 draw 所有的target data\n",
    "1. 实际上在`i < 50`里决定画前50 or what target data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "def draw_all_target_data():\n",
    "    # target data\n",
    "    # target_path = r\"../data/targets\"\n",
    "    target_path = r\"../data/targets_5\"\n",
    "    # target_path里有全部的target data地址\n",
    "    target_all_path = os.listdir(target_path)\n",
    "    len_target = len(target_all_path)\n",
    "    print(f\"一共有 *{len_target}* 组 target data\")\n",
    "\n",
    "    # Init\n",
    "    # viz_env_str = \"target_bar\"\n",
    "    # viz_env_str = \"target_line\"\n",
    "    # viz_env_str = \"target_hist\"\n",
    "    viz_env_str = \"target_scatter\"\n",
    "    viz = Visdom(env = viz_env_str)\n",
    "\n",
    "    for i in range(len_target):\n",
    "        target_path_i_path = os.path.join(target_path,target_all_path[i])\n",
    "        target_df = pd.read_csv(target_path_i_path,encoding=\"utf-8\")\n",
    "\n",
    "        # Init\n",
    "        win_str = str(target_all_path[i])\n",
    "        title_str = \"Target Distrb. of \"+win_str\n",
    "\n",
    "        if(i < 50):\n",
    "            # Hist plot which is not that great\n",
    "            if viz_env_str == \"target_hist\" :\n",
    "                viz.histogram(X = np.array(target_df.N), env=viz_env_str, win=win_str,\n",
    "                            opts= dict(title=title_str,numbins = 50))\n",
    "\n",
    "            # Line plot\n",
    "            if viz_env_str == \"target_line\" :\n",
    "                target_df.drop_duplicates(inplace=True)\n",
    "                viz.line(X = np.array(target_df.N),Y= np.array(target_df.P), env=viz_env_str, win=win_str,opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "            # Bar plot. 这个看起来是最准确的一个\n",
    "            if viz_env_str == \"target_bar\" :\n",
    "                target_df.drop_duplicates(inplace=True)\n",
    "                arr_str = [str(i) for i in np.array(target_df.N)]\n",
    "                viz.bar(X=np.array(target_df.P), env=viz_env_str, win=win_str,\n",
    "                        opts=dict(title=title_str,rownames=arr_str,stacked = False))\n",
    "\n",
    "            # Scatter plot\n",
    "            if viz_env_str == \"target_scatter\" :\n",
    "                target_df.drop_duplicates(inplace=True)\n",
    "                viz.scatter(X=np.array(target_df),env=viz_env_str, win=win_str,opts=dict(title=title_str,markersize = 3))\n",
    "    print(\"Done\")\n",
    "\n",
    "# draw_all_target_data()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 画target data的cdf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "def draw_all_target_data_cdf():\n",
    "    # target data\n",
    "    # target_path = r\"../data/targets\"\n",
    "    target_path = r\"../data/targets_5_cdf\"\n",
    "    # target_path里有全部的target data地址\n",
    "    target_all_path = os.listdir(target_path)\n",
    "    len_target = len(target_all_path)\n",
    "    print(f\"一共有 *{len_target}* 组 target data\")\n",
    "\n",
    "    # Init，只需要改这个str就ok\n",
    "    # viz_env_str = \"target_barCDF\"\n",
    "    # viz_env_str = \"target_lineCDF\"\n",
    "    # viz_env_str = \"target_histCDF\"\n",
    "    viz_env_str = \"target_scatterCDF\"\n",
    "    viz = Visdom(env = viz_env_str)\n",
    "\n",
    "    for i in range(len_target):\n",
    "        target_path_i_path = os.path.join(target_path,target_all_path[i])\n",
    "        target_df = pd.read_csv(target_path_i_path,encoding=\"utf-8\")\n",
    "\n",
    "        # Init\n",
    "        win_str = str(target_all_path[i])\n",
    "        title_str = \"Target Distrb. of \"+win_str\n",
    "\n",
    "        if(i < 50):\n",
    "            # Hist plot which is not that great\n",
    "            if viz_env_str == \"target_histCDF\" :\n",
    "                viz.histogram(X = np.array(target_df.N), env=viz_env_str, win=win_str,\n",
    "                            opts= dict(title=title_str,numbins = 50))\n",
    "\n",
    "            # Line plot\n",
    "            if viz_env_str == \"target_lineCDF\" :\n",
    "                target_df.drop_duplicates(inplace=True)\n",
    "                viz.line(X = np.array(target_df.N),Y= np.array(target_df.P), env=viz_env_str, win=win_str,opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "            # Bar plot. 这个看起来是最准确的一个\n",
    "            if viz_env_str == \"target_barCDF\" :\n",
    "                target_df.drop_duplicates(inplace=True)\n",
    "                arr_str = [str(i) for i in np.array(target_df.N)]\n",
    "                viz.bar(X=np.array(target_df.P), env=viz_env_str, win=win_str,\n",
    "                        opts=dict(title=title_str,rownames=arr_str,stacked = False))\n",
    "\n",
    "            # Scatter plot\n",
    "            if viz_env_str == \"target_scatterCDF\" :\n",
    "                target_df.drop_duplicates(inplace=True)\n",
    "                viz.scatter(X=np.array(target_df),env=viz_env_str, win=win_str,opts=dict(title=title_str,markersize = 3))\n",
    "    print(\"Done\")\n",
    "\n",
    "# draw_all_target_data_cdf()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 也是画target data，但是是平滑处理\n",
    "- `savgol_filter`参数解析:\n",
    "    - y：代表曲线点坐标（x,y）中的y\n",
    "    - window_length：窗口长度，该值需为正奇整数。值越小，曲线越贴近真实曲线；值越大，平滑效果越厉害\n",
    "    - polyorder: 对窗口内的数据点进行k阶多项式拟合，k的值需要小于window_length。值越大，曲线越贴近真实曲线；值越小，平滑效果越厉害\n",
    "    - mode：确定了要应用滤波器的填充信号的扩展类型。（This determines the type of extension to use for the padded signal to which the filter is applied. ）\n",
    "- **实际上没用到这个函数**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "def draw_smooth_target_data():\n",
    "    # target data\n",
    "    target_path = r\"../data/targets\"\n",
    "    # target_path里有全部的target data地址\n",
    "    target_all_path = os.listdir(target_path)\n",
    "    len_target = len(target_all_path)\n",
    "    print(f\"一共有 *{len_target}* 组 target data\")\n",
    "\n",
    "    # Init\n",
    "    viz_env_str = \"target_bar\"\n",
    "    # viz_env_str = \"target_distrb\"\n",
    "    # viz_env_str = \"target_hist\"\n",
    "    # viz_env_str = \"target_scatter\"\n",
    "    viz = Visdom(env = viz_env_str)\n",
    "\n",
    "    for i in range(len_target):\n",
    "        target_path_i_path = os.path.join(target_path,target_all_path[i])\n",
    "        target_df = pd.read_csv(target_path_i_path,encoding=\"utf-8\")\n",
    "\n",
    "        # Init\n",
    "        win_str = str(target_all_path[i])\n",
    "        title_str = \"Target Distrb. of \"+win_str\n",
    "\n",
    "        if(i < 100):\n",
    "            # Smoothen the data\n",
    "            y_smooth = scipy.signal.savgol_filter(target_df.P,window_length=53,polyorder=3)\n",
    "            # y_smooth = scipy.signal.savgol_filter(target_df.P, 99, 1, mode= 'nearest')\n",
    "            target_df.P = y_smooth\n",
    "\n",
    "            # Hist plot which is not that great\n",
    "            # viz.histogram(X = np.array(target_df.N), env=viz_env_str, win=win_str,\n",
    "            #             opts= dict(title=title_str,numbins = 50))\n",
    "\n",
    "            # Line plot\n",
    "            # target_df.drop_duplicates(inplace=True)\n",
    "            # viz.line(X = np.array(target_df.N),Y= np.array(target_df.P), env=viz_env_str, win=win_str,opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "            # Bar plot. 这个看起来是最准确的一个\n",
    "            target_df.drop_duplicates(inplace=True)\n",
    "            arr_str = [str(i) for i in np.array(target_df.N)]\n",
    "            viz.bar(X=np.array(target_df.P), env=viz_env_str, win=win_str,\n",
    "                    opts=dict(title=title_str,rownames=arr_str))\n",
    "\n",
    "            # Scatter plot\n",
    "            # target_df.drop_duplicates(inplace=True)\n",
    "            # viz.scatter(X=np.array(target_df),env=viz_env_str, win=win_str,opts=dict(title=title_str,markersize = 3))\n",
    "    print(\"Done\")\n",
    "\n",
    "# draw_smooth_target_data()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "#### Test for drawing\n",
    "def test_draw():\n",
    "    viz = Visdom(env=\"001\")\n",
    "\n",
    "    mu = torch.tensor([0,10,20])\n",
    "    sigma = torch.tensor([1,1,1])\n",
    "    duration = torch.tensor([0,1,2,0])\n",
    "    duration = torch.repeat_interleave(duration.unsqueeze(dim=1), repeats=3, dim=1)\n",
    "    m = torch.distributions.Normal(loc=mu, scale=sigma)\n",
    "    pi = torch.tensor([0.2,0.3,0.5])\n",
    "\n",
    "    # draw\n",
    "    x_0 = torch.tensor(np.arange(0,1000))\n",
    "    x = torch.repeat_interleave(x_0.unsqueeze(dim=1), repeats=3, dim=1)\n",
    "    y = torch.exp(m.log_prob(x))\n",
    "    y_sum = torch.unsqueeze(torch.sum(pi*y,dim=1),dim=1)\n",
    "    viz.line(X = x_0,Y= torch.cat([y,y_sum],dim = 1), env=\"001\", win=\"test_draw_2\",\n",
    "            opts= dict(title='test_draw', legend=['N1', 'N2', 'N3','NNN']))\n",
    "# test_draw()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.2 plot mdn\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2.1 mdn pdf的图像[with input data]\n",
    "1. 求解[1,6,11...]上的cdf：\n",
    "    - 求解[1,6,11..]的cdf，再求解[6,11,..]，两者相减"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "def draw_mdn_2(pi, mu, sigma, target, input, total_train_step, N_gaussians):\n",
    "    # 只画一个batch中的第一个\n",
    "\n",
    "    # The target distrb.\n",
    "    target = torch.unique(target,dim=0)    # drop the duplicate\n",
    "    n_target = target[:,0]\n",
    "    non_zero_idx = torch.nonzero(n_target)\n",
    "    n = n_target[non_zero_idx]\n",
    "\n",
    "    p_target = target[:,1]\n",
    "    p = p_target[non_zero_idx]\n",
    "    max_n = int(max(n).item())\n",
    "\n",
    "    # The predicted distrb.\n",
    "    m = torch.distributions.Normal(mu,sigma)          # Gaussian\n",
    "    # m = torch.distributions.Laplace(mu,sigma)           # Laplace\n",
    "\n",
    "    x_0 = torch.arange(1,(max_n+TARGET),TARGET).to(device=device)\n",
    "\n",
    "    x = torch.repeat_interleave(x_0.unsqueeze(dim=1), repeats=N_gaussians, dim=1)   # expand dim\n",
    "    # 求解每个长度为TARGET的区间上的cdf\n",
    "    y = (m.cdf(x+TARGET) - m.cdf(x)).to(device=device)\n",
    "\n",
    "    # 方法一：\n",
    "    y_pred_1 = torch.sum(pi*y,dim=1)\n",
    "    # 方法二：做一下(0,1)上的归一化\n",
    "    y_pred_2 = y_pred_1/y_pred_1.sum()\n",
    "\n",
    "    # The input distrb.\n",
    "    input_data = input[0:2,:]\n",
    "    x_input = torch.arange(1,(300+1),SCALE).to(device=device)\n",
    "    y_input_0 = (input_data[0,:]).to(device=device)\n",
    "    y_input_1 = (input_data[1,:]).to(device=device)\n",
    "\n",
    "    # Init\n",
    "    win_str = \"total_train_step = \"+str(total_train_step)\n",
    "    title_str = \"Distrb. in \"+win_str\n",
    "    viz.line(X = [0.],Y = [0.], env=\"001\", win=win_str, opts= dict(title=title_str))\n",
    "\n",
    "    # Visdom本身不能把hist和line画在一个window中\n",
    "    # 如果想画一起只能是两条line\n",
    "    # Plot y_target\n",
    "    # viz.histogram(X = n, env=\"001\", win=win_str,\n",
    "    #         opts= dict(title=title_str))\n",
    "    viz.line(X = n,Y= p, env=\"001\", win=win_str, update=\"append\", name='target',\n",
    "            opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "    # Plot y_pred\n",
    "    viz.line(X = x_0,Y= y_pred_1, env=\"001\", win=win_str, update=\"append\", name='pred-1',\n",
    "            opts= dict(title=title_str))\n",
    "\n",
    "    # Plot y_pred_2\n",
    "    viz.line(X = x_0,Y= y_pred_2, env=\"001\", win=win_str, update=\"append\", name='pred-2',\n",
    "            opts= dict(title=title_str))\n",
    "\n",
    "    # Plot y_input\n",
    "    # 如果input data长度短，全部画完\n",
    "    if(len(x_input) <= max_n):\n",
    "        viz.line(X = x_input,Y= y_input_0, env=\"001\", win=win_str, update=\"append\", name='GT-1', opts= dict(title=title_str))\n",
    "        viz.line(X = x_input,Y= y_input_1, env=\"001\", win=win_str, update=\"append\", name='GT-2', opts= dict(title=title_str))\n",
    "    # 如果input data长度长，则截断\n",
    "    else:\n",
    "        x_input_0 = x_input[0:max_n]\n",
    "        y_input_2 = y_input_0[0:max_n]/y_input_0[0:max_n].sum()\n",
    "        y_input_3 = y_input_1[0:max_n]/y_input_1[0:max_n].sum()\n",
    "        viz.line(X = x_input_0,Y= y_input_2, env=\"001\", win=win_str, update=\"append\", name='GT-1', opts= dict(title=title_str))\n",
    "        viz.line(X = x_input_0,Y= y_input_3, env=\"001\", win=win_str, update=\"append\", name='GT-2', opts= dict(title=title_str))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2.2 plot mdn cdf[无input data]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "def draw_mdn_cdf(pi, mu, sigma, target, input, total_train_step, N_gaussians):\n",
    "    # 只画一个batch中的第一个\n",
    "\n",
    "    # The target distrb.\n",
    "    target = torch.unique(target,dim=0)    # drop the duplicate\n",
    "    n_target = target[:,0]\n",
    "    non_zero_idx = torch.nonzero(n_target)\n",
    "    n = n_target[non_zero_idx]\n",
    "\n",
    "    p_target = target[:,1]\n",
    "    p = p_target[non_zero_idx]\n",
    "    max_n = max(n).item()               # 横轴长度\n",
    "\n",
    "    # The predicted distrb.\n",
    "    m = torch.distributions.Normal(mu,sigma)\n",
    "    x_0 = torch.arange(1,max_n).to(device=device)\n",
    "    x = torch.repeat_interleave(x_0.unsqueeze(dim=1), repeats=N_gaussians, dim=1)\n",
    "    y = m.cdf(x).to(device=device)                        # y:多条高斯曲线; y_pred: 一条GMM曲线\n",
    "    # y_pred = torch.unsqueeze(torch.sum(pi*y,dim=1),dim=1)                 # 维度相等才能cat\n",
    "    y_pred = torch.sum(pi*y,dim=1)\n",
    "\n",
    "    # Init\n",
    "    win_str = \"total_train_step = \"+str(total_train_step)\n",
    "    title_str = \"Distrb. in \"+win_str\n",
    "    viz.line(X = [0.],Y = [0.], env=\"001\", win=win_str, opts= dict(title=title_str))\n",
    "\n",
    "    # Visdom本身不能把hist和line画在一个window中\n",
    "    # 如果想画一起只能是两条line\n",
    "    # Plot y_target\n",
    "    # viz.histogram(X = n, env=\"001\", win=win_str,\n",
    "    #         opts= dict(title=title_str))\n",
    "    viz.line(X = n,Y= p, env=\"001\", win=win_str, update=\"append\", name='target',\n",
    "            opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "    # Plot y_pred\n",
    "    viz.line(X = x_0,Y= y_pred, env=\"001\", win=win_str, update=\"append\", name='pred',\n",
    "            opts= dict(title=title_str))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2.3 plot mdn pdf in TEST\n",
    "1. 读入2个model时，在test set上比较效果"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [],
   "source": [
    "test_env = \"003\"\n",
    "def draw_mdn_3(Pi1, Mu1, Sigma1, Pi2, Mu2, Sigma2, Target, Input, test_batch, N_gaussians):\n",
    "    for i in range(len(Pi1)):\n",
    "        # The target distrb.\n",
    "        target = Target[i]\n",
    "        target = torch.unique(target,dim=0)    # drop the duplicate\n",
    "        n_target = target[:,0]\n",
    "        non_zero_idx = torch.nonzero(n_target)\n",
    "        n = n_target[non_zero_idx]\n",
    "\n",
    "        p_target = target[:,1]\n",
    "        p = p_target[non_zero_idx]\n",
    "        max_n = int(max(n).item())\n",
    "\n",
    "        # The predicted distrb.\n",
    "        mu1 = Mu1[i]\n",
    "        mu2 = Mu2[i]\n",
    "        sigma1 = Sigma1[i]\n",
    "        sigma2 = Sigma2[i]\n",
    "        m1 = torch.distributions.Normal(mu1,sigma1)          # Gaussian\n",
    "        m2 = torch.distributions.Normal(mu2,sigma2)          # Gaussian\n",
    "        # m = torch.distributions.Laplace(mu,sigma)           # Laplace\n",
    "\n",
    "        x_0 = torch.arange(1,(max_n+TARGET),TARGET).to(device=device)\n",
    "\n",
    "        x = torch.repeat_interleave(x_0.unsqueeze(dim=1), repeats=N_gaussians, dim=1)   # expand dim\n",
    "        # 求解每个长度为TARGET的区间上的cdf\n",
    "        y1 = (m1.cdf(x+TARGET) - m1.cdf(x)).to(device=device)\n",
    "        y2 = (m2.cdf(x+TARGET) - m2.cdf(x)).to(device=device)\n",
    "\n",
    "        # 方法一：\n",
    "        pi1 = Pi1[i]\n",
    "        pi2 = Pi2[i]\n",
    "        y_mdn_1 = torch.sum(pi1*y1,dim=1)\n",
    "        y_mdn_2 = torch.sum(pi2*y2,dim=1)\n",
    "        # 方法二：做一下(0,1)上的归一化\n",
    "        y_pred_1 = y_mdn_1/y_mdn_1.sum()\n",
    "        y_pred_2 = y_mdn_2/y_mdn_2.sum()\n",
    "\n",
    "        # The input distrb.\n",
    "        input = Input[i]\n",
    "        input_data = input[0:2,:]\n",
    "        x_input = torch.arange(1,(300+1),SCALE).to(device=device)\n",
    "        y_input_0 = (input_data[0,:]).to(device=device)\n",
    "        y_input_1 = (input_data[1,:]).to(device=device)\n",
    "\n",
    "        # Init\n",
    "        win_str = \"Test batch = \"+str(test_batch)+\" idx = \"+str(i)\n",
    "        title_str = \"Distrb. of \"+win_str\n",
    "        viz.line(X = [0.],Y = [0.], env=test_env, win=win_str, opts= dict(title=title_str))\n",
    "\n",
    "        # Visdom本身不能把hist和line画在一个window中\n",
    "        # 如果想画一起只能是两条line\n",
    "        # Plot y_target\n",
    "        # viz.histogram(X = n, env=\"001\", win=win_str,\n",
    "        #         opts= dict(title=title_str))\n",
    "        viz.line(X = n,Y= p, env=test_env, win=win_str, update=\"append\", name='target',\n",
    "                opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "        # Plot y_pred\n",
    "        viz.line(X = x_0,Y= y_pred_1, env=test_env, win=win_str, update=\"append\", name='pred-MLE',opts= dict(title=title_str))\n",
    "        viz.line(X = x_0,Y= y_pred_2, env=test_env, win=win_str, update=\"append\", name='pred-WD',\n",
    "                opts= dict(title=title_str))\n",
    "\n",
    "        # Plot y_input\n",
    "        # 如果input data长度短，全部画完\n",
    "        if(len(x_input) <= max_n):\n",
    "            viz.line(X = x_input,Y= y_input_0, env=test_env, win=win_str, update=\"append\", name='GT-1', opts= dict(title=title_str))\n",
    "            viz.line(X = x_input,Y= y_input_1, env=test_env, win=win_str, update=\"append\", name='GT-2', opts= dict(title=title_str))\n",
    "        # 如果input data长度长，则截断\n",
    "        else:\n",
    "            x_input_0 = x_input[0:max_n]\n",
    "            y_input_2 = y_input_0[0:max_n]/y_input_0[0:max_n].sum()\n",
    "            y_input_3 = y_input_1[0:max_n]/y_input_1[0:max_n].sum()\n",
    "            viz.line(X = x_input_0,Y= y_input_2, env=test_env, win=win_str, update=\"append\", name='GT-1', opts= dict(title=title_str))\n",
    "            viz.line(X = x_input_0,Y= y_input_3, env=test_env, win=win_str, update=\"append\", name='GT-2', opts= dict(title=title_str))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.3 png格式的net structure"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "def draw_the_net_png():\n",
    "\n",
    "    x = torch.randn([5, 3, 300])  # 定义网络的输入值\n",
    "    mlp = MLP(N_gaussians)\n",
    "    y = mlp(x)                    # 获取网络的预测值\n",
    "\n",
    "    MyConvNetVis = make_dot(y, params=dict(list(mlp.named_parameters()) + [('x', x)]))\n",
    "    MyConvNetVis.format = \"png\"\n",
    "    # 指定文件生成的文件夹\n",
    "    MyConvNetVis.directory = \"data_pic\"\n",
    "    # 生成文件\n",
    "    MyConvNetVis.view()\n",
    "# draw_the_net_png()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.4 plot loss图像\n",
    "1. 直接根据参数决定draw什么loss，反正loss都画在一个window里面"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [],
   "source": [
    "win_train_loss_str = \"The Loss of BATCH in the Training Data\"\n",
    "win_vali_loss_str = \"The Loss in the Vali Data\"\n",
    "win_train_epoch_loss_str = \"The Loss of EPOCH in the Training Data\"\n",
    "\n",
    "def draw_loss(X_step, loss, win_str):\n",
    "    viz.line(X = [X_step], Y = [loss],win=win_str, update=\"append\",\n",
    "        opts= dict(title=win_str))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.5 grad graph\n",
    "https://github.com/t-vi/pytorch-tvmisc/blob/master/visualize/bad_grad_viz.ipynb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "\n",
    "def iter_graph(root, callback):\n",
    "    queue = [root]\n",
    "    seen = set()\n",
    "    while queue:\n",
    "        # 队头出fn然后判断fn是否seen\n",
    "        fn = queue.pop()\n",
    "        if fn in seen:\n",
    "            continue\n",
    "        # unseen加入queue，并且继续沿着fn向下递归（DFS？）\n",
    "        seen.add(fn)\n",
    "        for next_fn, _ in fn.next_functions:\n",
    "            # Alert：只有非none的才会加入queue，然后在register_grad里被fn_dict记录下grad_input\n",
    "            if next_fn is not None:\n",
    "                queue.append(next_fn)\n",
    "        callback(fn)\n",
    "\n",
    "def register_hooks(var):\n",
    "    fn_dict = {}  # 记录了grad名字和值（grad_input）\n",
    "    def hook_cb(fn):\n",
    "        def register_grad(grad_input, grad_output):\n",
    "            fn_dict[fn] = grad_input\n",
    "        fn.register_hook(register_grad)\n",
    "\n",
    "    # 1. 递归注册grad_input\n",
    "    iter_graph(var.grad_fn, hook_cb)\n",
    "\n",
    "    # def is_bad_grad(grad_output):\n",
    "    #     if grad_output is None:\n",
    "    #         return False\n",
    "    #     return grad_output.isnan().any() or (grad_output.abs() >= 1e6).any()\n",
    "    def is_bad_grad(grad_output):\n",
    "        if grad_output is None:\n",
    "                return True\n",
    "        grad_output = grad_output.data\n",
    "        return grad_output.ne(grad_output).any() or grad_output.gt(1e6).any()\n",
    "\n",
    "    # 2. graph已经从grad input构建完了，还有grad output的判断以及颜色的选择\n",
    "    def make_dot():\n",
    "        node_attr = dict(style='filled',\n",
    "                        shape='box',\n",
    "                        align='left',\n",
    "                        fontsize='12',\n",
    "                        ranksep='0.1',\n",
    "                        height='0.2')\n",
    "        dot = Digraph(node_attr=node_attr, graph_attr=dict(size=\"12,12\"))\n",
    "\n",
    "        def size_to_str(size):\n",
    "            return '('+(', ').join(map(str, size))+')'\n",
    "\n",
    "        def build_graph(fn):\n",
    "            if hasattr(fn, 'variable'):  # if GradAccumulator\n",
    "                u = fn.variable\n",
    "                node_name = 'Variable\\n ' + size_to_str(u.size())\n",
    "                dot.node(str(id(u)), node_name, fillcolor='lightblue')\n",
    "            else:  # 不是variable属性，白色\n",
    "                assert fn in fn_dict, fn   # 判断fn在不在fn_dict里面！\n",
    "                fillcolor = 'white'\n",
    "                if any(is_bad_grad(gi) for gi in fn_dict[fn]):\n",
    "                    fillcolor = 'red'\n",
    "                dot.node(str(id(fn)), str(type(fn).__name__), fillcolor=fillcolor)\n",
    "            for next_fn, _ in fn.next_functions:\n",
    "                if next_fn is not None:\n",
    "                    next_id = id(getattr(next_fn, 'variable', next_fn))\n",
    "                    dot.edge(str(next_id), str(id(fn)))\n",
    "        iter_graph(var.grad_fn, build_graph)\n",
    "\n",
    "        return dot\n",
    "\n",
    "    return make_dot\n",
    "\n",
    "# x = torch.randn(10, 10, requires_grad=True)\n",
    "# y = torch.randn(10, 10, requires_grad=True)\n",
    "#\n",
    "# z = x / (y * 0)\n",
    "# z = z.sum() * 2\n",
    "# get_dot = register_hooks(z)\n",
    "\n",
    "\n",
    "# with torch.autograd.detect_anomaly():\n",
    "#     z.backward()\n",
    "#     dot = get_dot()\n",
    "#     dot.save('tmp.dot') # to get .dot\n",
    "#     dot.render('tmp') # to get SVG\n",
    "#     # dot # in Jupyter, you can just render the variable"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6 Training\n",
    "## 6.1 Init"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "       BatchNorm1d-1               [-1, 3, 300]               0\n",
      "           Flatten-2                  [-1, 900]               0\n",
      "            Linear-3                  [-1, 300]         270,300\n",
      "           Dropout-4                  [-1, 300]               0\n",
      "          Softplus-5                  [-1, 300]               0\n",
      "            Linear-6                  [-1, 100]          30,100\n",
      "           Dropout-7                  [-1, 100]               0\n",
      "          Softplus-8                  [-1, 100]               0\n",
      "            Linear-9                   [-1, 30]           3,030\n",
      "          Dropout-10                   [-1, 30]               0\n",
      "         Softplus-11                   [-1, 30]               0\n",
      "           Linear-12                   [-1, 12]             372\n",
      "          Dropout-13                   [-1, 12]               0\n",
      "         Softplus-14                   [-1, 12]               0\n",
      "           Linear-15                    [-1, 3]              39\n",
      "          Softmax-16                    [-1, 3]               0\n",
      "           Linear-17                    [-1, 3]              39\n",
      "           Linear-18                    [-1, 3]              39\n",
      "================================================================\n",
      "Total params: 303,919\n",
      "Trainable params: 303,919\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 1.16\n",
      "Estimated Total Size (MB): 1.19\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "mlp = MLP_1_3(N_gaussians)\n",
    "\n",
    "# Init the params\n",
    "# # mlp = model_param_init(mlp)\n",
    "# Init the vis\n",
    "viz.line(X = [0.],Y = [0.], env=\"001\", win=win_train_loss_str, opts= dict(title=win_train_loss_str))\n",
    "viz.line(X = [0.],Y = [0.], env=\"001\", win=win_vali_loss_str, opts= dict(title=win_vali_loss_str))\n",
    "viz.line(X = [0.],Y = [0.], env=\"001\", win=win_train_epoch_loss_str, opts= dict(title=win_train_epoch_loss_str))\n",
    "\n",
    "# Save the init params\n",
    "# torch.save(mlp.state_dict(), 'mlp_init.pth')\n",
    "\n",
    "# Load the saved model\n",
    "# model_path_WD = \"mlp_train=300_WD.pth\"\n",
    "# model_data = torch.load(model_path_WD)\n",
    "# mlp.load_state_dict(model_data)\n",
    "\n",
    "mlp = mlp.to(device=device)\n",
    "summary(mlp, (3,300))\n",
    "\n",
    "############ learning rate strategy ############\n",
    "# 1.\n",
    "# Set different lr for params\n",
    "# id: id() 函数返回对象的唯一标识符，标识符是一个整数。返回对象的内存地址。\n",
    "mu_params = list(map(id, mlp.z_mu.parameters()))\n",
    "base_params = filter(lambda p: id(p) not in mu_params, mlp.parameters())\n",
    "params = [{'params': base_params},         # 如果对某个参数不指定学习率，就使用最外层的默认学习率\n",
    "        {'params': mlp.z_mu.parameters(), 'lr': lr_for_mu}]\n",
    "# optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "\n",
    "optimizer = torch.optim.Adagrad(params,lr=learning_rate)\n",
    "\n",
    "# 2.\n",
    "# Or set lr decay\n",
    "# StepLR为步进，每step_size个epoch，lr*gamma\n",
    "# scheduler  = torch.optim.lr_scheduler.StepLR(optimizer,step_size=10,gamma=0.8)\n",
    "# 下面这个比较好用\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[15,30], gamma=0.7, last_epoch=-1)\n",
    "\n",
    "# # Set the hooks\n",
    "# #mlp.conv1.register_forward_hook(hook_forward_fn)\n",
    "# mlp.conv1.register_full_backward_hook(hook_backward_fn)\n",
    "#\n",
    "# #mlp.linear1.register_forward_hook(hook_forward_fn)\n",
    "# #mlp.linear1.register_full_backward_hook(hook_backward_fn)\n",
    "#\n",
    "# #mlp.z_pi.register_forward_hook(hook_forward_fn)\n",
    "# mlp.z_pi.register_full_backward_hook(hook_backward_fn_pi)\n",
    "#\n",
    "# #mlp.z_mu.register_forward_hook(hook_forward_fn)\n",
    "# mlp.z_mu.register_full_backward_hook(hook_backward_fn_mu)\n",
    "#\n",
    "# mlp.z_sigma.register_forward_hook(hook_forward_fn)\n",
    "# mlp.z_sigma.register_full_backward_hook(hook_backward_fn_sigma)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.2 grad check\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [],
   "source": [
    "# # grad check\n",
    "# input_test = torch.randn((1,3,100), requires_grad=True,device=device)\n",
    "# test_ans = gradcheck(mlp.to(device), input_test, eps=1e-3)  #, eps=1e-6, atol=1e-4\n",
    "# print(\"Are the gradients correct: \", test_ans)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.3 START HERE\n",
    "1. 使用Laplace分布改：\n",
    "    - loss function和draw"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- 11 batch----\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Now this is EPOCH 0 ==========\n",
      "========== IN EPOCH 0 the total loss is 0.09649352030828595 ==========\n",
      "========== Now this is EPOCH 1 ==========\n",
      "========== IN EPOCH 1 the total loss is 0.08408396318554878 ==========\n",
      "========== Now this is EPOCH 2 ==========\n",
      "========== IN EPOCH 2 the total loss is 0.08184711856301874 ==========\n",
      "========== Now this is EPOCH 3 ==========\n",
      "========== IN EPOCH 3 the total loss is 0.08292771514970809 ==========\n",
      "========== Now this is EPOCH 4 ==========\n",
      "========== IN EPOCH 4 the total loss is 0.0834447464440018 ==========\n",
      "========== Now this is EPOCH 5 ==========\n",
      "========== IN EPOCH 5 the total loss is 0.07907172269187868 ==========\n",
      "========== Now this is EPOCH 6 ==========\n",
      "========== IN EPOCH 6 the total loss is 0.07597353018354625 ==========\n",
      "========== Now this is EPOCH 7 ==========\n",
      "========== IN EPOCH 7 the total loss is 0.07259535975754261 ==========\n",
      "========== Now this is EPOCH 8 ==========\n",
      "========== IN EPOCH 8 the total loss is 0.07312399614602327 ==========\n",
      "========== Now this is EPOCH 9 ==========\n",
      "========== IN EPOCH 9 the total loss is 0.07168730878038332 ==========\n",
      "========== Now this is EPOCH 10 ==========\n",
      "========== IN EPOCH 10 the total loss is 0.06493317033164203 ==========\n",
      "========== Now this is EPOCH 11 ==========\n",
      "========== IN EPOCH 11 the total loss is 0.06344828766305 ==========\n",
      "========== Now this is EPOCH 12 ==========\n",
      "========== IN EPOCH 12 the total loss is 0.06496225704904646 ==========\n",
      "========== Now this is EPOCH 13 ==========\n",
      "========== IN EPOCH 13 the total loss is 0.06454856670461595 ==========\n",
      "========== Now this is EPOCH 14 ==========\n",
      "========== IN EPOCH 14 the total loss is 0.059040883381385356 ==========\n",
      "========== Now this is EPOCH 15 ==========\n",
      "========== IN EPOCH 15 the total loss is 0.06076332647353411 ==========\n",
      "========== Now this is EPOCH 16 ==========\n",
      "========== IN EPOCH 16 the total loss is 0.0631785549921915 ==========\n",
      "========== Now this is EPOCH 17 ==========\n",
      "========== IN EPOCH 17 the total loss is 0.06467701174551621 ==========\n",
      "========== Now this is EPOCH 18 ==========\n",
      "========== IN EPOCH 18 the total loss is 0.05794981948565692 ==========\n",
      "========== Now this is EPOCH 19 ==========\n",
      "========== IN EPOCH 19 the total loss is 0.05626190867042169 ==========\n",
      "========== Now this is EPOCH 20 ==========\n",
      "========== IN EPOCH 20 the total loss is 0.059449233929626644 ==========\n",
      "========== Now this is EPOCH 21 ==========\n",
      "========== IN EPOCH 21 the total loss is 0.05624595552217215 ==========\n",
      "========== Now this is EPOCH 22 ==========\n",
      "========== IN EPOCH 22 the total loss is 0.05853118887171149 ==========\n",
      "========== Now this is EPOCH 23 ==========\n",
      "========== IN EPOCH 23 the total loss is 0.0540138027863577 ==========\n",
      "========== Now this is EPOCH 24 ==========\n",
      "========== IN EPOCH 24 the total loss is 0.06068920413963497 ==========\n",
      "========== Now this is EPOCH 25 ==========\n",
      "========== IN EPOCH 25 the total loss is 0.055282893939875066 ==========\n",
      "========== Now this is EPOCH 26 ==========\n",
      "========== IN EPOCH 26 the total loss is 0.05484951916150749 ==========\n",
      "========== Now this is EPOCH 27 ==========\n",
      "========== IN EPOCH 27 the total loss is 0.05411059135803953 ==========\n",
      "========== Now this is EPOCH 28 ==========\n",
      "========== IN EPOCH 28 the total loss is 0.05406765633961186 ==========\n",
      "========== Now this is EPOCH 29 ==========\n",
      "========== IN EPOCH 29 the total loss is 0.05513534293277189 ==========\n",
      "========== Now this is EPOCH 30 ==========\n",
      "========== IN EPOCH 30 the total loss is 0.05235983454622328 ==========\n",
      "========== Now this is EPOCH 31 ==========\n",
      "========== IN EPOCH 31 the total loss is 0.053405810438562185 ==========\n",
      "========== Now this is EPOCH 32 ==========\n",
      "========== IN EPOCH 32 the total loss is 0.05278146831551567 ==========\n",
      "========== Now this is EPOCH 33 ==========\n",
      "========== IN EPOCH 33 the total loss is 0.052162763429805636 ==========\n",
      "========== Now this is EPOCH 34 ==========\n",
      "========== IN EPOCH 34 the total loss is 0.05272416619118303 ==========\n",
      "========== Now this is EPOCH 35 ==========\n",
      "========== IN EPOCH 35 the total loss is 0.05362208292353898 ==========\n",
      "========== Now this is EPOCH 36 ==========\n",
      "========== IN EPOCH 36 the total loss is 0.05048673856072128 ==========\n",
      "========== Now this is EPOCH 37 ==========\n",
      "========== IN EPOCH 37 the total loss is 0.04958011762937531 ==========\n",
      "========== Now this is EPOCH 38 ==========\n",
      "========== IN EPOCH 38 the total loss is 0.05203409062232822 ==========\n",
      "========== Now this is EPOCH 39 ==========\n",
      "========== IN EPOCH 39 the total loss is 0.05082449194742367 ==========\n",
      "Total training time when epoch= *40* is *1005.1960380077362 *s\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# filename = \"../log_file.txt\"\n",
    "# f = open(filename,'w')\n",
    "total_train_step = 0\n",
    "EPOCH_NUM = 40\n",
    "\n",
    "train_start_time = time.time()\n",
    "mlp.train()\n",
    "\n",
    "for epoch in range(EPOCH_NUM):\n",
    "    # # Save the net structure\n",
    "    # net_file_name = \"net_before_epoch\"+str(epoch)+\".pth\"\n",
    "    # net_path = net_root_path + net_file_name\n",
    "    # torch.save(mlp.state_dict(), net_path)\n",
    "\n",
    "    epoch_train_loss = 0\n",
    "    print(f\"========== Now this is EPOCH {epoch} ==========\")\n",
    "    for batch_id,data in enumerate(train_loader):\n",
    "\n",
    "        input_data, target = data\n",
    "        # print(f\"---- {batch_id} batch----\")\n",
    "        # Do the inference\n",
    "        input_data = input_data.to(device)\n",
    "        target = target.to(device)\n",
    "        pi, mu, sigma = mlp(input_data)\n",
    "\n",
    "        # Save the params\n",
    "        # params = list(mlp.named_parameters())\n",
    "\n",
    "        # Cal the MLE loss and draw the distrb.\n",
    "        # loss = loss_fn_v2(pi, mu, sigma, target, N_gaussians)\n",
    "        # loss = loss_fn_v3(pi, mu, sigma, target, N_gaussians)\n",
    "        # loss = loss_fn_v5(pi, mu, sigma, target, N_gaussians)\n",
    "\n",
    "        # Cal the CDF loss and draw the distrb.\n",
    "        # loss = loss_fn_cdf(pi, mu, sigma, target, N_gaussians)\n",
    "\n",
    "        # Cal the CE or KL loss and draw the distrb.\n",
    "        # loss  = loss_fn_CE(pi, mu, sigma, target, N_gaussians)\n",
    "\n",
    "        # Cal the WD loss and draw the distrb.\n",
    "        loss  = loss_fn_WD(pi, mu, sigma, target, N_gaussians)\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "        draw_loss(total_train_step, loss.item(),win_train_loss_str)\n",
    "\n",
    "        # print(\"训练次数：{}，Loss：{}\".format(total_train_step, loss.item()))\n",
    "\n",
    "        ########### before step() ###############\n",
    "\n",
    "        # for name, parms in mlp.named_parameters():\n",
    "        #     print(\"=====before step()=====\")\n",
    "        #     print('-->name:', name)\n",
    "        #     print('-->params:', parms.data)\n",
    "        #     # print('-->grad_requirs:',parms.requires_grad)\n",
    "        #     print('-->grad_value:',parms.grad)\n",
    "        #     print(\"===\")\n",
    "\n",
    "        # Optim\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        ######\n",
    "\n",
    "        # 反向传播时检测是否有异常值，定位code\n",
    "        # with torch.autograd.detect_anomaly():\n",
    "        #     loss.backward()\n",
    "\n",
    "        ######\n",
    "        # get_dot = register_hooks(loss)\n",
    "        # dot = get_dot()\n",
    "        # dot.save('tmp.dot')   # to get .dot\n",
    "        # dot.render('tmp')     # to get pdf\n",
    "\n",
    "        ######\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # for name, parms in mlp.named_parameters():\n",
    "        #     print(\"=====After step()=====\")\n",
    "        #     print('-->name:', name)\n",
    "        #     print('-->params:', parms.data)\n",
    "        #     # print('-->grad_requirs:',parms.requires_grad)\n",
    "        #     print('-->grad_value:',parms.grad)\n",
    "        #     print(\"===\")\n",
    "\n",
    "        if total_train_step % 20 == 0:\n",
    "        # Only draw the 1st result in a training batch (5 in total)\n",
    "            with torch.no_grad():\n",
    "                # draw_mdn_2(pi[0,:].clone().detach(), mu[0,:].clone().detach(), sigma[0,:].clone().detach(), target[0].clone().detach(), input_data[0].clone().detach(),total_train_step, N_gaussians)\n",
    "                # draw_mdn_cdf(pi[0,:].detach(), m[0], target[0].detach(), total_train_step, loss_list[0],N_gaussians)\n",
    "\n",
    "                ########### Do validation\n",
    "                mlp.eval()\n",
    "                total_vali_loss = 0\n",
    "                for vali_batch_id, vali_data in enumerate(val_loader):\n",
    "                    vali_input_data, vali_target = vali_data\n",
    "                    vali_input_data = vali_input_data.to(device)\n",
    "                    vali_pi, vali_mu, vali_sigma = mlp(vali_input_data)\n",
    "\n",
    "                    # Cal the sum of vali loss instead of vali loss in a batch\n",
    "                    # vali_duration,vali_m  = loss_preparation(vali_pi.detach(), vali_mu.detach(), vali_sigma.detach(), vali_target.detach())\n",
    "                    vali_loss = loss_fn_v2(vali_pi, vali_mu, vali_sigma, vali_target, N_gaussians)\n",
    "                    total_vali_loss += vali_loss.item()\n",
    "\n",
    "                # Plot the vali loss\n",
    "                draw_loss(epoch, (total_vali_loss), win_vali_loss_str)\n",
    "                mlp.train()\n",
    "\n",
    "        total_train_step += 1\n",
    "    # Plot the loss in this EPOCH\n",
    "    print(f\"========== IN EPOCH {epoch} the total loss is {epoch_train_loss} ==========\")\n",
    "\n",
    "    # # Save the net structure\n",
    "    # net_file_name = \"net_after_epoch\"+str(epoch)+\".pth\"\n",
    "    # net_path = net_root_path + net_file_name\n",
    "    # torch.save(mlp.state_dict(), net_path)\n",
    "\n",
    "    # Plot loss of this EPOCH\n",
    "    draw_loss(epoch, epoch_train_loss,win_train_epoch_loss_str)\n",
    "    # writer.add_scalars(\"Loss of EPOCH\",{\"Train\":epoch_train_loss},epoch)\n",
    "\n",
    "    # Record the params\n",
    "    # for name,param in params:\n",
    "    #     #参数的梯度\n",
    "    #     #print(\"name:\",name)\n",
    "    #     #print(\"param.grad:\",param.grad)\n",
    "    #     # writer.add_histogram(tag = name +'_grad of EPOCH '+str(epoch),values=param.grad,global_step=epoch)\n",
    "    #     # #参数值\n",
    "    #     writer.add_histogram(tag = name +'data of EPOCH '+str(epoch),values=param.data,global_step=epoch)\n",
    "\n",
    "# f.close()\n",
    "train_end_time = time.time()\n",
    "print(f\"Total training time when epoch= *{EPOCH_NUM}* is *{train_end_time-train_start_time} *s\")\n",
    "writer.close()\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [],
   "source": [
    "model_path_MLE = \"mlp_train=300_MLE.pth\"\n",
    "# model_path_WD = \"mlp_train=300_WD_epoch=80.pth\"\n",
    "model_path_WD = \"mlp_train=300_WD.pth\"\n",
    "torch.save(mlp.state_dict(), model_path_WD)\n",
    "# torch.save(mlp.state_dict(), model_path_MLE)\n",
    "\n",
    "# viz.delete_env(\"001_test\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [],
   "source": [
    "# Gtmp = pgv.AGraph('tmp.dot')\n",
    "# G = nx.Graph(Gtmp)\n",
    "# nx.draw(G)\n",
    "# # plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "$\\mathcal{L}(y \\vert x) = - \\log\\bigg\\{\\sum_{k=1}^K \\pi_k(x)  \\mathcal{N}\\big(y \\vert \\mu_k(x), \\Sigma_k(x)\\big)\\bigg\\}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Load and test\n",
    "## 7.1 small batch\n",
    "- 对某个小batch画图，batch size可以=1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "outputs": [
    {
     "data": {
      "text/plain": "array([939])"
     },
     "execution_count": 646,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the data\n",
    "small_batch_size = 1\n",
    "\n",
    "small_test_idx = train_idx[3:4]\n",
    "small_test_loader = DataLoader(dataset = dataset,batch_size = small_batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(small_test_idx),collate_fn = my_collate_fn)\n",
    "small_test_idx"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "outputs": [],
   "source": [
    "# Plot\n",
    "test_env = \"002\"\n",
    "def draw_mdn_test(pi, m, target, input, epoch, idx,N_gaussians,flag):\n",
    "    # 只画一个batch中的第一个\n",
    "\n",
    "    # The target distrb.\n",
    "    target = torch.unique(target,dim=0)    # drop the duplicate\n",
    "    n_target = target[:,0]\n",
    "    non_zero_idx = torch.nonzero(n_target)\n",
    "    n = n_target[non_zero_idx]\n",
    "\n",
    "    p_target = target[:,1]\n",
    "    p = p_target[non_zero_idx]\n",
    "    max_n = max(n).item()               # 横轴长度\n",
    "\n",
    "    # The predicted distrb.\n",
    "    x_0 = torch.arange(1,max_n).to(device=device)\n",
    "    x = torch.repeat_interleave(x_0.unsqueeze(dim=1), repeats=N_gaussians, dim=1)\n",
    "    y = torch.exp(m.log_prob(x)).to(device=device)                          # y:多条高斯曲线; y_pred: 一条GMM曲线\n",
    "    # y_pred = torch.unsqueeze(torch.sum(pi*y,dim=1),dim=1)                 # 维度相等才能cat\n",
    "    y_pred = torch.sum(pi*y,dim=1)\n",
    "\n",
    "    # The input distrb.\n",
    "    input_data = input[0:2,:]\n",
    "    x_input = torch.arange(1,input_data.shape[1]+1).to(device=device)\n",
    "    y_input_0 = input_data[0,:].to(device=device)\n",
    "    y_input_1 = input_data[1,:].to(device=device)\n",
    "\n",
    "    # Init\n",
    "    win_str = flag+ \" epoch=\"+str(epoch)+\" | idx=\"+str(idx)\n",
    "    title_str = \"Distrb. \"+win_str\n",
    "    viz.line(X = [0.],Y = [0.], env=test_env, win=win_str, opts= dict(title=title_str))\n",
    "\n",
    "    # Visdom本身不能把hist和line画在一个window中\n",
    "    # 如果想画一起只能是两条line\n",
    "    # Plot y_target\n",
    "    # viz.histogram(X = n, env=\"001\", win=win_str,\n",
    "    #         opts= dict(title=title_str))\n",
    "    viz.line(X = n,Y= p, env=test_env, win=win_str, update=\"append\", name='target',\n",
    "            opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "    # Plot y_pred\n",
    "    viz.line(X = x_0,Y= y_pred, env=test_env, win=win_str, update=\"append\", name='pred',\n",
    "            opts= dict(title=title_str))\n",
    "\n",
    "    # Plot y_input\n",
    "    viz.line(X = x_input,Y= y_input_0, env=test_env, win=win_str, update=\"append\", name='GT-1', opts= dict(title=title_str))\n",
    "    viz.line(X = x_input,Y= y_input_1, env=test_env, win=win_str, update=\"append\", name='GT-2', opts= dict(title=title_str))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(EPOCH_NUM):\n",
    "\n",
    "    net_file_name_0 = \"net_before_epoch\"+str(epoch)+\".pth\"\n",
    "    net_file_name_1 = \"net_after_epoch\"+str(epoch)+\".pth\"\n",
    "    net_path0 = net_root_path + net_file_name_0\n",
    "    net_path1 = net_root_path + net_file_name_1\n",
    "\n",
    "    # Load the model\n",
    "    mlp0 = MLP(N_gaussians)\n",
    "    mlp1 = MLP(N_gaussians)\n",
    "    mlp0.load_state_dict(torch.load(net_path0))\n",
    "    mlp1.load_state_dict(torch.load(net_path1))\n",
    "    mlp0.to(device)\n",
    "    mlp1.to(device)\n",
    "\n",
    "    # 其实每个epoch只会执行一个batch\n",
    "    for batch_id,data in enumerate(small_test_loader):\n",
    "\n",
    "        # Predict\n",
    "        input_data, target = data\n",
    "        input_data = input_data.to(device)\n",
    "        target = target.to(device)\n",
    "        pi0, mu0, sigma0 = mlp0(input_data)\n",
    "        pi1, mu1, sigma1 = mlp1(input_data)\n",
    "        # Get the mdn\n",
    "        _,m0  = loss_preparation(pi0.detach(), mu0.detach(), sigma0.detach(), target.detach())\n",
    "        _,m1  = loss_preparation(pi1.detach(), mu1.detach(), sigma1.detach(), target.detach())\n",
    "\n",
    "        # Plot for 'small_batch_size' times\n",
    "        for j in range(small_batch_size):\n",
    "            draw_mdn_test(pi0[j,:].detach(), m0[j], target[j].detach(), input_data[j].detach(), epoch, j,N_gaussians,flag=\"before\")\n",
    "            draw_mdn_test(pi1[j,:].detach(), m1[j], target[j].detach(), input_data[j].detach(), epoch, j,N_gaussians,flag=\"after\")\n",
    "\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7.2 Test dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "outputs": [
    {
     "data": {
      "text/plain": "MLP_1_3(\n  (BN1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n  (BN2): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n  (drop): Dropout(p=0.5, inplace=False)\n  (linear1): Linear(in_features=900, out_features=300, bias=True)\n  (linear2): Linear(in_features=300, out_features=100, bias=True)\n  (linear3): Linear(in_features=100, out_features=30, bias=True)\n  (linear4): Linear(in_features=30, out_features=12, bias=True)\n  (ac_func): Softplus(beta=1, threshold=20)\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (z_pi): Sequential(\n    (0): Linear(in_features=12, out_features=3, bias=True)\n    (1): Softmax(dim=1)\n  )\n  (z_mu): Linear(in_features=12, out_features=3, bias=True)\n  (z_sigma): Linear(in_features=12, out_features=3, bias=True)\n)"
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved model\n",
    "# model_path_WD_2 = \"mlp_train=300_WD_epoch=80.pth\"\n",
    "model_path_WD = \"mlp_train=300_WD.pth\"\n",
    "model_path_MLE = \"mlp_train=300_MLE.pth\"\n",
    "\n",
    "\n",
    "mlp_1 = MLP_1_3(N_gaussians)\n",
    "mlp_2 = MLP_1_3(N_gaussians)\n",
    "\n",
    "model_data_1 = torch.load(model_path_MLE)\n",
    "# model_data_1 = torch.load(model_path_WD_2)\n",
    "model_data_2 = torch.load(model_path_WD)\n",
    "\n",
    "mlp_1.load_state_dict(model_data_1)\n",
    "mlp_2.load_state_dict(model_data_2)\n",
    "\n",
    "mlp_1.to(device)\n",
    "mlp_2.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_test_loss_1 WD-2: 245.71023178100586\n",
      "total_test_loss_2 WD : 360.57776641845703\n"
     ]
    }
   ],
   "source": [
    "total_test_loss_1 = 0\n",
    "total_test_loss_2 = 0\n",
    "i = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_id,data in enumerate(test_loader):\n",
    "            input_data, target = data\n",
    "            input_data = input_data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            pi_1, mu_1, sigma_1 = mlp_1(input_data)\n",
    "            pi_2, mu_2, sigma_2 = mlp_2(input_data)\n",
    "\n",
    "            draw_mdn_3(pi_1, mu_1, sigma_1, pi_2, mu_2, sigma_2, target, input_data,i, N_gaussians)\n",
    "\n",
    "            loss_1 = loss_fn_v2(pi_1, mu_1, sigma_1, target, N_gaussians)\n",
    "            loss_2 = loss_fn_v2(pi_2, mu_2, sigma_2, target, N_gaussians)\n",
    "\n",
    "            total_test_loss_1 += loss_1.item()\n",
    "            total_test_loss_2 += loss_2.item()\n",
    "\n",
    "            i += 1\n",
    "    # 这里print的loss可以视作平均到每个sample上的loss\n",
    "    print(\"total_test_loss_1 MLE:\",total_test_loss_1/len(test_loader))\n",
    "    print(\"total_test_loss_2 WD :\",total_test_loss_2/len(test_loader))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
