{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2022/6/7 15:17\n",
    "# @Author  : Wang Yujia\n",
    "# @File    : mlp.ipynb\n",
    "# @Description :\n",
    "# @TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0. what for\n",
    "1. loss部分逻辑重写，添加`no grad`\n",
    "2. 添加hooks\n",
    "3. 使用pad_seq函数取代自己写的pad函数\n",
    "4. 效果：可以train虽然loss不下降，不会出现NaN的问题"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Preparations\n",
    "## 1.1 global settings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# nums of Gaussian kernels\n",
    "N_gaussians = 3\n",
    "\n",
    "# dataset划分\n",
    "batch_size = 40\n",
    "train_pct = 0.7\n",
    "vali_pct = 0.2\n",
    "test_pct = 0.1\n",
    "\n",
    "# train and optim.\n",
    "learning_rate = 1e-2\n",
    "lr_for_mu = 1e-2   # 给mu单独设置learning rate\n",
    "total_train_step = 0\n",
    "total_test_step = 0\n",
    "EPOCH_NUM = 5\n",
    "MIN_LOSS = 1e-8\n",
    "\n",
    "# Range\n",
    "a = 0\n",
    "b = 255\n",
    "\n",
    "# training data的粒度，画图会使用到\n",
    "SCALE = 3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import torch.utils.data\n",
    "from mydataset import *\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from tensorboardX import SummaryWriter\n",
    "from visdom import Visdom\n",
    "from graphviz import Digraph\n",
    "from torchviz import make_dot\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pygraphviz as pgv\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "import scipy\n",
    "from torch.nn import KLDivLoss\n",
    "from torch.autograd.gradcheck import gradcheck\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 the data path\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# Training data\n",
    "train_path = r\"../data/train_100\"\n",
    "# train_path = r\"../data/train_60\"\n",
    "# train_path = r\"../data/train_300_v1\"\n",
    "\n",
    "# Target data\n",
    "# target_path = r\"../data/targets\"\n",
    "target_path = r\"../data/targets_5\"\n",
    "# target_path = r\"../data/targets_5_cdf\"\n",
    "# data keys\n",
    "data_key_path = \"../data/target_datakey.csv\"\n",
    "\n",
    "\n",
    "# Net path\n",
    "net_root_path = \"../net_saved/\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Dataset and Dataloader\n",
    "1. DataLoader中的shuffer=True表示在每一次epoch中都打乱所有数据的顺序，然后以batch为单位从头到尾按顺序取用数据。这样的结果就是不同epoch中的数据都是乱序的,设置随机种子的作用就是让你的每一次训练都乱的一样，\n",
    "\n",
    "## 2.1 Dataset and spliting\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 设置随机数种子"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup_seed(7)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 读取data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "dataset = myDataset(train_path, target_path, data_key_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 产生index的乱序排列"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 807  305  455  939  508  594  835 1082  598 1102   46  761  841  141\n",
      "  407  334  253  500  734  936  698  446  907 1087 1009  140  463  547\n",
      " 1155  856   34 1156  703 1121  751  587 1100  509  473  128  788   97\n",
      "  471  385 1085  525  679 1135  284 1146  186  318 1088 1113    9  457\n",
      " 1047  451  569 1120 1074  326  377  809  109   98  620 1194  825  828\n",
      "   83  113  556  674  568  853  351  558   54  656  804 1149  101  344\n",
      "  851  544  955   40 1021  489  626  664  657  868 1169  151  179 1130\n",
      "  564  713  743 1014  966  861 1132  146  610  408  662  551  172 1020\n",
      "  982 1148  431  517  270  858  170  374  816  618  205   17   53 1003\n",
      "  263  857  716  843  498  228  339  725  752  278  649 1017  108  642\n",
      " 1195 1174   99  530  632  888  189  961  358 1078  663  757 1051  204\n",
      "  409  283  562   23  619  216  474  921  950 1162  123  785  769  621\n",
      "  262  586 1178  541  795   70 1189  396  171  845  168 1125  361  224\n",
      "  125  706  164  231  264   42  746  872  998  132 1035  430   38  522\n",
      "  880  325  285  367  870  418   13  166  513  901  617  740  680 1126\n",
      "   20  372   71 1038  355   60  220  110    4 1115  572  415  336  316\n",
      "  449  412 1066  768 1055  704  869   94   95 1163 1145  648  884  885\n",
      " 1170  848   58  207  306 1067  729   24  644  633  252  832  244   25\n",
      " 1094   63  222 1070  996 1063  130  153 1180  922  399  997   75  894\n",
      " 1048  308  280  934  721  199  820  134  289  259 1159    3  510  492\n",
      "  196  565  563   26  913  483 1141  653   61  623  984  652  609  739\n",
      "  824 1018  891  467  842    8  503  714 1015  447  783  992   74 1184\n",
      "   57  978  484  297 1039  217  681 1027  960  350 1129 1045 1183  906\n",
      "  813  660  266    6  379  526  611  138  163  701 1031 1050   43  733\n",
      "  720  319 1158 1153  877  294  432  531  578  523   19  426  830  927\n",
      "  781 1033 1013  420  137  488  185  920  702 1134 1124  454  327  317\n",
      "  799  655  878  699  981  806  953  750  148  591  915  731  937  365\n",
      "  557  397 1173  779 1185  459  232  389 1147 1011  158  315  256  863\n",
      "   96 1157  445  384  511  817  328  314  357  347  452  298  899  887\n",
      " 1034  839  778  747  249  766   15  159  696  585  360   48  478  330\n",
      "  893  112  338  307  528  422 1142  971  506  504 1049  791  395  669\n",
      "  540  127 1160  902  507  980  237  167  602  983   80  414  640  300\n",
      "  977  324 1117  215 1058   30  242  697  860  951  209  671  742 1072\n",
      "  435   27 1060  518  122 1086  462 1028 1118  603  929  732  722  493\n",
      "  202  403 1089 1179  362  271  466  641 1061  744  272  277 1177   12\n",
      "  597  668  210  651  952  147  442  299  115  375  635  767  748 1080\n",
      "  755  177  472  685   51  810 1079  571  388   22  650  965  423  938\n",
      " 1112  643 1193  142   50  666 1143   81  482  321  948  687 1138   66\n",
      "  909  534  738  417 1005   64  895  886  918  491  700  790   65  968\n",
      "  627  999  296  419  438  261  837  281  631  677  188  881  251  694\n",
      "  514  590  487  273  943 1073  448  411 1131  933 1191  935  149  218\n",
      "  239  718  192  191  945  754  665  705  595  926  708  582 1046  323\n",
      " 1026 1071  658  437  770  833  341    2  229 1098 1152   76 1103  371\n",
      "  180  889  577  736  758 1019 1105 1010  589 1167  812  956  882   37\n",
      "  353 1095  236  928  499  553   55  302  286  800  219  789  387  737\n",
      "  390  352  193  235 1110  480  567  116 1057  957 1106  890  777  120\n",
      "  815  990  773   62  667  453  515  570  223  613  221  673  794  465\n",
      "   52 1093  485 1140 1053  155  912  600 1023  248  441  975 1164  542\n",
      "  117  724  628  712  916  291 1041  381  954  200  596  976 1137 1187\n",
      "  691  505  348 1190  340  615  154 1104  879  995  287  322  819  818\n",
      "  796   82  793  464  797  756 1168  771  333   10  470   32  782  413\n",
      "  580  382  930  760 1133  840 1139  378   89  792  601  213  332  133\n",
      "  346  855    7  941  320 1044  187  972  552  304   31  831  684 1065\n",
      "  240  993  174  245   56  630  583 1025  145   90 1114  392  682  310\n",
      "  359   93  947 1024  376  243 1076   87  214  444  173  425 1040  118\n",
      "  119 1151  905    0  394  401  404  836 1091  162  114  670  150  798\n",
      "  875 1064  368  543  477 1128  532  625  967  512  801   72  258  624\n",
      "  234  135  616  516  866  802  592 1056  501  538  897  728  581  343\n",
      "  925 1002  293  364  883  659   14  686   86  370  165  131  335   59\n",
      "  709  676  354  959  424  292   41  329  614  429   79]\n"
     ]
    }
   ],
   "source": [
    "shuffled_indices = np.random.permutation(dataset.__len__())\n",
    "\n",
    "train_idx = shuffled_indices[:int(train_pct*dataset.__len__())]\n",
    "# train_idx = shuffled_indices\n",
    "tmp = int((train_pct+vali_pct)*dataset.__len__())\n",
    "val_idx = shuffled_indices[int(train_pct*dataset.__len__()):tmp]\n",
    "\n",
    "test_idx = shuffled_indices[tmp:]\n",
    "print(train_idx)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 根据这个乱序排列抽取dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Dataloader and collating\n",
    "1. 主要是对label数据进行collate\n",
    "    - 按照batch中的最大target data长度进行padding，padding with 0\n",
    "2. 返回的结果多一个batch dim,比如下面的`5`\n",
    "    - After collating:\n",
    "        - `torch.Size([5, 3, 300]),torch.Size([5, 87, 2])`\n",
    "        - `87`是最长的targets data长度"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def test_pad_sequence():\n",
    "    seq1 = torch.tensor([[ 2., 0.04761905], [3., 0.14285714], [4., 0.04761905]])\n",
    "    seq2 = torch.tensor([[ 1., 0.04761905]])\n",
    "    seq3 = torch.tensor([[ 3., 0.14285714], [4., 0.04761905]])\n",
    "    ls = list((seq1,seq2,seq3))\n",
    "    ls_length = torch.tensor([3,1,2])\n",
    "    ans = pad_sequence(ls,batch_first=True)\n",
    "    print(ans)\n",
    "    # seq_pak = pack_padded_sequence(ans,ls_length,batch_first=True,enforce_sorted=False)\n",
    "    # seq_unpacked, lens_unpacked = pad_packed_sequence(seq_pak, batch_first=True)\n",
    "    # lens_unpacked"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.1 No rescale"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def my_collate_fn(data):\n",
    "# 这里的data是一个list， list的元素是元组: (self.data, self.label)\n",
    "# collate_fn的作用是把[(data, label),(data, label)...]转化成([data, data...],[label,label...])\n",
    "# 假设self.data的一个data的shape为(channels, length), 每一个channel的length相等,\n",
    "# data[索引到index(batch)][索引到data或者label][索引到channel]\n",
    "\n",
    "    data_list = []\n",
    "    target_list = []\n",
    "    data_len = len(data)      # 读进来的data batch的大小\n",
    "\n",
    "    batch = 0\n",
    "\n",
    "    while batch < data_len:\n",
    "        data_list.append(torch.tensor(data[batch][0]))\n",
    "        target_list.append(torch.tensor(data[batch][1]))\n",
    "        batch += 1\n",
    "\n",
    "    # Pad target data with zeros\n",
    "    target_padded = pad_sequence(target_list,batch_first=True)\n",
    "\n",
    "    data_tensor = torch.stack(data_list).float()\n",
    "    target_tensor = target_padded.float()\n",
    "\n",
    "    return data_tensor, target_tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.2 Rescale"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "def my_collate_fn_rescale(data):\n",
    "# 这里的data是一个list， list的元素是元组: (self.data, self.label)\n",
    "# collate_fn的作用是把[(data, label),(data, label)...]转化成([data, data...],[label,label...])\n",
    "# 假设self.data的一个data的shape为(channels, length), 每一个channel的length相等,\n",
    "# data[索引到index(batch)][索引到data或者label][索引到channel]\n",
    "\n",
    "    data_list = []\n",
    "    target_list = []\n",
    "    data_len = len(data)      # 读进来的data batch的大小\n",
    "\n",
    "    batch = 0\n",
    "\n",
    "    while batch < data_len:\n",
    "        data_df = pd.DataFrame(data[batch][0])\n",
    "        data_GT = data_df.iloc[0:2,:]\n",
    "\n",
    "        # Get the max/min in each chanel and broadcast it by hand\n",
    "        data_max = np.tile(pd.DataFrame(np.max(data_GT,axis=1)), (1, data_GT.shape[1]))\n",
    "        data_min = np.tile(pd.DataFrame(np.min(data_GT,axis=1)), (1, data_GT.shape[1]))\n",
    "        # Rescale\n",
    "        data_rescaled = a + (data_GT-data_min)*(b-a) / (data_max - data_min)\n",
    "        data_df.iloc[0:2,:] = data_rescaled\n",
    "\n",
    "        data_list.append(torch.tensor(np.array(data_df,dtype=float)))\n",
    "        # print(\"After scale:\",data_df)\n",
    "        target_list.append(torch.tensor(data[batch][1]))\n",
    "        batch += 1\n",
    "\n",
    "    # Pad target data with zeros\n",
    "    target_padded = pad_sequence(target_list,batch_first=True)\n",
    "\n",
    "    data_tensor = torch.stack(data_list).float()\n",
    "    target_tensor = target_padded.float()\n",
    "\n",
    "    return data_tensor, target_tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.3 log1p"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "def my_collate_fn_log1p(data):\n",
    "# 这里的data是一个list， list的元素是元组: (self.data, self.label)\n",
    "# collate_fn的作用是把[(data, label),(data, label)...]转化成([data, data...],[label,label...])\n",
    "# 假设self.data的一个data的shape为(channels, length), 每一个channel的length相等,\n",
    "# data[索引到index(batch)][索引到data或者label][索引到channel]\n",
    "\n",
    "    data_list = []\n",
    "    target_list = []\n",
    "    data_len = len(data)      # 读进来的data batch的大小\n",
    "\n",
    "    batch = 0\n",
    "\n",
    "    while batch < data_len:\n",
    "        data_df = pd.DataFrame(data[batch][0])\n",
    "        data_GT = data_df.iloc[0:2,:]\n",
    "\n",
    "        data_log1p = -np.log(data_GT+MIN_LOSS)\n",
    "        data_df.iloc[0:2,:] = data_log1p\n",
    "\n",
    "        data_list.append(torch.tensor(np.array(data_df,dtype=float)))\n",
    "        target_list.append(torch.tensor(data[batch][1]))\n",
    "        batch += 1\n",
    "\n",
    "    # Pad target data with zeros\n",
    "    target_padded = pad_sequence(target_list,batch_first=True)\n",
    "\n",
    "    data_tensor = torch.stack(data_list).float()\n",
    "    target_tensor = target_padded.float()\n",
    "\n",
    "    return data_tensor, target_tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.4 DataLoader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = dataset,batch_size = batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(train_idx), collate_fn = my_collate_fn)\n",
    "\n",
    "val_loader = DataLoader(dataset = dataset,batch_size = batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(val_idx),collate_fn = my_collate_fn)\n",
    "\n",
    "test_loader = DataLoader(dataset = dataset,batch_size = batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(test_idx),collate_fn = my_collate_fn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. The Net and Init\n",
    "1. BatchNorm1d: The mean and std are calculated per-dimension over the mini-batches\n",
    "2."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "### BatchNorm2d测试\n",
    "def test_BN():\n",
    "    m = nn.BatchNorm2d(3, affine=False)  # affine: With Learnable Parameters or not\n",
    "    print('m:', m)\n",
    "    # The mean and std are calculated per-dimension over the mini-batches\n",
    "    input = torch.tensor([\n",
    "        [[1.,2.,3.,4.],[1.,2.,3.,4.],[-1.,-2.,-3.,-4.]],\n",
    "        [[0.,0.,0.,0.],[0.,0.,0.,0.],[0.,0.,0.,0.]]\n",
    "    ], requires_grad=True)\n",
    "\n",
    "    print('input:', input.shape)\n",
    "    input = input.unsqueeze(dim=2)\n",
    "    print('input:', input.shape)\n",
    "    output = m(input) # 归一化\n",
    "    print('output:', output.shape)\n",
    "    print('output:', output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-0.1690,  0.5071,  1.1832,  1.8593]],\n\n        [[-0.8452, -0.8452, -0.8452, -0.8452]]])"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 复现batchNorm2d在input shape为3维的情况\n",
    "input = torch.tensor([[[1.,2.,3.,4.]],[[0.,0.,0.,0.]]])\n",
    "# print(input.shape)\n",
    "# torch.mean(input),torch.var(input,unbiased = False)\n",
    "(input-torch.mean(input))/ torch.sqrt(torch.var(input,unbiased = False))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 Init, Printer, Hook\n",
    "- 设置网络初始权重: 不太work"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "class model_param_init(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        assert isinstance(model, nn.Module), 'model not a class nn.Module'\n",
    "        self.net = model\n",
    "        self.initParam()\n",
    "\n",
    "    def initParam(self):\n",
    "        for param in self.net.parameters():\n",
    "            # nn.init.zeros_(param)\n",
    "            # nn.init.ones_(param)\n",
    "            # nn.init.normal_(param, mean=0, std=1)\n",
    "            # nn.init.uniform_(param, a=0, b=1)\n",
    "            # nn.init.constant_(param, val=1)   # 将所有权重初始化为1\n",
    "            # nn.init.eye_(param)  # 只能将二维的tensor初始化为单位矩阵\n",
    "            # nn.init.xavier_uniform_(param, gain=1)  # Glorot初始化  得到的张量是从-a——a中采用的\n",
    "            # nn.init.xavier_normal_(param, gain=1)   # 得到的张量是从0-std采样的\n",
    "            nn.init.kaiming_normal_(param, a=0, mode='fan_in', nonlinearity='relu') # he初始化方法\n",
    "            # nn.init.kaiming_uniform_(param)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- print网络每层结构"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "class PrintLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if(DEBUG):\n",
    "            print(\"This layer: \")\n",
    "            print(x)      #print(x.shape)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- hook_backward_fn: 输入端的grad，输出端的grad，这里称呼的**输入与输出是站在前向传播的角度的**。如果模块有多个输入与输出的话， 其`grad_input`和`grad_output`可以是tuple类型。\n",
    "- 与forward不同的是，backward传播的时候，**不仅反向传递input和output的grad，还会传递模块Parameter的grad**：\n",
    "    - 比如fc模块，其`grad_input`是一个三元组的tuple，（对bias的梯度，对输入的梯度，对w的梯度）；\n",
    "    - conv模块`grad_input`也是一个三元组tuple，为（对输入的梯度，对w的梯度，对bias的梯度"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# hook functions have to take these 3 input\n",
    "def hook_forward_fn(module, input, output):\n",
    "    print(\"It's forward: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"input: {input}\")\n",
    "    print(f\"output: {output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn(module, grad_input, grad_output):\n",
    "    print(\"It's backward: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn_pi(module, grad_input, grad_output):\n",
    "    print(\"It's backward in pi: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn_sigma(module, grad_input, grad_output):\n",
    "    print(\"It's backward in sigma: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn_mu(module, grad_input, grad_output):\n",
    "    print(\"It's backward in mu: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Conv结构-1\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# # Not Sequential\n",
    "# class MLP(nn.Module):\n",
    "#     # code->generate->override methods\n",
    "#     def __init__(self, n_gaussians) -> None:\n",
    "#         super().__init__()\n",
    "#         self.BN = nn.BatchNorm2d(num_features=3,affine=False)\n",
    "#         self.BN_aff = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "#         self.drop = nn.Dropout(0.3)\n",
    "#\n",
    "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#         self.ac_func = nn.Softplus()\n",
    "#\n",
    "#         self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,2), stride=(1,2), padding=0,bias=False)\n",
    "#\n",
    "#         self.conv3 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,5), stride=(1,5), padding=0,bias=False)\n",
    "#\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.linear1 = nn.Linear(30, 9)\n",
    "#\n",
    "#         self.z_pi = nn.Sequential(\n",
    "#             nn.Linear(9, n_gaussians),  # 30个params要learn\n",
    "#             nn.Softmax(dim=1)\n",
    "#         )\n",
    "#\n",
    "#         self.z_mu = nn.Linear(9, n_gaussians)\n",
    "#         self.z_sigma = nn.Linear(9, n_gaussians)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         # 加一个height维度\n",
    "#         x = torch.unsqueeze(x,dim=2)\n",
    "#         x = self.BN(x)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         x = self.BN_aff(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         x = self.BN_aff(x)\n",
    "#         x = self.conv3(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         x = self.flatten(x)\n",
    "#         x = self.linear1(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         pi = self.z_pi(x)\n",
    "#         mu = self.z_mu(x)\n",
    "#         sigma = torch.exp(self.z_sigma(x))\n",
    "#\n",
    "#         return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3 Conv结构-2\n",
    "1. 换了一下conv的结构"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# # 参数量747\n",
    "# class MLP(nn.Module):\n",
    "#     # code->generate->override methods\n",
    "#     def __init__(self, n_gaussians) -> None:\n",
    "#         super().__init__()\n",
    "#         self.BN1 = nn.BatchNorm1d(num_features=3,affine=False)\n",
    "#         self.BN2 = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "#         self.BN3 = nn.BatchNorm2d(num_features=6,affine=True)\n",
    "#         self.drop = nn.Dropout(0.3)\n",
    "#\n",
    "#         self.conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=(1,6), stride=(1,3), padding=0,bias=False)\n",
    "#         # self.ac_func = nn.Softplus()\n",
    "#         self.ac_func = nn.ReLU()\n",
    "#         # self.ac_func = nn.Tanh()\n",
    "#         # 3, 3, 99\n",
    "#         self.conv2 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=(3,6), stride=6, padding=0,bias=False)\n",
    "#         # 6,1,33\n",
    "#\n",
    "#         self.conv3 = nn.Conv2d(in_channels=6, out_channels=6, kernel_size=(1,3), stride=3, padding=0,bias=False)\n",
    "#\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         #self.linear1 = nn.Linear(48, 18)\n",
    "#\n",
    "#         # 无clip\n",
    "#         self.z_pi = nn.Sequential(\n",
    "#             nn.Linear(30, n_gaussians),\n",
    "#             nn.Softmax(dim=1)\n",
    "#         )\n",
    "#\n",
    "#         # 有clip\n",
    "#         # self.z_pi = nn.Linear(30, n_gaussians)\n",
    "#         self.z_mu = nn.Linear(30, n_gaussians)\n",
    "#         self.z_sigma = nn.Linear(30, n_gaussians)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         x = self.BN1(x)\n",
    "#         # 加一个height维度\n",
    "#\n",
    "#         x = torch.unsqueeze(x,dim=1)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.BN2(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.BN3(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         # x = torch.unsqueeze(x,dim=2)\n",
    "#         x = self.conv3(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         x = self.flatten(x)\n",
    "#         # x = self.linear1(x)\n",
    "#         # x = self.ac_func(self.drop(x))\n",
    "#         pi = self.z_pi(x)\n",
    "#         mu = self.z_mu(x)\n",
    "#         sigma = torch.exp(self.z_sigma(x))\n",
    "#\n",
    "#         # 加入clip\n",
    "#         # 对pi进行clip之前需要注释掉之前的code\n",
    "#         # pi = torch.nn.functional.softmax(torch.clip(pi,1e-3,1),dim=1)\n",
    "#         # mu = torch.maximum(torch.tensor(30.0),mu)\n",
    "#\n",
    "#         # return x\n",
    "#         return pi,mu,sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4 (3,100)的结构-3\n",
    "1. 适配(3,100)的training data长度而不是(3,300)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "outputs": [],
   "source": [
    "# 参数量：498\n",
    "class MLP(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "        self.BN = nn.BatchNorm2d(num_features=3,affine=False)\n",
    "        self.BN_aff1 = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "        self.BN_aff2 = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "        self.BN_aff3 = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "        # self.ac_func = nn.Softplus()\n",
    "        # self.ac_func = nn.Sigmoid()\n",
    "        self.ac_func = nn.Tanh()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,2), stride=(1,2), padding=0,bias=False)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        # self.linear1 = nn.Linear(30, 9)\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(48, n_gaussians),  # 30个params要learn\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(48, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(48, n_gaussians)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 加一个height维度\n",
    "        x = torch.unsqueeze(x,dim=2)\n",
    "        x = self.BN(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.BN_aff1(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        # x = self.BN_aff2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.BN_aff2(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        pi = self.z_pi(torch.clamp(x,1e-3))\n",
    "        mu = self.z_mu(x)\n",
    "        sigma = torch.exp(self.z_sigma(x))\n",
    "\n",
    "        # pi.retain_grad()\n",
    "        # mu.retain_grad()\n",
    "        # sigma.retain_grad()\n",
    "        # print(\"grad:\")\n",
    "        # print(\"pi.grad:\",pi.grad)\n",
    "        # print(\"mu.grad:\",mu.grad)\n",
    "        # print(\"sigma.grad:\",sigma.grad)\n",
    "\n",
    "        # return x\n",
    "        return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "# # 参数量：228\n",
    "# class MLP(nn.Module):\n",
    "#     # code->generate->override methods\n",
    "#     def __init__(self, n_gaussians) -> None:\n",
    "#         super().__init__()\n",
    "#         self.BN = nn.BatchNorm2d(num_features=3,affine=False)\n",
    "#         self.BN_aff = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "#         self.drop = nn.Dropout(0.2)\n",
    "#\n",
    "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#         # self.ac_func = nn.Softplus()\n",
    "#         self.ac_func = nn.LeakyReLU()\n",
    "#         self.ac_func2 = nn.ReLU()\n",
    "#\n",
    "#         # self.ac_func = nn.Sigmoid()\n",
    "#         # self.ac_func = nn.Tanh()\n",
    "#\n",
    "#         self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,2), stride=(1,2), padding=0,bias=False)\n",
    "#\n",
    "#         self.conv3 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#\n",
    "#         self.flatten = nn.Flatten()\n",
    "#\n",
    "#         self.z_pi = nn.Sequential(\n",
    "#             nn.Linear(15, n_gaussians),\n",
    "#             nn.Softmax(dim=1)\n",
    "#         )\n",
    "#\n",
    "#         self.z_mu = nn.Linear(15, n_gaussians)\n",
    "#         self.z_sigma = nn.Linear(15, n_gaussians)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         # 加一个height维度\n",
    "#         x = torch.unsqueeze(x,dim=2)\n",
    "#         x = self.BN(x)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         x = self.BN_aff(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         x = self.BN_aff(x)\n",
    "#         x = self.conv3(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#         #\n",
    "#         x = self.flatten(x)\n",
    "#\n",
    "#         pi = self.z_pi(x)\n",
    "#         mu = self.z_mu(x)\n",
    "#         sigma = torch.exp(self.z_sigma(x))\n",
    "#\n",
    "#         # return x\n",
    "#         return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- MLP结构+长度=100"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "# # Not Sequential\n",
    "# class MLP(nn.Module):\n",
    "#     # code->generate->override methods\n",
    "#     def __init__(self, n_gaussians) -> None:\n",
    "#         super().__init__()\n",
    "#         self.BN1 = nn.BatchNorm1d(num_features=3,affine=False)\n",
    "#         self.BN2 = nn.BatchNorm1d(num_features=1,affine=False)\n",
    "#         self.drop = nn.Dropout(0.3)\n",
    "#\n",
    "#         self.linear1 = nn.Linear(300, 100)\n",
    "#         self.linear2 = nn.Linear(100, 30)\n",
    "#         self.linear3 = nn.Linear(30, 12)\n",
    "#\n",
    "#         self.ac_func = nn.Softplus()\n",
    "#\n",
    "#         self.flatten = nn.Flatten()\n",
    "#\n",
    "#         self.z_pi = nn.Sequential(\n",
    "#             nn.Linear(12, n_gaussians),  # 30个params要learn\n",
    "#             nn.Softmax(dim=1)\n",
    "#         )\n",
    "#\n",
    "#         self.z_mu = nn.Linear(12, n_gaussians)\n",
    "#         self.z_sigma = nn.Linear(12, n_gaussians)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         x = self.BN1(x)\n",
    "#         x = self.flatten(x)\n",
    "#\n",
    "#         x = self.linear1(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         # x = self.BN2(x)\n",
    "#         x = self.linear2(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         # x = self.BN2(x)\n",
    "#         x = self.linear3(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         pi = self.z_pi(x)\n",
    "#         mu = self.z_mu(x)\n",
    "#         sigma = torch.exp(self.z_sigma(x))\n",
    "#\n",
    "#         # return x\n",
    "#         return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.5 (3,60)的结构-3\n",
    "1. 适配(3,60)的training data长度而不是(3,300)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "# # 参数量：174\n",
    "# class MLP(nn.Module):\n",
    "#     # code->generate->override methods\n",
    "#     def __init__(self, n_gaussians) -> None:\n",
    "#         super().__init__()\n",
    "#         self.BN = nn.BatchNorm2d(num_features=3,affine=False)\n",
    "#         self.BN_aff = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "#         self.drop = nn.Dropout(0.2)\n",
    "#\n",
    "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#         # self.ac_func = nn.Softplus()\n",
    "#         self.ac_func = nn.LeakyReLU()\n",
    "#         self.ac_func2 = nn.ReLU()\n",
    "#\n",
    "#         # self.ac_func = nn.Sigmoid()\n",
    "#         # self.ac_func = nn.Tanh()\n",
    "#\n",
    "#         self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,2), stride=(1,2), padding=0,bias=False)\n",
    "#\n",
    "#         self.conv3 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.linear = nn.Linear(24, 9)\n",
    "#\n",
    "#         self.z_pi = nn.Sequential(\n",
    "#             nn.Linear(9, n_gaussians),\n",
    "#             nn.Softmax(dim=1)\n",
    "#         )\n",
    "#\n",
    "#         self.z_mu = nn.Linear(9, n_gaussians)\n",
    "#         self.z_sigma = nn.Linear(9, n_gaussians)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         # 加一个height维度\n",
    "#         x = torch.unsqueeze(x,dim=2)\n",
    "#         x = self.BN(x)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.ac_func(self.drop(x))      # 3, 1, 20\n",
    "#\n",
    "#         x = self.BN_aff(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.ac_func(self.drop(x))      # 3, 1, 10\n",
    "#\n",
    "#         x = self.BN_aff(x)\n",
    "#         x = self.conv3(x)\n",
    "#         x = self.ac_func(self.drop(x))      # 3, 1, 8\n",
    "#\n",
    "#         x = self.flatten(x)\n",
    "#         # print(x.shape)\n",
    "#         # x = self.linear(x)\n",
    "#         #\n",
    "#         pi = self.z_pi(x)\n",
    "#         mu = self.z_mu(x)\n",
    "#         sigma = torch.exp(self.z_sigma(x))\n",
    "#\n",
    "#         # return x\n",
    "#         return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- MLP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "# # 参数量：5799\n",
    "# class MLP(nn.Module):\n",
    "#     # code->generate->override methods\n",
    "#     def __init__(self, n_gaussians) -> None:\n",
    "#         super().__init__()\n",
    "#         self.BN = nn.BatchNorm2d(num_features=3,affine=False)\n",
    "#         self.BN_aff = nn.BatchNorm2d(num_features=3,affine=True)\n",
    "#         self.drop = nn.Dropout(0.2)\n",
    "#\n",
    "#         self.linear1 = nn.Linear(180,30)\n",
    "#         self.linear2 = nn.Linear(30,9)\n",
    "#         # self.ac_func = nn.Softplus()\n",
    "#         self.ac_func = nn.LeakyReLU()\n",
    "#         self.ac_func2 = nn.ReLU()\n",
    "#\n",
    "#         # self.ac_func = nn.Sigmoid()\n",
    "#         # self.ac_func = nn.Tanh()\n",
    "#\n",
    "#         self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,2), stride=(1,2), padding=0,bias=False)\n",
    "#\n",
    "#         self.conv3 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(1,3), stride=(1,3), padding=0,bias=False)\n",
    "#\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.linear = nn.Linear(24, 9)\n",
    "#\n",
    "#         self.z_pi = nn.Sequential(\n",
    "#             nn.Linear(9, n_gaussians),\n",
    "#             nn.Softmax(dim=1)\n",
    "#         )\n",
    "#\n",
    "#         self.z_mu = nn.Linear(9, n_gaussians)\n",
    "#         self.z_sigma = nn.Linear(9, n_gaussians)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         # 加一个height维度\n",
    "#         x = torch.unsqueeze(x,dim=2)\n",
    "#         x = self.BN(x)\n",
    "#         x = self.flatten(x)\n",
    "#\n",
    "#         x = self.linear1(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         x = self.linear2(x)\n",
    "#         x = self.ac_func(self.drop(x))\n",
    "#\n",
    "#         pi = self.z_pi(x)\n",
    "#         mu = self.z_mu(x)\n",
    "#         sigma = torch.exp(self.z_sigma(x))\n",
    "#\n",
    "#         # return x\n",
    "#         return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input's shape is torch.Size([2, 3, 1, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": "torch.Size([2, 12])"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([\n",
    "    [[1.,2.,3.,4.],[1.,2.,3.,4.],[-1.,-2.,-3.,-4.]],\n",
    "    [[0.,0.,0.,0.],[0.,0.,0.,0.],[0.,0.,0.,0.]]\n",
    "], requires_grad=True)\n",
    "input = input.unsqueeze(dim=2)\n",
    "print(f\"input's shape is {input.shape}\")\n",
    "flt = nn.Flatten(start_dim=1)\n",
    "flt(input).shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. The Loss\n",
    "- `loss_preparation`用来做loss的前期data准备：\n",
    "    - 计算混合模型的分布`m`以及target data中的`duration`\n",
    "- `loss_fn`用来计算loss值\n",
    "\n",
    "## 4.1 初始版本\n",
    "### 4.1.1 version 1 [FAILED]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "# # 当input的shape是[50,3]时，输出应该是50个GMM\n",
    "# # 对这50个GMM看能生成什么output\n",
    "\n",
    "def loss_fn(Pi,duration,m,N_gaussians):\n",
    "    loss_list = []\n",
    "    with torch.no_grad():\n",
    "        # for each GMM\n",
    "        for i in range(len(Pi)):\n",
    "            target = duration[i,:]\n",
    "            pi = Pi[i,:]\n",
    "\n",
    "            # Drop padded data and Expanded to the same dim\n",
    "            target_nonzero = target[torch.nonzero(target)].squeeze_()\n",
    "            target_nonzero = torch.repeat_interleave(target_nonzero.unsqueeze(dim=1), repeats=N_gaussians, dim=1).to(device)\n",
    "            # len_target = len(target_nonzero)       # The length of target data\n",
    "\n",
    "            # loss_1 是高斯分布的概率密度value\n",
    "            loss_1 = torch.exp(m[i].log_prob(target_nonzero))\n",
    "\n",
    "            # loss_2 是MDN的概率密度value\n",
    "            loss_2 = torch.sum(loss_1 * pi, dim=1)\n",
    "\n",
    "            # loss_3 是值非0的概率密度value\n",
    "            loss_3 = loss_2[torch.nonzero(loss_2)].view(-1,1)                # 再去掉所有的log(0)\n",
    "\n",
    "            # 如果loss_2全是0则赋值为1e-40，否则赋值为loss的最小值\n",
    "            MIN_LOSS = torch.min(loss_3) if torch.min(loss_2)>0 else 1e-40\n",
    "            loss_padded = [MIN_LOSS]* (loss_2.shape[0]-loss_3.shape[0])\n",
    "            loss_padded_ts = torch.tensor(loss_padded,device=device)\n",
    "\n",
    "            # loss_4是用MIN_LOSS填充后的loss_3，和loss_2等长\n",
    "            loss_4 = torch.cat((loss_3[:,0],loss_padded_ts))\n",
    "            torch._assert(len(loss_4) == len(loss_2),\"cat不正确\")\n",
    "\n",
    "            # loss_5是log likelihood loss\n",
    "            loss_5 = torch.log(loss_4)\n",
    "            loss_list.append(-torch.mean((loss_5)).item())\n",
    "            # loss_list.append(-torch.sum((loss_5)).item()) # loss值会比较大，\n",
    "\n",
    "    # 最后loss求一下平均\n",
    "    loss = np.mean(loss_list)\n",
    "    loss_ts = torch.tensor(loss,device=device,requires_grad=True)\n",
    "    return loss_ts,loss_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1.2 version 2 [WORK]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "outputs": [],
   "source": [
    "def loss_fn_v2(Pi, Mu, Sigma, Duration, N_gaussians):\n",
    "\n",
    "    loss_sum = torch.tensor(0.,device=device,requires_grad=True)\n",
    "\n",
    "    for i in range(len(Pi)):\n",
    "\n",
    "        target = Duration[i,:,0]\n",
    "        pi = Pi[i,:]\n",
    "        m = torch.distributions.Normal(loc=Mu[i,:], scale=Sigma[i,:])\n",
    "\n",
    "        # Drop padded data and Expanded to the same dim\n",
    "        idx = torch.nonzero(target)\n",
    "        target_nonzero = target[idx]\n",
    "        # target_nonzero_2 = torch.repeat_interleave(target_nonzero.unsqueeze(dim=1), repeats=N_gaussians, dim=1).to(device)\n",
    "        target_nonzero_2 = target_nonzero.repeat(1,N_gaussians).to(device)\n",
    "\n",
    "        # loss_1 是高斯分布的概率密度value\n",
    "        loss_1 = torch.exp(m.log_prob(target_nonzero_2))\n",
    "\n",
    "        # loss_2 是MDN的概率密度value\n",
    "        loss_2 = torch.sum(loss_1 * pi, dim=1)\n",
    "\n",
    "        # # loss_3 是值非0的概率密度value\n",
    "        # # loss_3 = loss_2[torch.nonzero(loss_2)].view(-1,1)                # 再去掉所有的log(0)\n",
    "        # idx2 = torch.nonzero(loss_2)\n",
    "        # loss_3 = loss_2[idx2].reshape(-1,1)                # 再去掉所有的log(0)\n",
    "        #\n",
    "        # # 如果loss_2全是0则赋值为1e-40，否则赋值为loss的最小值\n",
    "        # # MIN_LOSS = torch.min(loss_3) if torch.min(loss_2)>0 else 1e-40\n",
    "        # loss_padded = [MIN_LOSS]* (loss_2.shape[0]-loss_3.shape[0])\n",
    "        # loss_padded_ts = torch.tensor(loss_padded,device=device)\n",
    "        #\n",
    "        # # loss_4是用MIN_LOSS填充后的loss_3，和loss_2等长\n",
    "        # loss_4 = torch.cat((loss_3[:,0],loss_padded_ts))\n",
    "        # # torch._assert(len(loss_4) == len(loss_2),\"cat不正确\")\n",
    "        #\n",
    "        # # loss_5是log likelihood loss\n",
    "        # # print(\"loss_4 Nan?\",torch.any(torch.isnan(loss_4)))\n",
    "        # loss_5 = torch.log(loss_4)  # +1e-8\n",
    "        # # loss_list.append(-torch.mean((loss_5)).item())\n",
    "        # loss_sum = -torch.mean(loss_5) + loss_sum\n",
    "\n",
    "        ## 两种截断的方式都ok：一种是在MDN的prob上设定最小值；一种是最后加一个safety数 MIN_LOSS\n",
    "        loss_4 = torch.clamp(loss_2,min=MIN_LOSS)\n",
    "        # loss_3 = torch.log(loss_2+MIN_LOSS)\n",
    "        loss_3 = torch.log(loss_4)  # +1e-8\n",
    "        loss_sum = -torch.mean(loss_3) + loss_sum\n",
    "\n",
    "    # 最后loss求一下平均\n",
    "    return loss_sum"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1.3 version 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "outputs": [],
   "source": [
    "def log_sum_exp(x, axis=None):\n",
    "    \"\"\"Log-sum-exp trick implementation\"\"\"\n",
    "    x_max = torch.max(x, dim=axis, keepdim=True)\n",
    "    return torch.log(torch.sum(torch.exp(x - x_max), dim=axis, keepdim=True))+x_max"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "outputs": [],
   "source": [
    "def loss_fn_v3(Pi, Mu, Sigma, Target, N_gaussians):\n",
    "\n",
    "    loss_sum = torch.tensor(0.,device=device,requires_grad=True)\n",
    "\n",
    "    for i in range(len(Pi)):\n",
    "\n",
    "        target = Target[i,:,0]\n",
    "        pi = Pi[i,:]\n",
    "\n",
    "        # Drop padded data and Expanded to the same dim\n",
    "        #### 写法一：work。无原地变换\n",
    "        idx = torch.nonzero(target)\n",
    "        target_nonzero = target[idx]\n",
    "        target_nonzero_2 = target_nonzero.repeat(1,N_gaussians)\n",
    "\n",
    "        # loss_1 是高斯分布的概率密度value\n",
    "\n",
    "        # loss_1 = gaussian_pdf(target_nonzero_2,Mu[i,:],Sigma[i,:],C)\n",
    "        # print(\"loss_0:\",gaussian_pdf(target_nonzero_2,Mu[i,:],Sigma[i,:],0))\n",
    "        # print(\"loss_1:\",loss_1)\n",
    "###########\n",
    "        # exponent?\n",
    "        v1 = .5 * torch.log(2 * torch.tensor(np.pi)) -torch.log(Sigma[i,:])\n",
    "        v2 = (target_nonzero_2 - Mu[i,:])\n",
    "        print(\"v1\",v1.shape)\n",
    "        print(\"v2\",v2.shape)\n",
    "\n",
    "        # v1 torch.Size([3])\n",
    "        # v2 torch.Size([44, 3])\n",
    "\n",
    "        # torch.sum(v2**2, dim=1)是叠加的后面的维度\n",
    "        exponent = torch.log(Pi[i,:]) -  v1\\\n",
    "                   - torch.sum(v2**2, dim=1)/(2*(Sigma[i,:])**2)\n",
    "        print(\"exponent shape:\",exponent.shape)\n",
    "        log_gauss = log_sum_exp(exponent, axis=1)\n",
    "        res = - torch.mean(log_gauss)\n",
    "#############\n",
    "        # loss_2 是MDN的概率密度value\n",
    "        loss_2 = torch.sum(0 * pi, dim=1)\n",
    "\n",
    "        #################### 直接使用loss_2\n",
    "        # 本来MIN_LOSS应该*MIN_LOSS但是mean了一下抵消了\n",
    "        # print(\"loss_2 check:\",i)\n",
    "        # print(torch.isnan(loss_2).any())\n",
    "        # print(torch.isnan(torch.log(loss_2)).any())\n",
    "\n",
    "        # loss_3 = torch.log(loss_2*torch.exp(torch.tensor(50000.)))-torch.tensor(50000.)\n",
    "        # print(\"loss_2:\",loss_2)\n",
    "\n",
    "        loss_3 = torch.log(loss_2)\n",
    "        # print(\"loss_3:\",loss_3)\n",
    "        loss_sum = -torch.mean(loss_3) + loss_sum\n",
    "\n",
    "    return loss_sum"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5])\n",
      "tensor([[4, 4, 4, 4, 4, 4]])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor(4.5528)"
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def argmax(vec):\n",
    "    # return the argmax as a python int\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    print(idx)\n",
    "    return idx.item()\n",
    "\n",
    "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    print(max_score_broadcast)\n",
    "    return max_score + \\\n",
    "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "\n",
    "input_test = torch.tensor([[1,2,3],[2,1,4]]).view(1, -1)\n",
    "log_sum_exp(input_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def log_sum_exp(x, axis=None):\n",
    "    \"\"\"Log-sum-exp trick implementation\"\"\"\n",
    "    x_max = torch.max(x, dim=axis, keepdim=True)\n",
    "\n",
    "    return torch.log(torch.sum(torch.exp(x - x_max),dim=axis, keepdim=True))+x_max\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 cdf loss\n",
    "1. 注意把targets data的路径改一下"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "# cdf loss\n",
    "def loss_fn_cdf(Pi, Mu, Sigma, Target, N_gaussians):\n",
    "\n",
    "    loss_sum = torch.tensor(0.,device=device,requires_grad=True)\n",
    "\n",
    "    # for each GMM\n",
    "    for i in range(len(Pi)):\n",
    "        target = Target[i,:,0]\n",
    "        prob_target = Target[i,:,1]\n",
    "        pi = Pi[i,:]\n",
    "        m = torch.distributions.Normal(loc=Mu[i,:], scale=Sigma[i,:])\n",
    "\n",
    "        # Drop padded data and Expanded to the same dim\n",
    "        #### 写法一：work。无原地变换\n",
    "        idx = torch.nonzero(target)\n",
    "        target_nonzero = torch.squeeze(target[idx])\n",
    "        prob_target_nonzero = torch.squeeze(prob_target[idx])\n",
    "        target_nonzero_2 = torch.repeat_interleave(target_nonzero.unsqueeze(dim=1), repeats=N_gaussians, dim=1).to(device)\n",
    "\n",
    "        # loss_1 是高斯分布的概率密度value\n",
    "        loss_1 = torch.exp(m.log_prob(target_nonzero_2))\n",
    "\n",
    "        # loss_2 是MDN的概率密度value\n",
    "        loss_2 = torch.sum(loss_1 * pi, dim=1)\n",
    "\n",
    "        # loss_3 是cdf的差值的一范数or二范数，表示的是MDN和target之间的总体差异\n",
    "        loss_3 = torch.norm((loss_2 - prob_target_nonzero), 1)\n",
    "\n",
    "        # loss_4 = -torch.log(loss_3)   # +MIN_LOSS\n",
    "        loss_sum = torch.log(loss_3) + loss_sum\n",
    "\n",
    "    # Batch平均loss\n",
    "\n",
    "    return loss_sum\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.3 交叉熵/ KL\n",
    "1. 自己写的，没有用torch的函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "# 当input的shape是[50,3]时，输出应该是50个GMM\n",
    "def loss_preparation_CE(pi, mu, sigma):\n",
    "\n",
    "    m=[]\n",
    "    for i in range(pi.shape[0]):\n",
    "        m.append(torch.distributions.Normal(loc=mu[i,:], scale=sigma[i,:]))\n",
    "\n",
    "    # target_packed = pack_padded_sequence(target,target_len,batch_first=True,enforce_sorted=False)  # 去掉padded 0并拉成一个vector\n",
    "    # seq_unpacked, lens_unpacked = pad_packed_sequence(target_packed, batch_first=True)             # 和上面互为逆运算\n",
    "\n",
    "    return m"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "# # 当input的shape是[50,3]时，输出应该是50个GMM\n",
    "# # 对这50个GMM看能生成什么output\n",
    "\n",
    "def loss_fn_CE(Pi, Mu, Sigma, Target, N_gaussians):\n",
    "\n",
    "    loss_sum = torch.tensor(0.,device=device,requires_grad=True)\n",
    "\n",
    "    # for each GMM\n",
    "    for i in range(len(Pi)):\n",
    "        target = Target[i,:,0]\n",
    "        prob_target = Target[i,:,1]\n",
    "        pi = Pi[i,:]\n",
    "        m = torch.distributions.Normal(loc=Mu[i,:], scale=Sigma[i,:])\n",
    "\n",
    "        # Drop padded data and Expanded to the same dim\n",
    "        #### 写法一：work。无原地变换\n",
    "        idx = torch.nonzero(target)\n",
    "        target_nonzero = torch.squeeze(target[idx])\n",
    "        prob_target_nonzero = torch.squeeze(prob_target[idx])\n",
    "\n",
    "        target_nonzero_2 = torch.repeat_interleave(target_nonzero.unsqueeze(dim=1), repeats=N_gaussians, dim=1).to(device)\n",
    "\n",
    "        # loss_1 是高斯分布的概率密度value\n",
    "        loss_1 = torch.exp(m.log_prob(target_nonzero_2))\n",
    "\n",
    "        # loss_2 是MDN的概率密度value\n",
    "        loss_2 = torch.sum(loss_1 * pi, dim=1)\n",
    "\n",
    "        # loss_4是KL loss\n",
    "        # loss_4 = -prob_target_nonzero*torch.log(loss_2) + prob_target_nonzero*torch.log(prob_target_nonzero)\n",
    "        # print(\"prob_target_nonzero shape:\",prob_target_nonzero.shape)\n",
    "        # print(\"loss_2 shape:\",loss_2.shape)\n",
    "\n",
    "        loss_4 = -prob_target_nonzero*torch.log(loss_2+MIN_LOSS)+ prob_target_nonzero*torch.log(prob_target_nonzero)\n",
    "        loss_sum = torch.sum(loss_4)+loss_sum\n",
    "\n",
    "    loss_ts = loss_sum/len(Pi)\n",
    "    return loss_ts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.4 log sum exp\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "# 使用log_sum_exp的loss\n",
    "def loss_fn_lse(Pi,duration,m,N_gaussians):\n",
    "    loss_list = []\n",
    "    # with torch.no_grad():\n",
    "    # for each GMM\n",
    "    # 后期肯定要矩阵化这个计算！\n",
    "    for i in range(len(m)):\n",
    "        target = duration[i,:]\n",
    "        pi = Pi[i,:]\n",
    "\n",
    "        # Drop padded data and Expanded to the same dim\n",
    "        target_nonzero = target[torch.nonzero(target)].squeeze_()\n",
    "        target_nonzero = torch.repeat_interleave(target_nonzero.unsqueeze(dim=1), repeats=N_gaussians, dim=1).to(device)\n",
    "        # len_target = len(target_nonzero)       # The length of target data\n",
    "\n",
    "        # loss_1 是高斯分布的log prob\n",
    "        loss_1 = m[i].log_prob(target_nonzero)\n",
    "\n",
    "        # loss_2 是MDN的log prob\n",
    "        loss_2 = torch.sum(loss_1, dim=1) + torch.log(pi)\n",
    "\n",
    "        # loss_3 是值非0的概率密度value\n",
    "        loss_3 = loss_2[torch.nonzero(loss_2)].view(-1,1)                # 再去掉所有的log(0)\n",
    "\n",
    "        # 如果loss_2全是0则赋值为1e-40，否则赋值为loss的最小值\n",
    "        MIN_LOSS = torch.min(loss_3) if torch.min(loss_2)>0 else 1e-40\n",
    "        loss_padded = [MIN_LOSS]* (loss_2.shape[0]-loss_3.shape[0])\n",
    "        loss_padded_ts = torch.tensor(loss_padded,device=device)\n",
    "\n",
    "        # loss_4是用MIN_LOSS填充后的loss_3，和loss_2等长\n",
    "        loss_4 = torch.cat((loss_3[:,0],loss_padded_ts))\n",
    "        torch._assert(len(loss_4) == len(loss_2),\"cat不正确\")\n",
    "\n",
    "        # loss_5是log likelihood loss\n",
    "        loss_5 = torch.log(loss_4)\n",
    "        loss_list.append(-torch.mean((loss_5)).item())\n",
    "        # loss_list.append(-torch.sum((loss_5)).item()) # loss值会比较大，\n",
    "\n",
    "    # 最后loss求一下平均\n",
    "    loss = np.mean(loss_list)\n",
    "\n",
    "    loss_ts = torch.tensor(loss,device=device,requires_grad=True)\n",
    "    return loss_ts,loss_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "def loss_test():\n",
    "    loss_2 = torch.tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "    loss_3 = loss_2[torch.nonzero(loss_2)].view(-1,1)         # 去掉所有的log(0)\n",
    "    print(\"loss_3:\",loss_3.shape)\n",
    "\n",
    "    MIN_LOSS = torch.min(loss_2) if torch.min(loss_2)>0 else 1e-20\n",
    "\n",
    "    print(\"MIN_LOSS：\",MIN_LOSS)\n",
    "    # 用MIN_loss替代loss_2为0的部分\n",
    "    loss_2[loss_2==0] = MIN_LOSS\n",
    "\n",
    "\n",
    "    loss_padded = [MIN_LOSS]* (loss_2.shape[0]-loss_3.shape[0])\n",
    "    loss_padded_ts = torch.tensor(loss_padded) # ,device=device\n",
    "    loss_3 = torch.cat((loss_3[:,0],loss_padded_ts))\n",
    "    loss_3.shape, torch.log(loss_3)\n",
    "    print(\"data: \",loss_3.data)\n",
    "    print(\"data: \",type(loss_3))\n",
    "# loss_test()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5 Plot\n",
    "1. draw:\n",
    "    - loss: training data的loss和test data的loss趋势\n",
    "    - MLP的网络结构（.png）\n",
    "    - target distrb.和pred. distrb.\n",
    "    - 所有target data的分布图\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(\"logs-MLP\")\n",
    "viz = Visdom(env=\"001\")\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.1 draw 所有的target data\n",
    "1. 实际上在`i < 50`里决定画前50 or what target data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "def draw_all_target_data():\n",
    "    # target data\n",
    "    # target_path = r\"../data/targets\"\n",
    "    target_path = r\"../data/targets_5\"\n",
    "    # target_path里有全部的target data地址\n",
    "    target_all_path = os.listdir(target_path)\n",
    "    len_target = len(target_all_path)\n",
    "    print(f\"一共有 *{len_target}* 组 target data\")\n",
    "\n",
    "    # Init\n",
    "    # viz_env_str = \"target_bar\"\n",
    "    # viz_env_str = \"target_line\"\n",
    "    # viz_env_str = \"target_hist\"\n",
    "    viz_env_str = \"target_scatter\"\n",
    "    viz = Visdom(env = viz_env_str)\n",
    "\n",
    "    for i in range(len_target):\n",
    "        target_path_i_path = os.path.join(target_path,target_all_path[i])\n",
    "        target_df = pd.read_csv(target_path_i_path,encoding=\"utf-8\")\n",
    "\n",
    "        # Init\n",
    "        win_str = str(target_all_path[i])\n",
    "        title_str = \"Target Distrb. of \"+win_str\n",
    "\n",
    "        if(i < 50):\n",
    "            # Hist plot which is not that great\n",
    "            if viz_env_str == \"target_hist\" :\n",
    "                viz.histogram(X = np.array(target_df.N), env=viz_env_str, win=win_str,\n",
    "                            opts= dict(title=title_str,numbins = 50))\n",
    "\n",
    "            # Line plot\n",
    "            if viz_env_str == \"target_line\" :\n",
    "                target_df.drop_duplicates(inplace=True)\n",
    "                viz.line(X = np.array(target_df.N),Y= np.array(target_df.P), env=viz_env_str, win=win_str,opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "            # Bar plot. 这个看起来是最准确的一个\n",
    "            if viz_env_str == \"target_bar\" :\n",
    "                target_df.drop_duplicates(inplace=True)\n",
    "                arr_str = [str(i) for i in np.array(target_df.N)]\n",
    "                viz.bar(X=np.array(target_df.P), env=viz_env_str, win=win_str,\n",
    "                        opts=dict(title=title_str,rownames=arr_str,stacked = False))\n",
    "\n",
    "            # Scatter plot\n",
    "            if viz_env_str == \"target_scatter\" :\n",
    "                target_df.drop_duplicates(inplace=True)\n",
    "                viz.scatter(X=np.array(target_df),env=viz_env_str, win=win_str,opts=dict(title=title_str,markersize = 3))\n",
    "    print(\"Done\")\n",
    "\n",
    "# draw_all_target_data()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 画target data的cdf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "def draw_all_target_data_cdf():\n",
    "    # target data\n",
    "    # target_path = r\"../data/targets\"\n",
    "    target_path = r\"../data/targets_5_cdf\"\n",
    "    # target_path里有全部的target data地址\n",
    "    target_all_path = os.listdir(target_path)\n",
    "    len_target = len(target_all_path)\n",
    "    print(f\"一共有 *{len_target}* 组 target data\")\n",
    "\n",
    "    # Init，只需要改这个str就ok\n",
    "    # viz_env_str = \"target_barCDF\"\n",
    "    # viz_env_str = \"target_lineCDF\"\n",
    "    # viz_env_str = \"target_histCDF\"\n",
    "    viz_env_str = \"target_scatterCDF\"\n",
    "    viz = Visdom(env = viz_env_str)\n",
    "\n",
    "    for i in range(len_target):\n",
    "        target_path_i_path = os.path.join(target_path,target_all_path[i])\n",
    "        target_df = pd.read_csv(target_path_i_path,encoding=\"utf-8\")\n",
    "\n",
    "        # Init\n",
    "        win_str = str(target_all_path[i])\n",
    "        title_str = \"Target Distrb. of \"+win_str\n",
    "\n",
    "        if(i < 50):\n",
    "            # Hist plot which is not that great\n",
    "            if viz_env_str == \"target_histCDF\" :\n",
    "                viz.histogram(X = np.array(target_df.N), env=viz_env_str, win=win_str,\n",
    "                            opts= dict(title=title_str,numbins = 50))\n",
    "\n",
    "            # Line plot\n",
    "            if viz_env_str == \"target_lineCDF\" :\n",
    "                target_df.drop_duplicates(inplace=True)\n",
    "                viz.line(X = np.array(target_df.N),Y= np.array(target_df.P), env=viz_env_str, win=win_str,opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "            # Bar plot. 这个看起来是最准确的一个\n",
    "            if viz_env_str == \"target_barCDF\" :\n",
    "                target_df.drop_duplicates(inplace=True)\n",
    "                arr_str = [str(i) for i in np.array(target_df.N)]\n",
    "                viz.bar(X=np.array(target_df.P), env=viz_env_str, win=win_str,\n",
    "                        opts=dict(title=title_str,rownames=arr_str,stacked = False))\n",
    "\n",
    "            # Scatter plot\n",
    "            if viz_env_str == \"target_scatterCDF\" :\n",
    "                target_df.drop_duplicates(inplace=True)\n",
    "                viz.scatter(X=np.array(target_df),env=viz_env_str, win=win_str,opts=dict(title=title_str,markersize = 3))\n",
    "    print(\"Done\")\n",
    "\n",
    "# draw_all_target_data_cdf()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 也是画target data，但是是平滑处理\n",
    "- `savgol_filter`参数解析:\n",
    "    - y：代表曲线点坐标（x,y）中的y\n",
    "    - window_length：窗口长度，该值需为正奇整数。值越小，曲线越贴近真实曲线；值越大，平滑效果越厉害\n",
    "    - polyorder: 对窗口内的数据点进行k阶多项式拟合，k的值需要小于window_length。值越大，曲线越贴近真实曲线；值越小，平滑效果越厉害\n",
    "    - mode：确定了要应用滤波器的填充信号的扩展类型。（This determines the type of extension to use for the padded signal to which the filter is applied. ）\n",
    "- **实际上没用到这个函数**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "def draw_smooth_target_data():\n",
    "    # target data\n",
    "    target_path = r\"../data/targets\"\n",
    "    # target_path里有全部的target data地址\n",
    "    target_all_path = os.listdir(target_path)\n",
    "    len_target = len(target_all_path)\n",
    "    print(f\"一共有 *{len_target}* 组 target data\")\n",
    "\n",
    "    # Init\n",
    "    viz_env_str = \"target_bar\"\n",
    "    # viz_env_str = \"target_distrb\"\n",
    "    # viz_env_str = \"target_hist\"\n",
    "    # viz_env_str = \"target_scatter\"\n",
    "    viz = Visdom(env = viz_env_str)\n",
    "\n",
    "    for i in range(len_target):\n",
    "        target_path_i_path = os.path.join(target_path,target_all_path[i])\n",
    "        target_df = pd.read_csv(target_path_i_path,encoding=\"utf-8\")\n",
    "\n",
    "        # Init\n",
    "        win_str = str(target_all_path[i])\n",
    "        title_str = \"Target Distrb. of \"+win_str\n",
    "\n",
    "        if(i < 100):\n",
    "            # Smoothen the data\n",
    "            y_smooth = scipy.signal.savgol_filter(target_df.P,window_length=53,polyorder=3)\n",
    "            # y_smooth = scipy.signal.savgol_filter(target_df.P, 99, 1, mode= 'nearest')\n",
    "            target_df.P = y_smooth\n",
    "\n",
    "            # Hist plot which is not that great\n",
    "            # viz.histogram(X = np.array(target_df.N), env=viz_env_str, win=win_str,\n",
    "            #             opts= dict(title=title_str,numbins = 50))\n",
    "\n",
    "            # Line plot\n",
    "            # target_df.drop_duplicates(inplace=True)\n",
    "            # viz.line(X = np.array(target_df.N),Y= np.array(target_df.P), env=viz_env_str, win=win_str,opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "            # Bar plot. 这个看起来是最准确的一个\n",
    "            target_df.drop_duplicates(inplace=True)\n",
    "            arr_str = [str(i) for i in np.array(target_df.N)]\n",
    "            viz.bar(X=np.array(target_df.P), env=viz_env_str, win=win_str,\n",
    "                    opts=dict(title=title_str,rownames=arr_str))\n",
    "\n",
    "            # Scatter plot\n",
    "            # target_df.drop_duplicates(inplace=True)\n",
    "            # viz.scatter(X=np.array(target_df),env=viz_env_str, win=win_str,opts=dict(title=title_str,markersize = 3))\n",
    "    print(\"Done\")\n",
    "\n",
    "# draw_smooth_target_data()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "#### Test for drawing\n",
    "def test_draw():\n",
    "    viz = Visdom(env=\"001\")\n",
    "\n",
    "    mu = torch.tensor([0,10,20])\n",
    "    sigma = torch.tensor([1,1,1])\n",
    "    duration = torch.tensor([0,1,2,0])\n",
    "    duration = torch.repeat_interleave(duration.unsqueeze(dim=1), repeats=3, dim=1)\n",
    "    m = torch.distributions.Normal(loc=mu, scale=sigma)\n",
    "    pi = torch.tensor([0.2,0.3,0.5])\n",
    "\n",
    "    # draw\n",
    "    x_0 = torch.tensor(np.arange(0,1000))\n",
    "    x = torch.repeat_interleave(x_0.unsqueeze(dim=1), repeats=3, dim=1)\n",
    "    y = torch.exp(m.log_prob(x))\n",
    "    y_sum = torch.unsqueeze(torch.sum(pi*y,dim=1),dim=1)\n",
    "    viz.line(X = x_0,Y= torch.cat([y,y_sum],dim = 1), env=\"001\", win=\"test_draw_2\",\n",
    "            opts= dict(title='test_draw', legend=['N1', 'N2', 'N3','NNN']))\n",
    "# test_draw()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.2 plot mdn\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2.2 mdn pdf的图像[with input data]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "def draw_mdn_2(pi, mu, sigma, target, input, total_train_step, N_gaussians):\n",
    "    # 只画一个batch中的第一个\n",
    "\n",
    "    # The target distrb.\n",
    "    target = torch.unique(target,dim=0)    # drop the duplicate\n",
    "    n_target = target[:,0]\n",
    "    non_zero_idx = torch.nonzero(n_target)\n",
    "    n = n_target[non_zero_idx]\n",
    "\n",
    "    p_target = target[:,1]\n",
    "    p = p_target[non_zero_idx]\n",
    "    max_n = max(n).item()               # 横轴长度\n",
    "\n",
    "    # The predicted distrb.\n",
    "    m = torch.distributions.Normal(mu,sigma)\n",
    "    x_0 = torch.arange(1,max_n).to(device=device)\n",
    "    x = torch.repeat_interleave(x_0.unsqueeze(dim=1), repeats=N_gaussians, dim=1)\n",
    "    y = torch.exp(m.log_prob(x)).to(device=device)                          # y:多条高斯曲线; y_pred: 一条GMM曲线\n",
    "    # y_pred = torch.unsqueeze(torch.sum(pi*y,dim=1),dim=1)                 # 维度相等才能cat\n",
    "    y_pred = torch.sum(pi*y,dim=1)\n",
    "\n",
    "    # The input distrb.\n",
    "    input_data = input[0:2,:]\n",
    "    x_input = torch.arange(1,(300+1),SCALE).to(device=device)\n",
    "    y_input_0 = (input_data[0,:]).to(device=device)\n",
    "    y_input_1 = (input_data[1,:]).to(device=device)\n",
    "\n",
    "    # Init\n",
    "    win_str = \"total_train_step = \"+str(total_train_step)\n",
    "    # title_str = \"Distrb. in \"+win_str+\"/ Loss=\"+str(loss)\n",
    "    title_str = \"Distrb. in \"+win_str\n",
    "    viz.line(X = [0.],Y = [0.], env=\"001\", win=win_str, opts= dict(title=title_str))\n",
    "\n",
    "    # Visdom本身不能把hist和line画在一个window中\n",
    "    # 如果想画一起只能是两条line\n",
    "    # Plot y_target\n",
    "    # viz.histogram(X = n, env=\"001\", win=win_str,\n",
    "    #         opts= dict(title=title_str))\n",
    "    viz.line(X = n,Y= p, env=\"001\", win=win_str, update=\"append\", name='target',\n",
    "            opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "    # Plot y_pred\n",
    "    viz.line(X = x_0,Y= y_pred, env=\"001\", win=win_str, update=\"append\", name='pred',\n",
    "            opts= dict(title=title_str))\n",
    "\n",
    "    # Plot y_input\n",
    "    viz.line(X = x_input,Y= y_input_0, env=\"001\", win=win_str, update=\"append\", name='GT-1', opts= dict(title=title_str))\n",
    "    viz.line(X = x_input,Y= y_input_1, env=\"001\", win=win_str, update=\"append\", name='GT-2', opts= dict(title=title_str))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2.3 plot mdn cdf[无input data]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "def draw_mdn_cdf(pi, mu, sigma, target, input, total_train_step, N_gaussians):\n",
    "    # 只画一个batch中的第一个\n",
    "\n",
    "    # The target distrb.\n",
    "    target = torch.unique(target,dim=0)    # drop the duplicate\n",
    "    n_target = target[:,0]\n",
    "    non_zero_idx = torch.nonzero(n_target)\n",
    "    n = n_target[non_zero_idx]\n",
    "\n",
    "    p_target = target[:,1]\n",
    "    p = p_target[non_zero_idx]\n",
    "    max_n = max(n).item()               # 横轴长度\n",
    "\n",
    "    # The predicted distrb.\n",
    "    m = torch.distributions.Normal(mu,sigma)\n",
    "    x_0 = torch.arange(1,max_n).to(device=device)\n",
    "    x = torch.repeat_interleave(x_0.unsqueeze(dim=1), repeats=N_gaussians, dim=1)\n",
    "    y = m.cdf(x).to(device=device)                        # y:多条高斯曲线; y_pred: 一条GMM曲线\n",
    "    # y_pred = torch.unsqueeze(torch.sum(pi*y,dim=1),dim=1)                 # 维度相等才能cat\n",
    "    y_pred = torch.sum(pi*y,dim=1)\n",
    "\n",
    "    # Init\n",
    "    win_str = \"total_train_step = \"+str(total_train_step)\n",
    "    title_str = \"Distrb. in \"+win_str\n",
    "    viz.line(X = [0.],Y = [0.], env=\"001\", win=win_str, opts= dict(title=title_str))\n",
    "\n",
    "    # Visdom本身不能把hist和line画在一个window中\n",
    "    # 如果想画一起只能是两条line\n",
    "    # Plot y_target\n",
    "    # viz.histogram(X = n, env=\"001\", win=win_str,\n",
    "    #         opts= dict(title=title_str))\n",
    "    viz.line(X = n,Y= p, env=\"001\", win=win_str, update=\"append\", name='target',\n",
    "            opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "    # Plot y_pred\n",
    "    viz.line(X = x_0,Y= y_pred, env=\"001\", win=win_str, update=\"append\", name='pred',\n",
    "            opts= dict(title=title_str))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.3 png格式的net structure"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "def draw_the_net_png():\n",
    "\n",
    "    x = torch.randn([5, 3, 300])  # 定义网络的输入值\n",
    "    mlp = MLP(N_gaussians)\n",
    "    y = mlp(x)                    # 获取网络的预测值\n",
    "\n",
    "    MyConvNetVis = make_dot(y, params=dict(list(mlp.named_parameters()) + [('x', x)]))\n",
    "    MyConvNetVis.format = \"png\"\n",
    "    # 指定文件生成的文件夹\n",
    "    MyConvNetVis.directory = \"data_pic\"\n",
    "    # 生成文件\n",
    "    MyConvNetVis.view()\n",
    "# draw_the_net_png()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.4 plot loss图像\n",
    "1. 直接根据参数决定draw什么loss，反正loss都画在一个window里面"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "win_train_loss_str = \"The Loss of BATCH in the Training Data\"\n",
    "win_vali_loss_str = \"The Loss in the Vali Data\"\n",
    "win_train_epoch_loss_str = \"The Loss of EPOCH in the Training Data\"\n",
    "\n",
    "def draw_loss(X_step, loss, win_str):\n",
    "    viz.line(X = [X_step], Y = [loss],win=win_str, update=\"append\",\n",
    "        opts= dict(title=win_str))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.5 grad graph\n",
    "https://github.com/t-vi/pytorch-tvmisc/blob/master/visualize/bad_grad_viz.ipynb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "outputs": [],
   "source": [
    "\n",
    "def iter_graph(root, callback):\n",
    "    queue = [root]\n",
    "    seen = set()\n",
    "    while queue:\n",
    "        # 队头出fn然后判断fn是否seen\n",
    "        fn = queue.pop()\n",
    "        if fn in seen:\n",
    "            continue\n",
    "        # unseen加入queue，并且继续沿着fn向下递归（DFS？）\n",
    "        seen.add(fn)\n",
    "        for next_fn, _ in fn.next_functions:\n",
    "            # Alert：只有非none的才会加入queue，然后在register_grad里被fn_dict记录下grad_input\n",
    "            if next_fn is not None:\n",
    "                queue.append(next_fn)\n",
    "        callback(fn)\n",
    "\n",
    "def register_hooks(var):\n",
    "    fn_dict = {}  # 记录了grad名字和值（grad_input）\n",
    "    def hook_cb(fn):\n",
    "        def register_grad(grad_input, grad_output):\n",
    "            fn_dict[fn] = grad_input\n",
    "        fn.register_hook(register_grad)\n",
    "\n",
    "    # 1. 递归注册grad_input\n",
    "    iter_graph(var.grad_fn, hook_cb)\n",
    "\n",
    "\n",
    "    # def is_bad_grad(grad_output):\n",
    "    #     if grad_output is None:\n",
    "    #         return False\n",
    "    #     return grad_output.isnan().any() or (grad_output.abs() >= 1e6).any()\n",
    "    def is_bad_grad(grad_output):\n",
    "        if grad_output is None:\n",
    "                return True\n",
    "        grad_output = grad_output.data\n",
    "        return grad_output.ne(grad_output).any() or grad_output.gt(1e6).any()\n",
    "\n",
    "    # 2. graph已经从grad input构建完了，还有grad output的判断以及颜色的选择\n",
    "    def make_dot():\n",
    "        node_attr = dict(style='filled',\n",
    "                        shape='box',\n",
    "                        align='left',\n",
    "                        fontsize='12',\n",
    "                        ranksep='0.1',\n",
    "                        height='0.2')\n",
    "        dot = Digraph(node_attr=node_attr, graph_attr=dict(size=\"12,12\"))\n",
    "\n",
    "        def size_to_str(size):\n",
    "            return '('+(', ').join(map(str, size))+')'\n",
    "\n",
    "        def build_graph(fn):\n",
    "            if hasattr(fn, 'variable'):  # if GradAccumulator\n",
    "                u = fn.variable\n",
    "                node_name = 'Variable\\n ' + size_to_str(u.size())\n",
    "                dot.node(str(id(u)), node_name, fillcolor='lightblue')\n",
    "            else:  # 不是variable属性，白色\n",
    "                assert fn in fn_dict, fn   # 判断fn在不在fn_dict里面！\n",
    "                fillcolor = 'white'\n",
    "                if any(is_bad_grad(gi) for gi in fn_dict[fn]):\n",
    "                    fillcolor = 'red'\n",
    "                dot.node(str(id(fn)), str(type(fn).__name__), fillcolor=fillcolor)\n",
    "            for next_fn, _ in fn.next_functions:\n",
    "                if next_fn is not None:\n",
    "                    next_id = id(getattr(next_fn, 'variable', next_fn))\n",
    "                    dot.edge(str(next_id), str(id(fn)))\n",
    "        iter_graph(var.grad_fn, build_graph)\n",
    "\n",
    "        return dot\n",
    "\n",
    "    return make_dot\n",
    "\n",
    "# x = torch.randn(10, 10, requires_grad=True)\n",
    "# y = torch.randn(10, 10, requires_grad=True)\n",
    "#\n",
    "# z = x / (y * 0)\n",
    "# z = z.sum() * 2\n",
    "# get_dot = register_hooks(z)\n",
    "\n",
    "\n",
    "# with torch.autograd.detect_anomaly():\n",
    "#     z.backward()\n",
    "#     dot = get_dot()\n",
    "#     dot.save('tmp.dot') # to get .dot\n",
    "#     dot.render('tmp') # to get SVG\n",
    "#     # dot # in Jupyter, you can just render the variable"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6 Training\n",
    "## 6.1 Init"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "       BatchNorm2d-1            [-1, 3, 1, 100]               0\n",
      "            Conv2d-2             [-1, 3, 1, 33]              27\n",
      "       BatchNorm2d-3             [-1, 3, 1, 33]               6\n",
      "           Dropout-4             [-1, 3, 1, 33]               0\n",
      "              Tanh-5             [-1, 3, 1, 33]               0\n",
      "            Conv2d-6             [-1, 3, 1, 16]              18\n",
      "       BatchNorm2d-7             [-1, 3, 1, 16]               6\n",
      "           Dropout-8             [-1, 3, 1, 16]               0\n",
      "              Tanh-9             [-1, 3, 1, 16]               0\n",
      "          Flatten-10                   [-1, 48]               0\n",
      "           Linear-11                    [-1, 3]             147\n",
      "          Softmax-12                    [-1, 3]               0\n",
      "           Linear-13                    [-1, 3]             147\n",
      "           Linear-14                    [-1, 3]             147\n",
      "================================================================\n",
      "Total params: 498\n",
      "Trainable params: 498\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.01\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "mlp = MLP(N_gaussians)\n",
    "\n",
    "# Init the params\n",
    "# # mlp = model_param_init(mlp)\n",
    "# Init the vis\n",
    "viz.line(X = [0.],Y = [0.], env=\"001\", win=win_train_loss_str, opts= dict(title=win_train_loss_str))\n",
    "viz.line(X = [0.],Y = [0.], env=\"001\", win=win_vali_loss_str, opts= dict(title=win_vali_loss_str))\n",
    "viz.line(X = [0.],Y = [0.], env=\"001\", win=win_train_epoch_loss_str, opts= dict(title=win_train_epoch_loss_str))\n",
    "\n",
    "# Save the init params\n",
    "torch.save(mlp.state_dict(), 'mlp_init.pth')\n",
    "\n",
    "# Load the saved model\n",
    "# model_data = torch.load('mlp_init_loss_17.pth')\n",
    "# mlp.load_state_dict(model_data)\n",
    "\n",
    "mlp = mlp.to(device=device)\n",
    "summary(mlp, (3,100))\n",
    "\n",
    "############ learning rate strategy ############\n",
    "# 1.\n",
    "# Set different lr for params\n",
    "# id: id() 函数返回对象的唯一标识符，标识符是一个整数。返回对象的内存地址。\n",
    "mu_params = list(map(id, mlp.z_mu.parameters()))\n",
    "base_params = filter(lambda p: id(p) not in mu_params, mlp.parameters())\n",
    "params = [{'params': base_params},         # 如果对某个参数不指定学习率，就使用最外层的默认学习率\n",
    "        {'params': mlp.z_mu.parameters(), 'lr': lr_for_mu}]\n",
    "# optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "\n",
    "optimizer = torch.optim.Adagrad(params,lr=learning_rate)\n",
    "\n",
    "# 2.\n",
    "# Or set lr decay\n",
    "# StepLR为步进，step_size=20表示每20个epoch，lr*gama\n",
    "# scheduler  = torch.optim.lr_scheduler.StepLR(optimizer,step_size=6,gamma=0.8)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10,20], gamma=0.75, last_epoch=-1)\n",
    "\n",
    "\n",
    "# # Set the hooks\n",
    "# #mlp.conv1.register_forward_hook(hook_forward_fn)\n",
    "# mlp.conv1.register_full_backward_hook(hook_backward_fn)\n",
    "#\n",
    "# #mlp.linear1.register_forward_hook(hook_forward_fn)\n",
    "# #mlp.linear1.register_full_backward_hook(hook_backward_fn)\n",
    "#\n",
    "# #mlp.z_pi.register_forward_hook(hook_forward_fn)\n",
    "# mlp.z_pi.register_full_backward_hook(hook_backward_fn_pi)\n",
    "#\n",
    "# #mlp.z_mu.register_forward_hook(hook_forward_fn)\n",
    "# mlp.z_mu.register_full_backward_hook(hook_backward_fn_mu)\n",
    "#\n",
    "#mlp.z_sigma.register_forward_hook(hook_forward_fn)\n",
    "# mlp.z_sigma.register_full_backward_hook(hook_backward_fn_sigma)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.2 grad check\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "outputs": [],
   "source": [
    "# # grad check\n",
    "# input_test = torch.randn((1,3,100), requires_grad=True,device=device)\n",
    "# test_ans = gradcheck(mlp.to(device), input_test, eps=1e-3)  #, eps=1e-6, atol=1e-4\n",
    "# print(\"Are the gradients correct: \", test_ans)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.3 START HERE"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- 11 batch----\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Now this is EPOCH 0 ==========\n",
      "---- 0 batch----\n",
      "训练次数：0，Loss：678.2802734375\n",
      "---- 1 batch----\n",
      "训练次数：1，Loss：670.2285766601562\n",
      "---- 2 batch----\n",
      "训练次数：2，Loss：680.2017211914062\n",
      "---- 3 batch----\n",
      "训练次数：3，Loss：659.6618041992188\n",
      "---- 4 batch----\n",
      "训练次数：4，Loss：666.0266723632812\n",
      "---- 5 batch----\n",
      "训练次数：5，Loss：656.9972534179688\n",
      "---- 6 batch----\n",
      "训练次数：6，Loss：652.8593139648438\n",
      "---- 7 batch----\n",
      "训练次数：7，Loss：611.0421142578125\n",
      "---- 8 batch----\n",
      "训练次数：8，Loss：668.8651733398438\n",
      "---- 9 batch----\n",
      "训练次数：9，Loss：676.1024780273438\n",
      "---- 10 batch----\n",
      "训练次数：10，Loss：677.3471069335938\n",
      "---- 11 batch----\n",
      "训练次数：11，Loss：661.8721923828125\n",
      "---- 12 batch----\n",
      "训练次数：12，Loss：656.8170776367188\n",
      "---- 13 batch----\n",
      "训练次数：13，Loss：635.7457275390625\n",
      "---- 14 batch----\n",
      "训练次数：14，Loss：650.7761840820312\n",
      "---- 15 batch----\n",
      "训练次数：15，Loss：596.2493286132812\n",
      "---- 16 batch----\n",
      "训练次数：16，Loss：676.0298461914062\n",
      "---- 17 batch----\n",
      "训练次数：17，Loss：666.6851196289062\n",
      "---- 18 batch----\n",
      "训练次数：18，Loss：655.9864501953125\n",
      "---- 19 batch----\n",
      "训练次数：19，Loss：651.4188232421875\n",
      "---- 20 batch----\n",
      "训练次数：20，Loss：590.460693359375\n",
      "========== IN EPOCH 0 the total loss is 13739.653930664062 ==========\n",
      "========== Now this is EPOCH 1 ==========\n",
      "---- 0 batch----\n",
      "训练次数：21，Loss：643.0526123046875\n",
      "---- 1 batch----\n",
      "训练次数：22，Loss：670.1532592773438\n",
      "---- 2 batch----\n",
      "训练次数：23，Loss：604.9129638671875\n",
      "---- 3 batch----\n",
      "训练次数：24，Loss：621.046875\n",
      "---- 4 batch----\n",
      "训练次数：25，Loss：615.5037231445312\n",
      "---- 5 batch----\n",
      "训练次数：26，Loss：638.7212524414062\n",
      "---- 6 batch----\n",
      "训练次数：27，Loss：634.5763549804688\n",
      "---- 7 batch----\n",
      "训练次数：28，Loss：654.8932495117188\n",
      "---- 8 batch----\n",
      "训练次数：29，Loss：627.4097900390625\n",
      "---- 9 batch----\n",
      "训练次数：30，Loss：638.4693603515625\n",
      "---- 10 batch----\n",
      "训练次数：31，Loss：652.5868530273438\n",
      "---- 11 batch----\n",
      "训练次数：32，Loss：642.4835815429688\n",
      "---- 12 batch----\n",
      "训练次数：33，Loss：688.866943359375\n",
      "---- 13 batch----\n",
      "训练次数：34，Loss：635.5963745117188\n",
      "---- 14 batch----\n",
      "训练次数：35，Loss：650.3472290039062\n",
      "---- 15 batch----\n",
      "训练次数：36，Loss：619.0978393554688\n",
      "---- 16 batch----\n",
      "训练次数：37，Loss：582.9326782226562\n",
      "---- 17 batch----\n",
      "训练次数：38，Loss：582.0632934570312\n",
      "---- 18 batch----\n",
      "训练次数：39，Loss：634.8605346679688\n",
      "---- 19 batch----\n",
      "训练次数：40，Loss：631.1318359375\n",
      "---- 20 batch----\n",
      "训练次数：41，Loss：577.2454833984375\n",
      "========== IN EPOCH 1 the total loss is 13245.952087402344 ==========\n",
      "========== Now this is EPOCH 2 ==========\n",
      "---- 0 batch----\n",
      "训练次数：42，Loss：611.287841796875\n",
      "---- 1 batch----\n",
      "训练次数：43，Loss：637.467529296875\n",
      "---- 2 batch----\n",
      "训练次数：44，Loss：599.5947265625\n",
      "---- 3 batch----\n",
      "训练次数：45，Loss：624.5914306640625\n",
      "---- 4 batch----\n",
      "训练次数：46，Loss：640.6372680664062\n",
      "---- 5 batch----\n",
      "训练次数：47，Loss：594.179931640625\n",
      "---- 6 batch----\n",
      "训练次数：48，Loss：599.7122192382812\n",
      "---- 7 batch----\n",
      "训练次数：49，Loss：636.6550903320312\n",
      "---- 8 batch----\n",
      "训练次数：50，Loss：636.9466552734375\n",
      "---- 9 batch----\n",
      "训练次数：51，Loss：619.8203125\n",
      "---- 10 batch----\n",
      "训练次数：52，Loss：602.681396484375\n",
      "---- 11 batch----\n",
      "训练次数：53，Loss：627.5702514648438\n",
      "---- 12 batch----\n",
      "训练次数：54，Loss：573.0473022460938\n",
      "---- 13 batch----\n",
      "训练次数：55，Loss：618.4602661132812\n",
      "---- 14 batch----\n",
      "训练次数：56，Loss：593.94580078125\n",
      "---- 15 batch----\n",
      "训练次数：57，Loss：611.9462890625\n",
      "---- 16 batch----\n",
      "训练次数：58，Loss：619.6265869140625\n",
      "---- 17 batch----\n",
      "训练次数：59，Loss：573.3043823242188\n",
      "---- 18 batch----\n",
      "训练次数：60，Loss：591.7077026367188\n",
      "---- 19 batch----\n",
      "训练次数：61，Loss：600.9398193359375\n",
      "---- 20 batch----\n",
      "训练次数：62，Loss：550.0879516601562\n",
      "========== IN EPOCH 2 the total loss is 12764.210754394531 ==========\n",
      "========== Now this is EPOCH 3 ==========\n",
      "---- 0 batch----\n",
      "训练次数：63，Loss：573.987060546875\n",
      "---- 1 batch----\n",
      "训练次数：64，Loss：633.2427978515625\n",
      "---- 2 batch----\n",
      "训练次数：65，Loss：593.3869018554688\n",
      "---- 3 batch----\n",
      "训练次数：66，Loss：547.660400390625\n",
      "---- 4 batch----\n",
      "训练次数：67，Loss：601.7314453125\n",
      "---- 5 batch----\n",
      "训练次数：68，Loss：581.9539794921875\n",
      "---- 6 batch----\n",
      "训练次数：69，Loss：576.96875\n",
      "---- 7 batch----\n",
      "训练次数：70，Loss：626.1082763671875\n",
      "---- 8 batch----\n",
      "训练次数：71，Loss：599.9132080078125\n",
      "---- 9 batch----\n",
      "训练次数：72，Loss：597.0960693359375\n",
      "---- 10 batch----\n",
      "训练次数：73，Loss：546.8226928710938\n",
      "---- 11 batch----\n",
      "训练次数：74，Loss：631.3991088867188\n",
      "---- 12 batch----\n",
      "训练次数：75，Loss：594.657470703125\n",
      "---- 13 batch----\n",
      "训练次数：76，Loss：582.314453125\n",
      "---- 14 batch----\n",
      "训练次数：77，Loss：574.9301147460938\n",
      "---- 15 batch----\n",
      "训练次数：78，Loss：553.96484375\n",
      "---- 16 batch----\n",
      "训练次数：79，Loss：574.1190185546875\n",
      "---- 17 batch----\n",
      "训练次数：80，Loss：563.8890380859375\n",
      "---- 18 batch----\n",
      "训练次数：81，Loss：545.4488525390625\n",
      "---- 19 batch----\n",
      "训练次数：82，Loss：540.8550415039062\n",
      "---- 20 batch----\n",
      "训练次数：83，Loss：555.6843872070312\n",
      "========== IN EPOCH 3 the total loss is 12196.133911132812 ==========\n",
      "========== Now this is EPOCH 4 ==========\n",
      "---- 0 batch----\n",
      "训练次数：84，Loss：567.0296630859375\n",
      "---- 1 batch----\n",
      "训练次数：85，Loss：620.6703491210938\n",
      "---- 2 batch----\n",
      "训练次数：86，Loss：522.394287109375\n",
      "---- 3 batch----\n",
      "训练次数：87，Loss：523.6499633789062\n",
      "---- 4 batch----\n",
      "训练次数：88，Loss：568.1214599609375\n",
      "---- 5 batch----\n",
      "训练次数：89，Loss：600.3616333007812\n",
      "---- 6 batch----\n",
      "训练次数：90，Loss：543.856689453125\n",
      "---- 7 batch----\n",
      "训练次数：91，Loss：567.6806030273438\n",
      "---- 8 batch----\n",
      "训练次数：92，Loss：596.0289306640625\n",
      "---- 9 batch----\n",
      "训练次数：93，Loss：552.67333984375\n",
      "---- 10 batch----\n",
      "训练次数：94，Loss：566.8253173828125\n",
      "---- 11 batch----\n",
      "训练次数：95，Loss：585.5950317382812\n",
      "---- 12 batch----\n",
      "训练次数：96，Loss：514.4459228515625\n",
      "---- 13 batch----\n",
      "训练次数：97，Loss：562.1268310546875\n",
      "---- 14 batch----\n",
      "训练次数：98，Loss：555.3910522460938\n",
      "---- 15 batch----\n",
      "训练次数：99，Loss：549.2703247070312\n",
      "---- 16 batch----\n",
      "训练次数：100，Loss：592.154052734375\n",
      "---- 17 batch----\n",
      "训练次数：101，Loss：560.9630737304688\n",
      "---- 18 batch----\n",
      "训练次数：102，Loss：548.0918579101562\n",
      "---- 19 batch----\n",
      "训练次数：103，Loss：513.9660034179688\n",
      "---- 20 batch----\n",
      "训练次数：104，Loss：488.4267883300781\n",
      "========== IN EPOCH 4 the total loss is 11699.723175048828 ==========\n",
      "========== Now this is EPOCH 5 ==========\n",
      "---- 0 batch----\n",
      "训练次数：105，Loss：537.4257202148438\n",
      "---- 1 batch----\n",
      "训练次数：106，Loss：539.7062377929688\n",
      "---- 2 batch----\n",
      "训练次数：107，Loss：544.973876953125\n",
      "---- 3 batch----\n",
      "训练次数：108，Loss：538.5447387695312\n",
      "---- 4 batch----\n",
      "训练次数：109，Loss：538.1533203125\n",
      "---- 5 batch----\n",
      "训练次数：110，Loss：518.8983764648438\n",
      "---- 6 batch----\n",
      "训练次数：111，Loss：553.9561767578125\n",
      "---- 7 batch----\n",
      "训练次数：112，Loss：543.81787109375\n",
      "---- 8 batch----\n",
      "训练次数：113，Loss：560.7229614257812\n",
      "---- 9 batch----\n",
      "训练次数：114，Loss：556.396484375\n",
      "---- 10 batch----\n",
      "训练次数：115，Loss：528.943115234375\n",
      "---- 11 batch----\n",
      "训练次数：116，Loss：510.2081604003906\n",
      "---- 12 batch----\n",
      "训练次数：117，Loss：481.990234375\n",
      "---- 13 batch----\n",
      "训练次数：118，Loss：478.2374572753906\n",
      "---- 14 batch----\n",
      "训练次数：119，Loss：508.6017761230469\n",
      "---- 15 batch----\n",
      "训练次数：120，Loss：592.0785522460938\n",
      "---- 16 batch----\n",
      "训练次数：121，Loss：552.1524047851562\n",
      "---- 17 batch----\n",
      "训练次数：122，Loss：514.6618041992188\n",
      "---- 18 batch----\n",
      "训练次数：123，Loss：520.2062377929688\n",
      "---- 19 batch----\n",
      "训练次数：124，Loss：542.2713623046875\n",
      "---- 20 batch----\n",
      "训练次数：125，Loss：434.6794128417969\n",
      "========== IN EPOCH 5 the total loss is 11096.626281738281 ==========\n",
      "========== Now this is EPOCH 6 ==========\n",
      "---- 0 batch----\n",
      "训练次数：126，Loss：501.9920349121094\n",
      "---- 1 batch----\n",
      "训练次数：127，Loss：553.80615234375\n",
      "---- 2 batch----\n",
      "训练次数：128，Loss：544.8990478515625\n",
      "---- 3 batch----\n",
      "训练次数：129，Loss：423.9552001953125\n",
      "---- 4 batch----\n",
      "训练次数：130，Loss：536.1398315429688\n",
      "---- 5 batch----\n",
      "训练次数：131，Loss：480.3037414550781\n",
      "---- 6 batch----\n",
      "训练次数：132，Loss：494.8306884765625\n",
      "---- 7 batch----\n",
      "训练次数：133，Loss：527.9232788085938\n",
      "---- 8 batch----\n",
      "训练次数：134，Loss：497.56683349609375\n",
      "---- 9 batch----\n",
      "训练次数：135，Loss：516.8923950195312\n",
      "---- 10 batch----\n",
      "训练次数：136，Loss：486.779541015625\n",
      "---- 11 batch----\n",
      "训练次数：137，Loss：529.2092895507812\n",
      "---- 12 batch----\n",
      "训练次数：138，Loss：553.9453735351562\n",
      "---- 13 batch----\n",
      "训练次数：139，Loss：527.3964233398438\n",
      "---- 14 batch----\n",
      "训练次数：140，Loss：510.7108459472656\n",
      "---- 15 batch----\n",
      "训练次数：141，Loss：460.43280029296875\n",
      "---- 16 batch----\n",
      "训练次数：142，Loss：498.4766845703125\n",
      "---- 17 batch----\n",
      "训练次数：143，Loss：577.4312744140625\n",
      "---- 18 batch----\n",
      "训练次数：144，Loss：514.1221313476562\n",
      "---- 19 batch----\n",
      "训练次数：145，Loss：481.95843505859375\n",
      "---- 20 batch----\n",
      "训练次数：146，Loss：410.2182922363281\n",
      "========== IN EPOCH 6 the total loss is 10628.990295410156 ==========\n",
      "========== Now this is EPOCH 7 ==========\n",
      "---- 0 batch----\n",
      "训练次数：147，Loss：559.3589477539062\n",
      "---- 1 batch----\n",
      "训练次数：148，Loss：500.03521728515625\n",
      "---- 2 batch----\n",
      "训练次数：149，Loss：473.63531494140625\n",
      "---- 3 batch----\n",
      "训练次数：150，Loss：465.1389465332031\n",
      "---- 4 batch----\n",
      "训练次数：151，Loss：510.0236511230469\n",
      "---- 5 batch----\n",
      "训练次数：152，Loss：476.968017578125\n",
      "---- 6 batch----\n",
      "训练次数：153，Loss：553.8525390625\n",
      "---- 7 batch----\n",
      "训练次数：154，Loss：530.3419799804688\n",
      "---- 8 batch----\n",
      "训练次数：155，Loss：475.7147216796875\n",
      "---- 9 batch----\n",
      "训练次数：156，Loss：504.85308837890625\n",
      "---- 10 batch----\n",
      "训练次数：157，Loss：487.6387023925781\n",
      "---- 11 batch----\n",
      "训练次数：158，Loss：489.456298828125\n",
      "---- 12 batch----\n",
      "训练次数：159，Loss：485.6798400878906\n",
      "---- 13 batch----\n",
      "训练次数：160，Loss：492.9400329589844\n",
      "---- 14 batch----\n",
      "训练次数：161，Loss：501.5789794921875\n",
      "---- 15 batch----\n",
      "训练次数：162，Loss：507.8674621582031\n",
      "---- 16 batch----\n",
      "训练次数：163，Loss：447.0927734375\n",
      "---- 17 batch----\n",
      "训练次数：164，Loss：408.406982421875\n",
      "---- 18 batch----\n",
      "训练次数：165，Loss：509.6390380859375\n",
      "---- 19 batch----\n",
      "训练次数：166，Loss：501.88946533203125\n",
      "---- 20 batch----\n",
      "训练次数：167，Loss：454.0507507324219\n",
      "========== IN EPOCH 7 the total loss is 10336.16275024414 ==========\n",
      "========== Now this is EPOCH 8 ==========\n",
      "---- 0 batch----\n",
      "训练次数：168，Loss：502.516357421875\n",
      "---- 1 batch----\n",
      "训练次数：169，Loss：521.2313842773438\n",
      "---- 2 batch----\n",
      "训练次数：170，Loss：471.0475158691406\n",
      "---- 3 batch----\n",
      "训练次数：171，Loss：530.322265625\n",
      "---- 4 batch----\n",
      "训练次数：172，Loss：450.47991943359375\n",
      "---- 5 batch----\n",
      "训练次数：173，Loss：447.8951416015625\n",
      "---- 6 batch----\n",
      "训练次数：174，Loss：465.4925537109375\n"
     ]
    }
   ],
   "source": [
    "# filename = \"../log_file.txt\"\n",
    "# f = open(filename,'w')\n",
    "total_train_step = 0\n",
    "EPOCH_NUM = 30\n",
    "\n",
    "mlp.train()\n",
    "\n",
    "for epoch in range(EPOCH_NUM):\n",
    "    # # Save the net structure\n",
    "    # net_file_name = \"net_before_epoch\"+str(epoch)+\".pth\"\n",
    "    # net_path = net_root_path + net_file_name\n",
    "    # torch.save(mlp.state_dict(), net_path)\n",
    "\n",
    "    epoch_train_loss = 0\n",
    "    print(f\"========== Now this is EPOCH {epoch} ==========\")\n",
    "    for batch_id,data in enumerate(train_loader):\n",
    "\n",
    "        input_data, target = data\n",
    "        print(f\"---- {batch_id} batch----\")\n",
    "        # Do the inference\n",
    "        input_data = input_data.to(device)\n",
    "        target = target.to(device)\n",
    "        pi, mu, sigma = mlp(input_data)\n",
    "\n",
    "        # Save the params\n",
    "        # params = list(mlp.named_parameters())\n",
    "\n",
    "        # Cal the MLE loss and draw the distrb.\n",
    "        loss = loss_fn_v2(pi, mu, sigma, target.detach(), N_gaussians)\n",
    "        # loss = loss_fn_v3(pi, mu, sigma, target, N_gaussians)\n",
    "\n",
    "        # Cal the CDF loss and draw the distrb.\n",
    "        # loss = loss_fn_cdf(pi, mu, sigma, target, N_gaussians)\n",
    "\n",
    "        # Cal the CE or KL loss and draw the distrb.\n",
    "        # loss  = loss_fn_CE(pi, mu, sigma, target, N_gaussians)\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "        draw_loss(total_train_step, loss.item(),win_train_loss_str)\n",
    "\n",
    "        print(\"训练次数：{}，Loss：{}\".format(total_train_step, loss.item()))\n",
    "\n",
    "        ########### before step() ###############\n",
    "\n",
    "        # for name, parms in mlp.named_parameters():\n",
    "        #     print(\"=====before step()=====\")\n",
    "        #     print('-->name:', name)\n",
    "        #     print('-->params:', parms.data)\n",
    "        #     # print('-->grad_requirs:',parms.requires_grad)\n",
    "        #     print('-->grad_value:',parms.grad)\n",
    "        #     print(\"===\")\n",
    "\n",
    "        # Optim\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        ######\n",
    "\n",
    "        # 反向传播时检测是否有异常值，定位code\n",
    "        # with torch.autograd.detect_anomaly():\n",
    "        #     loss.backward()\n",
    "\n",
    "\n",
    "        ######\n",
    "        # get_dot = register_hooks(loss)\n",
    "        # dot = get_dot()\n",
    "        # dot.save('tmp.dot')   # to get .dot\n",
    "        # dot.render('tmp')     # to get pdf\n",
    "\n",
    "        ######\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # for name, parms in mlp.named_parameters():\n",
    "        #     print(\"=====After step()=====\")\n",
    "        #     print('-->name:', name)\n",
    "        #     print('-->params:', parms.data)\n",
    "        #     # print('-->grad_requirs:',parms.requires_grad)\n",
    "        #     print('-->grad_value:',parms.grad)\n",
    "        #     print(\"===\")\n",
    "\n",
    "        if total_train_step % 20 == 0:\n",
    "        # Only draw the 1st result in a training batch (5 in total)\n",
    "            draw_mdn_2(pi[0,:].detach(), mu[0,:].detach(), sigma[0,:].detach(), target[0].detach(), input_data[0].detach(),total_train_step, N_gaussians)\n",
    "            # draw_mdn_cdf(pi[0,:].detach(), m[0], target[0].detach(), total_train_step, loss_list[0],N_gaussians)\n",
    "\n",
    "            ########### Do validation\n",
    "            mlp.eval()\n",
    "            total_vali_loss = 0\n",
    "            for vali_batch_id, vali_data in enumerate(val_loader):\n",
    "                vali_input_data, vali_target = vali_data\n",
    "                vali_input_data = vali_input_data.to(device)\n",
    "                vali_pi, vali_mu, vali_sigma = mlp(vali_input_data)\n",
    "\n",
    "                # Cal the sum of vali loss instead of vali loss in a batch\n",
    "                # vali_duration,vali_m  = loss_preparation(vali_pi.detach(), vali_mu.detach(), vali_sigma.detach(), vali_target.detach())\n",
    "                vali_loss = loss_fn_v2(vali_pi, vali_mu, vali_sigma, vali_target, N_gaussians)\n",
    "                total_vali_loss += vali_loss.item()\n",
    "            # Plot the vali loss\n",
    "            # total_vali_loss.item()/83.\n",
    "            draw_loss(epoch, (total_vali_loss), win_vali_loss_str)\n",
    "            mlp.train()\n",
    "\n",
    "        total_train_step += 1\n",
    "    # Plot the loss in this EPOCH\n",
    "    print(f\"========== IN EPOCH {epoch} the total loss is {epoch_train_loss} ==========\")\n",
    "\n",
    "    # # Save the net structure\n",
    "    # net_file_name = \"net_after_epoch\"+str(epoch)+\".pth\"\n",
    "    # net_path = net_root_path + net_file_name\n",
    "    # torch.save(mlp.state_dict(), net_path)\n",
    "\n",
    "    # Plot loss of this EPOCH\n",
    "    draw_loss(epoch, epoch_train_loss,win_train_epoch_loss_str)\n",
    "    writer.add_scalars(\"Loss of EPOCH\",{\"Train\":epoch_train_loss},epoch)\n",
    "\n",
    "    # Record the params\n",
    "    # for name,param in params:\n",
    "    #     #参数的梯度\n",
    "    #     #print(\"name:\",name)\n",
    "    #     #print(\"param.grad:\",param.grad)\n",
    "    #     # writer.add_histogram(tag = name +'_grad of EPOCH '+str(epoch),values=param.grad,global_step=epoch)\n",
    "    #     # #参数值\n",
    "    #     writer.add_histogram(tag = name +'data of EPOCH '+str(epoch),values=param.data,global_step=epoch)\n",
    "\n",
    "# f.close()\n",
    "\n",
    "writer.close()\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# torch.save(mlp.state_dict(), 'mlp_init_loss_17.pth')\n",
    "# viz.delete_env(\"001_test\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [],
   "source": [
    "# Gtmp = pgv.AGraph('tmp.dot')\n",
    "# G = nx.Graph(Gtmp)\n",
    "# nx.draw(G)\n",
    "# # plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "$\\mathcal{L}(y \\vert x) = - \\log\\bigg\\{\\sum_{k=1}^K \\pi_k(x)  \\mathcal{N}\\big(y \\vert \\mu_k(x), \\Sigma_k(x)\\big)\\bigg\\}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Load and test\n",
    "1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "outputs": [
    {
     "data": {
      "text/plain": "array([939])"
     },
     "execution_count": 646,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the data\n",
    "small_batch_size = 1\n",
    "\n",
    "small_test_idx = train_idx[3:4]\n",
    "small_test_loader = DataLoader(dataset = dataset,batch_size = small_batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(small_test_idx),collate_fn = my_collate_fn)\n",
    "small_test_idx"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "outputs": [],
   "source": [
    "# Plot\n",
    "test_env = \"002\"\n",
    "def draw_mdn_test(pi, m, target, input, epoch, idx,N_gaussians,flag):\n",
    "    # 只画一个batch中的第一个\n",
    "\n",
    "    # The target distrb.\n",
    "    target = torch.unique(target,dim=0)    # drop the duplicate\n",
    "    n_target = target[:,0]\n",
    "    non_zero_idx = torch.nonzero(n_target)\n",
    "    n = n_target[non_zero_idx]\n",
    "\n",
    "    p_target = target[:,1]\n",
    "    p = p_target[non_zero_idx]*\n",
    "    max_n = max(n).item()               # 横轴长度\n",
    "\n",
    "    # The predicted distrb.\n",
    "    x_0 = torch.arange(1,max_n).to(device=device)\n",
    "    x = torch.repeat_interleave(x_0.unsqueeze(dim=1), repeats=N_gaussians, dim=1)\n",
    "    y = torch.exp(m.log_prob(x)).to(device=device)                          # y:多条高斯曲线; y_pred: 一条GMM曲线\n",
    "    # y_pred = torch.unsqueeze(torch.sum(pi*y,dim=1),dim=1)                 # 维度相等才能cat\n",
    "    y_pred = torch.sum(pi*y,dim=1)\n",
    "\n",
    "    # The input distrb.\n",
    "    input_data = input[0:2,:]\n",
    "    x_input = torch.arange(1,input_data.shape[1]+1).to(device=device)\n",
    "    y_input_0 = input_data[0,:].to(device=device)\n",
    "    y_input_1 = input_data[1,:].to(device=device)\n",
    "\n",
    "    # Init\n",
    "    win_str = flag+ \" epoch=\"+str(epoch)+\" | idx=\"+str(idx)\n",
    "    title_str = \"Distrb. \"+win_str\n",
    "    viz.line(X = [0.],Y = [0.], env=test_env, win=win_str, opts= dict(title=title_str))\n",
    "\n",
    "    # Visdom本身不能把hist和line画在一个window中\n",
    "    # 如果想画一起只能是两条line\n",
    "    # Plot y_target\n",
    "    # viz.histogram(X = n, env=\"001\", win=win_str,\n",
    "    #         opts= dict(title=title_str))\n",
    "    viz.line(X = n,Y= p, env=test_env, win=win_str, update=\"append\", name='target',\n",
    "            opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "    # Plot y_pred\n",
    "    viz.line(X = x_0,Y= y_pred, env=test_env, win=win_str, update=\"append\", name='pred',\n",
    "            opts= dict(title=title_str))\n",
    "\n",
    "    # Plot y_input\n",
    "    viz.line(X = x_input,Y= y_input_0, env=test_env, win=win_str, update=\"append\", name='GT-1', opts= dict(title=title_str))\n",
    "    viz.line(X = x_input,Y= y_input_1, env=test_env, win=win_str, update=\"append\", name='GT-2', opts= dict(title=title_str))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(EPOCH_NUM):\n",
    "\n",
    "    net_file_name_0 = \"net_before_epoch\"+str(epoch)+\".pth\"\n",
    "    net_file_name_1 = \"net_after_epoch\"+str(epoch)+\".pth\"\n",
    "    net_path0 = net_root_path + net_file_name_0\n",
    "    net_path1 = net_root_path + net_file_name_1\n",
    "\n",
    "    # Load the model\n",
    "    mlp0 = MLP(N_gaussians)\n",
    "    mlp1 = MLP(N_gaussians)\n",
    "    mlp0.load_state_dict(torch.load(net_path0))\n",
    "    mlp1.load_state_dict(torch.load(net_path1))\n",
    "    mlp0.to(device)\n",
    "    mlp1.to(device)\n",
    "\n",
    "    # 其实每个epoch只会执行一个batch\n",
    "    for batch_id,data in enumerate(small_test_loader):\n",
    "\n",
    "        # Predict\n",
    "        input_data, target = data\n",
    "        input_data = input_data.to(device)\n",
    "        target = target.to(device)\n",
    "        pi0, mu0, sigma0 = mlp0(input_data)\n",
    "        pi1, mu1, sigma1 = mlp1(input_data)\n",
    "        # Get the mdn\n",
    "        _,m0  = loss_preparation(pi0.detach(), mu0.detach(), sigma0.detach(), target.detach())\n",
    "        _,m1  = loss_preparation(pi1.detach(), mu1.detach(), sigma1.detach(), target.detach())\n",
    "\n",
    "        # Plot for 'small_batch_size' times\n",
    "        for j in range(small_batch_size):\n",
    "            draw_mdn_test(pi0[j,:].detach(), m0[j], target[j].detach(), input_data[j].detach(), epoch, j,N_gaussians,flag=\"before\")\n",
    "            draw_mdn_test(pi1[j,:].detach(), m1[j], target[j].detach(), input_data[j].detach(), epoch, j,N_gaussians,flag=\"after\")\n",
    "\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
