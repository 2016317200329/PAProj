{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 0. Intro\n",
    "\n",
    "1. 只有1个GT模型input"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Preparations\n",
    "## 1.1 Import"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from importlib import reload\n",
    "import random\n",
    "import torch.utils.data\n",
    "from mydataset import *\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from tensorboardX import SummaryWriter\n",
    "from visdom import Visdom\n",
    "from graphviz import Digraph\n",
    "from torchviz import make_dot\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pygraphviz as pgv\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import geomloss\n",
    "import time\n",
    "\n",
    "import scipy\n",
    "from torch.nn import KLDivLoss\n",
    "from torch.autograd.gradcheck import gradcheck\n",
    "from config import bcolors\n",
    "\n",
    "import config\n",
    "import loss\n",
    "import plot"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Global settings and reloading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "reload(config)  # 必须reload！！\n",
    "reload(loss)    # 必须reload！！\n",
    "reload(plot)\n",
    "from config import DefaultConfig\n",
    "from loss import cal_metric\n",
    "from loss import loss_fn_v2\n",
    "from loss import validate\n",
    "from loss import loss_fn_WD\n",
    "from plot import plot_conv_weight\n",
    "from plot import plot_mu_weight\n",
    "from plot import plot_pi_weight\n",
    "from plot import plot_sigma_weight\n",
    "from plot import plot_net\n",
    "\n",
    "opt = DefaultConfig()\n",
    "\n",
    "\n",
    "# nums of Gaussian kernels\n",
    "N_gaussians = opt.N_gaussians\n",
    "\n",
    "total_train_step = 0\n",
    "total_test_step = 0\n",
    "\n",
    "# Range【一般用不到】\n",
    "a = 0\n",
    "b = 255\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Dataset and Dataloader\n",
    "1. DataLoader中的`shuffer=True`表示在每一次epoch中都打乱所有数据的顺序，然后以batch为单位从头到尾按顺序取用数据。这样的结果就是不同epoch中的数据都是乱序的,设置随机种子的作用就是让你的每一次训练都乱的一样，\n",
    "\n",
    "## 2.1 Dataset and spliting\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 设置随机数种子"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup_seed(4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [],
   "source": [
    "dataset = myDataset(opt.train_path, opt.target_path_metric, opt.target_path_loss, opt.data_key_path, opt.NLL_metric_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 489  794 1043   31  462  812  703 1040   92  682   35  685  315 1086\n",
      "  427  133  155  802  305  760  300  377 1007  138  998  227  988  347\n",
      "  916  161  404  584   76  838  695  280  976  260  824  162 1186  188\n",
      "  776  424 1121  663  948   58  166  766  293  645  375  861  995 1059\n",
      "  837  286  655  862 1025  330  231  523 1039  376  734  580  401  905\n",
      "  790  566  246  601  178   75  258   66  191  512   15  408  728  447\n",
      "  303  102 1094  342 1147  779  535  932  636  139 1016  630  516  750\n",
      " 1058  611  154  925   88 1084  152   98  884 1112  570  696 1171  777\n",
      "  233  409 1032  288  594  372  283  881 1131  354  676  164  693   19\n",
      "  110  575  362  987 1051  398  390  677   47 1103  574  914   65  473\n",
      "  963  683  882  873  515 1110  270  615  770 1089  406   63  558  763\n",
      "  623  237  222  333 1093  961 1038  902 1149  923   44  225  588  405\n",
      "  163  698 1062  781  436 1166  910  642  463 1123  934 1009  958 1122\n",
      "  701  949  112  328 1071 1044  430  554  744   33  115  147    2 1172\n",
      "  585 1152  759  352   59  957   72  832 1052  467 1120  618  370  279\n",
      "  509  971  879  926  251  238  124  544  672 1021  617 1128  726  953\n",
      "  774  897 1185  357  890  608  908   67  319  285  661  666    8  704\n",
      "  885  737 1189 1143  758  218  829  418  804  978  244  153 1088 1195\n",
      "  326  221  956  738 1144 1065  445  559  146   26  135  104  793  413\n",
      " 1158   50   40  336  840  943  374  479  419  322  179  265  985  962\n",
      "  429  724  904  669  215 1137  502  379 1008  967  826   17   25 1066\n",
      "  718  557  309  968 1018  819  931 1192  224   89  598  508  825  487\n",
      "   30  358  465  211 1081  157  740  955  811 1142  236  631  640   70\n",
      "  275 1100  582  247  937 1096  670 1132  848  441  659  651  571  549\n",
      "  202  841  699   55  253  815   74 1006  399  511  170  970 1033  420\n",
      "  767  898  849  577  486  900  287  538  681  780  878  964  641 1175\n",
      "  786  321  325  625   90  798  747  363   83  125  595  692  634  820\n",
      " 1104  545  708  859  364   13  306  385 1047  214  741   77  715  755\n",
      "  863  450  806 1118  675  846  940   18  852  127  310  753 1092   11\n",
      "  524  868 1087  721  411 1069  752  482  425 1159  722  917  813  678\n",
      "   36  643  665  652  380  262  290  437  254 1145  435   49  860  228\n",
      "  239  340   62  787 1035  526  657  789   78  803  952  175  193  889\n",
      "  353  248 1005 1154  771  189  395  649  539 1136   34  209  783  501\n",
      "  307 1155 1106  867  273  337  180  136  289  331  632  402  431  853\n",
      "  277  345  906  525 1077  518  578 1174  886  979  517 1177   42  304\n",
      "  534  687  194 1179 1020  299  496 1170 1055 1027  120  730  106  455\n",
      "  113  965  871  576  131  660  974 1023 1061 1014  521  684 1057  707\n",
      "  973  831  294  220  477  858  181  990  472  568  493   53  282  522\n",
      "  428  658  381  596 1153  295 1010  751  984 1188 1139  481  324  278\n",
      "  167  928  184  975  503  567   93 1037  219 1116  650  950  857  945\n",
      "  114  735  593   54  830  891  446 1048  111  213  736  982 1133  208\n",
      "  624 1028  761  474  203   24    7  768  475  725  944 1141  620  117\n",
      "   57  556  101  877  604   80 1075  212  520  384  616  108 1117  541\n",
      "  359  176  850 1001  423  490   45  159  266  320  592   81  391  268\n",
      " 1036  267   97  739  765  249  530 1003  942   61 1167  537  714   71\n",
      " 1127  939  628  169 1134 1183  383  348 1101  350  667  690  394   68\n",
      "  921  276 1190  434  378  452  216  836 1168  387  644 1109  694 1156\n",
      "  416  177  828  671  458 1105    3  263  432  814  504  646  782  495\n",
      "  773  261  397  834  903  226  444  536  484  788  433  874  200 1151\n",
      "  527  257 1063   14  229 1042  476 1115  126  144  563   99  459  507\n",
      "  457 1162  281  895  471  316   32  271 1079  407  123  654  656  864\n",
      "  470  158  629  732  317  600  168  930  922  382  274    6    4 1124\n",
      "  417  210  122  723    1  497  791 1165   64  461  311  234  100 1091\n",
      " 1173  318  981  338  403  389  103  196  800  590  334  872  966  550\n",
      "  743  621  134  772  341  855  492  700  192  142  410  298 1072  301\n",
      "  668  464  599  551  128  160  356  635  839 1169  999  612  822  614\n",
      "  105  132  941   73  235  960   85  365 1114  907  371  869   16   29\n",
      "   46  986  572  344  129  269  980  440 1090   94  468  936  252 1013\n",
      "  627  513  491  195 1180  573  332  242   79  547  918   43  597  801\n",
      "   20  205  997  206  792  892   91  292  605  150  664]\n"
     ]
    }
   ],
   "source": [
    "shuffled_indices = []\n",
    "DATA_len = 0\n",
    "# 使用全部的data\n",
    "if not opt.arr_flag:\n",
    "    DATA_len = dataset.__len__()\n",
    "    shuffled_indices = np.random.permutation(DATA_len)\n",
    "\n",
    "# 使用指定的data\n",
    "if opt.arr_flag:\n",
    "    shuffled_indices = np.load(opt.arr_path)\n",
    "    DATA_len = len(shuffled_indices)\n",
    "    # np.random.shuffle(shuffled_indices)\n",
    "\n",
    "train_idx = shuffled_indices[:int(opt.train_pct*DATA_len)]\n",
    "# train_idx = shuffled_indices\n",
    "tmp = int((opt.train_pct+opt.vali_pct)*DATA_len)\n",
    "val_idx = shuffled_indices[int(opt.train_pct*DATA_len):tmp]\n",
    "\n",
    "test_idx = shuffled_indices[tmp:]\n",
    "print(train_idx)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [],
   "source": [
    "np.save('shuffled_indices',shuffled_indices)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Dataloader and collating\n",
    "1. 主要是对label数据进行collate\n",
    "    - 按照batch中的最大target data长度进行padding，padding with 0\n",
    "2. 返回的结果多一个batch dim,比如下面的`5`\n",
    "    - After collating:\n",
    "        - `torch.Size([5, 3, 300]),torch.Size([5, 87, 2])`\n",
    "        - `87`是最长的targets data长度"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [],
   "source": [
    "def my_collate_fn(data):\n",
    "# 这里的data是一个list， list的元素是元组: (self.data, self.label)\n",
    "# collate_fn的作用是把[(data, label),(data, label)...]转化成([data, data...],[label,label...])\n",
    "# 假设self.data的一个data的shape为(channels, length), 每一个channel的length相等,\n",
    "# data[索引到index(batch)][索引到data或者label][索引到channel]\n",
    "\n",
    "    data_list = []\n",
    "    target_metric_list = []\n",
    "    target_loss_list = []\n",
    "    setting_list = []\n",
    "    metric_list = []\n",
    "\n",
    "    data_len = len(data)      # 读进来的data batch的大小\n",
    "\n",
    "    batch = 0\n",
    "\n",
    "    while batch < data_len:\n",
    "        # print(\"shape:\",data[batch][0].shape) #shape: (3, 300)\n",
    "\n",
    "        data_tmp = np.array([data[batch][0][opt.GT_CHOSEN,:],data[batch][0][2,:]])\n",
    "        data_list.append(torch.tensor(data_tmp))\n",
    "\n",
    "        target_metric_list.append(torch.tensor(data[batch][1]))\n",
    "        target_loss_list.append(torch.tensor(data[batch][2]))\n",
    "        setting_list.append(torch.tensor(data[batch][3]))\n",
    "        metric_list.append(torch.tensor(data[batch][4]))\n",
    "        batch += 1\n",
    "\n",
    "    # Pad target data with zeros\n",
    "    target_metric_padded = pad_sequence(target_metric_list,batch_first=True)\n",
    "    target_loss_padded = pad_sequence(target_loss_list,batch_first=True)\n",
    "    target_metric_tensor = target_metric_padded.float()\n",
    "    target_loss_padded = target_loss_padded.float()\n",
    "\n",
    "    data_tensor = torch.stack(data_list).float()\n",
    "    setting_tensor = torch.stack(setting_list).float()\n",
    "    metric_tensor = torch.stack(metric_list).float()\n",
    "\n",
    "    return data_tensor, target_metric_tensor, target_loss_padded, setting_tensor, metric_tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [],
   "source": [
    "def my_collate_fn_2(data):\n",
    "# 这里的data是一个list， list的元素是元组: (self.data, self.label)\n",
    "# collate_fn的作用是把[(data, label),(data, label)...]转化成([data, data...],[label,label...])\n",
    "# 假设self.data的一个data的shape为(channels, length), 每一个channel的length相等,\n",
    "# data[索引到index(batch)][索引到data或者label][索引到channel]\n",
    "\n",
    "    data_list = []              # training data\n",
    "    target_metric_list = []     # target data for computing metric of NN, (TARGET=1)\n",
    "    target_loss_list = []       # target data for computing loss of NN, (TARGET=5)\n",
    "    setting_list = []\n",
    "    metric_list = []\n",
    "\n",
    "    data_len = len(data)        # 读进来的data batch的大小\n",
    "\n",
    "    batch = 0\n",
    "\n",
    "    while batch < data_len:\n",
    "        # print(\"shape:\",data[batch][0].shape) #shape: (3, 300)\n",
    "\n",
    "        # 对于所有用0填充的data来说，最好用一个数字补齐这些0\n",
    "        data[batch][0][opt.GT_CHOSEN,np.where(data[batch][0][opt.GT_CHOSEN]==0)]= min(min(data[batch][0][opt.GT_CHOSEN]),opt.SAFETY)\n",
    "        # 然后再拼接\n",
    "        data_tmp = np.array([data[batch][0][opt.GT_CHOSEN,:],data[batch][0][2,:]])\n",
    "        data_list.append(torch.tensor(data_tmp))\n",
    "\n",
    "        data_list.append(torch.tensor(data[batch][0]))\n",
    "        target_metric_list.append(torch.tensor(data[batch][1]))\n",
    "        target_loss_list.append(torch.tensor(data[batch][2]))\n",
    "        setting_list.append(torch.tensor(data[batch][3]))\n",
    "        metric_list.append(torch.tensor(data[batch][4]))\n",
    "        batch += 1\n",
    "\n",
    "    # Pad target data with zeros\n",
    "    target_metric_padded = pad_sequence(target_metric_list,batch_first=True)\n",
    "    target_loss_padded = pad_sequence(target_loss_list,batch_first=True)\n",
    "    target_metric_tensor = target_metric_padded.float()\n",
    "    target_loss_padded = target_loss_padded.float()\n",
    "\n",
    "    data_tensor = torch.stack(data_list).float()\n",
    "    setting_tensor = torch.stack(setting_list).float()\n",
    "    metric_tensor = torch.stack(metric_list).float()\n",
    "\n",
    "    return data_tensor, target_metric_tensor, target_loss_padded, setting_tensor, metric_tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = dataset,batch_size = opt.batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(train_idx), collate_fn = my_collate_fn)\n",
    "\n",
    "val_loader = DataLoader(dataset = dataset,batch_size = opt.batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(val_idx),collate_fn = my_collate_fn)\n",
    "\n",
    "# 注意test_loader的batch size\n",
    "test_loader = DataLoader(dataset = dataset,batch_size = opt.batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(test_idx),collate_fn = my_collate_fn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. The Net and Init\n",
    "1. BatchNorm1d: The mean and std are calculated per-dimension over the mini-batches\n",
    "## 3.1 2层MLP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "# 参数量92,000+\n",
    "# 2层\n",
    "class MLP_1(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=2,affine=True)\n",
    "        self.BN2 = nn.BatchNorm1d(num_features=150,affine=True)\n",
    "        self.BN3 = nn.BatchNorm1d(num_features=12,affine=True)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "        self.linear1 = nn.Linear(600, 150)\n",
    "        self.linear2 = nn.Linear(150, 12)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(12, n_gaussians),  # 30个params要learn\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(12, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(12, n_gaussians)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.BN1(x)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.BN2(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = self.BN3(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        pi = self.ac_func(self.z_pi(x))\n",
    "        # pi = self.z_pi(torch.clamp(x,1e-3))\n",
    "        mu = self.ac_func(self.z_mu(x))\n",
    "        sigma = torch.exp(self.z_sigma(x))\n",
    "        sigma = torch.clamp(sigma,1e-4)\n",
    "\n",
    "        # return x\n",
    "        return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Conv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "# 967\n",
    "class Conv_block_1(nn.Module):\n",
    "    def __init__(self, ch_out=1,kernel_size=12, stride=3) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.kernel_size = (2,kernel_size)\n",
    "        self.stride = (2,stride)\n",
    "        self.ln_in = int((300-self.kernel_size[1])/self.stride[1]+1)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=ch_out, kernel_size=self.kernel_size, stride=self.stride, padding=0,bias=True)\n",
    "        self.BN_aff2 = nn.BatchNorm1d(num_features=self.ln_in,affine=True)\n",
    "        self.BN_aff1 = nn.BatchNorm1d(num_features=1,affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Conv=>BN=>AC\n",
    "        x = self.conv(x)\n",
    "        x = torch.squeeze(x,dim=1)\n",
    "        x = self.ac_func(self.BN_aff1(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "class Conv_1_1(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians, ch_out=1, kernel_size=12, stride=3) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.kernel_size = (2,kernel_size)\n",
    "        self.stride = (2,stride)\n",
    "        self.ln_in = int((300-self.kernel_size[1])/self.stride[1]+1)\n",
    "\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=2,affine=True)\n",
    "\n",
    "        self.layer_pi = Conv_block_1(ch_out=1,kernel_size=kernel_size,stride=stride)\n",
    "        self.layer_mu = Conv_block_1(ch_out=1,kernel_size=kernel_size,stride=stride)\n",
    "        self.layer_sigma = Conv_block_1(ch_out=1,kernel_size=kernel_size,stride=stride)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "        # self.ac_func = nn.ReLU()\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(self.ln_in, n_gaussians),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(self.ln_in, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(self.ln_in, n_gaussians)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.BN1(x)\n",
    "        x = torch.unsqueeze(x,dim=1)                     # torch.Size([B, 1, 3, 300])\n",
    "\n",
    "        x_pi = self.layer_pi(x)\n",
    "        x_mu = self.layer_pi(x)\n",
    "        x_sigma = self.layer_pi(x)\n",
    "\n",
    "        # 有必要吗ac：Definitely\n",
    "        # 哪一层起到了作用？pi和mu已知的，负值会造成影响的\n",
    "\n",
    "        pi = self.ac_func(self.z_pi(x_pi))\n",
    "        mu = self.ac_func(self.z_mu(x_mu))\n",
    "        # 不要给sigma加ac\n",
    "        # sigma = self.ac_func(self.z_sigma(x_sigma))\n",
    "        sigma = self.z_sigma(x_sigma)\n",
    "\n",
    "        sigma = torch.exp(sigma)\n",
    "        sigma = torch.clamp(sigma,1e-4)\n",
    "\n",
    "        # return x1_3\n",
    "        return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. The Loss\n",
    "\n",
    "## 4.0 The metric\n",
    "1. 以NLL作为metric\n",
    "2. 理论模型的metric可以提前算好，读进来，这里读setting是为了？IDK"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "1.0599999999999996"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6.54-5.48"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "# from loss import cal_metric"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1 NLL loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "# from loss import loss_fn_v2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.4 W-distance\n",
    "1. 拿到target data的两列，按照第一列去predict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "# from loss import loss_fn_WD"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5 Plot\n",
    "1. draw:\n",
    "    - loss: training data的loss和test data的loss趋势\n",
    "    - MLP的网络结构（.png）\n",
    "    - target distrb.和pred. distrb.\n",
    "    - 所有target data的分布图\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(\"logs-MLP\")\n",
    "env_str = \"003\"\n",
    "viz = Visdom(env=env_str)\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.2 plot mdn\n",
    "### 5.2.1 mdn pdf的图像[with input data]\n",
    "1. 求解[1,6,11...]上的cdf：\n",
    "    - 求解[1,6,11..]的cdf，再求解[6,11,..]，两者相减"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "def draw_mdn_2(pi, mu, sigma, target, input, total_train_step, N_gaussians):\n",
    "    \"\"\"\n",
    "    画两条曲线，pred-1表示归一化前，pred-2表示归一化后\n",
    "\n",
    "    :param pi:\n",
    "    :param mu:\n",
    "    :param sigma:\n",
    "    :param target:\n",
    "    :param input:\n",
    "    :param total_train_step:\n",
    "    :param N_gaussians:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 只画一个batch中的第一个\n",
    "\n",
    "    # The target distrb.\n",
    "    target = torch.unique(target,dim=0)    # drop the duplicate\n",
    "    n_target = target[:,0]\n",
    "    non_zero_idx = torch.nonzero(n_target)\n",
    "    n = n_target[non_zero_idx]\n",
    "\n",
    "    p_target = target[:,1]\n",
    "    p = p_target[non_zero_idx]\n",
    "    max_n = int(max(n).item())\n",
    "\n",
    "    # The predicted distrb.\n",
    "    m = torch.distributions.Normal(mu,sigma)          # Gaussian\n",
    "    # m = torch.distributions.Laplace(mu,sigma)           # Laplace\n",
    "\n",
    "    x_0 = torch.arange(1,(max_n+TARGET),TARGET).to(device=device)\n",
    "\n",
    "    x = torch.repeat_interleave(x_0.unsqueeze(dim=1), repeats=N_gaussians, dim=1)   # expand dim\n",
    "    # 求解每个长度为TARGET的区间上的cdf\n",
    "    y = (m.cdf(x+TARGET) - m.cdf(x)).to(device=device)\n",
    "\n",
    "    # 方法一：\n",
    "    y_pred_1 = torch.sum(pi*y,dim=1)\n",
    "    # 方法二：做一下(0,1)上的归一化\n",
    "    y_pred_2 = y_pred_1/y_pred_1.sum()\n",
    "\n",
    "    # The input distrb.\n",
    "    # input_data = input[0:2,:]\n",
    "    input_data = input[0,:]\n",
    "    x_input = torch.arange(1,(300+1),SCALE).to(device=device)\n",
    "    y_input_0 = input_data.to(device=device)\n",
    "    # y_input_0 = (input_data[0,:]).to(device=device)\n",
    "    # y_input_1 = (input_data[1,:]).to(device=device)\n",
    "\n",
    "    # Init\n",
    "    win_str = \"total_train_step = \"+str(total_train_step)\n",
    "    title_str = \"Distrb. in \"+win_str\n",
    "    viz.line(X = [0.],Y = [0.], env=\"001\", win=win_str, opts= dict(title=title_str))\n",
    "\n",
    "    # Visdom本身不能把hist和line画在一个window中\n",
    "    # 如果想画一起只能是两条line\n",
    "    # Plot y_target\n",
    "    # viz.histogram(X = n, env=\"001\", win=win_str,\n",
    "    #         opts= dict(title=title_str))\n",
    "    viz.line(X = n,Y= p, env=\"001\", win=win_str, update=\"append\", name='target',\n",
    "            opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "    # Plot y_pred\n",
    "    viz.line(X = x_0,Y= y_pred_1, env=\"001\", win=win_str, update=\"append\", name='pred-1',\n",
    "            opts= dict(title=title_str))\n",
    "\n",
    "    # Plot y_pred_2\n",
    "    viz.line(X = x_0,Y= y_pred_2, env=\"001\", win=win_str, update=\"append\", name='pred-2',\n",
    "            opts= dict(title=title_str))\n",
    "\n",
    "    # Plot y_input\n",
    "    # 如果input data长度短，全部画完\n",
    "    if(len(x_input) <= max_n):\n",
    "        viz.line(X = x_input,Y= y_input_0, env=\"001\", win=win_str, update=\"append\", name='GT-1', opts= dict(title=title_str))\n",
    "        # viz.line(X = x_input,Y= y_input_1, env=\"001\", win=win_str, update=\"append\", name='GT-2', opts= dict(title=title_str))\n",
    "    # 如果input data长度长，则截断\n",
    "    else:\n",
    "        x_input_0 = x_input[0:max_n]\n",
    "        y_input_2 = y_input_0[0:max_n]/y_input_0[0:max_n].sum()\n",
    "        # y_input_3 = y_input_1[0:max_n]/y_input_1[0:max_n].sum()\n",
    "        viz.line(X = x_input_0,Y= y_input_2, env=\"001\", win=win_str, update=\"append\", name='GT-1', opts= dict(title=title_str))\n",
    "        # viz.line(X = x_input_0,Y= y_input_3, env=\"001\", win=win_str, update=\"append\", name='GT-2', opts= dict(title=title_str))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.4 plot the loss\n",
    "1. 直接根据参数决定draw什么loss，反正loss都画在一个window里面"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "win_train_loss_str = \"The Loss of BATCH in the Training Data\"\n",
    "win_vali_loss_str = \"The Loss in the Vali Data\"\n",
    "win_train_epoch_loss_str = \"The Loss of EPOCH in the Training Data\"\n",
    "\n",
    "def draw_loss(X_step, loss, win_str):\n",
    "    viz.line(X = [X_step], Y = [loss],win=win_str, env = env_str,update=\"append\",\n",
    "        opts= dict(title=win_str))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.5 plot the metric"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "win_vali_metric_str = \"The NLL of ALL vali data\"\n",
    "\n",
    "def draw_metric(X_step, total_vali_metric, GT_metric):\n",
    "\n",
    "    viz.line(X = [X_step], Y = [[total_vali_metric,GT_metric[0,0],GT_metric[0,1]]],win=win_vali_metric_str, update=\"append\", env = env_str,opts= dict(title=win_vali_metric_str, legend=['pred','GT-1','GT-2'], showlegend=True,xlabel=\"epoch\", ylabel=\"NLL\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6 Training\n",
    "## 6.1 Init"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "       BatchNorm1d-1               [-1, 2, 300]               4\n",
      "            Conv2d-2             [-1, 1, 1, 97]              25\n",
      "       BatchNorm1d-3                [-1, 1, 97]               2\n",
      "          Softplus-4                [-1, 1, 97]               0\n",
      "      Conv_block_1-5                [-1, 1, 97]               0\n",
      "            Conv2d-6             [-1, 1, 1, 97]              25\n",
      "       BatchNorm1d-7                [-1, 1, 97]               2\n",
      "          Softplus-8                [-1, 1, 97]               0\n",
      "      Conv_block_1-9                [-1, 1, 97]               0\n",
      "           Conv2d-10             [-1, 1, 1, 97]              25\n",
      "      BatchNorm1d-11                [-1, 1, 97]               2\n",
      "         Softplus-12                [-1, 1, 97]               0\n",
      "     Conv_block_1-13                [-1, 1, 97]               0\n",
      "           Linear-14                 [-1, 1, 3]             294\n",
      "          Softmax-15                 [-1, 1, 3]               0\n",
      "         Softplus-16                 [-1, 1, 3]               0\n",
      "           Linear-17                 [-1, 1, 3]             294\n",
      "         Softplus-18                 [-1, 1, 3]               0\n",
      "           Linear-19                 [-1, 1, 3]             294\n",
      "================================================================\n",
      "Total params: 967\n",
      "Trainable params: 967\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.02\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "mlp = Conv_1_1(N_gaussians)\n",
    "# mlp = MLP_1(N_gaussians)\n",
    "\n",
    "# Init the params\n",
    "# # mlp = model_param_init(mlp)\n",
    "\n",
    "# Save the init params\n",
    "# torch.save(mlp.state_dict(), 'mlp_init.pth')\n",
    "\n",
    "# Load the saved model\n",
    "# model_path_LSE = \"mlp_train=300_LSE.pth\"\n",
    "# model_data = torch.load(model_path_LSE)\n",
    "# mlp.load_state_dict(model_data)\n",
    "\n",
    "mlp = mlp.to(device=device)\n",
    "summary(mlp, (2,300))\n",
    "\n",
    "############ learning rate strategy ############\n",
    "# 1.\n",
    "# Set different lr for params\n",
    "# id: id() 函数返回对象的唯一标识符，标识符是一个整数。返回对象的内存地址。\n",
    "mu_params = list(map(id, mlp.z_mu.parameters()))\n",
    "sigma_params = list(map(id, mlp.z_sigma.parameters()))\n",
    "pi_params = list(map(id, mlp.z_pi.parameters()))\n",
    "\n",
    "params_id = mu_params + sigma_params + pi_params\n",
    "lr = 5e-2\n",
    "base_params = filter(lambda p: id(p) not in params_id, mlp.parameters())\n",
    "params = [{'params': base_params},         # 如果对某个参数不指定学习率，就使用最外层的默认学习率\n",
    "          {'params': mlp.z_pi.parameters(), 'lr': opt.learning_rate},\n",
    "        {'params': mlp.z_mu.parameters(), 'lr': opt.learning_rate},\n",
    "          {'params': mlp.z_sigma.parameters(), 'lr': opt.lr_for_sigma}]\n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr=opt.learning_rate,weight_decay=opt.weight_decay)\n",
    "\n",
    "# 2.\n",
    "# Or set lr decay\n",
    "# StepLR为步进，每step_size个epoch，lr*gamma\n",
    "# scheduler  = torch.optim.lr_scheduler.StepLR(optimizer,step_size=10,gamma=0.8)\n",
    "# 下面这个比较好用\n",
    "scheduler  = torch.optim.lr_scheduler.StepLR(optimizer,step_size=opt.StepLR_step_size,gamma=opt.StepLR_gamma)\n",
    "\n",
    "# # Set the hooks\n",
    "# #mlp.conv1.register_forward_hook(hook_forward_fn)\n",
    "# mlp.conv1.register_full_backward_hook(hook_backward_fn)\n",
    "#\n",
    "# #mlp.linear1.register_forward_hook(hook_forward_fn)\n",
    "# #mlp.linear1.register_full_backward_hook(hook_backward_fn)\n",
    "#\n",
    "# #mlp.z_pi.register_forward_hook(hook_forward_fn)\n",
    "# mlp.z_pi.register_full_backward_hook(hook_backward_fn_pi)\n",
    "#\n",
    "# #mlp.z_mu.register_forward_hook(hook_forward_fn)\n",
    "# mlp.z_mu.register_full_backward_hook(hook_backward_fn_mu)\n",
    "#\n",
    "# mlp.z_sigma.register_forward_hook(hook_forward_fn)\n",
    "# mlp.z_sigma.register_full_[2, 3, 300]backward_hook(hook_backward_fn_sigma)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# writer = SummaryWriter(log_dir=\"logs-MLP/\"+opt.logs_str,flush_secs=60)\n",
    "#\n",
    "# Init the vis win\n",
    "viz.line(X = [0.],Y = [0.], env=env_str, win=win_train_loss_str, opts= dict(title=win_train_loss_str))\n",
    "viz.line(X = [0.],Y = [0.], env=env_str, win=win_train_epoch_loss_str, opts= dict(title=win_train_epoch_loss_str))\n",
    "viz.line(X = [0.],Y = [[0.,0.,0.]], env=env_str, win=win_vali_metric_str, opts= dict(title=win_vali_metric_str,legend=['pred','GT-1','GT-2'], showlegend=True,xlabel=\"epoch\", ylabel=\"NLL\"))\n",
    "\n",
    "mlp.eval()\n",
    "with torch.no_grad():\n",
    "    total_vali_metric, GT_metric = validate(mlp,val_loader,opt.N_gaussians, opt.MIN_LOSS,device)\n",
    "    draw_metric(0, total_vali_metric.cpu(), GT_metric)\n",
    "# plot_net(writer,mlp,torch.randn((2,3,300),device=device))\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.3 START HERE\n",
    "1. 使用Laplace分布改：\n",
    "    - loss function和draw"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Now this is EPOCH 0 ==========\n",
      "========== IN EPOCH 0 the total loss is 97180.13366699219 ==========\n",
      "========== IN EPOCH 0 the vali NLL loss is 37.742225646972656 ==========\n",
      "========== IN EPOCH 0 the GT metric is [[ 6.7786484 10.165821 ]] ==========\n",
      "========== Now this is EPOCH 1 ==========\n",
      "========== IN EPOCH 1 the total loss is 63588.51971435547 ==========\n",
      "========== IN EPOCH 1 the vali NLL loss is 17.370405197143555 ==========\n",
      "========== IN EPOCH 1 the GT metric is [[ 6.778648 10.16582 ]] ==========\n",
      "========== Now this is EPOCH 2 ==========\n",
      "========== IN EPOCH 2 the total loss is 17415.564849853516 ==========\n",
      "========== IN EPOCH 2 the vali NLL loss is 6.6178741455078125 ==========\n",
      "========== IN EPOCH 2 the GT metric is [[ 6.7786484 10.16582  ]] ==========\n",
      "========== Now this is EPOCH 3 ==========\n",
      "========== IN EPOCH 3 the total loss is 9341.57601928711 ==========\n",
      "========== IN EPOCH 3 the vali NLL loss is 6.129843235015869 ==========\n",
      "========== IN EPOCH 3 the GT metric is [[ 6.7786484 10.16582  ]] ==========\n",
      "========== Now this is EPOCH 4 ==========\n",
      "========== IN EPOCH 4 the total loss is 8827.331954956055 ==========\n",
      "========== IN EPOCH 4 the vali NLL loss is 5.963498592376709 ==========\n",
      "========== IN EPOCH 4 the GT metric is [[ 6.7786484 10.16582  ]] ==========\n",
      "========== Now this is EPOCH 5 ==========\n",
      "========== IN EPOCH 5 the total loss is 8738.789749145508 ==========\n",
      "========== IN EPOCH 5 the vali NLL loss is 5.863375186920166 ==========\n",
      "========== IN EPOCH 5 the GT metric is [[ 6.778648 10.16582 ]] ==========\n",
      "========== Now this is EPOCH 6 ==========\n",
      "========== IN EPOCH 6 the total loss is 8520.10481262207 ==========\n",
      "========== IN EPOCH 6 the vali NLL loss is 5.819209098815918 ==========\n",
      "========== IN EPOCH 6 the GT metric is [[ 6.7786484 10.16582  ]] ==========\n",
      "========== Now this is EPOCH 7 ==========\n",
      "========== IN EPOCH 7 the total loss is 8250.596206665039 ==========\n",
      "========== IN EPOCH 7 the vali NLL loss is 5.745464324951172 ==========\n",
      "========== IN EPOCH 7 the GT metric is [[ 6.778648 10.16582 ]] ==========\n",
      "========== Now this is EPOCH 8 ==========\n",
      "========== IN EPOCH 8 the total loss is 8156.348403930664 ==========\n",
      "========== IN EPOCH 8 the vali NLL loss is 5.725822925567627 ==========\n",
      "========== IN EPOCH 8 the GT metric is [[ 6.7786484 10.16582  ]] ==========\n",
      "========== Now this is EPOCH 9 ==========\n",
      "========== IN EPOCH 9 the total loss is 8048.414642333984 ==========\n",
      "========== IN EPOCH 9 the vali NLL loss is 5.683831214904785 ==========\n",
      "========== IN EPOCH 9 the GT metric is [[ 6.778648 10.16582 ]] ==========\n",
      "========== Now this is EPOCH 10 ==========\n",
      "========== IN EPOCH 10 the total loss is 8001.746383666992 ==========\n",
      "========== IN EPOCH 10 the vali NLL loss is 5.671930313110352 ==========\n",
      "========== IN EPOCH 10 the GT metric is [[ 6.7786484 10.16582  ]] ==========\n",
      "========== Now this is EPOCH 11 ==========\n",
      "========== IN EPOCH 11 the total loss is 7889.808883666992 ==========\n",
      "========== IN EPOCH 11 the vali NLL loss is 5.650012016296387 ==========\n",
      "========== IN EPOCH 11 the GT metric is [[ 6.778648 10.16582 ]] ==========\n",
      "========== Now this is EPOCH 12 ==========\n",
      "========== IN EPOCH 12 the total loss is 7820.996490478516 ==========\n",
      "========== IN EPOCH 12 the vali NLL loss is 5.617096424102783 ==========\n",
      "========== IN EPOCH 12 the GT metric is [[ 6.7786484 10.16582  ]] ==========\n",
      "========== Now this is EPOCH 13 ==========\n",
      "========== IN EPOCH 13 the total loss is 7939.8848876953125 ==========\n",
      "========== IN EPOCH 13 the vali NLL loss is 5.602811336517334 ==========\n",
      "========== IN EPOCH 13 the GT metric is [[ 6.778649 10.16582 ]] ==========\n",
      "========== Now this is EPOCH 14 ==========\n",
      "========== IN EPOCH 14 the total loss is 7699.580215454102 ==========\n",
      "========== IN EPOCH 14 the vali NLL loss is 5.580092906951904 ==========\n",
      "========== IN EPOCH 14 the GT metric is [[ 6.7786484 10.16582  ]] ==========\n",
      "========== Now this is EPOCH 15 ==========\n",
      "========== IN EPOCH 15 the total loss is 7750.189544677734 ==========\n",
      "========== IN EPOCH 15 the vali NLL loss is 5.578495979309082 ==========\n",
      "========== IN EPOCH 15 the GT metric is [[ 6.7786484 10.16582  ]] ==========\n",
      "========== Now this is EPOCH 16 ==========\n",
      "========== IN EPOCH 16 the total loss is 7685.49821472168 ==========\n",
      "========== IN EPOCH 16 the vali NLL loss is 5.560449600219727 ==========\n",
      "========== IN EPOCH 16 the GT metric is [[ 6.778648 10.16582 ]] ==========\n",
      "========== Now this is EPOCH 17 ==========\n",
      "========== IN EPOCH 17 the total loss is 7664.017181396484 ==========\n",
      "========== IN EPOCH 17 the vali NLL loss is 5.559277534484863 ==========\n",
      "========== IN EPOCH 17 the GT metric is [[ 6.7786484 10.16582  ]] ==========\n",
      "========== Now this is EPOCH 18 ==========\n",
      "========== IN EPOCH 18 the total loss is 7559.778366088867 ==========\n",
      "========== IN EPOCH 18 the vali NLL loss is 5.55085563659668 ==========\n",
      "========== IN EPOCH 18 the GT metric is [[ 6.778648 10.16582 ]] ==========\n",
      "========== Now this is EPOCH 19 ==========\n",
      "========== IN EPOCH 19 the total loss is 7584.944320678711 ==========\n",
      "========== IN EPOCH 19 the vali NLL loss is 5.555320739746094 ==========\n",
      "========== IN EPOCH 19 the GT metric is [[ 6.7786484 10.16582  ]] ==========\n",
      "========== Now this is EPOCH 20 ==========\n",
      "========== IN EPOCH 20 the total loss is 7642.06428527832 ==========\n",
      "========== IN EPOCH 20 the vali NLL loss is 5.543972969055176 ==========\n",
      "========== IN EPOCH 20 the GT metric is [[ 6.7786484 10.16582  ]] ==========\n",
      "========== Now this is EPOCH 21 ==========\n",
      "========== IN EPOCH 21 the total loss is 7572.316940307617 ==========\n",
      "========== IN EPOCH 21 the vali NLL loss is 5.549380302429199 ==========\n",
      "========== IN EPOCH 21 the GT metric is [[ 6.778649 10.165821]] ==========\n",
      "========== Now this is EPOCH 22 ==========\n",
      "========== IN EPOCH 22 the total loss is 7543.103958129883 ==========\n",
      "========== IN EPOCH 22 the vali NLL loss is 5.531127452850342 ==========\n",
      "========== IN EPOCH 22 the GT metric is [[ 6.7786474 10.16582  ]] ==========\n",
      "========== Now this is EPOCH 23 ==========\n",
      "========== IN EPOCH 23 the total loss is 7519.091857910156 ==========\n",
      "========== IN EPOCH 23 the vali NLL loss is 5.536889553070068 ==========\n",
      "========== IN EPOCH 23 the GT metric is [[ 6.7786484 10.165821 ]] ==========\n",
      "========== Now this is EPOCH 24 ==========\n",
      "========== IN EPOCH 24 the total loss is 7478.4569091796875 ==========\n",
      "========== IN EPOCH 24 the vali NLL loss is 5.531306266784668 ==========\n",
      "========== IN EPOCH 24 the GT metric is [[ 6.778648 10.165819]] ==========\n",
      "========== Now this is EPOCH 25 ==========\n",
      "========== IN EPOCH 25 the total loss is 7541.904815673828 ==========\n",
      "========== IN EPOCH 25 the vali NLL loss is 5.534793376922607 ==========\n",
      "========== IN EPOCH 25 the GT metric is [[ 6.7786484 10.16582  ]] ==========\n",
      "========== Now this is EPOCH 26 ==========\n",
      "========== IN EPOCH 26 the total loss is 7492.390884399414 ==========\n",
      "========== IN EPOCH 26 the vali NLL loss is 5.540966510772705 ==========\n",
      "========== IN EPOCH 26 the GT metric is [[ 6.7786484 10.16582  ]] ==========\n",
      "========== Now this is EPOCH 27 ==========\n",
      "========== IN EPOCH 27 the total loss is 7446.950134277344 ==========\n",
      "========== IN EPOCH 27 the vali NLL loss is 5.5345563888549805 ==========\n",
      "========== IN EPOCH 27 the GT metric is [[ 6.7786484 10.16582  ]] ==========\n",
      "========== Now this is EPOCH 28 ==========\n",
      "========== IN EPOCH 28 the total loss is 7541.320266723633 ==========\n",
      "========== IN EPOCH 28 the vali NLL loss is 5.5234246253967285 ==========\n",
      "========== IN EPOCH 28 the GT metric is [[ 6.7786474 10.16582  ]] ==========\n",
      "========== Now this is EPOCH 29 ==========\n",
      "========== IN EPOCH 29 the total loss is 7476.708847045898 ==========\n",
      "========== IN EPOCH 29 the vali NLL loss is 5.5250020027160645 ==========\n",
      "========== IN EPOCH 29 the GT metric is [[ 6.7786474 10.165821 ]] ==========\n",
      "========== Now this is EPOCH 30 ==========\n",
      "========== IN EPOCH 30 the total loss is 7543.095748901367 ==========\n",
      "========== IN EPOCH 30 the vali NLL loss is 5.521219253540039 ==========\n",
      "========== IN EPOCH 30 the GT metric is [[ 6.7786484 10.165821 ]] ==========\n",
      "========== Now this is EPOCH 31 ==========\n",
      "========== IN EPOCH 31 the total loss is 7518.2137451171875 ==========\n",
      "========== IN EPOCH 31 the vali NLL loss is 5.520851135253906 ==========\n",
      "========== IN EPOCH 31 the GT metric is [[ 6.778648 10.16582 ]] ==========\n",
      "========== Now this is EPOCH 32 ==========\n",
      "========== IN EPOCH 32 the total loss is 7440.291534423828 ==========\n",
      "========== IN EPOCH 32 the vali NLL loss is 5.512021064758301 ==========\n",
      "========== IN EPOCH 32 the GT metric is [[ 6.7786484 10.165819 ]] ==========\n",
      "========== Now this is EPOCH 33 ==========\n",
      "========== IN EPOCH 33 the total loss is 7419.892318725586 ==========\n",
      "========== IN EPOCH 33 the vali NLL loss is 5.510164737701416 ==========\n",
      "========== IN EPOCH 33 the GT metric is [[ 6.778648 10.165821]] ==========\n",
      "========== Now this is EPOCH 34 ==========\n",
      "========== IN EPOCH 34 the total loss is 7430.228759765625 ==========\n",
      "========== IN EPOCH 34 the vali NLL loss is 5.506589412689209 ==========\n",
      "========== IN EPOCH 34 the GT metric is [[ 6.7786484 10.16582  ]] ==========\n",
      "========== Now this is EPOCH 35 ==========\n",
      "========== IN EPOCH 35 the total loss is 7446.784515380859 ==========\n",
      "========== IN EPOCH 35 the vali NLL loss is 5.523390293121338 ==========\n",
      "========== IN EPOCH 35 the GT metric is [[ 6.778648 10.16582 ]] ==========\n",
      "========== Now this is EPOCH 36 ==========\n",
      "========== IN EPOCH 36 the total loss is 7396.303955078125 ==========\n",
      "========== IN EPOCH 36 the vali NLL loss is 5.5165228843688965 ==========\n",
      "========== IN EPOCH 36 the GT metric is [[ 6.7786484 10.165821 ]] ==========\n",
      "========== Now this is EPOCH 37 ==========\n",
      "========== IN EPOCH 37 the total loss is 7389.001998901367 ==========\n",
      "========== IN EPOCH 37 the vali NLL loss is 5.508799076080322 ==========\n",
      "========== IN EPOCH 37 the GT metric is [[ 6.7786484 10.165821 ]] ==========\n",
      "========== Now this is EPOCH 38 ==========\n",
      "========== IN EPOCH 38 the total loss is 7402.157745361328 ==========\n",
      "========== IN EPOCH 38 the vali NLL loss is 5.500985622406006 ==========\n",
      "========== IN EPOCH 38 the GT metric is [[ 6.7786484 10.16582  ]] ==========\n",
      "========== Now this is EPOCH 39 ==========\n",
      "========== IN EPOCH 39 the total loss is 7473.029281616211 ==========\n",
      "========== IN EPOCH 39 the vali NLL loss is 5.507489204406738 ==========\n",
      "========== IN EPOCH 39 the GT metric is [[ 6.7786484 10.16582  ]] ==========\n",
      "========== Now this is EPOCH 40 ==========\n",
      "========== IN EPOCH 40 the total loss is 7366.051742553711 ==========\n",
      "========== IN EPOCH 40 the vali NLL loss is 5.508383274078369 ==========\n",
      "========== IN EPOCH 40 the GT metric is [[ 6.778648 10.16582 ]] ==========\n",
      "========== Now this is EPOCH 41 ==========\n",
      "========== IN EPOCH 41 the total loss is 7424.134094238281 ==========\n",
      "========== IN EPOCH 41 the vali NLL loss is 5.50137996673584 ==========\n",
      "========== IN EPOCH 41 the GT metric is [[ 6.7786484 10.16582  ]] ==========\n",
      "========== Now this is EPOCH 42 ==========\n",
      "========== IN EPOCH 42 the total loss is 7441.780822753906 ==========\n",
      "========== IN EPOCH 42 the vali NLL loss is 5.494414329528809 ==========\n",
      "========== IN EPOCH 42 the GT metric is [[ 6.778648 10.16582 ]] ==========\n",
      "========== Now this is EPOCH 43 ==========\n",
      "========== IN EPOCH 43 the total loss is 7440.7412109375 ==========\n",
      "========== IN EPOCH 43 the vali NLL loss is 5.49135684967041 ==========\n",
      "========== IN EPOCH 43 the GT metric is [[ 6.7786484 10.165821 ]] ==========\n",
      "========== Now this is EPOCH 44 ==========\n",
      "========== IN EPOCH 44 the total loss is 7391.987922668457 ==========\n",
      "========== IN EPOCH 44 the vali NLL loss is 5.500060081481934 ==========\n",
      "========== IN EPOCH 44 the GT metric is [[ 6.7786484 10.16582  ]] ==========\n",
      "========== Now this is EPOCH 45 ==========\n",
      "========== IN EPOCH 45 the total loss is 7405.340309143066 ==========\n",
      "========== IN EPOCH 45 the vali NLL loss is 5.499406337738037 ==========\n",
      "========== IN EPOCH 45 the GT metric is [[ 6.7786484 10.16582  ]] ==========\n",
      "========== Now this is EPOCH 46 ==========\n",
      "========== IN EPOCH 46 the total loss is 7376.409591674805 ==========\n",
      "========== IN EPOCH 46 the vali NLL loss is 5.489940643310547 ==========\n",
      "========== IN EPOCH 46 the GT metric is [[ 6.778648 10.16582 ]] ==========\n",
      "========== Now this is EPOCH 47 ==========\n",
      "========== IN EPOCH 47 the total loss is 7430.190399169922 ==========\n",
      "========== IN EPOCH 47 the vali NLL loss is 5.498593807220459 ==========\n",
      "========== IN EPOCH 47 the GT metric is [[ 6.7786484 10.16582  ]] ==========\n",
      "========== Now this is EPOCH 48 ==========\n",
      "========== IN EPOCH 48 the total loss is 7366.180572509766 ==========\n",
      "========== IN EPOCH 48 the vali NLL loss is 5.483247756958008 ==========\n",
      "========== IN EPOCH 48 the GT metric is [[ 6.7786484 10.16582  ]] ==========\n",
      "========== Now this is EPOCH 49 ==========\n",
      "========== IN EPOCH 49 the total loss is 7440.184341430664 ==========\n",
      "========== IN EPOCH 49 the vali NLL loss is 5.48690128326416 ==========\n",
      "========== IN EPOCH 49 the GT metric is [[ 6.778648 10.16582 ]] ==========\n",
      "========== Now this is EPOCH 50 ==========\n",
      "========== IN EPOCH 50 the total loss is 7340.649963378906 ==========\n",
      "========== IN EPOCH 50 the vali NLL loss is 5.489601135253906 ==========\n",
      "========== IN EPOCH 50 the GT metric is [[ 6.7786484 10.16582  ]] ==========\n",
      "========== Now this is EPOCH 51 ==========\n",
      "========== IN EPOCH 51 the total loss is 7401.136810302734 ==========\n",
      "========== IN EPOCH 51 the vali NLL loss is 5.495340824127197 ==========\n",
      "========== IN EPOCH 51 the GT metric is [[ 6.7786484 10.16582  ]] ==========\n",
      "========== Now this is EPOCH 52 ==========\n",
      "========== IN EPOCH 52 the total loss is 7346.962356567383 ==========\n",
      "========== IN EPOCH 52 the vali NLL loss is 5.4804277420043945 ==========\n",
      "========== IN EPOCH 52 the GT metric is [[ 6.7786484 10.16582  ]] ==========\n",
      "========== Now this is EPOCH 53 ==========\n",
      "========== IN EPOCH 53 the total loss is 7340.190002441406 ==========\n",
      "========== IN EPOCH 53 the vali NLL loss is 5.4702887535095215 ==========\n",
      "========== IN EPOCH 53 the GT metric is [[ 6.778649 10.16582 ]] ==========\n",
      "========== Now this is EPOCH 54 ==========\n",
      "========== IN EPOCH 54 the total loss is 7280.648147583008 ==========\n",
      "========== IN EPOCH 54 the vali NLL loss is 5.471719264984131 ==========\n",
      "========== IN EPOCH 54 the GT metric is [[ 6.7786484 10.16582  ]] ==========\n",
      "========== Now this is EPOCH 55 ==========\n",
      "========== IN EPOCH 55 the total loss is 7321.242889404297 ==========\n",
      "========== IN EPOCH 55 the vali NLL loss is 5.465691566467285 ==========\n",
      "========== IN EPOCH 55 the GT metric is [[ 6.778648 10.16582 ]] ==========\n",
      "========== Now this is EPOCH 56 ==========\n",
      "========== IN EPOCH 56 the total loss is 7292.363327026367 ==========\n",
      "========== IN EPOCH 56 the vali NLL loss is 5.476737022399902 ==========\n",
      "========== IN EPOCH 56 the GT metric is [[ 6.778648 10.165821]] ==========\n",
      "========== Now this is EPOCH 57 ==========\n",
      "========== IN EPOCH 57 the total loss is 7339.298675537109 ==========\n",
      "========== IN EPOCH 57 the vali NLL loss is 5.472233772277832 ==========\n",
      "========== IN EPOCH 57 the GT metric is [[ 6.7786484 10.16582  ]] ==========\n",
      "========== Now this is EPOCH 58 ==========\n",
      "========== IN EPOCH 58 the total loss is 7377.864547729492 ==========\n",
      "========== IN EPOCH 58 the vali NLL loss is 5.472507476806641 ==========\n",
      "========== IN EPOCH 58 the GT metric is [[ 6.778648 10.16582 ]] ==========\n",
      "========== Now this is EPOCH 59 ==========\n",
      "========== IN EPOCH 59 the total loss is 7318.584060668945 ==========\n",
      "========== IN EPOCH 59 the vali NLL loss is 5.4680962562561035 ==========\n",
      "========== IN EPOCH 59 the GT metric is [[ 6.7786484 10.16582  ]] ==========\n",
      "Total training time when epoch= *60* is *856.158328294754 *s\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# filename = \"../log_file.txt\"\n",
    "# f = open(filename,'w')\n",
    "total_train_step = 0\n",
    "EPOCH_NUM = 60\n",
    "\n",
    "train_start_time = time.time()\n",
    "mlp.train()\n",
    "\n",
    "for epoch in range(EPOCH_NUM):\n",
    "    # # Save the net structure\n",
    "    # net_file_name = \"net_before_epoch\"+str(epoch)+\".pth\"\n",
    "    # net_path = net_root_path + net_file_name\n",
    "    # torch.save(mlp.state_dict(), net_path)\n",
    "\n",
    "    epoch_train_loss = 0\n",
    "    print(f\"========== Now this is EPOCH {epoch} ==========\")\n",
    "    for batch_id,data in enumerate(train_loader):\n",
    "\n",
    "        input_data, _, target_loss, setting, _ = data\n",
    "        # print(f\"---- {batch_id} batch----\")\n",
    "        # Do the inference\n",
    "        input_data = input_data.to(device)\n",
    "        target_loss = target_loss.to(device)\n",
    "        pi, mu, sigma = mlp(input_data)\n",
    "\n",
    "        # Save the params\n",
    "        # params = list(mlp.named_parameters())\n",
    "\n",
    "        # Cal the MLE loss and draw the distrb.\n",
    "        loss = loss_fn_v2(pi, mu, sigma, target_loss, opt.N_gaussians,opt.TARGET, opt.SAFETY, device)\n",
    "        # loss = loss_fn_v3(pi, mu, sigma, target, N_gaussians)\n",
    "        # loss = loss_fn_v5(pi, mu, sigma, target, N_gaussians)\n",
    "\n",
    "        # Cal the CDF loss and draw the distrb.\n",
    "        # loss = loss_fn_cdf(pi, mu, sigma, target, N_gaussians)\n",
    "\n",
    "        # Cal the CE or KL loss and draw the distrb.\n",
    "        # loss  = loss_fn_CE(pi, mu, sigma, target, N_gaussians)\n",
    "\n",
    "        # Cal the WD loss and draw the distrb.\n",
    "        # loss  = loss_fn_WD(pi, mu, sigma, target, N_gaussians)\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "        draw_loss(total_train_step, loss.item(),win_train_loss_str)\n",
    "\n",
    "        # print(\"训练次数：{}，Loss：{}\".format(total_train_step, loss.item()))\n",
    "\n",
    "        # Optim\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        ########### before step() ###############\n",
    "\n",
    "        # for name, parms in mlp.named_parameters():\n",
    "        #     print(\"=====before step()=====\")\n",
    "        #     print('-->name:', name)\n",
    "        #     print('-->params:', parms.data)\n",
    "        #     # print('-->grad_requirs:',parms.requires_grad)\n",
    "        #     print('-->grad_value:',parms.grad)\n",
    "        #     print(\"===\")\n",
    "\n",
    "        ######\n",
    "\n",
    "        # 反向传播时检测是否有异常值，定位code\n",
    "        # with torch.autograd.detect_anomaly():\n",
    "        #     loss.backward()\n",
    "\n",
    "        ######\n",
    "        # get_dot = register_hooks(loss)\n",
    "        # dot = get_dot()\n",
    "        # dot.save('tmp.dot')   # to get .dot\n",
    "        # dot.render('tmp')     # to get pdf\n",
    "\n",
    "        ######\n",
    "        # Print grad check\n",
    "        # v_n = []    # name\n",
    "        # v_v = []    # value\n",
    "        # v_g = []    # grad\n",
    "        # for name, parameter in mlp.named_parameters():\n",
    "        #     v_n.append(name)\n",
    "        #     v_v.append(parameter.detach().cpu().numpy() if parameter is not None else [0])\n",
    "        #     v_g.append(parameter.grad.detach().cpu().numpy() if parameter.grad is not None else [0])\n",
    "        # for i in range(len(v_n)):\n",
    "        #     # if np.max(v_v[i]).item() - np.min(v_v[i]).item() < 1e-6:\n",
    "        #     #     color = bcolors.FAIL + '*'\n",
    "        #     if np.isnan(v_g[i]).any() or np.isnan(v_v[i]).any():\n",
    "        #         color = bcolors.FAIL + '*'\n",
    "        #     else:\n",
    "        #         color = bcolors.OKGREEN + ' '\n",
    "        #     print('%svalue %s: %.3e ~ %.3e' % (color, v_n[i], np.min(v_v[i]).item(), np.max(v_v[i]).item()))\n",
    "        #     print('%sgrad  %s: %.3e ~ %.3e' % (color, v_n[i], np.min(v_g[i]).item(), np.max(v_g[i]).item()))\n",
    "\n",
    "        ######\n",
    "        optimizer.step()\n",
    "\n",
    "        # for name, parms in mlp.named_parameters():\n",
    "        #     print(\"=====After step()=====\")\n",
    "        #     print('-->name:', name)\n",
    "        #     print('-->params:', parms.data)\n",
    "        #     # print('-->grad_requirs:',parms.requires_grad)\n",
    "        #     print('-->grad_value:',parms.grad)\n",
    "        #     print(\"===\")\n",
    "\n",
    "        # Only draw the 1st result in a training batch (5 in total)\n",
    "        # if total_train_step % 20 == 0:\n",
    "        #     with torch.no_grad():\n",
    "        #         draw_mdn_2(pi[0,:].clone().detach(), mu[0,:].clone().detach(), sigma[0,:].clone().detach(), target[0].clone().detach(), input_data[0].clone().detach(),total_train_step, N_gaussians)\n",
    "        #         # draw_mdn_cdf(pi[0,:].detach(), m[0], target[0].detach(), total_train_step, loss_list[0],N_gaussians)\n",
    "        total_train_step += 1\n",
    "\n",
    "    ########### Do validation\n",
    "    mlp.eval()\n",
    "    with torch.no_grad():\n",
    "        total_vali_metric, GT_metric = validate(mlp,val_loader,opt.N_gaussians, opt.MIN_LOSS,device)\n",
    "        draw_metric(epoch+1, total_vali_metric.cpu(), GT_metric)\n",
    "        # writer.add_scalars(\"metric/\"+opt.tag_str,{\"pred\":total_vali_metric.cpu(),\n",
    "        #                             \"GT-1\":GT_metric[0,0],\n",
    "        #                             \"GT-2\":GT_metric[0,1]},epoch)\n",
    "    mlp.train()\n",
    "\n",
    "    # Plot the loss in this EPOCH\n",
    "    print(f\"========== IN EPOCH {epoch} the total loss is {epoch_train_loss} ==========\")\n",
    "    print(f\"========== IN EPOCH {epoch} the vali NLL loss is {total_vali_metric.detach().cpu().numpy()} ==========\")\n",
    "    print(f\"========== IN EPOCH {epoch} the GT metric is {GT_metric.detach().cpu().numpy()} ==========\")\n",
    "\n",
    "    # # Save the net structure\n",
    "    # net_file_name = \"net_after_epoch\"+str(epoch)+\".pth\"\n",
    "    # net_path = net_root_path + net_file_name\n",
    "    # torch.save(mlp.state_dict(), net_path)\n",
    "\n",
    "    # Plot loss of this EPOCH\n",
    "    draw_loss(epoch, epoch_train_loss,win_train_epoch_loss_str)\n",
    "    # writer.add_scalars(\"Loss of EPOCH\",{\"Train\":epoch_train_loss},epoch)\n",
    "    scheduler.step()\n",
    "\n",
    "    # Record the params\n",
    "    # for name,param in params:\n",
    "    #     #参数的梯度\n",
    "    #     #print(\"name:\",name)\n",
    "    #     #print(\"param.grad:\",param.grad)\n",
    "    #     # writer.add_histogram(tag = name +'_grad of EPOCH '+str(epoch),values=param.grad,global_step=epoch)\n",
    "    #     # #参数值\n",
    "    #     writer.add_histogram(tag = name +'data of EPOCH '+str(epoch),values=param.data,global_step=epoch)\n",
    "\n",
    "# f.close()\n",
    "train_end_time = time.time()\n",
    "print(f\"Total training time when epoch= *{EPOCH_NUM}* is *{train_end_time-train_start_time} *s\")\n",
    "writer.close()\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- save"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 2.,  4.,  9.],\n        [12., 14., 24.]])"
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = torch.tensor([[1.,2,3],[6,7,8]])\n",
    "pi = torch.tensor([2.,2,3,])\n",
    "input_data*pi"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_path_MLE = \"mlp_train=300_MLE.pth\"\n",
    "# model_path_WD = \"mlp_train=300_WD_epoch=80.pth\"\n",
    "model_path_WD = \"mlp_train=300_WD.pth\"\n",
    "model_path_LSE = \"mlp_train=300_LSE.pth\"\n",
    "torch.save(mlp.state_dict(), model_path_LSE)\n",
    "# torch.save(mlp.state_dict(), model_path_MLE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
