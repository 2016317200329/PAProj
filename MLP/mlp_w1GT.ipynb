{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 0.\n",
    "\n",
    "1. 只有1个GT模型input"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Preparations\n",
    "## 1.1 Import"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from importlib import reload\n",
    "import random\n",
    "import torch.utils.data\n",
    "from mydataset import *\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from tensorboardX import SummaryWriter\n",
    "from visdom import Visdom\n",
    "from graphviz import Digraph\n",
    "from torchviz import make_dot\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pygraphviz as pgv\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import geomloss\n",
    "import time\n",
    "\n",
    "import scipy\n",
    "from torch.nn import KLDivLoss\n",
    "from torch.autograd.gradcheck import gradcheck\n",
    "from config import bcolors\n",
    "\n",
    "import config\n",
    "import loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Global settings and reloading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "reload(config)  # 必须reload！！\n",
    "reload(loss)    # 必须reload！！\n",
    "from config import DefaultConfig\n",
    "from loss import cal_metric\n",
    "from loss import loss_fn_v2\n",
    "opt = DefaultConfig()\n",
    "\n",
    "\n",
    "# 选择哪一个GT model: 0 或者 1\n",
    "GT_CHOSEN = opt.GT_CHOSEN\n",
    "\n",
    "# nums of Gaussian kernels\n",
    "N_gaussians = opt.N_gaussians\n",
    "\n",
    "# dataset划分\n",
    "batch_size = opt.batch_size\n",
    "train_pct = opt.train_pct\n",
    "vali_pct = opt.vali_pct\n",
    "test_pct = opt.test_pct\n",
    "\n",
    "# train and optim.\n",
    "learning_rate = opt.learning_rate\n",
    "lr_for_mu = opt.lr_for_mu   # 给mu单独设置learning rate\n",
    "\n",
    "total_train_step = 0\n",
    "total_test_step = 0\n",
    "\n",
    "MIN_LOSS = opt.MIN_LOSS\n",
    "SAFETY = opt.SAFETY\n",
    "\n",
    "# Range【一般用不到】\n",
    "a = 0\n",
    "b = 255\n",
    "\n",
    "# Training data的粒度，画图会使用到\n",
    "SCALE = opt.SCALE\n",
    "# Target data是target_5时\n",
    "TARGET = opt.TARGET\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 the data path\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# Training data\n",
    "train_path = opt.train_path\n",
    "\n",
    "# Target data\n",
    "target_path_metric = opt.target_path_metric\n",
    "target_path_loss = opt.target_path_loss\n",
    "\n",
    "# data keys\n",
    "data_key_path = opt.data_key_path\n",
    "\n",
    "# NLL metric\n",
    "NLL_metric_path = opt.NLL_metric_path\n",
    "\n",
    "# Net path\n",
    "net_root_path = opt.net_root_path"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Dataset and Dataloader\n",
    "1. DataLoader中的`shuffer=True`表示在每一次epoch中都打乱所有数据的顺序，然后以batch为单位从头到尾按顺序取用数据。这样的结果就是不同epoch中的数据都是乱序的,设置随机种子的作用就是让你的每一次训练都乱的一样，\n",
    "\n",
    "## 2.1 Dataset and spliting\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 设置随机数种子"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup_seed(7)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "dataset = myDataset(train_path, target_path_metric, target_path_loss, data_key_path, NLL_metric_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  56  561  857  852  492  251   89  542  784  626  957  764 1053  934\n",
      "  585  341  264  669  247  697  279  266  330  228 1105  985  506  297\n",
      " 1083   99  309  452  858  925  329  441  656 1116  869   59  568   75\n",
      "  587  409 1152  342 1137  394  276  862  146  349 1099  230 1026  403\n",
      "  267  318 1087  788  150  737  193  339  783  556  284  242  543  907\n",
      "  378 1003  944  833  597  119  800 1034  225  660    5  201  291  584\n",
      "  808  164  370  244  530  280   78  923  236 1170  918  411  281  275\n",
      "  987  651  490  163    7  206  998  359  714  218  426  410    4  270\n",
      " 1080  924   38 1171  996  252  433   65  686  531  424  208 1191  812\n",
      "  769  735  219  703  971 1114  797  233  915  670  582  170  181   91\n",
      "  226   71 1145  528 1194  152 1135  750   74  871  355  754  693  180\n",
      "   35   60  748  458  534  724   11  338  184  632  887  747  320  356\n",
      " 1049  956  895 1045  793   51  268 1148  256  977  548  187 1140  188\n",
      "  732   81  562  546  566  678 1167   15  189 1061  168  240  480 1074\n",
      "  742  892  766  638  357 1023 1084  503  216 1062  838  822  128  806\n",
      "  249  749  841  798  903  581   92  335   84  927  558  952  257 1059\n",
      "  287  272  488  346  517  836 1032  160  883  829  966  535  317 1122\n",
      " 1055  336  313  274  289  781 1118 1173 1054  900  803 1073  999  207\n",
      " 1112 1102  479  665  464 1161   86  258  689   96 1141  282  232  819\n",
      "   28  327 1065  200  753  337  949  214   80  434 1016   36  263  345\n",
      "   45  725  937  110  802  751   41  269  238  612 1066  212 1033  969\n",
      "  363  933  707   22 1060 1052  679  334   31  955 1086  375   93   37\n",
      "  717   63  785  321  882  698   39  358  692  744  454  449  438 1030\n",
      "  108  379  826  607  169  975  109  974  124  876 1113  733  407  569\n",
      "   48  326  672  178  938  654  875   23  813  315  650 1056  593  981\n",
      "  450  621  580 1090  890  983  382  745  391  432  340  807 1058  951\n",
      "  627   98  940  368   53  176  202  718  283  250  842 1093  702  366\n",
      "  498 1139  834   67  367  522   64  105  810  344  142  312  140  954\n",
      "  400  254  578  229  736  473 1008  828  227  175  197  786  851  415\n",
      "  961  417  350  617  122  856  894  652  912  708  222  700 1076  383\n",
      "  641  161  141   46  820 1131  630  771  533  120  106  909  884  171\n",
      "  849  211 1157  873  739  374  151  262  158 1107  827  316  825 1169\n",
      "  364 1128  659 1186  487 1017 1044  195 1178  399  616  412  466  343\n",
      " 1070  319  663  653  960  870  130  237  811 1136  835  837  209  720\n",
      "  540  539  551  470  722  682  331  324  323  608  223 1144  636  231\n",
      "  668  859  997  103  245  886  552  637  271  295  579  111   55  959\n",
      " 1075  774  380  779 1057  376  101 1156   85  789  885  325  302 1106\n",
      " 1077  445  398  861 1129 1048 1123  290  288  846  156  126 1162  704\n",
      " 1147  965  486   66   95  541   25  465  719 1126  776  721  457  213\n",
      "   76  205 1138 1020  844 1069  311 1094  215    0  513  695  926  945\n",
      " 1125 1088  135  385   44  794 1072  583  847 1043  104 1134   61 1091\n",
      " 1146  507  921   88 1037  980  591  706  796  600  658  173  831   18\n",
      "  527  442   69 1143   72   82 1019 1042  989  917  594  688  958 1182\n",
      "  782  943   14  194  863  183  791  645  392 1040  136  772  681 1031\n",
      "  387  625   32  293  877  973   77  129 1104 1154  165  880  127  524\n",
      "   17  948  121  114  310  799 1036  192  147 1050  908 1183  390   90\n",
      "  738  629 1187  134  644    9  333  139   94  795  234 1119  510  655\n",
      "  765  100  602  559  743  666  982  483  727  519    6  149  942  550\n",
      "  905  586  153  705  277  610  913  741  322  922  221  294  816  107\n",
      "  860  936  154  384  348  497  879  618  687  994  976  911  495 1177\n",
      "  694  567  839  777  462  685  137  968  571  713 1001  431  639 1111\n",
      "   58  962 1068  396  393  155  167   21  422  734  113    1  675  615\n",
      " 1188  115  684  204  102   49 1064 1028  978   30  482  261  716   26\n",
      "  640  891  939  511   50  112   79   73  481 1096  967  430  509  420\n",
      "  123   33  572  162  898  361  830  815  159  609  145  314  529  631\n",
      "   43  440  888 1067   16 1185 1011  914   12  308  690  255  853  995\n",
      "  767 1115  990  768  196   54  605  893  709 1159   10  419   87  253\n",
      "   34  821  405 1021   19  906  920  360  371  574  525  416  444  429\n",
      "   52  143  460  241  673  606  347 1051  305  964  950 1046  866  953\n",
      " 1018  780  116 1000  172  935  199  117  468  485  545]\n"
     ]
    }
   ],
   "source": [
    "# 使用全部的data\n",
    "# DATA_len = dataset.__len__()\n",
    "# shuffled_indices = np.random.permutation(DATA_len)\n",
    "\n",
    "# 使用指定的data\n",
    "shuffled_indices = np.load(\"../BasicInfo/arr.npy\")\n",
    "DATA_len = len(shuffled_indices)\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "train_idx = shuffled_indices[:int(train_pct*dataset.__len__())]\n",
    "tmp = int((train_pct+vali_pct)*dataset.__len__())\n",
    "val_idx = shuffled_indices[int(train_pct*dataset.__len__()):tmp]\n",
    "\n",
    "test_idx = shuffled_indices[tmp:]\n",
    "print(train_idx)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Dataloader and collating\n",
    "1. 主要是对label数据进行collate\n",
    "    - 按照batch中的最大target data长度进行padding，padding with 0\n",
    "2. 返回的结果多一个batch dim,比如下面的`5`\n",
    "    - After collating:\n",
    "        - `torch.Size([5, 3, 300]),torch.Size([5, 87, 2])`\n",
    "        - `87`是最长的targets data长度"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "def my_collate_fn(data):\n",
    "# 这里的data是一个list， list的元素是元组: (self.data, self.label)\n",
    "# collate_fn的作用是把[(data, label),(data, label)...]转化成([data, data...],[label,label...])\n",
    "# 假设self.data的一个data的shape为(channels, length), 每一个channel的length相等,\n",
    "# data[索引到index(batch)][索引到data或者label][索引到channel]\n",
    "\n",
    "    data_list = []\n",
    "    target_metric_list = []\n",
    "    target_loss_list = []\n",
    "    setting_list = []\n",
    "    metric_list = []\n",
    "\n",
    "    data_len = len(data)      # 读进来的data batch的大小\n",
    "\n",
    "    batch = 0\n",
    "\n",
    "    while batch < data_len:\n",
    "        # print(\"shape:\",data[batch][0].shape) #shape: (3, 300)\n",
    "\n",
    "        data_tmp = np.array([data[batch][0][GT_CHOSEN,:],data[batch][0][2,:]])\n",
    "        data_list.append(torch.tensor(data_tmp))\n",
    "\n",
    "        target_metric_list.append(torch.tensor(data[batch][1]))\n",
    "        target_loss_list.append(torch.tensor(data[batch][2]))\n",
    "        setting_list.append(torch.tensor(data[batch][3]))\n",
    "        metric_list.append(torch.tensor(data[batch][4]))\n",
    "        batch += 1\n",
    "\n",
    "    # Pad target data with zeros\n",
    "    target_metric_padded = pad_sequence(target_metric_list,batch_first=True)\n",
    "    target_loss_padded = pad_sequence(target_loss_list,batch_first=True)\n",
    "    target_metric_tensor = target_metric_padded.float()\n",
    "    target_loss_padded = target_loss_padded.float()\n",
    "\n",
    "    data_tensor = torch.stack(data_list).float()\n",
    "    setting_tensor = torch.stack(setting_list).float()\n",
    "    metric_tensor = torch.stack(metric_list).float()\n",
    "\n",
    "    return data_tensor, target_metric_tensor, target_loss_padded, setting_tensor, metric_tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = dataset,batch_size = batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(train_idx), collate_fn = my_collate_fn)\n",
    "\n",
    "val_loader = DataLoader(dataset = dataset,batch_size = batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(val_idx),collate_fn = my_collate_fn)\n",
    "\n",
    "# 注意test_loader的batch size\n",
    "test_loader = DataLoader(dataset = dataset,batch_size = batch_size, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(test_idx),collate_fn = my_collate_fn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. The Net and Init\n",
    "1. BatchNorm1d: The mean and std are calculated per-dimension over the mini-batches\n",
    "## 3.1 2层MLP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# 参数量92,000+\n",
    "# 2层\n",
    "class MLP_1(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self, n_gaussians) -> None:\n",
    "        super().__init__()\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=2,affine=True)\n",
    "        self.BN2 = nn.BatchNorm1d(num_features=150,affine=True)\n",
    "        self.BN3 = nn.BatchNorm1d(num_features=12,affine=True)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "        self.linear1 = nn.Linear(600, 150)\n",
    "        self.linear2 = nn.Linear(150, 12)\n",
    "\n",
    "        self.ac_func = nn.Softplus()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.z_pi = nn.Sequential(\n",
    "            nn.Linear(12, n_gaussians),  # 30个params要learn\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.z_mu = nn.Linear(12, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(12, n_gaussians)\n",
    "\n",
    "        # SELU Init\n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, (nn.Linear)):\n",
    "        #         nn.init.kaiming_normal_(m.weight, nonlinearity='linear')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.BN1(x)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.BN2(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = self.BN3(x)\n",
    "        x = self.ac_func(self.drop(x))\n",
    "\n",
    "        pi = self.z_pi(x)\n",
    "        # pi = self.z_pi(torch.clamp(x,1e-3))\n",
    "        mu = self.z_mu(x)\n",
    "        # sigma = torch.exp(self.z_sigma(x))\n",
    "        sigma = F.elu(self.z_sigma(x)) + 1\n",
    "        sigma = torch.clamp(sigma,1e-4)\n",
    "\n",
    "        # return x\n",
    "        return pi, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. The Loss\n",
    "\n",
    "## 4.0 The metric\n",
    "1. 以NLL作为metric\n",
    "2. 理论模型的metric可以提前算好，读进来，这里读setting是为了？IDK"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# from loss import cal_metric"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1 NLL loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# from loss import loss_fn_v2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.4 W-distance\n",
    "1. 拿到target data的两列，按照第一列去predict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from loss import loss_fn_WD"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5 Plot\n",
    "1. draw:\n",
    "    - loss: training data的loss和test data的loss趋势\n",
    "    - MLP的网络结构（.png）\n",
    "    - target distrb.和pred. distrb.\n",
    "    - 所有target data的分布图\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(\"logs-MLP\")\n",
    "viz = Visdom(env=\"001\")\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.2 plot mdn\n",
    "### 5.2.1 mdn pdf的图像[with input data]\n",
    "1. 求解[1,6,11...]上的cdf：\n",
    "    - 求解[1,6,11..]的cdf，再求解[6,11,..]，两者相减"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [],
   "source": [
    "def draw_mdn_2(pi, mu, sigma, target, input, total_train_step, N_gaussians):\n",
    "    \"\"\"\n",
    "    画两条曲线，pred-1表示归一化前，pred-2表示归一化后\n",
    "\n",
    "    :param pi:\n",
    "    :param mu:\n",
    "    :param sigma:\n",
    "    :param target:\n",
    "    :param input:\n",
    "    :param total_train_step:\n",
    "    :param N_gaussians:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 只画一个batch中的第一个\n",
    "\n",
    "    # The target distrb.\n",
    "    target = torch.unique(target,dim=0)    # drop the duplicate\n",
    "    n_target = target[:,0]\n",
    "    non_zero_idx = torch.nonzero(n_target)\n",
    "    n = n_target[non_zero_idx]\n",
    "\n",
    "    p_target = target[:,1]\n",
    "    p = p_target[non_zero_idx]\n",
    "    max_n = int(max(n).item())\n",
    "\n",
    "    # The predicted distrb.\n",
    "    m = torch.distributions.Normal(mu,sigma)          # Gaussian\n",
    "    # m = torch.distributions.Laplace(mu,sigma)           # Laplace\n",
    "\n",
    "    x_0 = torch.arange(1,(max_n+TARGET),TARGET).to(device=device)\n",
    "\n",
    "    x = torch.repeat_interleave(x_0.unsqueeze(dim=1), repeats=N_gaussians, dim=1)   # expand dim\n",
    "    # 求解每个长度为TARGET的区间上的cdf\n",
    "    y = (m.cdf(x+TARGET) - m.cdf(x)).to(device=device)\n",
    "\n",
    "    # 方法一：\n",
    "    y_pred_1 = torch.sum(pi*y,dim=1)\n",
    "    # 方法二：做一下(0,1)上的归一化\n",
    "    y_pred_2 = y_pred_1/y_pred_1.sum()\n",
    "\n",
    "    # The input distrb.\n",
    "    # input_data = input[0:2,:]\n",
    "    input_data = input[0,:]\n",
    "    x_input = torch.arange(1,(300+1),SCALE).to(device=device)\n",
    "    y_input_0 = input_data.to(device=device)\n",
    "    # y_input_0 = (input_data[0,:]).to(device=device)\n",
    "    # y_input_1 = (input_data[1,:]).to(device=device)\n",
    "\n",
    "    # Init\n",
    "    win_str = \"total_train_step = \"+str(total_train_step)\n",
    "    title_str = \"Distrb. in \"+win_str\n",
    "    viz.line(X = [0.],Y = [0.], env=\"001\", win=win_str, opts= dict(title=title_str))\n",
    "\n",
    "    # Visdom本身不能把hist和line画在一个window中\n",
    "    # 如果想画一起只能是两条line\n",
    "    # Plot y_target\n",
    "    # viz.histogram(X = n, env=\"001\", win=win_str,\n",
    "    #         opts= dict(title=title_str))\n",
    "    viz.line(X = n,Y= p, env=\"001\", win=win_str, update=\"append\", name='target',\n",
    "            opts= dict(title=title_str,markers = True,markersize = 7,markersymbol = \"cross-thin-open\"))\n",
    "\n",
    "    # Plot y_pred\n",
    "    viz.line(X = x_0,Y= y_pred_1, env=\"001\", win=win_str, update=\"append\", name='pred-1',\n",
    "            opts= dict(title=title_str))\n",
    "\n",
    "    # Plot y_pred_2\n",
    "    viz.line(X = x_0,Y= y_pred_2, env=\"001\", win=win_str, update=\"append\", name='pred-2',\n",
    "            opts= dict(title=title_str))\n",
    "\n",
    "    # Plot y_input\n",
    "    # 如果input data长度短，全部画完\n",
    "    if(len(x_input) <= max_n):\n",
    "        viz.line(X = x_input,Y= y_input_0, env=\"001\", win=win_str, update=\"append\", name='GT-1', opts= dict(title=title_str))\n",
    "        # viz.line(X = x_input,Y= y_input_1, env=\"001\", win=win_str, update=\"append\", name='GT-2', opts= dict(title=title_str))\n",
    "    # 如果input data长度长，则截断\n",
    "    else:\n",
    "        x_input_0 = x_input[0:max_n]\n",
    "        y_input_2 = y_input_0[0:max_n]/y_input_0[0:max_n].sum()\n",
    "        # y_input_3 = y_input_1[0:max_n]/y_input_1[0:max_n].sum()\n",
    "        viz.line(X = x_input_0,Y= y_input_2, env=\"001\", win=win_str, update=\"append\", name='GT-1', opts= dict(title=title_str))\n",
    "        # viz.line(X = x_input_0,Y= y_input_3, env=\"001\", win=win_str, update=\"append\", name='GT-2', opts= dict(title=title_str))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.4 plot the loss\n",
    "1. 直接根据参数决定draw什么loss，反正loss都画在一个window里面"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "win_train_loss_str = \"The Loss of BATCH in the Training Data\"\n",
    "win_vali_loss_str = \"The Loss in the Vali Data\"\n",
    "win_train_epoch_loss_str = \"The Loss of EPOCH in the Training Data\"\n",
    "\n",
    "def draw_loss(X_step, loss, win_str):\n",
    "    viz.line(X = [X_step], Y = [loss],win=win_str, update=\"append\",\n",
    "        opts= dict(title=win_str))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.5 plot the metric"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "win_vali_metric_str = \"The NLL of ALL vali data\"\n",
    "\n",
    "def draw_metric(X_step, total_vali_metric, GT_metric):\n",
    "\n",
    "    viz.line(X = [X_step], Y = [[total_vali_metric,GT_metric[0,0],GT_metric[0,1]]],win=win_vali_metric_str, update=\"append\", opts= dict(title=win_vali_metric_str, legend=['pred','GT-1','GT-2'], showlegend=True,xlabel=\"epoch\", ylabel=\"NLL\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6 Training\n",
    "## 6.1 Init"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "       BatchNorm1d-1               [-1, 2, 300]               4\n",
      "           Flatten-2                  [-1, 600]               0\n",
      "            Linear-3                  [-1, 150]          90,150\n",
      "       BatchNorm1d-4                  [-1, 150]             300\n",
      "           Dropout-5                  [-1, 150]               0\n",
      "          Softplus-6                  [-1, 150]               0\n",
      "            Linear-7                   [-1, 12]           1,812\n",
      "       BatchNorm1d-8                   [-1, 12]              24\n",
      "           Dropout-9                   [-1, 12]               0\n",
      "         Softplus-10                   [-1, 12]               0\n",
      "           Linear-11                    [-1, 3]              39\n",
      "          Softmax-12                    [-1, 3]               0\n",
      "           Linear-13                    [-1, 3]              39\n",
      "           Linear-14                    [-1, 3]              39\n",
      "================================================================\n",
      "Total params: 92,407\n",
      "Trainable params: 92,407\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.35\n",
      "Estimated Total Size (MB): 0.37\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "mlp = MLP_1(N_gaussians)\n",
    "\n",
    "# Init the params\n",
    "# # mlp = model_param_init(mlp)\n",
    "# Init the vis\n",
    "viz.line(X = [0.],Y = [0.], env=\"001\", win=win_train_loss_str, opts= dict(title=win_train_loss_str))\n",
    "# viz.line(X = [0.],Y = [0.], env=\"001\", win=win_vali_loss_str, opts= dict(title=win_vali_loss_str))\n",
    "viz.line(X = [0.],Y = [0.], env=\"001\", win=win_train_epoch_loss_str, opts= dict(title=win_train_epoch_loss_str))\n",
    "viz.line(X = [0.],Y = [[0.,0.,0.]], env=\"001\", win=win_vali_metric_str, opts= dict(title=win_vali_metric_str,legend=['pred','GT-1','GT-2'], showlegend=True,xlabel=\"epoch\", ylabel=\"NLL\"))\n",
    "# Save the init params\n",
    "# torch.save(mlp.state_dict(), 'mlp_init.pth')\n",
    "\n",
    "# Load the saved model\n",
    "# model_path_WD = \"mlp_train=300_WD.pth\"\n",
    "# model_path_LSE = \"mlp_train=300_LSE.pth\"\n",
    "# model_data = torch.load(model_path_LSE)\n",
    "# mlp.load_state_dict(model_data)\n",
    "\n",
    "mlp = mlp.to(device=device)\n",
    "summary(mlp, (2,300))\n",
    "\n",
    "############ learning rate strategy ############\n",
    "# 1.\n",
    "# Set different lr for params\n",
    "# id: id() 函数返回对象的唯一标识符，标识符是一个整数。返回对象的内存地址。\n",
    "mu_params = list(map(id, mlp.z_mu.parameters()))\n",
    "base_params = filter(lambda p: id(p) not in mu_params, mlp.parameters())\n",
    "params = [{'params': base_params},         # 如果对某个参数不指定学习率，就使用最外层的默认学习率\n",
    "        {'params': mlp.z_mu.parameters(), 'lr': lr_for_mu}]\n",
    "optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "\n",
    "# optimizer = torch.optim.Adagrad(params,lr=learning_rate,weight_decay = 1e-4)\n",
    "# optimizer = torch.optim.Adagrad(params,lr=learning_rate)\n",
    "\n",
    "# 2.\n",
    "# Or set lr decay\n",
    "# StepLR为步进，每step_size个epoch，lr*gamma\n",
    "# scheduler  = torch.optim.lr_scheduler.StepLR(optimizer,step_size=10,gamma=0.8)\n",
    "# 下面这个比较好用\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10,20], gamma=0.7, last_epoch=-1)\n",
    "\n",
    "# # Set the hooks\n",
    "# #mlp.conv1.register_forward_hook(hook_forward_fn)\n",
    "# mlp.conv1.register_full_backward_hook(hook_backward_fn)\n",
    "#\n",
    "# #mlp.linear1.register_forward_hook(hook_forward_fn)\n",
    "# #mlp.linear1.register_full_backward_hook(hook_backward_fn)\n",
    "#\n",
    "# #mlp.z_pi.register_forward_hook(hook_forward_fn)\n",
    "# mlp.z_pi.register_full_backward_hook(hook_backward_fn_pi)\n",
    "#\n",
    "# #mlp.z_mu.register_forward_hook(hook_forward_fn)\n",
    "# mlp.z_mu.register_full_backward_hook(hook_backward_fn_mu)\n",
    "#\n",
    "# mlp.z_sigma.register_forward_hook(hook_forward_fn)\n",
    "# mlp.z_sigma.register_full_[2, 3, 300]backward_hook(hook_backward_fn_sigma)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.3 START HERE\n",
    "1. 使用Laplace分布改：\n",
    "    - loss function和draw"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Now this is EPOCH 0 ==========\n",
      "========== IN EPOCH 0 the total loss is 102651.25854492188 ==========\n",
      "========== Now this is EPOCH 1 ==========\n",
      "========== IN EPOCH 1 the total loss is 88084.49365234375 ==========\n",
      "========== Now this is EPOCH 2 ==========\n",
      "========== IN EPOCH 2 the total loss is 79805.162109375 ==========\n",
      "========== Now this is EPOCH 3 ==========\n",
      "========== IN EPOCH 3 the total loss is 71919.93408203125 ==========\n",
      "========== Now this is EPOCH 4 ==========\n",
      "========== IN EPOCH 4 the total loss is 66164.00671386719 ==========\n",
      "========== Now this is EPOCH 5 ==========\n",
      "========== IN EPOCH 5 the total loss is 59705.01416015625 ==========\n",
      "========== Now this is EPOCH 6 ==========\n",
      "========== IN EPOCH 6 the total loss is 54603.12744140625 ==========\n",
      "========== Now this is EPOCH 7 ==========\n",
      "========== IN EPOCH 7 the total loss is 48713.150390625 ==========\n",
      "========== Now this is EPOCH 8 ==========\n",
      "========== IN EPOCH 8 the total loss is 46429.885803222656 ==========\n",
      "========== Now this is EPOCH 9 ==========\n",
      "========== IN EPOCH 9 the total loss is 41195.34735107422 ==========\n",
      "========== Now this is EPOCH 10 ==========\n",
      "========== IN EPOCH 10 the total loss is 39813.633361816406 ==========\n",
      "========== Now this is EPOCH 11 ==========\n",
      "========== IN EPOCH 11 the total loss is 37292.67059326172 ==========\n",
      "========== Now this is EPOCH 12 ==========\n",
      "========== IN EPOCH 12 the total loss is 34071.37170410156 ==========\n",
      "========== Now this is EPOCH 13 ==========\n",
      "========== IN EPOCH 13 the total loss is 31006.68243408203 ==========\n",
      "========== Now this is EPOCH 14 ==========\n",
      "========== IN EPOCH 14 the total loss is 30128.59017944336 ==========\n",
      "========== Now this is EPOCH 15 ==========\n",
      "========== IN EPOCH 15 the total loss is 29999.643005371094 ==========\n",
      "========== Now this is EPOCH 16 ==========\n",
      "========== IN EPOCH 16 the total loss is 28011.002685546875 ==========\n",
      "========== Now this is EPOCH 17 ==========\n",
      "========== IN EPOCH 17 the total loss is 27370.95672607422 ==========\n",
      "========== Now this is EPOCH 18 ==========\n",
      "========== IN EPOCH 18 the total loss is 25312.14239501953 ==========\n",
      "========== Now this is EPOCH 19 ==========\n",
      "========== IN EPOCH 19 the total loss is 24995.5380859375 ==========\n",
      "========== Now this is EPOCH 20 ==========\n",
      "========== IN EPOCH 20 the total loss is 23908.670806884766 ==========\n",
      "========== Now this is EPOCH 21 ==========\n",
      "========== IN EPOCH 21 the total loss is 23092.497680664062 ==========\n",
      "========== Now this is EPOCH 22 ==========\n",
      "========== IN EPOCH 22 the total loss is 23062.854431152344 ==========\n",
      "========== Now this is EPOCH 23 ==========\n",
      "========== IN EPOCH 23 the total loss is 21999.654693603516 ==========\n",
      "========== Now this is EPOCH 24 ==========\n",
      "========== IN EPOCH 24 the total loss is 21661.370819091797 ==========\n",
      "========== Now this is EPOCH 25 ==========\n",
      "========== IN EPOCH 25 the total loss is 20988.55126953125 ==========\n",
      "========== Now this is EPOCH 26 ==========\n",
      "========== IN EPOCH 26 the total loss is 20278.138122558594 ==========\n",
      "========== Now this is EPOCH 27 ==========\n",
      "========== IN EPOCH 27 the total loss is 20162.80337524414 ==========\n",
      "========== Now this is EPOCH 28 ==========\n",
      "========== IN EPOCH 28 the total loss is 19960.404296875 ==========\n",
      "========== Now this is EPOCH 29 ==========\n",
      "========== IN EPOCH 29 the total loss is 19399.221069335938 ==========\n",
      "========== Now this is EPOCH 30 ==========\n",
      "========== IN EPOCH 30 the total loss is 18802.603958129883 ==========\n",
      "========== Now this is EPOCH 31 ==========\n",
      "========== IN EPOCH 31 the total loss is 18869.313507080078 ==========\n",
      "========== Now this is EPOCH 32 ==========\n",
      "========== IN EPOCH 32 the total loss is 18762.127227783203 ==========\n",
      "========== Now this is EPOCH 33 ==========\n",
      "========== IN EPOCH 33 the total loss is 18624.687133789062 ==========\n",
      "========== Now this is EPOCH 34 ==========\n",
      "========== IN EPOCH 34 the total loss is 17925.076751708984 ==========\n",
      "========== Now this is EPOCH 35 ==========\n",
      "========== IN EPOCH 35 the total loss is 17620.238677978516 ==========\n",
      "========== Now this is EPOCH 36 ==========\n",
      "========== IN EPOCH 36 the total loss is 17502.459503173828 ==========\n",
      "========== Now this is EPOCH 37 ==========\n",
      "========== IN EPOCH 37 the total loss is 17308.226348876953 ==========\n",
      "========== Now this is EPOCH 38 ==========\n",
      "========== IN EPOCH 38 the total loss is 17132.265014648438 ==========\n",
      "========== Now this is EPOCH 39 ==========\n",
      "========== IN EPOCH 39 the total loss is 17042.390502929688 ==========\n",
      "========== Now this is EPOCH 40 ==========\n",
      "========== IN EPOCH 40 the total loss is 16681.875701904297 ==========\n",
      "========== Now this is EPOCH 41 ==========\n",
      "========== IN EPOCH 41 the total loss is 16611.021545410156 ==========\n",
      "========== Now this is EPOCH 42 ==========\n",
      "========== IN EPOCH 42 the total loss is 16126.596496582031 ==========\n",
      "========== Now this is EPOCH 43 ==========\n",
      "========== IN EPOCH 43 the total loss is 15901.884735107422 ==========\n",
      "========== Now this is EPOCH 44 ==========\n",
      "========== IN EPOCH 44 the total loss is 15912.414306640625 ==========\n",
      "========== Now this is EPOCH 45 ==========\n",
      "========== IN EPOCH 45 the total loss is 15610.186279296875 ==========\n",
      "========== Now this is EPOCH 46 ==========\n",
      "========== IN EPOCH 46 the total loss is 15422.675048828125 ==========\n",
      "========== Now this is EPOCH 47 ==========\n",
      "========== IN EPOCH 47 the total loss is 15504.30258178711 ==========\n",
      "========== Now this is EPOCH 48 ==========\n",
      "========== IN EPOCH 48 the total loss is 15454.274200439453 ==========\n",
      "========== Now this is EPOCH 49 ==========\n",
      "========== IN EPOCH 49 the total loss is 15207.232452392578 ==========\n",
      "Total training time when epoch= *50* is *657.2214457988739 *s\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# filename = \"../log_file.txt\"\n",
    "# f = open(filename,'w')\n",
    "total_train_step = 0\n",
    "EPOCH_NUM = 50\n",
    "\n",
    "train_start_time = time.time()\n",
    "mlp.train()\n",
    "\n",
    "for epoch in range(EPOCH_NUM):\n",
    "    # # Save the net structure\n",
    "    # net_file_name = \"net_before_epoch\"+str(epoch)+\".pth\"\n",
    "    # net_path = net_root_path + net_file_name\n",
    "    # torch.save(mlp.state_dict(), net_path)\n",
    "\n",
    "    epoch_train_loss = 0\n",
    "    print(f\"========== Now this is EPOCH {epoch} ==========\")\n",
    "    for batch_id,data in enumerate(train_loader):\n",
    "\n",
    "        input_data, _, target_loss, setting, _ = data\n",
    "        # print(f\"---- {batch_id} batch----\")\n",
    "        # Do the inference\n",
    "        input_data = input_data.to(device)\n",
    "        target_loss = target_loss.to(device)\n",
    "        pi, mu, sigma = mlp(input_data)\n",
    "\n",
    "        # Save the params\n",
    "        # params = list(mlp.named_parameters())\n",
    "\n",
    "        # Cal the MLE loss and draw the distrb.\n",
    "        loss = loss_fn_v2(pi, mu, sigma, target_loss, N_gaussians,TARGET, SAFETY, device)\n",
    "        # loss = loss_fn_v3(pi, mu, sigma, target, N_gaussians)\n",
    "        # loss = loss_fn_v5(pi, mu, sigma, target, N_gaussians)\n",
    "\n",
    "        # Cal the CDF loss and draw the distrb.\n",
    "        # loss = loss_fn_cdf(pi, mu, sigma, target, N_gaussians)\n",
    "\n",
    "        # Cal the CE or KL loss and draw the distrb.\n",
    "        # loss  = loss_fn_CE(pi, mu, sigma, target, N_gaussians)\n",
    "\n",
    "        # Cal the WD loss and draw the distrb.\n",
    "        # loss  = loss_fn_WD(pi, mu, sigma, target, N_gaussians)\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "        draw_loss(total_train_step, loss.item(),win_train_loss_str)\n",
    "\n",
    "        # print(\"训练次数：{}，Loss：{}\".format(total_train_step, loss.item()))\n",
    "\n",
    "        # Optim\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        ########### before step() ###############\n",
    "\n",
    "        # for name, parms in mlp.named_parameters():\n",
    "        #     print(\"=====before step()=====\")\n",
    "        #     print('-->name:', name)\n",
    "        #     print('-->params:', parms.data)\n",
    "        #     # print('-->grad_requirs:',parms.requires_grad)\n",
    "        #     print('-->grad_value:',parms.grad)\n",
    "        #     print(\"===\")\n",
    "\n",
    "        ######\n",
    "\n",
    "        # 反向传播时检测是否有异常值，定位code\n",
    "        # with torch.autograd.detect_anomaly():\n",
    "        #     loss.backward()\n",
    "\n",
    "        ######\n",
    "        # get_dot = register_hooks(loss)\n",
    "        # dot = get_dot()\n",
    "        # dot.save('tmp.dot')   # to get .dot\n",
    "        # dot.render('tmp')     # to get pdf\n",
    "\n",
    "        ######\n",
    "        # Print grad check\n",
    "        # v_n = []    # name\n",
    "        # v_v = []    # value\n",
    "        # v_g = []    # grad\n",
    "        # for name, parameter in mlp.named_parameters():\n",
    "        #     v_n.append(name)\n",
    "        #     v_v.append(parameter.detach().cpu().numpy() if parameter is not None else [0])\n",
    "        #     v_g.append(parameter.grad.detach().cpu().numpy() if parameter.grad is not None else [0])\n",
    "        # for i in range(len(v_n)):\n",
    "        #     # if np.max(v_v[i]).item() - np.min(v_v[i]).item() < 1e-6:\n",
    "        #     #     color = bcolors.FAIL + '*'\n",
    "        #     if np.isnan(v_g[i]).any() or np.isnan(v_v[i]).any():\n",
    "        #         color = bcolors.FAIL + '*'\n",
    "        #     else:\n",
    "        #         color = bcolors.OKGREEN + ' '\n",
    "        #     print('%svalue %s: %.3e ~ %.3e' % (color, v_n[i], np.min(v_v[i]).item(), np.max(v_v[i]).item()))\n",
    "        #     print('%sgrad  %s: %.3e ~ %.3e' % (color, v_n[i], np.min(v_g[i]).item(), np.max(v_g[i]).item()))\n",
    "\n",
    "        ######\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # for name, parms in mlp.named_parameters():\n",
    "        #     print(\"=====After step()=====\")\n",
    "        #     print('-->name:', name)\n",
    "        #     print('-->params:', parms.data)\n",
    "        #     # print('-->grad_requirs:',parms.requires_grad)\n",
    "        #     print('-->grad_value:',parms.grad)\n",
    "        #     print(\"===\")\n",
    "\n",
    "        # Only draw the 1st result in a training batch (5 in total)\n",
    "        # if total_train_step % 20 == 0:\n",
    "        #     with torch.no_grad():\n",
    "        #         draw_mdn_2(pi[0,:].clone().detach(), mu[0,:].clone().detach(), sigma[0,:].clone().detach(), target[0].clone().detach(), input_data[0].clone().detach(),total_train_step, N_gaussians)\n",
    "        #         # draw_mdn_cdf(pi[0,:].detach(), m[0], target[0].detach(), total_train_step, loss_list[0],N_gaussians)\n",
    "        total_train_step += 1\n",
    "\n",
    "    ########### Do validation\n",
    "    with torch.no_grad():\n",
    "        mlp.eval()\n",
    "        total_vali_metric = 0\n",
    "        GT_metric = torch.tensor([0.,0.]).reshape(1,2)\n",
    "\n",
    "        for vali_batch_id, vali_data in enumerate(val_loader):\n",
    "            vali_input_data, vali_target, _, vali_setting , vali_metric = vali_data\n",
    "            vali_input_data = vali_input_data.to(device)\n",
    "            vali_target = vali_target.to(device)\n",
    "\n",
    "            vali_pi, vali_mu, vali_sigma = mlp(vali_input_data)\n",
    "\n",
    "            # Compute the error/ metric\n",
    "            vali_nll = cal_metric(vali_pi, vali_mu, vali_sigma, vali_target, N_gaussians, vali_setting,MIN_LOSS,device)\n",
    "            total_vali_metric += vali_nll\n",
    "\n",
    "            # Sum up NLL of all vali data\n",
    "            GT_metric += torch.sum(vali_metric,dim=0)\n",
    "        # total_vali_metric = total_vali_metric/len(val_idx)\n",
    "        # 注意只有1个model时是GT_metric.item()\n",
    "        GT_metric = GT_metric/len(val_idx)\n",
    "        total_vali_metric = total_vali_metric/len(val_idx)\n",
    "\n",
    "        draw_metric(epoch, total_vali_metric.cpu(), GT_metric)\n",
    "        mlp.train()\n",
    "\n",
    "    # Plot the loss in this EPOCH\n",
    "    print(f\"========== IN EPOCH {epoch} the total loss is {epoch_train_loss} ==========\")\n",
    "\n",
    "    # # Save the net structure\n",
    "    # net_file_name = \"net_after_epoch\"+str(epoch)+\".pth\"\n",
    "    # net_path = net_root_path + net_file_name\n",
    "    # torch.save(mlp.state_dict(), net_path)\n",
    "\n",
    "    # Plot loss of this EPOCH\n",
    "    draw_loss(epoch, epoch_train_loss,win_train_epoch_loss_str)\n",
    "    # writer.add_scalars(\"Loss of EPOCH\",{\"Train\":epoch_train_loss},epoch)\n",
    "\n",
    "    # Record the params\n",
    "    # for name,param in params:\n",
    "    #     #参数的梯度\n",
    "    #     #print(\"name:\",name)\n",
    "    #     #print(\"param.grad:\",param.grad)\n",
    "    #     # writer.add_histogram(tag = name +'_grad of EPOCH '+str(epoch),values=param.grad,global_step=epoch)\n",
    "    #     # #参数值\n",
    "    #     writer.add_histogram(tag = name +'data of EPOCH '+str(epoch),values=param.data,global_step=epoch)\n",
    "\n",
    "# f.close()\n",
    "train_end_time = time.time()\n",
    "print(f\"Total training time when epoch= *{EPOCH_NUM}* is *{train_end_time-train_start_time} *s\")\n",
    "writer.close()\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_path_MLE = \"mlp_train=300_MLE.pth\"\n",
    "# model_path_WD = \"mlp_train=300_WD_epoch=80.pth\"\n",
    "model_path_WD = \"mlp_train=300_WD.pth\"\n",
    "model_path_LSE = \"mlp_train=300_LSE.pth\"\n",
    "torch.save(mlp.state_dict(), model_path_LSE)\n",
    "# torch.save(mlp.state_dict(), model_path_MLE)\n",
    "\n",
    "# viz.delete_env(\"001_test\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
