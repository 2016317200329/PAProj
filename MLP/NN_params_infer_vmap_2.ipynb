{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "#-*- coding : utf-8 -*-\n",
    "# coding: utf-8\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0. what for\n",
    "1. 用于infer GT-2的params\n",
    "# 1. preparations\n",
    "## 1.1 全局设置"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import torch.utils.data\n",
    "import mydataset_GT2\n",
    "from functorch import vmap\n",
    "from mydataset_GT2 import myDataset\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from tensorboardX import SummaryWriter\n",
    "from visdom import Visdom\n",
    "from graphviz import Digraph\n",
    "import torchvision\n",
    "from torchviz import make_dot\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pygraphviz as pgv\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import geomloss\n",
    "import time\n",
    "# from GT_model.GT_2.SA_for_PT_funcs_delta_eq1 import *\n",
    "from GT_model.GT_2 import SA_for_PT_funcs_delta_eq1\n",
    "from importlib import reload\n",
    "from functools import partial\n",
    "# import logging\n",
    "# import pytorch_warmup as warmup\n",
    "# logging.basicConfig(filename=\"loss_info\",filemode=\"w\",level=logging.DEBUG)\n",
    "# logger = logging.getLogger(__name__)\n",
    "from torch.cuda.amp import GradScaler,autocast\n",
    "scaler = GradScaler(backoff_factor = 0.1)\n",
    "import scipy\n",
    "from torch.autograd.gradcheck import gradcheck\n",
    "\n",
    "from Config import config_GT2\n",
    "import loss\n",
    "import plot\n",
    "\n",
    "# import GT_model.GT_2.SA_for_PT_funcs_delta_eq1\n",
    "# from GT_model.GT_2.SA_for_PT_funcs_delta_eq1 import *\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Global settings and reloading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "reload(config_GT2)  # 必须reload！！\n",
    "reload(loss)    # 必须reload！！\n",
    "reload(plot)\n",
    "reload(mydataset_GT2)\n",
    "reload(SA_for_PT_funcs_delta_eq1)\n",
    "\n",
    "\n",
    "# from loss import validate\n",
    "from plot import plot_net\n",
    "from plot import plot_alpha\n",
    "from plot import plot_labda\n",
    "\n",
    "opt = config_GT2.DefaultConfig()\n",
    "\n",
    "total_train_step = 0\n",
    "total_test_step = 0\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "bound_alpha = torch.tensor([-0.3,0.3],device=device)\n",
    "bound_labda = torch.tensor([0.01,18],device=device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Dataset and Dataloader\n",
    "1. DataLoader中的`shuffer=True`表示在每一次epoch中都打乱所有数据的顺序，然后以batch为单位从头到尾按顺序取用数据。这样的结果就是不同epoch中的数据都是乱序的,设置随机种子的作用就是让你的每一次训练都乱的一样，\n",
    "\n",
    "## 2.1 Dataset and spliting\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 2., 3.]])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_metric_list=[]\n",
    "data_1 = [1,2,3]\n",
    "target_metric_list.append(torch.tensor(data_1, dtype=torch.int64))\n",
    "target_metric_tensor = torch.stack(target_metric_list).float()\n",
    "del target_metric_list\n",
    "target_metric_tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed = 26\n",
    "setup_seed(seed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 读取data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "dataset = myDataset(opt.train_path, opt.target_path_metric,opt.target_path_loss, opt.params_opitim_path, opt.data_key_path, opt.NLL_metric_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 产生index的乱序排列"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 493  129  826  231  500  334 1244  835  580  989  479  419  804  670\n",
      "   69  674  611  894  276 1049  153  425  321  935  782 1170 1069 1037\n",
      "  478  434 1203 1220  636    4  262  682  516 1097 1082   63  898 1204\n",
      " 1239  812  716   50  272  846 1199 1116  596  111  901  925  687 1180\n",
      "  986  363  311  664  273  730 1292  737  396  190  285 1241  338 1096\n",
      "  693 1190 1034 1281 1171  414  348  232  532  535 1166   32  483  926\n",
      "  953 1163  110  562 1173  374  341 1025  697 1168  105 1031 1035  726\n",
      "   93 1086  771  827  579 1129   74 1238  328 1023  767 1211   44  843\n",
      "  116 1104  678  108  174  976  359 1046  983  457  647 1134  654  159\n",
      "  604 1164 1277 1209  447  605  526  234  946  969  295 1028  191  905\n",
      "  442 1218  961  233   57  563  757 1274  863  630  489 1193  367  164\n",
      " 1059  155 1078  525  197  181  281 1165  956   17  947  623 1206  777\n",
      "  732  797  984  227  634  944  606  199  899  426  375  783  954 1077\n",
      "  924 1074 1039  694  666 1118  149  655  286  465  381  798  909  758\n",
      "  218  890  619  274  522 1283  539  102 1167    5  101  383  247  696\n",
      "  449   49  167 1248  641  585 1041  113  848 1092  441  501  371 1000\n",
      "  417   71   94  796  923  581  854  133 1237 1284 1067  962  870  892\n",
      "  292  811  829 1040  194  575  861  244 1080 1188  992 1070  212  484\n",
      "  968  573  548  264  823  862 1210 1083  699   40   55  583  903  736\n",
      "  701  210  599  211  922   80  397  760 1122   72  134  921  195 1261\n",
      "  519  582  427  324  561 1051 1084  385  331  628  550 1055 1216  665\n",
      "  939  613  911 1149  347 1032  871  377  570  251 1251  104 1120  781\n",
      " 1197  206  810 1291  474  200  776  428  139 1138  667  702  490 1172\n",
      "  893    2  517  955 1229 1150 1269  839   67  395  635  683  475  429\n",
      " 1257  421 1152  866  713 1162   76  184  248  751  162  145 1081  163\n",
      "  624  239  491  734  178  496 1015  904 1088   62  960  207  836  373\n",
      "  832 1064  936  979  380   48  456  157  219 1011  653  940  626  822\n",
      "  378  245 1293   53  966   64  499 1181  891  877 1252  235  182   26\n",
      "  270 1115 1001  593  467  309  471  221  967 1021  770  352  988  114\n",
      " 1160  271 1091  469 1217  547  106  799 1111  130  824  188  120  806\n",
      "   10  497   70  842  512   35  679 1250   46  632  942 1105 1221  551\n",
      "  314  350 1050  138  931  485  275  205  379   84   18 1159  346  741\n",
      "  420  222  601 1057  640 1245  290   25  656  873  267  364  650  597\n",
      " 1061 1056  289   66 1048  462 1144  316  662 1108  369  633  735  107\n",
      "  791   11 1228  229  146  997 1072  998  283  121  156 1094  637  937\n",
      " 1014 1124  568 1145  657 1253  712  780  841  685  144  642 1213 1142\n",
      " 1201  951  977  332  520  391  537  508  772 1185  355 1066  152 1297\n",
      "  450  453 1133  745 1004 1200  125  530  339  785  934 1263  123  158\n",
      "  887  790 1233   58  659  557  980  881  306  408  513  753  112  778\n",
      " 1119  859  337  386  840   19  773 1036 1302  763  192  413  845  574\n",
      "  805  533  406 1235 1043  738  136  661   77 1026  724 1158 1127 1225\n",
      "  349  671 1268 1029  750  558  298 1090  795  186  914  756 1130 1296\n",
      "  354  917  884  948  132  480  691  294  126  838  246  365  587 1240\n",
      " 1045  933  415  761  625 1098  370   12  629 1246  627 1161 1151  103\n",
      "  261  994   39  433   85  818  612  718 1141 1103  602  329   14  888\n",
      "  639 1264  552  269  615 1179 1273 1075  688  230  834   27  768  317\n",
      "  451  455  676  719 1112  335  236  284  727 1305 1258  784 1030  684\n",
      "  459  266  431  932  591  509  492  800  902 1114 1016 1198  407  488\n",
      " 1073  124  646  907   43  333   81   42  538 1234  468  242  970 1224\n",
      "  857  543  849  506  224  816  487  572  746  376 1020  297   59  308\n",
      "  438  622  361 1085 1282  577  486  589 1276 1013  618  387  555  366\n",
      " 1140 1247  357   87  389 1008  344 1178  154  240 1219  165  692   36\n",
      " 1068  410  950  393   91  584   68  223  910  895  952 1280  400  498\n",
      "  930  250  856 1131  521  559  554  161 1123  322  964  482 1019  808\n",
      "  740 1132  620 1121  225 1054  920  177  999 1047 1052  709  788 1271\n",
      "  131   33 1195  173  886  473  853  813  814  310  733  544  187 1298\n",
      "  801 1191   82  889 1205  327  238  353  996  700 1071  528 1128 1278\n",
      "  912 1012   61 1236  594  943  541 1266 1187  115   21  504  981 1113\n",
      "  531  566 1186  446  603  394  578  237 1060  411  463  707 1176 1107\n",
      "  278  792  963 1143   29 1215 1267 1087  323  793  345  179  743  710\n",
      "  717   92  445 1153  171 1192 1017  794  722 1005  755 1010  189   15\n",
      "  876  444  418  305  766  330  304  928 1301 1157  978  815 1155 1102\n",
      "  243  631  729 1256  958  725  254  985   52 1024   88  990  176   30\n",
      "  714   86  927  649  869  959  540  302  830  196  852  392 1058  608\n",
      " 1289  645  680  973  203  454  127  398  546  277  135  215  362   37\n",
      "  128 1196 1304  343]\n"
     ]
    }
   ],
   "source": [
    "DATA_len = dataset.__len__()\n",
    "shuffled_indices = np.random.permutation(DATA_len)\n",
    "\n",
    "train_idx = shuffled_indices[:int(opt.train_pct*DATA_len)]\n",
    "tmp = int((opt.train_pct+opt.vali_pct)*DATA_len)\n",
    "val_idx = shuffled_indices[int(opt.train_pct*DATA_len):tmp]\n",
    "test_idx = shuffled_indices[tmp:]\n",
    "\n",
    "print(train_idx)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 根据这个乱序排列抽取dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Dataloader and collating\n",
    "1. 主要是对label数据进行collate\n",
    "    - 按照batch中的最大target data长度进行padding，padding with 0\n",
    "2. 返回的结果多一个batch dim,比如下面的`5`\n",
    "    - After collating:\n",
    "        - `torch.Size([5, 3, 300]),torch.Size([5, 87, 2])`\n",
    "        - `87`是最长的targets data长度"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def my_collate_fn(data):\n",
    "# 这里的data是一个list， list的元素是元组: (self.data, self.label)\n",
    "# collate_fn的作用是把[(data, label),(data, label)...]转化成([data, data...],[label,label...])\n",
    "# 假设self.data的一个data的shape为(channels, length), 每一个channel的length相等,\n",
    "# data[索引到index(batch)][索引到data或者label][索引到channel]\n",
    "\n",
    "    data_list = []              # training data\n",
    "    target_metric_list = []     # target data for computing metric of NN\n",
    "    target_loss_list = []       # target data for computing loss of NN, (ls T)\n",
    "\n",
    "    target_params_list = []       # target data for computing loss of NN, (TARGET=5 or 1)\n",
    "    setting_list = []\n",
    "    metric_list = []\n",
    "\n",
    "    data_len = len(data)        # 读进来的data batch的大小\n",
    "\n",
    "    batch = 0\n",
    "\n",
    "    while batch < data_len:\n",
    "        data_list.append(torch.tensor(data[batch][0]))\n",
    "        # target_data只保留N这一列\n",
    "        # print(\"target[0] shape:\",torch.tensor(data[batch][1][:,0]).shape)\n",
    "        target_metric_list.append(torch.tensor(data[batch][1][:,0], dtype=torch.int64))\n",
    "        target_loss_list.append(torch.tensor(data[batch][2][:,0], dtype=torch.int64))\n",
    "        target_params_list.append(torch.tensor(data[batch][3]))\n",
    "        setting_list.append(torch.tensor(data[batch][4]))\n",
    "        metric_list.append(torch.tensor(data[batch][5]))\n",
    "        batch += 1\n",
    "\n",
    "\n",
    "    # Pad target data with zeros\n",
    "    target_metric_padded = pad_sequence(target_metric_list,batch_first=True)\n",
    "    target_loss_padded = pad_sequence(target_loss_list,batch_first=True)\n",
    "    target_metric_tensor = target_metric_padded.float()\n",
    "    target_loss_padded = target_loss_padded.float()\n",
    "\n",
    "    data_tensor = torch.stack(data_list).float()\n",
    "    target_params_tensor = torch.stack(target_params_list).float()\n",
    "    setting_tensor = torch.stack(setting_list).float()\n",
    "    metric_tensor = torch.stack(metric_list).float()\n",
    "\n",
    "\n",
    "    return data_tensor, target_metric_tensor, target_loss_padded, target_params_tensor, setting_tensor, metric_tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.4 DataLoader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = dataset,batch_size = 10, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(train_idx), collate_fn = my_collate_fn)\n",
    "\n",
    "val_loader = DataLoader(dataset = dataset,batch_size = 10, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(val_idx),collate_fn = my_collate_fn)\n",
    "\n",
    "test_loader = DataLoader(dataset = dataset,batch_size = 1, shuffle=False, num_workers=0, drop_last=False, sampler=SubsetRandomSampler(test_idx),collate_fn = my_collate_fn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. The Net and Init\n",
    "1. BatchNorm1d: The mean and std are calculated per-dimension over the mini-batches\n",
    "## 3.2 MLP-1\n",
    "### 3.2.1 1层MLP\n",
    "1. output dim=2，一层线性层"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# 参数量34\n",
    "# 1层MLP+MDN\n",
    "class MLP_1_1(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=8,affine=True)\n",
    "\n",
    "        self.block_alpha = nn.Sequential(\n",
    "            nn.Linear(8, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.block_labda = nn.Sequential(\n",
    "            nn.Linear(8, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.squeeze(x,dim=1)\n",
    "        x = self.BN1(x)\n",
    "\n",
    "        alpha = self.block_alpha(x)\n",
    "        labda = self.block_labda(x)\n",
    "\n",
    "        # Clamp\n",
    "        alpha = torch.clamp(alpha,min=bound_alpha[0],max=bound_alpha[1])\n",
    "        labda = torch.clamp(labda,min=bound_labda[0],max=bound_labda[1])\n",
    "\n",
    "        # alpha.register_hook(save_grad(\"alpha\"))\n",
    "        # labda.register_hook(save_grad(\"labda\"))\n",
    "        return alpha,labda"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "       BatchNorm1d-1                    [-1, 8]              16\n",
      "            Linear-2                    [-1, 1]               9\n",
      "              Tanh-3                    [-1, 1]               0\n",
      "            Linear-4                    [-1, 1]               9\n",
      "              ReLU-5                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 34\n",
      "Trainable params: 34\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP_1_1()\n",
    "mlp = mlp.to(device=device)\n",
    "summary(mlp, (1,8))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2.2 二层"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# 98\n",
    "# 1层MLP+MDN\n",
    "class MLP_1_2(nn.Module):\n",
    "    # code->generate->override methods\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.BN1 = nn.BatchNorm1d(num_features=8,affine=True)\n",
    "\n",
    "        self.block_alpha = nn.Sequential(\n",
    "            nn.Linear(8, 4),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(4, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.block_labda = nn.Sequential(\n",
    "            nn.Linear(8, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.squeeze(x,dim=1)\n",
    "        x = self.BN1(x)\n",
    "\n",
    "        alpha = self.block_alpha(x)\n",
    "        labda = self.block_labda(x)\n",
    "\n",
    "        # Clamp\n",
    "        alpha = torch.clamp(alpha,min=bound_alpha[0],max=bound_alpha[1])\n",
    "        labda = torch.clamp(labda,min=bound_labda[0],max=bound_labda[1])\n",
    "        return alpha,labda"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "       BatchNorm1d-1                    [-1, 8]              16\n",
      "            Linear-2                    [-1, 4]              36\n",
      "              Tanh-3                    [-1, 4]               0\n",
      "            Linear-4                    [-1, 1]               5\n",
      "              Tanh-5                    [-1, 1]               0\n",
      "            Linear-6                    [-1, 4]              36\n",
      "              ReLU-7                    [-1, 4]               0\n",
      "            Linear-8                    [-1, 1]               5\n",
      "              ReLU-9                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 98\n",
      "Trainable params: 98\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP_1_2()\n",
    "mlp = mlp.to(device=device)\n",
    "summary(mlp, (1,8))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Loss\n",
    "## 4.1 grad hooks\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "############# Tensor #############:\n",
    "def C(t,b):\n",
    "    return 0.2*t*b\n",
    "\n",
    "def f_ts(x, alpha, device):\n",
    "    # -alpha*x不能超过85，否则overflow\n",
    "    y1 = torch.clamp(-alpha*x,max=torch.tensor(85.,device=device))#.to(device)       # torch.exp(x), x<85\n",
    "    y2 = (1-torch.exp(y1))  #.to(device)\n",
    "\n",
    "    return y2\n",
    "\n",
    "def f_2_ts(x, alpha,device):\n",
    "    # -alpha*x不能超过85，否则overflow\n",
    "    y1 = torch.min(-alpha*x,torch.tensor(85.)).to(device)       # torch.exp(x), x<85\n",
    "    y2 = (torch.exp(y1))#.to(device)\n",
    "\n",
    "    return y2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# root = U_{t-1}\n",
    "def f_Equi_ts(t, T, v, d, b, alpha, labda, device):\n",
    "\n",
    "    tmp = (v-d*(t-1)-C((t-2),b) - b).to(device)\n",
    "\n",
    "    if (tmp>0):\n",
    "\n",
    "        root = (labda*f_ts(C(t-2,b),alpha,device) + f_ts(((v-d*(t-1)-C((t-2),b) - b)),alpha,device)) / (labda*f_ts(C(t-2,b)+b,alpha,device) + f_ts(((v-d*(t-1)-C((t-2),b) - b)),alpha,device))\n",
    "\n",
    "    else:\n",
    "\n",
    "        root = (1- f_2_ts(-(v-(t-1)*d-b),alpha,device)) / (f_2_ts(b,alpha,device) - f_2_ts(-(v-(t-1)*d-b),alpha,device))\n",
    "\n",
    "    assert not torch.isnan(root.detach()),f\"root = {root.detach()} and t, T, v, d, b, alpha, labda = {t, T, v, d, b, alpha.detach(), labda.detach()}\"\n",
    "    return root\n",
    "\n",
    "# 返回log(root)\n",
    "def f_Equi_ts_log(t, T, v, d, b, alpha, labda, device):\n",
    "\n",
    "    tmp = (v-d*(t-1)-C((t-2),b) - b).to(device)\n",
    "\n",
    "    if (tmp>0):\n",
    "\n",
    "        root = torch.log(torch.abs((labda*f_ts(C(t-2,b),alpha,device) + f_ts(((v-d*(t-1)-C((t-2),b) - b)),alpha,device)))) \\\n",
    "            - torch.log(torch.abs((labda*f_ts(C(t-2,b)+b,alpha,device) + f_ts(((v-d*(t-1)-C((t-2),b) - b)),alpha,device))))\n",
    "\n",
    "    else:\n",
    "        # 分母!=0\n",
    "        # root = torch.log(torch.abs((1- f_2_ts(-(v-(t-1)*d-b),alpha,device)))) \\\n",
    "        #     - (-alpha*b)- torch.log(torch.abs((1 - f_2_ts(-(v-(t-1)*d),alpha,device))))\n",
    "\n",
    "        root = torch.log(torch.abs((1- f_2_ts(-(v-(t-1)*d-b),alpha,device)))) \\\n",
    "            - torch.log(torch.abs((f_2_ts(b,alpha,device) - f_2_ts(-(v-(t-1)*d-b),alpha,device))))\n",
    "\n",
    "    assert not torch.isnan(root.detach()),f\"root = {root.detach()}, tmp = {tmp}, and t, T, v, d, b, alpha, labda = {t, T, v, d, b, alpha.detach(), labda.detach()}\"\n",
    "    assert not torch.isinf(root.detach()),f\"root = {root.detach()}, tmp = {tmp}, and t, T, v, d, b, alpha, labda = {t, T, v, d, b, alpha.detach(), labda.detach()}\"\n",
    "\n",
    "    return root\n",
    "\n",
    "\n",
    "def f_Equi_ts_log_vmap(t, v, d, b, alpha, labda):       # , device\n",
    "    root = torch.where((v-d*(t-1)-C((t-2),b) - b)>0,\n",
    "                        torch.log(torch.abs(\n",
    "                                (labda*f_ts(C(t-2,b),alpha,device) + f_ts(((v-d*(t-1)-C((t-2),b) - b)),alpha,device)) )) \\\n",
    "                        - torch.log(torch.abs(\n",
    "                                (labda*f_ts(C(t-2,b)+b,alpha,device) + f_ts(((v-d*(t-1)-C((t-2),b) - b)),alpha,device)) )),\n",
    "                        torch.log(torch.abs(\n",
    "                            (1- f_2_ts(-(v-(t-1)*d-b),alpha,device)) )) \\\n",
    "                        - torch.log(torch.abs((f_2_ts(b,alpha,device) - f_2_ts(-(v-(t-1)*d-b),alpha,device)) ))\n",
    "                        )\n",
    "    return root"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "\n",
    "def save_grad(name):\n",
    "    print(\"****\")\n",
    "    def hook(grad):\n",
    "        print(f\"name={name}, grad={grad}\")\n",
    "    return hook\n",
    "\n",
    "\n",
    "def hook_forward_fn(module, input, output):\n",
    "    print(\"It's forward: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"input: {input}\")\n",
    "    print(f\"output: {output}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "def hook_backward_fn(module, grad_input, grad_output):\n",
    "    print(\"It's backward: \")\n",
    "    print(f\"module: {module}\")\n",
    "    print(f\"grad_input: {grad_input}\")\n",
    "    print(f\"grad_output: {grad_output}\")\n",
    "    print(\"=\"*20)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 get U and LEN,T\n",
    "- 注意T的逻辑：\n",
    "    - 1.首先计算T。而且target data中没有包含>T的部分\n",
    "    - 2.再之，计算LEN: LEN=max(target_T)，没必要计算多余的U[t]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def get_LEN_T_ts(v,b,d, max_T):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        v:\n",
    "        b:\n",
    "        d:\n",
    "        max_T:\n",
    "\n",
    "    Returns:\n",
    "        LEN: Length of U vector\n",
    "        T: max possible duration: max_T or theoratic T\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if d == 0:          # fixed-price\n",
    "        # T = np.inf\n",
    "        T = max_T       # 最多计算target data需要的duration\n",
    "    else:               # asc-price\n",
    "        T = torch.floor((v - b) / d)\n",
    "\n",
    "    LEN = max_T\n",
    "    return LEN,T"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def get_U_GT2_ts(LEN, T,v, d, b, alpha, labda, eps = 1e-30, device = device):\n",
    "\n",
    "    # U: the prob. that someone offers a bid in t_th round\n",
    "    # U_i = torch.tensor([eps] * (int(LEN.detach().cpu()) + 2),device=device)\n",
    "    # U_i[0], U_i[1] = torch.tensor(0.), torch.tensor(1.)  # u[0]用不到，设为1,u[1]=1保证auction至少1轮\n",
    "    # idx = torch.arange(2,len(U_i))\n",
    "    # U_i[2:] = torch.tensor([f_Equi_ts(t, T, v, d, b, alpha, labda,device) for t in idx])\n",
    "\n",
    "    U_head = torch.tensor([1.,1.],requires_grad=True,device=device)  # u[0]用不到,u[1]=1保证auction至少1轮\n",
    "\n",
    "    idx = torch.arange(2,LEN+1)\n",
    "    for t in idx:\n",
    "        # 记录一下，work了。。。。\n",
    "        if t <= T:\n",
    "            U_head = torch.cat([U_head, f_Equi_ts(t, T, v, d, b, alpha, labda,device)])\n",
    "        else:   # 计算metric才需要这个else\n",
    "            U_head = torch.cat([U_head,torch.tensor([eps],requires_grad=True,device=device)])\n",
    "    # 这样写不work？？but why\n",
    "    # U_tail = torch.tensor([f_Equi_ts(t, T,v, d, b, alpha, labda,device) for t in idx],requires_grad=True)#.to(device)\n",
    "    # alpha.register_hook(save_grad(\"alpha\"))       # OK\n",
    "    # labda.register_hook(save_grad(\"labda\"))       # OK\n",
    "    U_T_1 = torch.tensor([eps],requires_grad=True,device=device)  # u[T+1]表示最后拍卖在T+1轮发生的概率为0\n",
    "    # U_head.register_hook(save_grad(\"U_head\"))\n",
    "    # print(\"U_head:\",U_head)\n",
    "    U_i = torch.concat([U_head,U_T_1])#.to(device)     # 减少原地修改，使用concat拼接\n",
    "    U_i = U_i.to(device)\n",
    "\n",
    "    # eps < U < 1-eps\n",
    "    # 实际上在pytorch中 torch.tensor(1-eps)=1.\n",
    "    U = torch.clip(U_i,torch.tensor(eps,device=device),torch.tensor(1-eps,device=device))\n",
    "    return U"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 返回U_log"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def get_U_GT2_ts_log(LEN, T, v, d, b, alpha, labda, eps = 1e-30, device = device):\n",
    "\n",
    "    # U: the prob. that someone offers a bid in t_th round\n",
    "    # U_i = torch.tensor([eps] * (int(LEN.detach().cpu()) + 2),device=device)\n",
    "    # U_i[0], U_i[1] = torch.tensor(0.), torch.tensor(1.)  # u[0]用不到，设为1,u[1]=1保证auction至少1轮\n",
    "    # idx = torch.arange(2,len(U_i))\n",
    "    # U_i[2:] = torch.tensor([f_Equi_ts(t, T, v, d, b, alpha, labda,device) for t in idx])\n",
    "\n",
    "    # log(1) = 0.\n",
    "    # U_head = torch.tensor([0.,0.],requires_grad=True,device=device)  # u[0]用不到,u[1]=1保证auction至少1轮\n",
    "    U_head = torch.tensor([0.,0.],requires_grad=True)  # u[0]用不到,u[1]=1保证auction至少1轮\n",
    "\n",
    "    idx = torch.arange(2,LEN+1).to(device)\n",
    "\n",
    "    # 方法一：\n",
    "    for t in idx:\n",
    "        # 记录一下，work了。。。。\n",
    "        U_head = torch.cat([U_head, f_Equi_ts_log(t, T, v, d, b, alpha, labda,device)])\n",
    "\n",
    "    # 这样写不work？？but why\n",
    "    # U_tail = torch.tensor([f_Equi_ts(t, T,v, d, b, alpha, labda,device) for t in idx],requires_grad=True)#.to(device)\n",
    "    # alpha.register_hook(save_grad(\"alpha\"))       # OK\n",
    "    # labda.register_hook(save_grad(\"labda\"))       # OK\n",
    "    U_T_1 = torch.log(torch.tensor([eps],requires_grad=True,device=device))  # u[T+1]表示最后拍卖在T+1轮发生的概率为0\n",
    "    # U_head.register_hook(save_grad(\"U_head\"))\n",
    "    # print(\"U_head:\",U_head)\n",
    "    U_i_log = torch.concat([U_head,U_T_1])#.to(device)     # 减少原地修改，使用concat拼接\n",
    "\n",
    "    U_i_log = U_i_log.to(device)\n",
    "\n",
    "    # eps < U < 1-eps\n",
    "    # 实际上在pytorch中 torch.tensor(1-eps)=1.\n",
    "    # U = torch.clip(U_i,torch.tensor(eps,device=device),torch.tensor(1-eps,device=device))\n",
    "    return U_i_log"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 返回U_log，并且使用vmap：\n",
    "    - 弊端，device是个bug，目前没办法传"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def get_U_GT2_ts_log_vmap(LEN, T, v, d, b, alpha, labda, eps = 1e-30, device = device):\n",
    "\n",
    "    # U: the prob. that someone offers a bid in t_th round\n",
    "    # log(1) = 0.\n",
    "    U_head = torch.tensor([0.,0.],requires_grad=True,device=device)  # u[0]用不到,u[1]=1保证auction至少1轮\n",
    "\n",
    "    idx = torch.arange(2,LEN+1).to(device)\n",
    "\n",
    "    # 方法二：使用vmap：\n",
    "    v_extend = torch.expand_copy(v,size=idx.shape).to(device)    # 必须扩张到和idx同一个size否则报错\n",
    "    b_extend = torch.expand_copy(b,size=idx.shape).to(device)\n",
    "    d_extend = torch.expand_copy(d,size=idx.shape).to(device)\n",
    "    alpha_extend = torch.expand_copy(alpha,size=idx.shape).to(device)\n",
    "    labda_extend = torch.expand_copy(labda,size=idx.shape).to(device)\n",
    "\n",
    "    f_Equi_vec = vmap(f_Equi_ts_log_vmap)  # [N, D], [N, D] -> [N]\n",
    "    U_root = f_Equi_vec(idx, v_extend, d_extend, b_extend, alpha_extend, labda_extend)\n",
    "\n",
    "    U_T_1 = torch.log(torch.tensor([eps],requires_grad=True,device=device))  # u[T+1]表示最后拍卖在T+1轮发生的概率为0\n",
    "    U_i_log = torch.concat([U_head,U_root,U_T_1])                            # 减少原地修改，使用concat拼接\n",
    "\n",
    "    U_i_log = U_i_log.to(device)\n",
    "\n",
    "    # eps < U < 1-eps\n",
    "    # 实际上在pytorch中 torch.tensor(1-eps)=1.\n",
    "    # U = torch.clip(U_i,torch.tensor(eps,device=device),torch.tensor(1-eps,device=device))\n",
    "\n",
    "    del idx\n",
    "    del v_extend\n",
    "    del b_extend\n",
    "    del d_extend\n",
    "    return U_i_log"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# 求和从1到target[i]\n",
    "# def get_sum_1_vmap(U_log_cumsum, target):\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.3 NLL loss\n",
    "1. loss 用于处理NLL loss\n",
    "2. loss function:\n",
    "\n",
    "- 注意凡是计算中会出现nan（root求解，log of U的求解等，都已经用`assert`提示），因此出现nan问题，一定不是数值计算的问题"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def loss_fn_2(input_data, Alpha, Labda, Target_data, eps, device):\n",
    "    # 注意：P[i][t] = U[i][1]*U[i][2]*...*(1-U[i][t+1])\n",
    "    loss_sum = torch.tensor(0.,device=device,requires_grad=True)\n",
    "    for i in range(len(input_data)):\n",
    "\n",
    "        # Get target data\n",
    "        target = Target_data[i, :].long()\n",
    "\n",
    "        # Solve for U from Equi. condt.\n",
    "        # Get settings\n",
    "        setting = input_data[i, :, 0:3]\n",
    "        d = setting[0, 0]\n",
    "        b = setting[0, 1]\n",
    "        v = setting[0, 2]\n",
    "\n",
    "        # 1.首先计算LEN,T\n",
    "        LEN,T = get_LEN_T_ts(v,b,d,max(target))\n",
    "        # 2.然后筛选target data：大于T的没必要，算不了\n",
    "\n",
    "        idx = torch.nonzero(target)\n",
    "        target_nonzero = target[idx]  #.reshape(1,-1).squeeze().long()      # int\n",
    "\n",
    "        # U_i最大必须可以访问到U_i[LEN+1],因为计算P[LEN]需要用到U[LEN+1]\n",
    "\n",
    "        # 方法一：先求U再求U_log\n",
    "        U = get_U_GT2_ts(LEN, T, v, d, b, Alpha[i,:], Labda[i,:], eps = eps, device=device)\n",
    "        assert not (torch.any(torch.isnan(U.detach()))), f\"U has NaN and U = {U.detach()}, and It should NOT be NaN actually\"\n",
    "        # U.register_hook(save_grad('U'))\n",
    "        # U_i最大必须可以访问到U_i[LEN+1],因为计算P[LEN]需要用到U[LEN+1]\n",
    "        assert max(target_nonzero) + 2 == len(U.detach()),\"U长度不正确\"\n",
    "\n",
    "        U_log = torch.log(U)#.to(device)\n",
    "        assert not (torch.any(torch.isnan(U_log.detach()))), f\"U_log has NaN and U_log = {U_log.detach()}, and U = {U.detach()}\"\n",
    "\n",
    "        U_log_cumsum = torch.cumsum(U_log,dim=0)#.to(device)\n",
    "        # assert U_log_cumsum.shape == U.shape, \"U_log_cumsum.shape != U.shape\"\n",
    "\n",
    "        # U_log_cumsum_extend.shape: [len(target_nonzero), LEN]\n",
    "        U_log_cumsum_extend = torch.repeat_interleave(U_log_cumsum[None,:], repeats=len(target_nonzero),dim=0)#.to(device)\n",
    "\n",
    "        # print(\"U_log_cumsum_extend.shape shoule be: [len(target_nonzero),len(U)]\")\n",
    "        # U_log_cumsum.register_hook(save_grad('U_log_cumsum'))\n",
    "\n",
    "        U_sum_1_idx = target_nonzero\n",
    "        U_sum_2_idx = target_nonzero.squeeze()+1\n",
    "        U_sum_1 = torch.gather(U_log_cumsum_extend,dim=1,index=U_sum_1_idx)#.to(device)\n",
    "        U_sum_2 = torch.log(torch.max(1-torch.gather(U,0,index=U_sum_2_idx),torch.tensor(eps,device=device)))#.to(device)\n",
    "        loss_sum  = U_sum_1.sum() + U_sum_2.sum() + loss_sum\n",
    "        assert not torch.isnan(U_sum_1.detach().sum()),\"U_sum_1=nan\"\n",
    "        assert not torch.isnan(U_sum_2.detach().sum()),\"U_sum_1=nan\"\n",
    "        # loss_sum.register_hook(save_grad('loss_sum'))\n",
    "\n",
    "    # 除以batchsize\n",
    "    # loss_sum.register_hook(save_grad('loss_sum'))\n",
    "    # print(\"ALPHA:register_hook\")\n",
    "    # handler = Alpha.register_hook(save_grad('Alpha'))\n",
    "\n",
    "    return -loss_sum/len(input_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 直接得到U_log\n",
    "- 忽略U_sum_2？"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def loss_fn_3(input_data, Alpha, Labda, Target_data, eps, device):\n",
    "    # 注意：P[i][t] = U[i][1]*U[i][2]*...*(1-U[i][t+1])\n",
    "    loss_sum = torch.tensor(0.,device=device,requires_grad=True)\n",
    "    for i in range(len(input_data)):\n",
    "\n",
    "        # Get target data\n",
    "        target = Target_data[i, :].long()\n",
    "\n",
    "        # Solve for U from Equi. condt.\n",
    "        # Get settings\n",
    "        setting = input_data[i, :, 0:3]\n",
    "        d = setting[0, 0]\n",
    "        b = setting[0, 1]\n",
    "        v = setting[0, 2]\n",
    "\n",
    "        # 1.首先计算LEN,T\n",
    "        LEN,T = get_LEN_T_ts(v,b,d,max(target))\n",
    "        # 2.然后筛选target data：大于T的没必要，算不了\n",
    "\n",
    "        idx = torch.nonzero(target)\n",
    "        target_nonzero = target[idx]  #.reshape(1,-1).squeeze().long()      # int\n",
    "\n",
    "        # U_i最大必须可以访问到U_i[LEN+1],因为计算P[LEN]需要用到U[LEN+1]\n",
    "        # 方法二：直接求U_log，并且使用vmap进行并行运算\n",
    "        U_log = get_U_GT2_ts_log_vmap(LEN, T, v, d, b, Alpha[i,:], Labda[i,:], eps = eps, device=device)\n",
    "        # assert not (torch.any(torch.isnan(U_log.detach()))), f\"U_log has NaN and U_log = {U_log.detach()}\"\n",
    "\n",
    "        U_log_cumsum = torch.cumsum(U_log,dim=0)#.to(device)\n",
    "        # assert U_log_cumsum.shape == U.shape, \"U_log_cumsum.shape != U.shape\"\n",
    "\n",
    "        # U_log_cumsum_extend.shape: [len(target_nonzero), LEN]\n",
    "\n",
    "        # 减少显存的方法一：\n",
    "        # 无非是求和from 1 to i in target_nonzero\n",
    "        # U_sum_1 = get_sum_1_vmap(U_log_cumsum,target_nonzero)\n",
    "\n",
    "        # 通过复制的方法二：\n",
    "        U_log_cumsum_extend = torch.repeat_interleave(U_log_cumsum[None,:], repeats=len(target_nonzero),dim=0)#.to(device)\n",
    "\n",
    "        # print(\"U_log_cumsum_extend.shape shoule be: [len(target_nonzero),len(U)]\")\n",
    "        # U_log_cumsum.register_hook(save_grad('U_log_cumsum'))\n",
    "\n",
    "        U_sum_1_idx = target_nonzero\n",
    "        U_sum_2_idx = target_nonzero.squeeze()+1\n",
    "        U_sum_1 = torch.gather(U_log_cumsum_extend,dim=1,index=U_sum_1_idx)#.to(device)\n",
    "        # U_sum_2 = torch.log(torch.max(1-torch.gather(U,0,index=U_sum_2_idx),torch.tensor(eps,device=device)))#.to(device)\n",
    "        # U_sum_2 = 1-torch.gather(U_log,0,index=U_sum_2_idx)#.to(device)\n",
    "        # 或许可以：用log(U[t+1])表示log(1-U[t+1])\n",
    "        # 潜在的问题：gather之后的值太小，因此exp之后非常接近1，用1-去之后是一个接近0的值，log=inf\n",
    "        U_sum_2 = torch.log(torch.max(1-torch.exp(torch.gather(U_log,0,index=U_sum_2_idx)),torch.tensor(opt.MIN_LOSS)))#.to(device)\n",
    "        # print(f\"torch.gather(U_log,0,index=U_sum_2_idx)={torch.gather(U_log,0,index=U_sum_2_idx)}\")\n",
    "        loss_sum = U_sum_1.sum() + U_sum_2.sum() + loss_sum\n",
    "\n",
    "        # print(f\"old method: U_sum_1.sum() ={U_sum_1.sum().detach().cpu()}, U_sum_2.sum()={U_sum_2.sum().detach().cpu()}\")\n",
    "\n",
    "        # assert not torch.isnan(U_sum_1.detach().sum()),f\"U_sum_1={U_sum_1}\"\n",
    "        # assert not torch.isinf(U_sum_1.detach().sum()),f\"U_sum_1={U_sum_1}\"\n",
    "        # assert not torch.isnan(U_sum_2.detach().sum()),f\"U_sum_2={U_sum_2}\"\n",
    "        # assert not torch.isinf(U_sum_2.detach().sum()),f\"U_sum_2={U_sum_2}\"\n",
    "        # loss_sum.register_hook(save_grad('loss_sum'))\n",
    "\n",
    "    # 除以batchsize\n",
    "    # loss_sum.register_hook(save_grad('loss_sum'))\n",
    "    # print(\"ALPHA:register_hook\")\n",
    "    # handler = Alpha.register_hook(save_grad('Alpha'))\n",
    "    return -loss_sum/len(input_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.3 validation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def validate_params(mlp, test_loader, eps, device):\n",
    "\n",
    "    loss_sum = torch.tensor(0., device=device, requires_grad=False)\n",
    "    # GT_metric = torch.tensor([0.,0.]).reshape(1,2)\n",
    "    GT_metric = torch.tensor([0.,0.,0.,0.,0.]).reshape(1,-1)\n",
    "    cnt = len(test_idx)\n",
    "    for batch_id, data in enumerate(test_loader):\n",
    "\n",
    "        input_data, target_metric,_, target_params_data, _, metric_data= data\n",
    "\n",
    "        # print(f\"---- {batch_id} batch----\")\n",
    "        # Do the inference\n",
    "        input_data = input_data.to(device)\n",
    "        target_data = target_metric.to(device)\n",
    "        # target_params_data = target_params_data.to(device)      # params inferred by SA\n",
    "\n",
    "        Alpha, Labda = mlp(input_data)\n",
    "\n",
    "        Alpha = Alpha.detach().cpu().numpy()\n",
    "        Labda = Labda.detach().cpu().numpy()\n",
    "\n",
    "        GT_metric += torch.sum(metric_data,dim=0)\n",
    "        # print(f\"Alpha, Labda = {Alpha, Labda}\")\n",
    "        for i in range(len(Alpha)):\n",
    "            # Get target data\n",
    "            target = target_data[i, :]\n",
    "            idx = torch.nonzero(target)\n",
    "            target_nonzero = target[idx].detach().cpu().squeeze().numpy()\n",
    "            target_ls = [int(x) for x in target_nonzero]        # 统一一下，target在这里是int list\n",
    "\n",
    "            # Solve for U from Equi. condt.\n",
    "            # Get settings\n",
    "            setting = input_data[i, :, 0:3]\n",
    "            d = setting[0, 0].detach().cpu().numpy().item()\n",
    "            b = setting[0, 1].detach().cpu().numpy().item()\n",
    "            v = setting[0, 2].detach().cpu().numpy().item()\n",
    "\n",
    "            #### metric计算参考自 GT_2_1\n",
    "            LEN,T =  SA_for_PT_funcs_delta_eq1.get_LEN_T(v,b,d,max(target_ls))\n",
    "\n",
    "            # Solve for U\n",
    "            U = SA_for_PT_funcs_delta_eq1.get_U_GT2(LEN,v,d,b,Alpha[i].item(),Labda[i].item(),eps=0.)\n",
    "\n",
    "            # 返回值是正值\n",
    "            nll_metric = SA_for_PT_funcs_delta_eq1.get_nll_meric(target_ls, U, LEN,TARGET = 1)\n",
    "\n",
    "            loss_sum = nll_metric + loss_sum\n",
    "\n",
    "    return loss_sum / cnt, GT_metric/ cnt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Plot\n",
    "## 5.4 plot the loss\n",
    "1. 直接根据参数决定draw什么loss，反正loss都画在一个window里面"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "env_str = \"params_NLL_4\"\n",
    "viz = Visdom(env = env_str)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "win_train_loss_str = \"The Loss of BATCH in the Training Data\"\n",
    "win_vali_loss_str = \"The Loss in the Vali Data\"\n",
    "win_train_epoch_loss_str = \"The Loss of EPOCH in the Training Data\"\n",
    "\n",
    "def draw_loss(X_step, loss, win_str):\n",
    "    viz.line(X = [X_step], Y = [loss],win=win_str, update=\"append\",\n",
    "        opts= dict(title=win_str))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.5 plot the metric"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "win_vali_metric_str = \"The NLL of ALL vali data\"\n",
    "def draw_metric(X_step, total_vali_metric):\n",
    "\n",
    "    viz.line(X = [X_step], Y = [total_vali_metric],win=win_vali_metric_str, update=\"append\", opts= dict(title=win_vali_metric_str, legend=['pred','GT-1','GT-2'], showlegend=True,xlabel=\"epoch\", ylabel=\"NLL\"))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6 Training\n",
    "## 6.1 Init"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "       BatchNorm1d-1                    [-1, 8]              16\n",
      "            Linear-2                    [-1, 1]               9\n",
      "              Tanh-3                    [-1, 1]               0\n",
      "            Linear-4                    [-1, 1]               9\n",
      "              ReLU-5                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 34\n",
      "Trainable params: 34\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "mlp = MLP_1_1()\n",
    "# mlp = MLP_1_2()\n",
    "mlp = mlp.to(device=device)\n",
    "\n",
    "summary(mlp, (1,8))\n",
    "# mlp.half()\n",
    "# mlp.block_alpha.register_forward_hook(hook_forward_fn)\n",
    "# mlp.block_labda.register_forward_hook(hook_forward_fn)\n",
    "\n",
    "# Init the params\n",
    "# mlp = model_param_init(mlp)\n",
    "\n",
    "# Save the init params\n",
    "# torch.save(mlp.state_dict(), 'mlp_init.pth')\n",
    "\n",
    "# Load the saved model\n",
    "# model_path_WD = \"mlp_train=300_WD.pth\"\n",
    "# model_path_init = 'mlp_init.pth'\n",
    "# model_path_LSE = \"mlp_train=300_LSE.pth\"\n",
    "# model_path_MLE = \"mlp_train=300_MLE.pth\"\n",
    "model_path_MLE = \"NN_params_infer_seed=26epoch=0.pth\"\n",
    "#\n",
    "model_data = torch.load(model_path_MLE)\n",
    "mlp.load_state_dict(model_data)\n",
    "\n",
    "############ learning rate strategy ############\n",
    "\n",
    "alpha_params = list(map(id,mlp.block_alpha.parameters()))\n",
    "labda_params = list(map(id,mlp.block_labda.parameters()))\n",
    "\n",
    "params_id = alpha_params + labda_params\n",
    "other_params = filter(lambda p: id(p) not in params_id, mlp.parameters())\n",
    "params = [{'params':other_params},\n",
    "        {'params':mlp.block_alpha.parameters(),'lr':opt.lr_for_alpha},\n",
    "        {'params':mlp.block_labda.parameters(),'lr':opt.lr_for_labda}]\n",
    "optimizer = torch.optim.AdamW(params, lr=opt.learning_rate,weight_decay=opt.weight_decay)\n",
    "\n",
    "# 2.\n",
    "# Or set lr decay\n",
    "# StepLR为步进，每step_size个epoch，lr*gamma\n",
    "# scheduler  = torch.optim.lr_scheduler.StepLR(optimizer,step_size=3,gamma=opt.StepLR_gamma)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.95, last_epoch=-1)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=5, verbose=False, threshold=1e-3, threshold_mode='abs', cooldown=0, min_lr=1e-7, eps=1e-7)\n",
    "# threshold:只关注超过阈值的显著变化；cooldown：触发一次条件后，等待一定epoch再进行检测，避免lr下降过速；\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "'The Loss of EPOCH in the Training Data'"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# writer = SummaryWriter(log_dir=\"logs-MLP/\"+opt.logs_str,flush_secs=60)\n",
    "\n",
    "# Init the vis win\n",
    "viz.line(X = [0.],Y = [0.], env=env_str, win=win_train_loss_str, opts= dict(title=win_train_loss_str))\n",
    "viz.line(X = [0.],Y = [0.], env=env_str, win=win_train_epoch_loss_str, opts= dict(title=win_train_epoch_loss_str))\n",
    "\n",
    "\n",
    "# mlp.eval()\n",
    "# with torch.no_grad():\n",
    "#     total_vali_metric, GT_metric = validate(mlp,val_loader,opt.N_gaussians, opt.MIN_LOSS,device)\n",
    "#     draw_metric(0, total_vali_metric.cpu(), GT_metric)\n",
    "# plot_net(writer,mlp,torch.randn((2,3,300),device=device))\n",
    "# print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 2]' is invalid for input of size 5",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_1176\\2596047382.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mno_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m     \u001B[1;31m#Compute the metric\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m     \u001B[0mtotal_test_metric\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mGT_metric\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mvalidate_params\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmlp\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mtest_loader\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mopt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mMIN_LOSS\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"========== IN Test dataset, NN Model:  {total_test_metric.detach().cpu().numpy()} ==========\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"========== IN Test dataset, the GTs: {GT_metric.detach().cpu().numpy()} ==========\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_1176\\559866464.py\u001B[0m in \u001B[0;36mvalidate_params\u001B[1;34m(mlp, test_loader, eps, device)\u001B[0m\n\u001B[0;32m      3\u001B[0m     \u001B[0mloss_sum\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0.\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrequires_grad\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m     \u001B[1;31m# GT_metric = torch.tensor([0.,0.]).reshape(1,2)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m     \u001B[0mGT_metric\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0.\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m0.\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m0.\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m0.\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m0.\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m     \u001B[0mcnt\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtest_idx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mbatch_id\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtest_loader\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: shape '[1, 2]' is invalid for input of size 5"
     ]
    }
   ],
   "source": [
    "mlp.eval()\n",
    "with torch.no_grad():\n",
    "    #Compute the metric\n",
    "    total_test_metric, GT_metric,_ = validate_params(mlp,test_loader,opt.MIN_LOSS,device)\n",
    "    print(f\"========== IN Test dataset, NN Model:  {total_test_metric.detach().cpu().numpy()} ==========\")\n",
    "    print(f\"========== IN Test dataset, the GTs: {GT_metric.detach().cpu().numpy()} ==========\")\n",
    "    print(f\"========== The diff of metric: NN Model - GT-2(common): {GT_metric.detach().cpu().numpy()[0,1] - total_test_metric.detach().cpu().numpy()} ==========\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.3 START HERE\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Now this is EPOCH 1 ,lr is [0.05, 0.05, 0.05]==========\n",
      "========== IN EPOCH 1 the total loss is 10737239.0 ==========\n",
      "========== Now this is EPOCH 2 ,lr is [0.0475, 0.0475, 0.0475]==========\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.35 GiB (GPU 0; 8.00 GiB total capacity; 3.98 GiB already allocated; 2.23 GiB free; 4.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_2236\\1440710076.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     58\u001B[0m         \u001B[1;31m# 如果unscale_()没有被明确调用，梯度将在step()过程中被自动取消缩放。\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     59\u001B[0m         \u001B[1;31m# scaler.unscale_(optimizer)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 60\u001B[1;33m         \u001B[0mscaler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mscale\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# scaler实现的反向误差传播\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     61\u001B[0m         \u001B[0mscaler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# 优化器中的值也需要放缩\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     62\u001B[0m         \u001B[0mscaler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# 更新scaler\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\Lib\\site-packages\\torch\\_tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    485\u001B[0m                 \u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    486\u001B[0m             )\n\u001B[1;32m--> 487\u001B[1;33m         torch.autograd.backward(\n\u001B[0m\u001B[0;32m    488\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    489\u001B[0m         )\n",
      "\u001B[1;32mD:\\Anaconda\\Lib\\site-packages\\torch\\autograd\\__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    195\u001B[0m     \u001B[1;31m# some Python versions print out the first line of a multi-line function\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    196\u001B[0m     \u001B[1;31m# calls in the traceback and some print out the last line\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 197\u001B[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001B[0m\u001B[0;32m    198\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    199\u001B[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 3.35 GiB (GPU 0; 8.00 GiB total capacity; 3.98 GiB already allocated; 2.23 GiB free; 4.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# filename = \"../log_file.txt\"\n",
    "# f = open(filename,'w')\n",
    "total_train_step = 0\n",
    "EPOCH_NUM = 20\n",
    "\n",
    "mlp.train()\n",
    "\n",
    "for epoch in range(1,EPOCH_NUM):\n",
    "\n",
    "    epoch_train_loss = 0\n",
    "    # plot_conv(writer,mlp,epoch)\n",
    "    print(f\"========== Now this is EPOCH {epoch} ,lr is {scheduler.get_last_lr()}==========\")\n",
    "    for batch_id, data in enumerate(train_loader):\n",
    "\n",
    "        input_data, _,target_loss, _, _, _= data\n",
    "\n",
    "        # print(f\"---- {batch_id} batch----\")\n",
    "        # Do the inference\n",
    "        input_data = input_data.to(device)\n",
    "        target_loss = target_loss.to(device)\n",
    "        # target_params_data = target_params_data.to(device)\n",
    "\n",
    "\n",
    "        with autocast():\n",
    "            alpha, labda = mlp(input_data)\n",
    "            # Cal the MLE loss and draw the distrb.\n",
    "            loss = loss_fn_3(input_data, alpha, labda, target_loss, opt.MIN_LOSS, device)\n",
    "\n",
    "        epoch_train_loss += loss.detach().cpu()     # 能放在cpu上就不放在gpu上\n",
    "        # print(\"loss.detach().item():\",loss.detach().item())\n",
    "        draw_loss(total_train_step, loss.detach().cpu(),win_train_loss_str)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # backward之前retain_graph才有用！\n",
    "        # alpha.retain_grad()\n",
    "        # labda.retain_grad()\n",
    "\n",
    "        # 执行完backward之后才会register tensors的hooks\n",
    "        # loss.backward()\n",
    "        # mlp.block_alpha.register_full_backward_hook(hook_backward_fn)\n",
    "        # mlp.block_labda.register_full_backward_hook(hook_backward_fn)\n",
    "\n",
    "        # 对所有的梯度乘以一个clip_coef = max_norm/total_norm,\n",
    "        # 因此max_norm越大，对于梯度爆炸的解决越柔和\n",
    "\n",
    "        # alpha.register_hook(save_grad(\"alpha\"))\n",
    "        # labda.register_hook(save_grad(\"labda\"))\n",
    "\n",
    "        # print(f\"alpha.grad = {alpha.grad}, labda.grad = {labda.grad}\")\n",
    "        # torch.nn.utils.clip_grad_norm_(mlp.parameters(), max_norm=50, norm_type=2)\n",
    "        # optimizer.step()\n",
    "        # mlp.block_alpha.register_full_backward_hook(hook_backward_fn)\n",
    "        # mlp.block_labda.register_full_backward_hook(hook_backward_fn)\n",
    "        # print(\"alpha.grad:\",alpha.grad)\n",
    "        # print(\"labda.grad:\",labda.grad)\n",
    "\n",
    "        # 如果unscale_()没有被明确调用，梯度将在step()过程中被自动取消缩放。\n",
    "        # scaler.unscale_(optimizer)\n",
    "        scaler.scale(loss).backward()  # scaler实现的反向误差传播\n",
    "        scaler.step(optimizer)  # 优化器中的值也需要放缩\n",
    "        scaler.update()  # 更新scaler\n",
    "        # torch.nn.utils.clip_grad_norm_(mlp.parameters(), max_norm=50, norm_type=2)\n",
    "\n",
    "        total_train_step += 1\n",
    "\n",
    "        del loss\n",
    "        del data\n",
    "        torch.cuda.empty_cache()        # 会变慢不少\n",
    "    ########### Do validation\n",
    "\n",
    "    # Plot the loss in this EPOCH\n",
    "    print(f\"========== IN EPOCH {epoch} the total loss is {epoch_train_loss} ==========\")\n",
    "    # print(f\"========== IN EPOCH {epoch} the vali NLL loss is {total_vali_metric.detach().cpu().numpy()} ==========\")\n",
    "\n",
    "    # Plot loss of this EPOCH\n",
    "    draw_loss(epoch, epoch_train_loss,win_train_epoch_loss_str)\n",
    "    # writer.add_scalars(\"Loss of EPOCH\",{\"Train\":epoch_train_loss},epoch)\n",
    "    # scheduler.step(total_vali_metric)\n",
    "    scheduler.step()\n",
    "    # 每跑完一个epoch，存一下\n",
    "    model_params_MLP = \"NN_params_infer_seed=\" + str(seed) +\"epoch=\"+str(epoch)+\".pth\"\n",
    "    torch.save(mlp.state_dict(), model_params_MLP)\n",
    "\n",
    "    # if epoch%3==0:\n",
    "        # Record the weight\n",
    "    # plot_alpha(writer,mlp,epoch,opt.tag_str)\n",
    "    # plot_labda(writer,mlp,epoch,opt.tag_str)\n",
    "    torch.cuda.empty_cache()        # 会变慢不少\n",
    "\n",
    "# f.close()\n",
    "# writer.close()\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- save"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [],
   "source": [
    "model_params_MLP = \"NN_params_infer_seed=\" + str(seed) +\".pth\"\n",
    "torch.save(mlp.state_dict(), model_params_MLP)\n",
    "\n",
    "# viz.delete_env(\"001_test\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Load and test\n",
    "## 7.1 load and init"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "Model_name = \"Conv4_N_g=2\"\n",
    "\n",
    "model_params_MLP = \"mlp_train=8_MLP_nll_2.pth\"\n",
    "\n",
    "mlp = MLP_1_1()\n",
    "model_data = torch.load(model_params_MLP)\n",
    "mlp.load_state_dict(model_data)\n",
    "mlp = mlp.to(device=device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== IN Test dataset, NN Model:  6.756695747375488 ==========\n",
      "========== IN Test dataset, the GTs: [[ 6.7116656  8.723391   7.5438957 -0.8322304 -2.0117228]] ==========\n",
      "========== The diff of metric: NN Model - GT-2(common): 1.9666948318481445 ==========\n"
     ]
    }
   ],
   "source": [
    "mlp.eval()\n",
    "with torch.no_grad():\n",
    "    #Compute the metric\n",
    "    total_test_metric, GT_metric,_ = validate_params(mlp,test_loader,opt.MIN_LOSS,device)\n",
    "    print(f\"========== IN Test dataset, NN Model:  {total_test_metric.detach().cpu().numpy()} ==========\")\n",
    "    print(f\"========== IN Test dataset, the GTs: {GT_metric.detach().cpu().numpy()} ==========\")\n",
    "    print(f\"========== The diff of metric: NN Model - GT-2(common): {GT_metric.detach().cpu().numpy()[0,1] - total_test_metric.detach().cpu().numpy()} ==========\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8. Generate input data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
